TY  - JOUR
T1  - Lessons Learned about the Application of Adaptive Testing in Several First-Year University Courses
AN  - 2699690168; EJ1340821
AB  - The adoption of computerised adaptive testing (CAT) instead of classical testing (FIT) raises questions from both teachers' and students' perspectives. The scientific literature shows that teachers using CAT instead of FIT should experience shorter times to complete the assessment and obtain more precise evaluations. As for the students, adaptive testing seems to increase their engagement, whereas the impossibility to revise the already given questions is usually seen as a detrimental characteristic. In such a context, the paper reports on a study concerning the aforementioned points. The outcomes seem almost all inline with the literature: no particular usability issues were detected, CAT is faster than FIT, and CAT does not seem more engaging than FIT. All these findings are reported in the conclusions as a list of suggestions to teachers interested in switching from FIT to CAT.
JF  - International Journal of Learning Technology
AU  - Angelone, Anna Maria
AU  - Galassi, Alessandra
AU  - Vittorini, Pierpaolo
Y1  - 2022
PY  - 2022
DA  - 2022
SP  - 3
EP  - 26
PB  - Inderscience Publishers
VL  - 17
IS  - 1
SN  - 1477-8386, 1477-8386
KW  - ERIC, Current Index to Journals in Education (CIJE)
KW  - Higher Education
KW  - Postsecondary Education
KW  - Teacher Attitudes
KW  - Learner Engagement
KW  - Educational Change
KW  - Student Evaluation
KW  - Computer Software
KW  - Student Attitudes
KW  - Grades (Scholastic)
KW  - Adaptive Testing
KW  - College Freshmen
KW  - Computer Assisted Testing
KW  - College Faculty
KW  - Evaluation Methods
KW  - Usability
UR  - https://www.proquest.com/scholarly-journals/lessons-learned-about-application-adaptive/docview/2699690168/se-2?accountid=10957
L2  - http://www.redi-bw.de/links/unifm?url_ver=Z39.88-2004&rft_val_fmt=info:ofi/fmt:kev:mtx:journal&genre=article&sid=ProQ:ProQ%3Aeric&atitle=Lessons+Learned+about+the+Application+of+Adaptive+Testing+in+Several+First-Year+University+Courses&title=International+Journal+of+Learning+Technology&issn=14778386&date=2022-01-01&volume=17&issue=1&spage=3&au=Angelone%2C+Anna+Maria%3BGalassi%2C+Alessandra%3BVittorini%2C+Pierpaolo&isbn=&jtitle=International+Journal+of+Learning+Technology&btitle=&rft_id=info:eric/EJ1340821&rft_id=info:doi/10.1504%2FIJLT.2022.123696
LA  - English
DB  - ERIC
N1  - Zuletzt aktualisiert - 2022-10-11
N1  - SubjectsTermNotLitGenreText - 140ER1 10919ER1 6526ER1 6663ER1 138ER5 10827ER5 6467ER5 6603ER5; 2043ER1 2099ER1 2098ER1 10811ER1 10919ER1 6526ER1 6663ER1 2029ER5 2085ER5 2084ER5 10719ER5 10827ER5 6467ER5 6603ER5; 1798ER1 1829ER1 10411ER1 8114ER1 4605ER1 1784ER5 1815ER5 10321ER5 8043ER5 4558ER5; 10309ER1 740ER1 10219ER5 732ER5; 1797ER1 3832ER1 8373ER1 3464ER1 5777ER1 4975ER1 9030ER1 8114ER1 4605ER1 9360ER1 1783ER5 3795ER5 9280ER5 3431ER5 5722ER5 4925ER5 8950ER5 8043ER5 4558ER5 8297ER5; 10617ER1 740ER1 10525ER5 732ER5; 3219ER1 1404ER1 3190ER5 1393ER5; 3679ER1 6663ER1 3643ER5 6603ER5; 10337ER1 3676ER1 10247ER5 3640ER5; 2084ER1 2070ER5; 11329ER1 3677ER1 2445ER1 10154ER1 7809ER1 5194ER1 11237ER5 3641ER5 2427ER5 10069ER5 7739ER5 5144ER5; 5954ER1 5899ER5; 4506ER1 4459ER5
DO  - https://doi.org/10.1504/IJLT.2022.123696
ER  - 



TY  - JOUR
T1  - The Optimal Item Pool Design in Multistage Computerized Adaptive Tests with the "p"-Optimality Method
AN  - 2458992849; EJ1263711
AB  - The present study extended the "p"-optimality method to the multistage computerized adaptive test (MST) context in developing optimal item pools to support different MST panel designs under different test configurations. Using the Rasch model, simulated optimal item pools were generated with and without practical constraints of exposure control. A total number of 72 simulated optimal item pools were generated and evaluated by an overall sample and conditional sample using various statistical measures. Results showed that the optimal item pools built with the "p"-optimality method provide sufficient measurement accuracy under all simulated MST panel designs. Exposure control affected the item pool size, but not the item distributions and item pool characteristics. This study demonstrated that the "p"-optimality method can adapt to MST item pool design, facilitate the MST assembly process, and improve its scoring accuracy.
JF  - Educational and Psychological Measurement
AU  - Yang, Lihong
AU  - Reckase, Mark D.
Y1  - 2020/10//
PY  - 2020
DA  - Oct 2020
SP  - 955
EP  - 974
PB  - SAGE Publications
VL  - 80
IS  - 5
SN  - 0013-1644, 0013-1644
KW  - ERIC, Current Index to Journals in Education (CIJE)
KW  - Adaptive Testing
KW  - Accuracy
KW  - Test Construction
KW  - Item Banks
KW  - Computer Assisted Testing
KW  - Item Response Theory
UR  - https://www.proquest.com/scholarly-journals/optimal-item-pool-design-multistage-computerized/docview/2458992849/se-2?accountid=10957
L2  - http://www.redi-bw.de/links/unifm?url_ver=Z39.88-2004&rft_val_fmt=info:ofi/fmt:kev:mtx:journal&genre=article&sid=ProQ:ProQ%3Aeric&atitle=The+Optimal+Item+Pool+Design+in+Multistage+Computerized+Adaptive+Tests+with+the+%26quot%3Bp%26quot%3B-Optimality+Method&title=Educational+and+Psychological+Measurement&issn=00131644&date=2020-10-01&volume=80&issue=5&spage=955&au=Yang%2C+Lihong%3BReckase%2C+Mark+D.&isbn=&jtitle=Educational+and+Psychological+Measurement&btitle=&rft_id=info:eric/EJ1263711&rft_id=info:doi/10.1177%2F0013164419901292
LA  - English
DB  - ERIC
N1  - Zuletzt aktualisiert - 2021-05-24
N1  - SubjectsTermNotLitGenreText - 5593ER1 5540ER5; 140ER1 10919ER1 6526ER1 6663ER1 138ER5 10827ER5 6467ER5 6603ER5; 2043ER1 2099ER1 2098ER1 10811ER1 10919ER1 6526ER1 6663ER1 2029ER5 2085ER5 2084ER5 10719ER5 10827ER5 6467ER5 6603ER5; 5598ER1 10914ER1 10966ER1 5545ER5 10822ER5 10874ER5; 96ER1 3677ER1 2445ER1 10154ER1 94ER5 3641ER5 2427ER5 10069ER5; 10895ER1 6468ER1 2820ER1 10803ER5 6409ER5 2799ER5
DO  - https://doi.org/10.1177/0013164419901292
ER  - 



TY  - THES
T1  - Investigating Hybrid Test Designs in Passage-Based Adaptive Tests
AN  - 2449482082
AB  - Computerized adaptive testing (CAT) selects each item to match with an examinee’s ability level during the test. In contrast, multistage adaptive testing (MST) administers a group of items instead of each individual item adaptively. A hybrid design combines CAT and MST borrowing strengths from both testing modes. However, existing research studies indicate that there is lack of knowledge on hybrid designs in passage-based adaptive testing, including situations where misrouting occurs. Misrouting refers to when examinees get routed to paths that don’t match with their true ability levels. Therefore, the purpose of this study is to evaluate the proposed hybrid designs (HMCAT designs) with respect to ability estimation accuracy under different MST configurations and their performance when misrouting occurs.
The HMCAT designs under passage-based adaptive testing control test adaptability at both passage/stimulus level and item level. Four proposed HMCAT designs differ by implementing the item level CAT at different stages of the MST. Specifically, among all four designs, the two most extreme designs are the one implementing CAT across all stages (CC) and the one not implementing CAT across any stage (PP). The two hybrid designs include implementing CAT only in the routing stage (CP) and implementing CAT only in the last stage (PC). The hybrid designs’ performance compared to that of the CC design and the PP design when interacting with different MST configurations and when misrouting occurs are the two main research questions of the study. 
The three-phase simulation analysis indicates that (1) the PC design achieves more accurate final ability estimation results under the three-stage and the four-stage MST configurations; (2) although the CP design fails to perform effectively when misrouting occurs under the three-stage MST configuration, it is still able to achieve accurate final ability estimation results under both two-stage MST configurations; (3) the CC design and the PP design achieve similar and the most accurate final ability estimation results under all four MST configurations; (4) the PC design reaches similar accuracy as the CC design and the PP design under the three-stage and the four-stage MST configurations; (5) the CP design reaches a similar accuracy level as the CC design and the PP design under both two-stage MST configurations; (6) the CP design is affected the most when misrouting occurs in the three-stage MST configuration, whereas the other three HMCAT designs are not affected. 
In general, findings from simulation analyses suggest that the HMCAT designs are effective enough to use in passage/stimulus-based adaptive tests with appropriate MST configurations. They also contain advantages such as greater flexibility with respect to content balancing, test security and control on item administration. However, it is important to understand that the performance of HMCAT designs can heavily depend on the MST configuration, the item pool characteristics, the test design and components in the test administration algorithm, such as content balancing strategies. Results from the current study provide implications for practitioners on how to decide on an appropriate HMCAT design under real testing contexts and how to evaluate and maintain the selected HMCAT design to effectively implement the design in practice.
JF  - ProQuest Dissertations and Theses
AU  - Ma, Ye (Cheryl)
A3  - Dunbar, Stephen B.
A3  - Harris, Deborah J.
Y1  - 2020
PY  - 2020
DA  - 2020
SP  - 224
CY  - United States -- Iowa
PB  - The University of Iowa
PP  - United States -- Iowa
SN  - 9798672162072
KW  - Computerized Adaptive Testing
KW  - Multistage adaptive testing
KW  - Passage/stimulus-based tests
KW  - Simulation study
KW  - Test assembly
KW  - Test designs
KW  - Educational tests & measurements
KW  - Computer engineering
KW  - Educational technology
KW  - 0710:Educational technology
KW  - 0288:Educational tests & measurements
KW  - 0464:Computer Engineering
UR  - https://www.proquest.com/dissertations-theses/investigating-hybrid-test-designs-passage-based/docview/2449482082/se-2?accountid=10957
L2  - http://www.redi-bw.de/links/unifm?url_ver=Z39.88-2004&rft_val_fmt=info:ofi/fmt:kev:mtx:dissertation&genre=dissertations&sid=ProQ:ProQuest+Dissertations+%26+Theses+Global&atitle=&title=Investigating+Hybrid+Test+Designs+in+Passage-Based+Adaptive+Tests&issn=&date=2020-01-01&volume=&issue=&spage=&au=Ma%2C+Ye+%28Cheryl%29&isbn=9798672162072&jtitle=&btitle=&rft_id=info:eric/&rft_id=info:doi/
LA  - English
DB  - ProQuest Dissertations & Theses Global
N1  - Copyright - Database copyright ProQuest LLC; ProQuest does not claim copyright in the individual underlying works.
N1  - Zuletzt aktualisiert - 2022-01-19
M3  - Ph.D.
M1  - 27997446
ER  - 



TY  - THES
T1  - Diagnostic Tools for Improving the Amount of Adaptation in Adaptive Tests Using Overall and Conditional Indices of Adaptation
AN  - 2288850151
AB  - In recent years, computerized adaptive testing (CAT) has been widely used in educational and clinical settings. The basic idea of CAT is relatively straightforward. A computer is used to administer items tailored for individuals to maximize the measurement precision of their proficiency estimates. However, the administration of CAT is not so simple. Those who administer CATs must, while trying to optimize an item selection criterion, consider a variety of practical issues such as test security, content balancing, the purpose of testing, and other test specifications. Such extraneous factors make it possible that a CAT might have so many constraints that in practice it is barely adaptive at all. This concern is at the forefront of the current study, which poses two key questions: How adaptive is a highly adaptive test really? How can the level of adaptation be improved? 
This study aims to develop three new statistical indicators to measure the amount of adaptation conditional on the examinees’ proficiency levels in CAT. It also aims to evaluate the feasibility and utility of these adaptation measures in helping to diagnose and improve adaptivity that occurs during the CAT administration. Extending work done by Reckase, Ju, and Kim (2018), the proposed measures are based on three components—the differences in the locations between the selected items and the examinee’s current proficiency estimates, the variations in the item locations administered to each examinee, and the magnitude of information that the test presents to each examinee. Hence, they can be used to assess adaptivity during the CAT process, as well as to identify differences in the level of adaptation for individuals or subgroups of examinees. 
To demonstrate the performance of the proposed adaptation indices, this study conducted analyses of real operational testing data from a healthcare licensure examination, as well as comprehensive simulation studies under various conditions that affect adaptivity in a CAT. The key findings of the study suggest that the proposed adaptation indices are likely to function as intended to sensitively detect the magnitude of adaptivity for a CAT over the proficiency continuum. These new measures shed light on how much adaptation of a given test occurs across individual proficiency levels or subpopulations. With some guidelines for the interpretation of these measures recommended in this study, the adaptation indices can also readily serve as diagnostic tools in practice for helping test practitioners design item pools and adaptive tests that support high adaptivity.
JF  - ProQuest Dissertations and Theses
AU  - Ju, Unhee
A3  - Reckase, Mark D.
Y1  - 2019
PY  - 2019
DA  - 2019
SP  - 180
CY  - United States -- Michigan
PB  - Michigan State University
PP  - United States -- Michigan
SN  - 9781085696203
KW  - Adaptation Indices
KW  - Computerized adaptive testing
KW  - Item selection criterion
KW  - Educational tests & measurements
KW  - 0288:Educational tests & measurements
UR  - https://www.proquest.com/dissertations-theses/diagnostic-tools-improving-amount-adaptation/docview/2288850151/se-2?accountid=10957
L2  - http://www.redi-bw.de/links/unifm?url_ver=Z39.88-2004&rft_val_fmt=info:ofi/fmt:kev:mtx:dissertation&genre=dissertations&sid=ProQ:ProQuest+Dissertations+%26+Theses+Global&atitle=&title=Diagnostic+Tools+for+Improving+the+Amount+of+Adaptation+in+Adaptive+Tests+Using+Overall+and+Conditional+Indices+of+Adaptation&issn=&date=2019-01-01&volume=&issue=&spage=&au=Ju%2C+Unhee&isbn=9781085696203&jtitle=&btitle=&rft_id=info:eric/&rft_id=info:doi/
LA  - English
DB  - ProQuest Dissertations & Theses Global
N1  - Copyright - Database copyright ProQuest LLC; ProQuest does not claim copyright in the individual underlying works.
N1  - Zuletzt aktualisiert - 2022-12-20
M3  - Ph.D.
M1  - 22620004
ER  - 



TY  - JOUR
T1  - An Adaptive Test Analysis Based on Students' Motivation
AN  - 2461133504; EJ1195643
AB  - Computerized Adaptive Testing (CAT) is now widely used. However, inserting new items into the question bank of a CAT requires a great effort that makes impractical the wide application of CAT in classroom teaching. One solution would be to use the tacit knowledge of the teachers or experts for a pre-classification and calibrate during the execution of tests with these items. Thus, this research consists of a comparative case study between a Stratified Adaptive Test (SAT), based on the tacit knowledge of a teacher, and a CAT based on Item Response Theory (IRT). The tests were applied in seven Computer Networks courses. The results indicate that levels of "anxiety" expressed in the use of the SAT were better than those using the CAT, in addition to being simpler to implement. In this way, it is recommended the implementation of a SAT, where the strata are initially based on the tacit knowledge of the teacher and later, as a result of an IRT calibration.
JF  - Informatics in Education
AU  - Yoshioka, Sérgio R. I.
AU  - Ishitani, Lucila
Y1  - 2018
PY  - 2018
DA  - 2018
SP  - 381
EP  - 404
PB  - Vilnius University Institute of Mathematics and Informatics
VL  - 17
IS  - 2
SN  - 1648-5831, 1648-5831
KW  - ERIC, Current Index to Journals in Education (CIJE)
KW  - Adaptive Testing
KW  - Student Motivation
KW  - Test Theory
KW  - Comparative Analysis
KW  - Test Reliability
KW  - Computer Assisted Testing
KW  - Item Banks
KW  - Test Validity
KW  - Anxiety
KW  - Computer Attitudes
KW  - Item Response Theory
UR  - https://www.proquest.com/scholarly-journals/adaptive-test-analysis-based-on-students/docview/2461133504/se-2?accountid=10957
L2  - http://www.redi-bw.de/links/unifm?url_ver=Z39.88-2004&rft_val_fmt=info:ofi/fmt:kev:mtx:journal&genre=article&sid=ProQ:ProQ%3Aeric&atitle=An+Adaptive+Test+Analysis+Based+on+Students%27+Motivation&title=Informatics+in+Education&issn=16485831&date=2018-01-01&volume=17&issue=2&spage=381&au=Yoshioka%2C+S%C3%A9rgio+R.+I.%3BIshitani%2C+Lucila&isbn=&jtitle=Informatics+in+Education&btitle=&rft_id=info:eric/EJ1195643&rft_id=info:doi/
LA  - English
DB  - ERIC
N1  - Anzahl der Quellenangaben - 48
N1  - Zuletzt aktualisiert - 2021-02-11
N1  - SubjectsTermNotLitGenreText - 10358ER1 6915ER1 10268ER5 6852ER5; 140ER1 10919ER1 6526ER1 6663ER1 138ER5 10827ER5 6467ER5 6603ER5; 2043ER1 2099ER1 2098ER1 10811ER1 10919ER1 6526ER1 6663ER1 2029ER5 2085ER5 2084ER5 10719ER5 10827ER5 6467ER5 6603ER5; 5598ER1 10914ER1 10966ER1 5545ER5 10822ER5 10874ER5; 554ER1 8522ER1 549ER5 8446ER5; 2044ER1 740ER1 2030ER5 732ER5; 10914ER1 10966ER1 10822ER5 10874ER5; 10905ER1 8884ER1 3677ER1 2445ER1 10154ER1 10813ER5 8808ER5 3641ER5 2427ER5 10069ER5; 10917ER1 11346ER1 3677ER1 2445ER1 10154ER1 10825ER5 11254ER5 3641ER5 2427ER5 10069ER5; 1980ER1 3679ER1 6663ER1 1966ER5 3643ER5 6603ER5; 5593ER1 5540ER5
ER  - 



TY  - JOUR
T1  - Mediating Effects of Individuals' Ability Levels on the Relationship of Reflective-Impulsive Cognitive Style and Item Response Time in CAT
AN  - 2155989774; EJ1193878
AB  - This study focused on the effect of examinees' ability levels on the relationship between Reflective-Impulsive (RI) cognitive style and item response time in computerized adaptive testing (CAT). The total of 56 students majoring in Educational Technology from Shandong Normal University participated in this study, and their RI cognitive styles were diagnosed using the Matching Familiar Figures Test-20 (MFFT-20). Examinees' ability values and average item response time were recorded by the computerized adaptive testing system. Then mediation analysis was implemented and the findings revealed that there was direct and indirect effect between RI cognitive style and item response time in CAT. What is more, RI cognitive style also directly affected the ability levels, and then the ability levels impacted on item response time. So, examinee's ability level was partly a mediator between RI cognitive style and item response time. Furthermore, RI cognitive style of the examinees might also be diagnosed according to ability values and average item response time. The relevant research and implications were further discussed.
JF  - Educational Technology & Society
AU  - Wang, Chao
AU  - Lu, Hong
Y1  - 2018
PY  - 2018
DA  - 2018
SP  - 89
EP  - 99
PB  - International Forum of Educational Technology & Society
VL  - 21
IS  - 4
KW  - China
KW  - ERIC, Current Index to Journals in Education (CIJE)
KW  - Correlation
KW  - Cognitive Style
KW  - Problem Solving
KW  - Probability
KW  - Reaction Time
KW  - Foreign Countries
KW  - Statistics
KW  - Adaptive Testing
KW  - Computer Assisted Testing
KW  - Educational Technology
KW  - Test Items
KW  - Item Response Theory
KW  - Mathematical Models
UR  - https://www.proquest.com/scholarly-journals/mediating-effects-individuals-ability-levels-on/docview/2155989774/se-2
L2  - http://www.redi-bw.de/links/unifm?url_ver=Z39.88-2004&rft_val_fmt=info:ofi/fmt:kev:mtx:journal&genre=article&sid=ProQ:ProQ%3Aeric&atitle=Mediating+Effects+of+Individuals%27+Ability+Levels+on+the+Relationship+of+Reflective-Impulsive+Cognitive+Style+and+Item+Response+Time+in+CAT&title=Educational+Technology+%26+Society&issn=1436-4522&date=2018-01-01&volume=21&issue=4&spage=89&au=Wang%2C+Chao%3BLu%2C+Hong&isbn=&jtitle=Educational+Technology+%26+Society&btitle=&rft_id=info:eric/EJ1193878&rft_id=info:doi/
LA  - English
DB  - ERIC
N1  - Anzahl der Quellenangaben - 52
N1  - Zuletzt aktualisiert - 2021-02-11
N1  - SubjectsTermNotLitGenreText - 5598ER1 10914ER1 10966ER1 5545ER5 10822ER5 10874ER5; 2043ER1 2099ER1 2098ER1 10811ER1 10919ER1 6526ER1 6663ER1 2029ER5 2085ER5 2084ER5 10719ER5 10827ER5 6467ER5 6603ER5; 1739ER1 8516ER1 5118ER1 1725ER5 8440ER5 5068ER5; 2295ER1 10210ER1 2604ER1 3679ER1 6663ER1 2278ER5 10125ER5 2586ER5 3643ER5 6603ER5; 4166ER1 4394ER1 4124ER5 4350ER5; 3311ER1 10805ER1 3282ER5 10713ER5; 10900ER1 10808ER5; 8327ER1 6490ER1 6042ER1 8252ER5 6431ER5 5983ER5; 10226ER1 6490ER1 6042ER1 10140ER5 6431ER5 5983ER5; 6484ER1 6838ER1 9769ER1 6663ER1 6425ER5 6775ER5 9687ER5 6603ER5; 8718ER1 8642ER5; 8338ER1 1733ER1 8263ER5 1719ER5; 140ER1 10919ER1 6526ER1 6663ER1 138ER5 10827ER5 6467ER5 6603ER5; China
ER  - 



TY  - JOUR
T1  - Computer-Adaptive Testing: Implications for Students' Achievement, Motivation, Engagement, and Subjective Test Experience
AN  - 2013521072; EJ1166166
AB  - The present study investigated the implications of computer-adaptive testing (operationalized by way of multistage adaptive testing; MAT) and "conventional" fixed order computer testing for various test-relevant outcomes in numeracy, including achievement, test-relevant motivation and engagement, and subjective test experience. It did so among N = 12,736 Australian elementary (years 3 and 5) and secondary (years 7 and 9) school students. Multilevel modeling assessed the extent to which Level 1 (student) test condition (fixed order vs. adaptive), gender, and year group factors and Level 2 (school) socioeducational advantage, location, structure, and size factors predicted students' test-relevant outcomes. In terms of statistically significant main effects, students in the computer-adaptive testing condition generated lower achievement error rates (i.e., higher measurement precision). Other statistically significant computer-adaptive test effects emerged as a function of year-level and gender, with positive effects of computer-adaptive testing being relatively greater for females and older students: these students achieved more highly (year 9 students), reported higher test-relevant motivation and engagement (year 9 students), and reported more positive subjective test experience (females and year 9 students). These findings (a) confirm that computer-adaptive testing yields greater achievement measurement precision, (b) suggest some positive test-relevant motivation and engagement effects from computer-adaptive testing, (c) counter claims that computer-adaptive testing reduces students' test-relevant motivation, engagement, and subjective experience, and (d) suggest positive computer-adaptive testing effects for older students at a developmental stage when they are typically less motivated and engaged.
JF  - Journal of Educational Psychology
AU  - Martin, Andrew J.
AU  - Lazendic, Goran
Y1  - 2018/01//
PY  - 2018
DA  - Jan 2018
SP  - 27
EP  - 45
PB  - American Psychological Association
VL  - 110
IS  - 1
SN  - 0022-0663, 0022-0663
KW  - Australia
KW  - ERIC, Current Index to Journals in Education (CIJE)
KW  - Elementary Education
KW  - Learner Engagement
KW  - Elementary School Students
KW  - Achievement Tests
KW  - Socioeconomic Status
KW  - School Size
KW  - Foreign Countries
KW  - Statistical Significance
KW  - Secondary School Students
KW  - Student Experience
KW  - Gender Differences
KW  - Geographic Location
KW  - Student Motivation
KW  - Computer Assisted Testing
KW  - Instructional Program Divisions
KW  - Academic Achievement
KW  - Predictor Variables
KW  - Numeracy
KW  - Statistical Analysis
UR  - https://www.proquest.com/scholarly-journals/computer-adaptive-testing-implications-students/docview/2013521072/se-2?accountid=10957
L2  - http://www.redi-bw.de/links/unifm?url_ver=Z39.88-2004&rft_val_fmt=info:ofi/fmt:kev:mtx:journal&genre=article&sid=ProQ:ProQ%3Aeric&atitle=Computer-Adaptive+Testing%3A+Implications+for+Students%27+Achievement%2C+Motivation%2C+Engagement%2C+and+Subjective+Test+Experience&title=Journal+of+Educational+Psychology&issn=00220663&date=2018-01-01&volume=110&issue=1&spage=27&au=Martin%2C+Andrew+J.%3BLazendic%2C+Goran&isbn=&jtitle=Journal+of+Educational+Psychology&btitle=&rft_id=info:eric/EJ1166166&rft_id=info:doi/10.1037%2Fedu0000205
LA  - English
DB  - ERIC
N1  - Anzahl der Quellenangaben - 102
N1  - Zuletzt aktualisiert - 2021-02-11
N1  - SubjectsTermNotLitGenreText - 2043ER1 2099ER1 2098ER1 10811ER1 10919ER1 6526ER1 6663ER1 2029ER5 2085ER5 2084ER5 10719ER5 10827ER5 6467ER5 6603ER5; 3407ER1 10411ER1 8114ER1 4605ER1 3377ER5 10321ER5 8043ER5 4558ER5; 9534ER1 10411ER1 8114ER1 4605ER1 9453ER5 10321ER5 8043ER5 4558ER5; 4348ER1 4305ER5; 5331ER1 5281ER5; 9957ER1 10228ER1 9873ER5 10141ER5; 4392ER1 4348ER5; 8200ER1 8128ER5; 4166ER1 4394ER1 4124ER5 4350ER5; 29ER1 98ER1 28ER5 96ER5; 10358ER1 6915ER1 10268ER5 6852ER5; 5954ER1 5899ER5; 7286ER1 7222ER5; 10340ER1 3736ER1 865ER1 10250ER5 3700ER5 856ER5; 109ER1 10925ER1 6527ER1 107ER5 10833ER5 6468ER5; 9396ER1 5288ER1 9316ER5 5238ER5; 10210ER1 2604ER1 3679ER1 6663ER1 10125ER5 2586ER5 3643ER5 6603ER5; 10222ER1 10210ER1 2604ER1 3679ER1 6663ER1 10136ER5 10125ER5 2586ER5 3643ER5 6603ER5; Australia
DO  - https://doi.org/10.1037/edu0000205
ER  - 



TY  - JOUR
T1  - An overview of differential item functioning in multistage computer adaptive testing using three-parameter logistic item response theory
AN  - 1957207565
AB  - Testing is growing by leaps and bounds across the world. There is a realization that a nation’s well-being depends crucially on the educational achievement of its population. Valid tests are an essential tool to evaluate a nation’s educational standing and to implement efficacious educational reforms. Because tests consume time that otherwise could be devoted to instruction, it is important to devise tests that are efficient. Doing so requires a careful balancing of the contributions of technology, psychometrics, test design, and the learning sciences. Computer adaptive multistage testing (MSCAT) fits the bill extraordinarily well; unlike other forms of adaptive testing, it can be adapted to educational surveys and student testing. Research in this area will be an evidence that the methodologies and underlying technology that surround MSCAT have reached maturity and that there is a growing acceptance by the field of this type of test design. This state-of-the-art paper aims to present an overview of differential item functioning (DIF) in MSCAT using three-parameter logistic item response theory (IRT), offering suggestions to implement it in practice with a hope to motivate testing and assessment researchers and practitioners to initiate projects in this under-practiced area by helping them to better understand some of the relevant technical concepts.
JF  - Language Testing in Asia
AU  - Sadeghi, Karim
AU  - Abolfazli Khonbi, Zainab
AD  - Department of English Language and Literature, Faculty of Humanities, Urmia University, Urmia, Iran ; Department of English Language and Literature, Faculty of Humanities, Kosar University of Bojnord, Bojnord, Iran ; Department of English Language and Literature, Faculty of Humanities, Urmia University, Urmia, Iran
Y1  - 2017/05//
PY  - 2017
DA  - May 2017
SP  - 1
EP  - 16
CY  - Heidelberg
PB  - Springer Nature B.V.
PP  - Heidelberg
VL  - 7
IS  - 1
KW  - Linguistics
KW  - Computer adaptive testing
KW  - Differential item functioning
KW  - Item bias
KW  - Item response theory
KW  - Multistage computer adaptive testing
KW  - Education reform
KW  - Achievement tests
KW  - Algorithms
KW  - Test validity and reliability
KW  - Language tests
KW  - Education policy
UR  - https://www.proquest.com/scholarly-journals/overview-differential-item-functioning-multistage/docview/1957207565/se-2?accountid=10957
L2  - http://www.redi-bw.de/links/unifm?url_ver=Z39.88-2004&rft_val_fmt=info:ofi/fmt:kev:mtx:journal&genre=article&sid=ProQ:ProQ%3Allba&atitle=An+overview+of+differential+item+functioning+in+multistage+computer+adaptive+testing+using+three-parameter+logistic+item+response+theory&title=Language+Testing+in+Asia&issn=&date=2017-05-01&volume=7&issue=1&spage=1&au=Sadeghi%2C+Karim%3BAbolfazli+Khonbi%2C+Zainab&isbn=&jtitle=Language+Testing+in+Asia&btitle=&rft_id=info:eric/&rft_id=info:doi/10.1186%2Fs40468-017-0038-z
LA  - English
DB  - Linguistics and Language Behavior Abstracts (LLBA)
N1  - Copyright - Language Testing in Asia is a copyright of Springer, 2017.
N1  - Zuletzt aktualisiert - 2022-08-07
DO  - https://doi.org/10.1186/s40468-017-0038-z
ER  - 



TY  - JOUR
T1  - An Overview of Differential Item Functioning in Multistage Computer Adaptive Testing Using Three-Parameter Logistic Item Response Theory
AN  - 2396852167; EJ1246463
AB  - As perfectly summarised by Ida Lawrence, "Testing is growing by leaps and bounds across the world. There is a realization that a nation's well-being depends crucially on the educational achievement of its population. Valid tests are an essential tool to evaluate a nation's educational standing and to implement efficacious educational reforms. Because tests consume time that otherwise could be devoted to instruction, it is important to devise tests that are efficient. Doing so requires a careful balancing of the contributions of technology, psychometrics, test design, and the learning sciences. Computer adaptive multistage testing (MSCAT) fits the bill extraordinarily well; unlike other forms of adaptive testing, it can be adapted to educational surveys and student testing. Research in this area will be an evidence that the methodologies and underlying technology that surround MSCAT have reached maturity and that there is a growing acceptance by the field of this type of test design" (from the Foreword to Y. Duanli, A. A. von Davier, & L. Charles (Eds.), "Computerized multistage testing: theory and application"). This state-of-the-art paper aims to present an overview of differential item functioning (DIF) in MSCAT using three-parameter logistic item response theory (IRT), offering suggestions to implement it in practice with a hope to motivate testing and assessment researchers and practitioners to initiate projects in this under-practiced area by helping them to better understand some of the relevant technical concepts.
JF  - Language Testing in Asia
AU  - Sadeghi, Karim
AU  - Abolfazli Khonbi, Zainab
Y1  - 2017
PY  - 2017
DA  - 2017
SP  - 1
EP  - 16
PB  - Springer
VL  - 7
KW  - ERIC, Current Index to Journals in Education (CIJE)
KW  - Adaptive Testing
KW  - Computer Assisted Testing
KW  - Psychometrics
KW  - Test Items
KW  - Test Bias
KW  - Item Response Theory
UR  - https://www.proquest.com/scholarly-journals/overview-differential-item-functioning-multistage/docview/2396852167/se-2?accountid=10957
L2  - http://www.redi-bw.de/links/unifm?url_ver=Z39.88-2004&rft_val_fmt=info:ofi/fmt:kev:mtx:journal&genre=article&sid=ProQ:ProQ%3Aeric&atitle=An+Overview+of+Differential+Item+Functioning+in+Multistage+Computer+Adaptive+Testing+Using+Three-Parameter+Logistic+Item+Response+Theory&title=Language+Testing+in+Asia&issn=2229-0443&date=2017-01-01&volume=7&issue=&spage=1&au=Sadeghi%2C+Karim%3BAbolfazli+Khonbi%2C+Zainab&isbn=&jtitle=Language+Testing+in+Asia&btitle=&rft_id=info:eric/EJ1246463&rft_id=info:doi/10.1186%2Fs40468-017-0038-z
LA  - English
DB  - ERIC
N1  - Anzahl der Quellenangaben - -1
N1  - Zuletzt aktualisiert - 2021-02-11
N1  - SubjectsTermNotLitGenreText - 10900ER1 10808ER5; 5598ER1 10914ER1 10966ER1 5545ER5 10822ER5 10874ER5; 2043ER1 2099ER1 2098ER1 10811ER1 10919ER1 6526ER1 6663ER1 2029ER5 2085ER5 2084ER5 10719ER5 10827ER5 6467ER5 6603ER5; 140ER1 10919ER1 6526ER1 6663ER1 138ER5 10827ER5 6467ER5 6603ER5; 10891ER1 955ER1 10799ER5 946ER5; 8530ER1 8529ER1 938ER1 9466ER1 6042ER1 8454ER5 8453ER5 929ER5 9385ER5 5983ER5
DO  - https://doi.org/10.1186/s40468-017-0038-z
ER  - 



TY  - THES
T1  - Nurse Anesthesia Pedagogy: Curriculum Review for Preparation for the Self-Evaluation Exam
AN  - 2157892297
AB  - The self-evaluation exam (SEE) is a high stake 240 item computer adaptive test that all student registered nurse anesthetists enrolled in nurse anesthesia programs must take. The SEE is utilized as an evaluative tool to measure preparedness for the national certifying exam; an examination that all student registered nurse anesthetists must pass in order to become certified to administer anesthesia. The curriculum review seeks to improve student learning and retention of knowledge of the SEE content by adapting useful study strategies and organizational skills, and increasing motivation and self-efficacy.
JF  - ProQuest Dissertations and Theses
AU  - Smith-Aifesehi, Tahira Ayanna
A3  - Hirabayashi, Kimberly
Y1  - 2017
PY  - 2017
DA  - 2017
SP  - 297
CY  - United States -- California
PB  - University of Southern California
PP  - United States -- California
SN  - 979-8-209-57078-3
KW  - (UMI)AAI11015994
KW  - Health and environmental sciences
KW  - Education
KW  - Nursing education
KW  - Pedagogy
KW  - Nursing
KW  - Health education
KW  - Self evaluation
KW  - Learning
KW  - 0680:Health education
KW  - 0569:Nursing
KW  - 0456:Pedagogy
UR  - https://www.proquest.com/dissertations-theses/nurse-anesthesia-pedagogy-curriculum-review/docview/2157892297/se-2?accountid=10957
L2  - http://www.redi-bw.de/links/unifm?url_ver=Z39.88-2004&rft_val_fmt=info:ofi/fmt:kev:mtx:dissertation&genre=dissertations&sid=ProQ:ProQuest+Dissertations+%26+Theses+Global&atitle=&title=Nurse+Anesthesia+Pedagogy%3A+Curriculum+Review+for+Preparation+for+the+Self-Evaluation+Exam&issn=&date=2017-01-01&volume=&issue=&spage=&au=Smith-Aifesehi%2C+Tahira+Ayanna&isbn=979-8-209-57078-3&jtitle=&btitle=&rft_id=info:eric/&rft_id=info:doi/
LA  - English
DB  - ProQuest Dissertations & Theses Global
N1  - Copyright - Database copyright ProQuest LLC; ProQuest does not claim copyright in the individual underlying works.
N1  - Analytdeskriptor - N.A.
N1  - Zuletzt aktualisiert - 2022-07-01
M3  - Ed.D.
M1  - 11015994
ER  - 



TY  - GEN
T1  - Exploring the Opportunities for the Adaptive Assessment of Vocabulary Knowledge in an Educational Context
AN  - 2461131892; ED596664
AB  - Vocabulary knowledge assessment methods and instruments have gone through a significant evolution. Computer-based tests offer more opportunities than their paper-and-pencil counterparts, however, most digital vocabulary assessments are linear and adaptive solutions in this domain are scarce. The aims of this study were to compare the effectiveness of a multi-stage adaptive and a linear test system of vocabulary knowledge both at test and individual level and to examine the impact of the used assessment method on students' performance and motivation measured through the number of the right answers within a test. Data show that the adaptive test system provided a more accurate ability estimation with higher reliability, a more motivating test environment and more precision than the linear version.
JF  - AERA Online Paper Repository
AU  - Molnar, Gyongyver
AU  - Hodi, Agnes
AU  - Magyar, Andrea
Y1  - 2016///Apr 8, 2016 - Apr 12,
PY  - 2016
DA  - Apr 8, 2016 - Apr 12, 2016
SP  - 1
EP  - 7
PB  - AERA Online Paper Repository
KW  - Hungary
KW  - ERIC, Resources in Education (RIE)
KW  - Elementary Education
KW  - Grade 4
KW  - Intermediate Grades
KW  - Grade 5
KW  - Middle Schools
KW  - Student Evaluation
KW  - Elementary School Students
KW  - Academic Ability
KW  - Foreign Countries
KW  - Adaptive Testing
KW  - Accuracy
KW  - Knowledge Level
KW  - Computer Assisted Testing
KW  - Difficulty Level
KW  - Vocabulary Skills
KW  - Grade 4
KW  - Student Evaluation
KW  - Grade 5
KW  - Hungary
KW  - Elementary School Students
KW  - Academic Ability
KW  - Foreign Countries
KW  - Adaptive Testing
KW  - Accuracy
KW  - Knowledge Level
KW  - Computer Assisted Testing
KW  - Difficulty Level
KW  - Vocabulary Skills
UR  - https://www.proquest.com/speeches-presentations/exploring-opportunities-adaptive-assessment/docview/2461131892/se-2?accountid=10957
L2  - http://www.redi-bw.de/links/unifm?url_ver=Z39.88-2004&rft_val_fmt=info:ofi/fmt:kev:mtx:journal&genre=unknown&sid=ProQ:ProQ%3Aeric&atitle=Exploring+the+Opportunities+for+the+Adaptive+Assessment+of+Vocabulary+Knowledge+in+an+Educational+Context&title=AERA+Online+Paper+Repository&issn=&date=2016-04-08&volume=&issue=&spage=1&au=Molnar%2C+Gyongyver%3BHodi%2C+Agnes%3BMagyar%2C+Andrea&isbn=&jtitle=AERA+Online+Paper+Repository&btitle=&rft_id=info:eric/ED596664&rft_id=info:doi/
LA  - English
DB  - ERIC
N1  - Anzahl der Quellenangaben - -1
N1  - Zuletzt aktualisiert - 2021-02-11
ER  - 



TY  - THES
T1  - Enhancing Item Pool Utilization When Designing Multistage Computerized Adaptive Tests
AN  - 2461125600; ED589689
AB  - In recent years, the multistage adaptive test (MST) has gained increasing popularity in the field of educational measurement and operational testing. MST refers to a test in which pre-constructed sets of items are administered adaptively and are scored as a unit (Hendrickson, 2007). As a special case of Computerized Adaptive Testing (CAT), a MST program needs the following components: an item response theory (IRT) model or non-IRT-based alternatives; an item pool design; module assembly; ability estimation; routing algorithm; and scoring (Yan et al., 2014). A significant amount of research has been conducted on components like module assembly, ability estimation, routing and scoring, but few studies have addressed the component of item pool design. An item pool is defined as consisting of a maximal number of combinations of items that meet all content specifications for a test and provide sufficient item information for estimation at a series of ability levels (van der Linden et al., 2006). An item pool design is very important because any successful MST assembly is inseparable from an optimal item pool that provides sufficient and high-quality items (Luecht & Nungester, 1998). Reckase (2003, 2010) developed the p-optimality method to design optimal item pools using the unidimensional Rasch model in CAT, and it has been proved to be efficient for different item types and IRT models. The present study extended this method to MST context in supporting and developing different MST panel designs under different test configurations. The study compared the performance of the MST assembled under the most popularly studied panel designs in the literature, such as 1-2, 1-3, 1-2-2, and 1-2-3. A combination of short, medium and long tests with different routing test proportions were used to build up different tests. Using one of the most popularly investigated IRT models, the Rasch model, simulated optimal item pools were generated with and without practical constraints of exposure control. A total number of 72 optimal items pools were generated and the measurement accuracy was evaluated by an overall sample and conditional sample using various statistical measures. The p-optimality method was also applied in an operational MST licensure test to see if it is feasible in supporting test assembly and achieving sufficient measurement accuracy in practice. Results showed that the different MST panel designs achieved sufficient measurement accuracy by using the items from the optimal item pools built with the p-optimality method. The same was true with the operational item pool. Measurement accuracy was related to test length, but not so much to the routing test proportions. Exposure control affected the item pool size, but the distributions of the item parameters and item pool characteristics for all the MST panel designs were similar under the two conditions. The item pool sizes under the exposure control conditions were several times larger than those under no exposure control, depending on the types of MST panel designs and routing test proportions. The results from this study provide information for how to enhance item pool utilization when designing multistage computerized adaptive tests, facilitating the MST assembly process, and improving the scoring accuracy. [The dissertation citations contained here are published with the permission of ProQuest LLC. Further reproduction is prohibited without permission. Copies of dissertations may be obtained by Telephone (800) 1-800-521-0600. Web page: http://www.proquest.com/en-US/products/dissertations/individuals.shtml.]
JF  - ProQuest LLC
AU  - Yang, Lihong
Y1  - 2016
PY  - 2016
DA  - 2016
SP  - 1
EP  - 142
SN  - 9781369051353
KW  - ERIC, Resources in Education (RIE)
KW  - Adaptive Testing
KW  - Measurement
KW  - Test Construction
KW  - Accuracy
KW  - Item Banks
KW  - Computer Assisted Testing
KW  - Licensing Examinations (Professions)
KW  - Item Response Theory
UR  - https://www.proquest.com/dissertations-theses/enhancing-item-pool-utilization-when-designing/docview/2461125600/se-2?accountid=10957
L2  - http://www.redi-bw.de/links/unifm?url_ver=Z39.88-2004&rft_val_fmt=info:ofi/fmt:kev:mtx:dissertation&genre=dissertations&sid=ProQ:ERIC&atitle=&title=Enhancing+Item+Pool+Utilization+When+Designing+Multistage+Computerized+Adaptive+Tests&issn=&date=2016-01-01&volume=&issue=&spage=&au=Yang%2C+Lihong&isbn=9781369051353&jtitle=&btitle=&rft_id=info:eric/ED589689&rft_id=info:doi/
LA  - English
DB  - ERIC
N1  - Zuletzt aktualisiert - 2021-02-11
N1  - Erste Seite - 1
N1  - SubjectsTermNotLitGenreText - 5593ER1 5540ER5; 2043ER1 2099ER1 2098ER1 10811ER1 10919ER1 6526ER1 6663ER1 2029ER5 2085ER5 2084ER5 10719ER5 10827ER5 6467ER5 6603ER5; 140ER1 10919ER1 6526ER1 6663ER1 138ER5 10827ER5 6467ER5 6603ER5; 10895ER1 6468ER1 2820ER1 10803ER5 6409ER5 2799ER5; 5598ER1 10914ER1 10966ER1 5545ER5 10822ER5 10874ER5; 96ER1 3677ER1 2445ER1 10154ER1 94ER5 3641ER5 2427ER5 10069ER5; 6520ER1 6461ER5; 6119ER1 10925ER1 6527ER1 6060ER5 10833ER5 6468ER5
ER  - 



TY  - THES
T1  - Enhancing item pool utilization when designing multistage computerized adaptive tests
AN  - 1814744084
AB  - In recent years, the multistage adaptive test (MST) has gained increasing popularity in the field of educational measurement and operational testing. MST refers to a test in which pre-constructed sets of items are administered adaptively and are scored as a unit (Hendrickson, 2007). As a special case of Computerized Adaptive Testing (CAT), a MST program needs the following components: an item response theory (IRT) model or non-IRT-based alternatives; an item pool design; module assembly; ability estimation; routing algorithm; and scoring (Yan et al., 2014). A significant amount of research has been conducted on components like module assembly, ability estimation, routing and scoring, but few studies have addressed the component of item pool design. An item pool is defined as consisting of a maximal number of combinations of items that meet all content specifications for a test and provide sufficient item information for estimation at a series of ability levels (van der Linden et al., 2006). An item pool design is very important because any successful MST assembly is inseparable from an optimal item pool that provides sufficient and high-quality items (Luecht & Nungester, 1998).     Reckase (2003, 2010) developed the p-optimality method to design optimal item pools using the unidimensional Rasch model in CAT, and it has been proved to be efficient for different item types and IRT models. The present study extended this method to MST context in supporting and developing different MST panel designs under different test configurations. The study compared the performance of the MST assembled under the most popularly studied panel designs in the literature, such as 1-2, 1-3, 1-2-2, and 1-2-3. A combination of short, medium and long tests with different routing test proportions were used to build up different tests. Using one of the most popularly investigated IRT models, the Rasch model, simulated optimal item pools were generated with and without practical constraints of exposure control. A total number of 72 optimal items pools were generated and the measurement accuracy was evaluated by an overall sample and conditional sample using various statistical measures. The p-optimality method was also applied in an operational MST licensure test to see if it is feasible in supporting test assembly and achieving sufficient measurement accuracy in practice.     Results showed that the different MST panel designs achieved sufficient measurement accuracy by using the items from the optimal item pools built with the p-optimality method. The same was true with the operational item pool. Measurement accuracy was related to test length, but not so much to the routing test proportions. Exposure control affected the item pool size, but the distributions of the item parameters and item pool characteristics for all the MST panel designs were similar under the two conditions. The item pool sizes under the exposure control conditions were several times larger than those under no exposure control, depending on the types of MST panel designs and routing test proportions. The results from this study provide information for how to enhance item pool utilization when designing multistage computerized adaptive tests, facilitating the MST assembly process, and improving the scoring accuracy.
JF  - ProQuest Dissertations and Theses
AU  - Yang, Lihong
A3  - Reckase, Mark D.
Y1  - 2016
PY  - 2016
DA  - 2016
SP  - 142
CY  - United States -- Michigan
PB  - Michigan State University
PP  - United States -- Michigan
SN  - 978-1-369-05135-3
KW  - Education
KW  - MST test development
KW  - Multistage adaptive tests
KW  - Routing
KW  - Educational tests & measurements
KW  - Educational psychology
KW  - 0525:Educational psychology
KW  - 0288:Educational tests & measurements
UR  - https://www.proquest.com/dissertations-theses/enhancing-item-pool-utilization-when-designing/docview/1814744084/se-2?accountid=10957
L2  - http://www.redi-bw.de/links/unifm?url_ver=Z39.88-2004&rft_val_fmt=info:ofi/fmt:kev:mtx:dissertation&genre=dissertations&sid=ProQ:ProQuest+Dissertations+%26+Theses+Global&atitle=&title=Enhancing+item+pool+utilization+when+designing+multistage+computerized+adaptive+tests&issn=&date=2016-01-01&volume=&issue=&spage=&au=Yang%2C+Lihong&isbn=978-1-369-05135-3&jtitle=&btitle=&rft_id=info:eric/&rft_id=info:doi/
LA  - English
DB  - ProQuest Dissertations & Theses Global
N1  - Copyright - Database copyright ProQuest LLC; ProQuest does not claim copyright in the individual underlying works.
N1  - Zuletzt aktualisiert - 2022-12-20
M3  - Ph.D.
M1  - 10150094
ER  - 



TY  - THES
T1  - Using the Rasch Model in a Computer Adaptive Testing Application to Enhance the Measurement Quality of Emotional Intelligence
AN  - 2566224349
AB  - Background: The use of objective measurement via Rasch models in self-report scales has increased in the recent past, which in turn has enabled the development of computer adaptive tests (CATs). Despite significant advances in both objective measurement and computer adaptive applications few studies have ventured into the personality domain and the emergent field of emotional intelligence. The development of CATs of personality attributes holds advantages such as the reduction in items used for the assessment, reduction of respondent fatigue, and greater cooperation from respondents in the assessment. Research purpose: The aim of this study was to develop a computer adaptive test of the trait Self-control sub-scale of a trait-based emotional intelligence inventory (Trait Emotional Intelligence Questionnaire: TEIQue). Secondary objectives were to examine the functioning of the CAT by (a) comparing the CAT with a static version, and (b) to establish a practical approach to developing a computer adaptive solution to existing static fixed format self-report inventories. Research design: Participants were 681 working South African adults who participated in a local validation research project of the TEIQue. All respondents completed an informed consent form indicating willingness to participate in the research process. The self-control scale consists of 31 items. Each item employs a 7-point Likert-type response format. The data analysis entailed three steps. Step 1 focussed on establishing a benchmark based on the static measurement of trait based self-control. The analysis entailed calibrating and composing a core self-control item bank by means of a Rasch rating scale analysis. The Rasch analysis focussed on how well the observed data fitted the measurement model and identifying items that meet the criteria for inclusion into the item bank. The data were analysed for individual item fit, unidimensionality, local independence, and differential item functioning across gender. Items that did not meet the criteria for good fit were excluded from the item bank. The rating scale analysis yielded 16 items that met the requirements of the Rasch model. These items constituted the static item bank. Step 2 involved obtaining person measures and standard errors for the static item bank. Step 3 involved a post hoc CAT simulation of the responses of the 681 persons to the 16 items, where different item selection criteria and stopping rules were applied to obtain CAT based person measures and standard errors. All simulations were carried out with the Firestar software. The aim of this step was to evaluate the correspondence between static person measures (i.e. measures obtained with the full item bank) and adaptive person measures (i.e. measures obtained with the simulated CAT). Main findings: Overall, high correlations were found between person measures of the static inventory and the CAT version. With a 13 out of 16 item strategy a very high correlation (r = .97) was obtained. The 7 out of 16 item strategy yielded a satisfactory, but somewhat weaker result (r = .91). Results showed that on average about ten items were required to obtain a standard error of person measures < .40. Different initial item starting and subsequent item selection strategies yielded largely similar results with no one single strategy clearly appearing to be the best strategy.
JF  - PQDT - Global
AU  - Hobson, Ernest Guy
A3  - De Bruin, Gideon P.
Y1  - 2015
PY  - 2015
DA  - 2015
SP  - 224
CY  - South Africa
PB  - University of Johannesburg (South Africa)
PP  - South Africa
SN  - 9798744425616
KW  - Information processing
KW  - Principal components analysis
KW  - Self control
KW  - Cognitive ability
KW  - Quantitative psychology
KW  - Personality
KW  - Emotional intelligence
KW  - Psychology
KW  - Cognitive psychology
KW  - Research
KW  - 0633:Cognitive psychology
KW  - 0632:Quantitative psychology
KW  - 0621:Psychology
UR  - https://www.proquest.com/dissertations-theses/using-rasch-model-computer-adaptive-testing/docview/2566224349/se-2?accountid=10957
L2  - http://www.redi-bw.de/links/unifm?url_ver=Z39.88-2004&rft_val_fmt=info:ofi/fmt:kev:mtx:dissertation&genre=dissertations&sid=ProQ:ProQuest+Dissertations+%26+Theses+Global&atitle=&title=Using+the+Rasch+Model+in+a+Computer+Adaptive+Testing+Application+to+Enhance+the+Measurement+Quality+of+Emotional+Intelligence&issn=&date=2015-01-01&volume=&issue=&spage=&au=Hobson%2C+Ernest+Guy&isbn=9798744425616&jtitle=&btitle=&rft_id=info:eric/&rft_id=info:doi/
L2  - https://ujcontent.uj.ac.za/vital/access/manager/Repository/uj:13971?site_name=GlobalView
LA  - English
DB  - ProQuest Dissertations & Theses Global
N1  - Copyright - Database copyright ProQuest LLC; ProQuest does not claim copyright in the individual underlying works.
N1  - Zuletzt aktualisiert - 2021-09-26
M3  - Ph.D.
M1  - 28332601
ER  - 



TY  - THES
T1  - Data-adaptive SNP-set-based association tests of longitudinal traits
AN  - 1796355291
AB  - Genome-wide association studies (GWASs) have been largely limited to investigating traits with a single time measurement. However, many prospective cohort studies and electronic health record (EHR)-based cohorts with GWAS data have collected traits with repeated measurements across follow-up time. Effectively utilizing the information embedded in the time trajectory of measurements could greatly increase the power of association testing. Local association signal patterns across the whole genome are usually variable, complicated and unpredictable, which underscores the need for a data adaptive test capable of maintaining high statistical power across different genetic architecture and association signal patterns. Furthermore, complex diseases are usually affected by multiple variants in a gene and multiple genes in a biological pathway. In addition, traditional single SNP-based association testing has very limited statistical power for variants of low to rare minor allele frequency (MAF < 5% or 1%). A SNP-set-based association test, e.g., based on genes or pathways, could boost the statistical power by aggregating individual weak to moderate association signals across a region of interest. In this dissertation, I have developed such powerful data-adaptive tests that address these analytical challenges facing association testing between longitudinal traits and rare or common variants. I implemented extensive simulation studies to evaluate the performance of the proposed tests, and illustrated their applications in the Atherosclerosis Risk in Communities (ARIC) study. I also produced a software package with documentation to implement the proposed tests in high performance computing platform. In conclusion, this dissertation paves a new path in extending the traditional association tests to longitudinal traits, helps identify novel genes and explain the missing heritability in human complex diseases.
JF  - ProQuest Dissertations and Theses
AU  - Yang, Yang
A3  - Wei, Peng
Y1  - 2015
PY  - 2015
DA  - 2015
SP  - 192
CY  - United States -- Texas
PB  - The University of Texas School of Public Health
PP  - United States -- Texas
SN  - 978-1-339-72944-2
KW  - Pure sciences
KW  - Biological sciences
KW  - Health and environmental sciences
KW  - Association test
KW  - Data-adaptive
KW  - Longitudinal
KW  - Pathway
KW  - Rare variant
KW  - Set-based
KW  - Genetics
KW  - Statistics
KW  - Epidemiology
KW  - 0766:Epidemiology
KW  - 0463:Statistics
KW  - 0369:Genetics
UR  - https://www.proquest.com/dissertations-theses/data-adaptive-snp-set-based-association-tests/docview/1796355291/se-2?accountid=10957
L2  - http://www.redi-bw.de/links/unifm?url_ver=Z39.88-2004&rft_val_fmt=info:ofi/fmt:kev:mtx:dissertation&genre=dissertations&sid=ProQ:ProQuest+Dissertations+%26+Theses+Global&atitle=&title=Data-adaptive+SNP-set-based+association+tests+of+longitudinal+traits&issn=&date=2015-01-01&volume=&issue=&spage=&au=Yang%2C+Yang&isbn=978-1-339-72944-2&jtitle=&btitle=&rft_id=info:eric/&rft_id=info:doi/
LA  - English
DB  - ProQuest Dissertations & Theses Global
N1  - Copyright - Database copyright ProQuest LLC; ProQuest does not claim copyright in the individual underlying works.
N1  - Zuletzt aktualisiert - 2022-11-02
M3  - Ph.D.
M1  - 10109675
ER  - 



TY  - THES
T1  - Using Item Response Theory to Evaluate a Scale of Internalized Distress for Computer Adaptive Administration
AN  - 1748052920
AB  - There has been a notable lack of change in psychological assessment practices over the last several decades despite vast improvements in science and technology. The use of item response theory (IRT) and computer adaptive testing (CAT) in other testing domains has yielded positive results, but these practices have not gained widespread use in psychological assessment. Research has shown that the item response theory methodology can yield important information about assessments not available to researchers through classical test theory, and computer adaptive testing can result in significant item reductions and shorter test administration times. Thus, the current study evaluated whether a single set of items that measure internalizing symptoms in a clinical sample with differential item functioning less than |±0.3| across the three demographic variables of gender, age, and education can be identified. Research has consistently shown that an internalized distress dimension is prevalent in the literature on self-rated mood, as well as in the MMPI-2 item pool. IRT analyses were used to investigate for age, gender, and education bias in a sample of 4,493 participants with 250 participants for most combinations of demographic variables (age [3], education [3], gender [2]). Age, education, and gender bias was most often seen in comparisons for participants in the 60 to 79 year old age group for all three scales. The Welsh Anxiety (A) scale showed the greatest number of items (19) with DIF greater than |±0.3|across the three demographic variables. Differential item functioning was most prevalent when there were significant disparities between the two groups being evaluated. Overall, results showed that there were 22 items that were relatively unbiased for each combination of demographic variables. Internalized Distress (ID) had the greatest number of unbiased items across the various combinations of demographic variables, with 19 items showing relatively no bias. These findings indicate that it is possible to develop a computer adaptive assessment to measure internalizing symptoms in a clinical sample with enough items that are free from bias for age, education, and gender. Further exploration of computer-adaptive administration of the MMPI-2 and other self-report inventories seems warranted because of its ability to provide comparable information frequently with considerable time saving for individuals.
JF  - ProQuest Dissertations and Theses
AU  - Sommers, Lacey M.
A3  - Greene, Roger L.
Y1  - 2015
PY  - 2015
DA  - 2015
SP  - 247
CY  - United States -- California
PB  - Palo Alto University
PP  - United States -- California
SN  - 978-1-339-27887-2
KW  - Psychology
KW  - Adaptive
KW  - Administration
KW  - Computer
KW  - Distress
KW  - Internalized
KW  - 0621:Psychology
UR  - https://www.proquest.com/dissertations-theses/using-item-response-theory-evaluate-scale/docview/1748052920/se-2?accountid=10957
L2  - http://www.redi-bw.de/links/unifm?url_ver=Z39.88-2004&rft_val_fmt=info:ofi/fmt:kev:mtx:dissertation&genre=dissertations&sid=ProQ:ProQuest+Dissertations+%26+Theses+Global&atitle=&title=Using+Item+Response+Theory+to+Evaluate+a+Scale+of+Internalized+Distress+for+Computer+Adaptive+Administration&issn=&date=2015-01-01&volume=&issue=&spage=&au=Sommers%2C+Lacey+M.&isbn=978-1-339-27887-2&jtitle=&btitle=&rft_id=info:eric/&rft_id=info:doi/
LA  - English
DB  - ProQuest Dissertations & Theses Global
N1  - Copyright - Database copyright ProQuest LLC; ProQuest does not claim copyright in the individual underlying works.
N1  - Zuletzt aktualisiert - 2022-08-30
M3  - Ph.D.
M1  - 3737795
ER  - 



TY  - THES
T1  - Dimensionality of the Neuroticism Basic Traits Inventory Scale
AN  - 2572696586
AB  - This study was conducted to examine the dimensionality of the Neuroticism scale of the Basic Traits Inventory in order to contribute to the larger project of developing computer adaptive testing for the five scales of the Basic Traits Inventory. The Neuroticism scale comprises the general factor of Neuroticism and four group factors or facets, namely, Affective Instability, Depression, Self-Consciousness, and Anxiety (Taylor & De Bruin, 2006; Taylor & De Bruin, 2013).Personality constructs and scales are complex as they typically comprise heterogeneous items for reliability and validity purposes (Reise, Morizot, & Hays, 2007). It is therefore to be expected that some studies present evidence of a scale that measures an overarching single dimension – unidimensionality – whilst other studies find evidence of multiple dimensions – multidimensionality (Gibbons et al., 2007; Immekus & Imbrie, 2008; Reise, Moore, & Haviland, 2010). Consequently, specification of a strictly unidimensional model is often not realistic for complex personality constructs and scales. However, test-developers and researchers aspire to unidimensionality (Zinbarg, Yovel, Revelle, & McDonald, 2006) as unidimensionality allows for the unambiguous interpretation of scale scores (Reise et al., 2010), is considered a precondition to reliability and validity, encourages the use of a total score (Netemeyer, Bearden, & Sharma, 2003), meets the unidimensional assumption of item response theory models, and is the prerequisite for the development of computer adaptive testing (Thomas, 1990; Weiss & Gibbons, 2007).Against this background, the key question that was posed in this study was not whether the Neuroticism scale was strictly unidimensional or not, but rather if the scale was unidimensional enough to allow for the meaningful interpretation of a total score (Morizot, Ainsworth, & Reise, 2009). This study focussed on attaining evidence of the presence of a strong general factor which would justify the interpretation of a total score and for applying unidimensional item response theory models to the scale (Reise, 2012).The Basic Traits Inventory was completed by 1 966 participants from various industries and organisations within South Africa. Statistical analyses were performed on the responses to the Neuroticism scale. Confirmatory factor analysis was used to investigate the unidimensionality and strength of the general factor of the scale, while evaluating the influence of multidimensionality on the interpretation of the general factor or the total score. This dimensionality investigation of the scale was achieved by comparing three confirmatory factor models using the goodness-of-fit indices and the standardised factor loadings of the models. The competing structural models were: a one-factor or unidimensional model which represented the general factor and interpretation of a total score for the scale; a four-factor or multidimensional model which represented the multiple factors of the scale and therefore the interpretation of the group factors or facets; and a bifactor model which allowed for the examination of the strength of the general factor in comparison to that of the group factors of the scale.The results of this study confirmed that the Neuroticism scale is multidimensional as the scale comprises multiple group factors or facets. However, the findings of the study revealed the strong presence of a general factor that runs through all the items of the scale, and consequently suggest that efforts to make use of and interpret separate subscale scores lack the needed empirical evidence to do so. In essence, the findings of the study demonstrated that the presence of unidimensionality in the scale is prominent despite the multidimensionality of the scale, which confirmed that the interpretation of the total score for the scale is justified and preferred irrespective of the multidimensionality of the scale.Overall, the findings and evidence of this study proposed that the Neuroticism scale is unidimensional enough to meet the unidimensional assumption of item response theory models. Accordingly, these results support the advancement of adapting the Neuroticism scale of the Basic Traits Inventory for computer adaptive testing (Reise, 2012; Thomas, 1990; Weiss & Gibbons, 2007).This study further confirmed that when developing a multifaceted scale, it is essential to analyse the dimensionality of the scale in order to determine what the scale is actually measuring. The bi factor model can be employed in the development, exploration and confirmation of the dimensionality of the scale to examine the strength of the general factor relative to the multiple group factors of the scale.
JF  - PQDT - Global
AU  - Clifton, Sebastian
A3  - De Bruin, G. P.
Y1  - 2013
PY  - 2013
DA  - 2013
SP  - 83
CY  - South Africa
PB  - University of Johannesburg (South Africa)
PP  - South Africa
SN  - 9798738616341
KW  - Language
KW  - Anxieties
KW  - Personality traits
KW  - Personality
KW  - Psychologists
KW  - Validation studies
KW  - Clinical psychology
KW  - Personality psychology
KW  - Psychology
KW  - 0625:Personality psychology
KW  - 0622:Clinical psychology
KW  - 0621:Psychology
KW  - 0679:Language
UR  - https://www.proquest.com/dissertations-theses/dimensionality-neuroticism-basic-traits-inventory/docview/2572696586/se-2?accountid=10957
L2  - http://www.redi-bw.de/links/unifm?url_ver=Z39.88-2004&rft_val_fmt=info:ofi/fmt:kev:mtx:dissertation&genre=dissertations&sid=ProQ:ProQuest+Dissertations+%26+Theses+Global&atitle=&title=Dimensionality+of+the+Neuroticism+Basic+Traits+Inventory+Scale&issn=&date=2014-01-01&volume=&issue=&spage=&au=Clifton%2C+Sebastian&isbn=9798738616341&jtitle=&btitle=&rft_id=info:eric/&rft_id=info:doi/
L2  - https://ujcontent.uj.ac.za/vital/access/manager/Repository/uj:11549?site_name=GlobalView
LA  - English
DB  - ProQuest Dissertations & Theses Global
N1  - Copyright - Database copyright ProQuest LLC; ProQuest does not claim copyright in the individual underlying works.
N1  - Zuletzt aktualisiert - 2021-11-26
M3  - M.Com.
M1  - 28374975
ER  - 



TY  - THES
T1  - The Effects of Item-by-Item Feedback during a Computer Adaptive Test
AN  - 1660170726
AB  - Decisions involved in designing high-stakes tests include whether to give examinees feedback and, if it is given, how to display the feedback to them. Researchers have yet to examine the effects of item response theory-related feedback on state test anxiety and examinees’ perceptions of fairness when said feedback is given on an item-by-item basis – such as during a computer adaptive test. To address these gaps in the literature, this experiment examined the effects of feedback on state test anxiety and perceived test fairness, accounting for individual differences in feedback acceptance, exam performance, and performance goal orientation. These effects were examined across three modes of item-by-item feedback (i.e., text feedback, graphical feedback, and text/graphical feedback together) during a computer adaptive test in an online sample (N = 338). A three-way interaction occurred between text and graphical feedback, test performance, and prove performance goal orientation. As expected, among those who received feedback, there was a significant, positive relationship between feedback acceptance and perceived test fairness. Limitations and future directions are discussed.
JF  - ProQuest Dissertations and Theses
AU  - Gasperson, Sean Morgan
A3  - Thompson, Lori Foster
Y1  - 2014
PY  - 2014
DA  - 2014
SP  - 128
CY  - United States -- North Carolina
PB  - North Carolina State University
PP  - United States -- North Carolina
SN  - 978-1-321-58320-5
KW  - Psychology
KW  - Anxiety
KW  - Computer adaptive testing
KW  - Feedback
KW  - High-stakes testing environment
KW  - Clinical psychology
KW  - Quantitative psychology
KW  - Cognitive psychology
KW  - 0633:Cognitive psychology
KW  - 0632:Quantitative psychology
KW  - 0622:Clinical psychology
UR  - https://www.proquest.com/dissertations-theses/effects-item-feedback-during-computer-adaptive/docview/1660170726/se-2?accountid=10957
L2  - http://www.redi-bw.de/links/unifm?url_ver=Z39.88-2004&rft_val_fmt=info:ofi/fmt:kev:mtx:dissertation&genre=dissertations&sid=ProQ:ProQuest+Dissertations+%26+Theses+Global&atitle=&title=The+Effects+of+Item-by-Item+Feedback+during+a+Computer+Adaptive+Test&issn=&date=2014-01-01&volume=&issue=&spage=&au=Gasperson%2C+Sean+Morgan&isbn=978-1-321-58320-5&jtitle=&btitle=&rft_id=info:eric/&rft_id=info:doi/
LA  - English
DB  - ProQuest Dissertations & Theses Global
N1  - Copyright - Database copyright ProQuest LLC; ProQuest does not claim copyright in the individual underlying works.
N1  - Zuletzt aktualisiert - 2022-11-02
M3  - Ph.D.
M1  - 3690266
ER  - 



TY  - JOUR
T1  - Mutual Information Item Selection Method in Cognitive Diagnostic Computerized Adaptive Testing with Short Test Length
AN  - 1651841074; EJ1019117
AB  - Cognitive diagnostic computerized adaptive testing (CD-CAT) purports to combine the strengths of both CAT and cognitive diagnosis. Cognitive diagnosis models aim at classifying examinees into the correct mastery profile group so as to pinpoint the strengths and weakness of each examinee whereas CAT algorithms choose items to determine those strengths and weakness as efficiently as possible. Most of the existing CD-CAT item selection algorithms are evaluated when test length is relatively long whereas several applications of CD-CAT, such as in interim assessment, require an item selection algorithm that is able to accurately recover examinees' mastery profile with short test length. In this article, we introduce the mutual information item selection method in the context of CD-CAT and then provide a computationally easier formula to make the method more amenable in real time. Mutual information is then evaluated against common item selection methods, such as Kullback-Leibler information, posterior weighted Kullback-Leibler information, and Shannon entropy. Based on our simulations, mutual information consistently results in nearly the highest attribute and pattern recovery rate in more than half of the conditions. We conclude by discussing how the number of attributes, Q-matrix structure, correlations among the attributes, and item quality affect estimation accuracy.
JF  - Educational and Psychological Measurement
AU  - Wang, Chun
Y1  - 2013/12//
PY  - 2013
DA  - Dec 2013
SP  - 1017
EP  - 1035
PB  - SAGE Publications
VL  - 73
IS  - 6
SN  - 0013-1644, 0013-1644
KW  - ERIC, Current Index to Journals in Education (CIJE)
KW  - Correlation
KW  - Diagnostic Tests
KW  - Selection
KW  - Mathematics
KW  - Cognitive Tests
KW  - Adaptive Testing
KW  - Test Length
KW  - Accuracy
KW  - Computation
KW  - Computer Assisted Testing
KW  - Test Items
KW  - Matrices
UR  - https://www.proquest.com/scholarly-journals/mutual-information-item-selection-method/docview/1651841074/se-2?accountid=10957
L2  - http://www.redi-bw.de/links/unifm?url_ver=Z39.88-2004&rft_val_fmt=info:ofi/fmt:kev:mtx:journal&genre=article&sid=ProQ:ProQ%3Aeric&atitle=Mutual+Information+Item+Selection+Method+in+Cognitive+Diagnostic+Computerized+Adaptive+Testing+with+Short+Test+Length&title=Educational+and+Psychological+Measurement&issn=00131644&date=2013-12-01&volume=73&issue=6&spage=1017&au=Wang%2C+Chun&isbn=&jtitle=Educational+and+Psychological+Measurement&btitle=&rft_id=info:eric/EJ1019117&rft_id=info:doi/10.1177%2F0013164413498256
LA  - English
DB  - ERIC
N1  - Anzahl der Quellenangaben - 44
N1  - Zuletzt aktualisiert - 2021-02-11
N1  - SubjectsTermNotLitGenreText - 2043ER1 2099ER1 2098ER1 10811ER1 10919ER1 6526ER1 6663ER1 2029ER5 2085ER5 2084ER5 10719ER5 10827ER5 6467ER5 6603ER5; 140ER1 10919ER1 6526ER1 6663ER1 138ER5 10827ER5 6467ER5 6603ER5; 1741ER1 10925ER1 6527ER1 1727ER5 10833ER5 6468ER5; 2847ER1 10925ER1 6527ER1 2826ER5 10833ER5 6468ER5; 10901ER1 10809ER5; 10900ER1 10808ER5; 9550ER1 9469ER5; 6505ER1 409ER1 6490ER1 6042ER1 6446ER5 404ER5 6431ER5 5983ER5; 2295ER1 10210ER1 2604ER1 3679ER1 6663ER1 2278ER5 10125ER5 2586ER5 3643ER5 6603ER5; 96ER1 3677ER1 2445ER1 10154ER1 94ER5 3641ER5 2427ER5 10069ER5; 2028ER1 6474ER1 2014ER5 6415ER5; 6490ER1 6042ER1 6431ER5 5983ER5
DO  - https://doi.org/10.1177/0013164413498256
ER  - 



TY  - JOUR
T1  - Test Anxiety, Computer-Adaptive Testing and the Common Core
AN  - 1697499495; EJ1054865
AB  - This paper highlights the current findings and issues regarding the role of computer-adaptive testing in test anxiety. The computer-adaptive test (CAT) proposed by one of the Common Core consortia brings these issues to the forefront. Research has long indicated that test anxiety impairs student performance. More recent research indicates that taking a test in a CAT format can affect the ability estimates of students with test anxiety. Inaccurate measures of ability are disconcerting because of the threat they pose to the validity of test score interpretation. This paper raises concerns regarding how the implementation of a computer-adaptive test for a large-scale common core assessment system could differentially affect students with test anxiety. Issues of fairness and score comparability are raised, and the implications of these issues are discussed.
JF  - Journal of Education and Training Studies
AU  - Colwell, Nicole Makas
Y1  - 2013/10//
PY  - 2013
DA  - Oct 2013
SP  - 50
EP  - 60
PB  - Redfame Publishing Inc
VL  - 1
IS  - 2
SN  - 2324-805X, 2324-805X
KW  - ERIC, Current Index to Journals in Education (CIJE)
KW  - Elementary Secondary Education
KW  - Student Evaluation
KW  - Scores
KW  - Socioeconomic Status
KW  - High Stakes Tests
KW  - Test Bias
KW  - Ability Identification
KW  - Computer Assisted Testing
KW  - Educational Technology
KW  - Effect Size
KW  - Standardized Tests
KW  - Summative Evaluation
KW  - Test Items
KW  - Evaluation Methods
KW  - Difficulty Level
KW  - Test Anxiety
UR  - https://www.proquest.com/scholarly-journals/test-anxiety-computer-adaptive-testing-common/docview/1697499495/se-2?accountid=10957
L2  - http://www.redi-bw.de/links/unifm?url_ver=Z39.88-2004&rft_val_fmt=info:ofi/fmt:kev:mtx:journal&genre=article&sid=ProQ:ProQ%3Aeric&atitle=Test+Anxiety%2C+Computer-Adaptive+Testing+and+the+Common+Core&title=Journal+of+Education+and+Training+Studies&issn=2324805X&date=2013-10-01&volume=1&issue=2&spage=50&au=Colwell%2C+Nicole+Makas&isbn=&jtitle=Journal+of+Education+and+Training+Studies&btitle=&rft_id=info:eric/EJ1054865&rft_id=info:doi/
LA  - English
DB  - ERIC
N1  - Anzahl der Quellenangaben - 41
N1  - Zuletzt aktualisiert - 2021-02-11
N1  - SubjectsTermNotLitGenreText - 10890ER1 554ER1 8522ER1 10798ER5 549ER5 8446ER5; 2043ER1 2099ER1 2098ER1 10811ER1 10919ER1 6526ER1 6663ER1 2029ER5 2085ER5 2084ER5 10719ER5 10827ER5 6467ER5 6603ER5; 3679ER1 6663ER1 3643ER5 6603ER5; 10153ER1 10925ER1 6527ER1 10068ER5 10833ER5 6468ER5; 10891ER1 955ER1 10799ER5 946ER5; 3311ER1 10805ER1 3282ER5 10713ER5; 10900ER1 10808ER5; 2883ER1 2862ER5; 10337ER1 3676ER1 10247ER5 3640ER5; 4ER1 5021ER1 3ER5 4971ER5; 3412ER1 3192ER1 3382ER5 3164ER5; 3324ER1 10210ER1 2604ER1 3679ER1 6663ER1 3295ER5 10125ER5 2586ER5 3643ER5 6603ER5; 9485ER1 2602ER1 9404ER5 2584ER5; 9957ER1 10228ER1 9873ER5 10141ER5; 4807ER1 10925ER1 6527ER1 4759ER5 10833ER5 6468ER5; 10460ER1 3676ER1 10369ER5 3640ER5
ER  - 



TY  - THES
T1  - An Investigation on Computer-Adaptive Multistage Testing Panels for Multidimensional Assessment
AN  - 1826527360; ED565591
AB  - The computer-adaptive multistage testing (ca-MST) has been developed as an alternative to computerized adaptive testing (CAT), and been increasingly adopted in large-scale assessments. Current research and practice only focus on ca-MST panels for credentialing purposes. The ca-MST test mode, therefore, is designed to gauge a single scale. The present study is the first step to investigate ca-MST panels for diagnostic purposes, which involve the assessment of multiple attributes in the same test. This study employed computer simulation to compare multidimensional ca-MST panels and their unidimensional counterparts, and to explore the factors that affect the accuracy and efficiency of multidimensional ca-MST. Nine multidimensional ca-MST panel designs--which differed in configuration and test length--were simulated under varied attribute correlation scenarios. In addition, item pools with different qualities were studied to suggest appropriate item bank design. The comparison between the multidimensional ca-MST and a sequential of unidimensional ca-MST suggested that when attributes correlated moderate to high, employing a multidimensional ca-MST provided more accurate and efficient scoring decisions than several unidimensional ca-MST with IRT scoring. However, a multidimensional ca-MST did not perform better than its unidimensional counterpart with MIRT scoring. Nevertheless, multidimensional panels are still promising for diagnostic purposes given practical considerations. The investigation on multidimensional ca-MST design indicated the following: Higher attribute correlation was associated with better scoring decision because more information carried by a correlation matrix was available for estimation. This held true across all item pool conditions. An optimal item pool would be the one that was discriminative, appropriately located and specifically designed for a configuration. The accuracy and efficiency of a multidimensional ca-MST panel would be diminished if its item pool was too easy, or not informative. According to the results, the 1-2-3 configuration design was most promising. In terms of test length, an appropriate decision would largely depend on the attribute correlation and the item pool characteristics. [The dissertation citations contained here are published with the permission of ProQuest LLC. Further reproduction is prohibited without permission. Copies of dissertations may be obtained by Telephone (800) 1-800-521-0600. Web page: http://www.proquest.com/en-US/products/dissertations/individuals.shtml.]
JF  - ProQuest LLC
AU  - Wang, Xinrui
Y1  - 2013
PY  - 2013
DA  - 2013
SP  - 1
EP  - 91
SN  - 9781303685835
KW  - ERIC, Resources in Education (RIE)
KW  - Adaptive Testing
KW  - Comparative Analysis
KW  - Accuracy
KW  - Correlation
KW  - Diagnostic Tests
KW  - Test Format
KW  - Efficiency
KW  - Computer Assisted Testing
KW  - Item Banks
KW  - Scoring
KW  - Test Items
UR  - https://www.proquest.com/dissertations-theses/investigation-on-computer-adaptive-multistage/docview/1826527360/se-2?accountid=10957
L2  - http://www.redi-bw.de/links/unifm?url_ver=Z39.88-2004&rft_val_fmt=info:ofi/fmt:kev:mtx:dissertation&genre=dissertations&sid=ProQ:ERIC&atitle=&title=An+Investigation+on+Computer-Adaptive+Multistage+Testing+Panels+for+Multidimensional+Assessment&issn=&date=2013-01-01&volume=&issue=&spage=&au=Wang%2C+Xinrui&isbn=9781303685835&jtitle=&btitle=&rft_id=info:eric/ED565591&rft_id=info:doi/
LA  - English
DB  - ERIC
N1  - Zuletzt aktualisiert - 2021-02-11
N1  - Erste Seite - 1
N1  - SubjectsTermNotLitGenreText - 2043ER1 2099ER1 2098ER1 10811ER1 10919ER1 6526ER1 6663ER1 2029ER5 2085ER5 2084ER5 10719ER5 10827ER5 6467ER5 6603ER5; 140ER1 10919ER1 6526ER1 6663ER1 138ER5 10827ER5 6467ER5 6603ER5; 2847ER1 10925ER1 6527ER1 2826ER5 10833ER5 6468ER5; 1980ER1 3679ER1 6663ER1 1966ER5 3643ER5 6603ER5; 96ER1 3677ER1 2445ER1 10154ER1 94ER5 3641ER5 2427ER5 10069ER5; 3329ER1 3300ER5; 10898ER1 10806ER5; 10900ER1 10808ER5; 5593ER1 5540ER5; 2295ER1 10210ER1 2604ER1 3679ER1 6663ER1 2278ER5 10125ER5 2586ER5 3643ER5 6603ER5; 9486ER1 6520ER1 9405ER5 6461ER5
ER  - 



TY  - THES
T1  - Using a computer-adaptive test simulation to investigate test coordinators' perceptions of a high-stakes computer-based testing program
AN  - 1609013939
AB  - This case study examined the efficiency and precision of computer classification and adaptive testing to elicit responses from test coordinators on implementing high-stakes computer-based testing. Test coordinators from five elementary schools located in a Georgia school district participated in the study. The school district administered state-made, high-stakes tests using paper and pencil; locally-developed tests via the computer or paper and pencil. A post-hoc simulation program, Comprehensive Simulation of Computerized Adaptive Testing, used 586 student item responses to produce results with a variable termination point and a classification termination point. Results from the simulation were analyzed and used in the case study to elicit interview responses from test coordinators. The photographs of computer-labs and test schedule documents were collected and analyzed to validate school test coordinators’ responses.    Test coordinators responded positively to the efficiency and precision of simulation results. Some test coordinators preferred the use of computer-adaptive tests for diagnostic purposes only. Test coordinators’ experiences focused on the security, the emotions, and the management of testing. The findings of this study will benefit those interested in implementing a high-stakes, computer-based testing program by recommending a simulation study be conducted and feedback be solicited from test coordinators prior to an operational test administration.
JF  - ProQuest Dissertations and Theses
AU  - Hogan, Tiffany E.
A3  - Oshima, T. Chris
Y1  - 2013
PY  - 2013
DA  - 2013
SP  - 120
CY  - United States -- Georgia
PB  - Georgia State University
PP  - United States -- Georgia
SN  - 978-1-321-18943-8
KW  - Education
KW  - Case study
KW  - Computer-adaptive test
KW  - Computer-based testing
KW  - Computerized classification test
KW  - Test coordinator's perceptions
KW  - Educational tests & measurements
KW  - Elementary education
KW  - Education policy
KW  - 0458:Education Policy
KW  - 0288:Educational tests & measurements
KW  - 0524:Elementary education
UR  - https://www.proquest.com/dissertations-theses/using-computer-adaptive-test-simulation/docview/1609013939/se-2?accountid=10957
L2  - http://www.redi-bw.de/links/unifm?url_ver=Z39.88-2004&rft_val_fmt=info:ofi/fmt:kev:mtx:dissertation&genre=dissertations&sid=ProQ:ProQuest+Dissertations+%26+Theses+Global&atitle=&title=Using+a+computer-adaptive+test+simulation+to+investigate+test+coordinators%27+perceptions+of+a+high-stakes+computer-based+testing+program&issn=&date=2013-01-01&volume=&issue=&spage=&au=Hogan%2C+Tiffany+E.&isbn=978-1-321-18943-8&jtitle=&btitle=&rft_id=info:eric/&rft_id=info:doi/
LA  - English
DB  - ProQuest Dissertations & Theses Global
N1  - Copyright - Database copyright ProQuest LLC; ProQuest does not claim copyright in the individual underlying works.
N1  - Zuletzt aktualisiert - 2022-11-01
M3  - Ph.D.
M1  - 3583649
ER  - 



TY  - THES
T1  - An investigation on computer-adaptive multistage testing panels for multidimensional assessment
AN  - 1497280615
AB  - The computer-adaptive multistage testing (ca-MST) has been developed as an alternative to computerized adaptive testing (CAT), and been increasingly adopted in large-scale assessments. Current research and practice only focus on ca-MST panels for credentialing purposes. The ca-MST test mode, therefore, is designed to gauge a single scale. The present study is the first step to investigate ca-MST panels for diagnostic purposes, which involve the assessment of multiple attributes in the same test.   This study employed computer simulation to compare multidimensional ca-MST panels and their unidimensional counterparts, and to explore the factors that affect the accuracy and efficiency of multidimensional ca-MST. Nine multidimensional ca-MST panel designs—which differed in configuration and test length—were simulated under varied attribute correlation scenarios. In addition, item pools with different qualities were studied to suggest appropriate item bank design.   The comparison between the multidimensional ca-MST and a sequential of unidimensional ca-MST suggested that when attributes correlated moderate to high, employing a multidimensional ca-MST provided more accurate and efficient scoring decisions than several unidimensional ca-MST with IRT scoring. However, a multidimensional ca-MST did not perform better than its unidimensional counterpart with MIRT scoring. Nevertheless, multidimensional panels are still promising for diagnostic purposes given practical considerations.   The investigation on multidimensional ca-MST design indicated the following: Higher attribute correlation was associated with better scoring decision because more information carried by a correlation matrix was available for estimation. This held true across all item pool conditions. An optimal item pool would be the one that was discriminative, appropriately located and specifically designed for a configuration. The accuracy and efficiency of a multidimensional ca-MST panel would be diminished if its item pool was too easy, or not informative. According to the results, the 1-2-3 configuration design was most promising. In terms of test length, an appropriate decision would largely depend on the attribute correlation and the item pool characteristics.
JF  - ProQuest Dissertations and Theses
AU  - Wang, Xinrui
A3  - Luecht, Richard M.
Y1  - 2013
PY  - 2013
DA  - 2013
SP  - 91
CY  - United States -- North Carolina
PB  - The University of North Carolina at Greensboro
PP  - United States -- North Carolina
SN  - 978-1-303-68583-5
KW  - Education
KW  - Computer-adaptive testing
KW  - Multidimensional assessment
KW  - Multistage testing
KW  - Educational tests & measurements
KW  - 0288:Educational tests & measurements
UR  - https://www.proquest.com/dissertations-theses/investigation-on-computer-adaptive-multistage/docview/1497280615/se-2?accountid=10957
L2  - http://www.redi-bw.de/links/unifm?url_ver=Z39.88-2004&rft_val_fmt=info:ofi/fmt:kev:mtx:dissertation&genre=dissertations&sid=ProQ:ProQuest+Dissertations+%26+Theses+Global&atitle=&title=An+investigation+on+computer-adaptive+multistage+testing+panels+for+multidimensional+assessment&issn=&date=2013-01-01&volume=&issue=&spage=&au=Wang%2C+Xinrui&isbn=978-1-303-68583-5&jtitle=&btitle=&rft_id=info:eric/&rft_id=info:doi/
LA  - English
DB  - ProQuest Dissertations & Theses Global
N1  - Copyright - Database copyright ProQuest LLC; ProQuest does not claim copyright in the individual underlying works.
N1  - Zuletzt aktualisiert - 2022-10-31
M3  - Ph.D.
M1  - 3609605
ER  - 



TY  - JOUR
T1  - Identifying Differential Item Functioning in Multi-Stage Computer Adaptive Testing
AN  - 1347458951; EJ996865
AB  - The purpose of this study is to evaluate the performance of CATSIB (Computer Adaptive Testing-Simultaneous Item Bias Test) for detecting differential item functioning (DIF) when items in the matching and studied subtest are administered adaptively in the context of a realistic multi-stage adaptive test (MST). MST was simulated using a 4-item module in a 7-panel administration. Three independent variables, expected to affect DIF detection rates, were manipulated: item difficulty, sample size, and balanced/unbalanced design. CATSIB met the acceptable criteria, meaning that the Type I error and power rates met 5% and 80%, respectively, for the large reference/moderate focal sample and the large reference/large focal sample conditions. These results indicate that CATSIB can be used to consistently and accurately detect DIF on an MST, but only with moderate to large samples. (Contains 4 tables and 2 figures.)
JF  - Educational Research and Evaluation
AU  - Gierl, Mark J.
AU  - Lai, Hollis
AU  - Li, Johnson
Y1  - 2013
PY  - 2013
DA  - 2013
SP  - 188
EP  - 203
PB  - Routledge
VL  - 19
IS  - 2-3
SN  - 1380-3611, 1380-3611
KW  - ERIC, Current Index to Journals in Education (CIJE)
KW  - Adaptive Testing
KW  - Error of Measurement
KW  - Computer Assisted Testing
KW  - Psychometrics
KW  - Test Items
KW  - Test Bias
UR  - https://www.proquest.com/scholarly-journals/identifying-differential-item-functioning-multi/docview/1347458951/se-2?accountid=10957
L2  - http://www.redi-bw.de/links/unifm?url_ver=Z39.88-2004&rft_val_fmt=info:ofi/fmt:kev:mtx:journal&genre=article&sid=ProQ:ProQ%3Aeric&atitle=Identifying+Differential+Item+Functioning+in+Multi-Stage+Computer+Adaptive+Testing&title=Educational+Research+and+Evaluation&issn=13803611&date=2013-01-01&volume=19&issue=2-3&spage=188&au=Gierl%2C+Mark+J.%3BLai%2C+Hollis%3BLi%2C+Johnson&isbn=&jtitle=Educational+Research+and+Evaluation&btitle=&rft_id=info:eric/EJ996865&rft_id=info:doi/10.1080%2F13803611.2013.767622
LA  - English
DB  - ERIC
N1  - Anzahl der Quellenangaben - 33
N1  - Zuletzt aktualisiert - 2021-02-11
N1  - SubjectsTermNotLitGenreText - 140ER1 10919ER1 6526ER1 6663ER1 138ER5 10827ER5 6467ER5 6603ER5; 10891ER1 955ER1 10799ER5 946ER5; 2043ER1 2099ER1 2098ER1 10811ER1 10919ER1 6526ER1 6663ER1 2029ER5 2085ER5 2084ER5 10719ER5 10827ER5 6467ER5 6603ER5; 10900ER1 10808ER5; 3621ER1 10210ER1 2604ER1 3679ER1 6663ER1 3586ER5 10125ER5 2586ER5 3643ER5 6603ER5; 8530ER1 8529ER1 938ER1 9466ER1 6042ER1 8454ER5 8453ER5 929ER5 9385ER5 5983ER5
DO  - https://doi.org/10.1080/13803611.2013.767622
ER  - 



TY  - THES
T1  - Item selection methods in multidimensional computerized adaptive testing adopting polytomously-scored items under multidimensional generalized partial credit model
AN  - 1447013044
AB  - Four item selection methods are compared and investigated under three test formats in the context of Multidimensional Computerized Adaptive Testing (MCAT) delivering polytomous items partially or completely in tests. Item selection methods examined include Fisher information based D-optimality (D-optimality), Kullback-Leibler information index (KI), mutual information (MI), and continuous entropy method (CEM). The three test formats considered are the POLYTYPE format that contains polytomous items with three response categories, the DPMIX format that delivers dichotomous items at the beginning and polytomous items at the final stage, and the PDMIX format that has the reverse order as DPMIX. In general, D-optimality shows the best estimation accuracy and conditional estimation accuracy. D-optimality, MI, and CEM are similar in terms of ability estimation accuracy and tendency in selecting items when the item bank size is large. For both dichotomous and polytomous items, KI is mostly outperformed by the other three methods in terms of ability estimation precision. When sub-thetas in both dimensions are equal, however, KI shows the best performance for polytomous items. In this study, which item type, dichotomous or polytomous, being administered first does not affect the estimation accuracy. However, if the test length is much longer or shorter than the test length of the current study, it is possible that the estimation accuracy could be affected by the order of delivering different item types. Both DPMIX and PDMIX formats yield similar conditional estimation accuracy pattern and precision. In addition, the item bank size does affect the estimation precision. These conclusions, however, might not be applied to MCAT testing with different test designs or item pool structures. More studies are needed in MCAT combining with polytomous items to further facilitate the development and improvement of the next-generation assessments such as formative assessment or testing for diagnosis.
JF  - ProQuest Dissertations and Theses
AU  - Lin, Haiyan
A3  - Ryan, Katherine
Y1  - 2012
PY  - 2012
DA  - 2012
SP  - 122
CY  - United States -- Illinois
PB  - University of Illinois at Urbana-Champaign
PP  - United States -- Illinois
SN  - 978-1-303-50911-7
KW  - Psychology
KW  - Education
KW  - Computerized adaptive testing
KW  - Generalized partial credit model
KW  - Item response theory
KW  - Item selection methods
KW  - Multidimensional item response theory
KW  - Educational tests & measurements
KW  - Educational psychology
KW  - Quantitative psychology
KW  - 0632:Quantitative psychology
KW  - 0525:Educational psychology
KW  - 0288:Educational tests & measurements
UR  - https://www.proquest.com/dissertations-theses/item-selection-methods-multidimensional/docview/1447013044/se-2?accountid=10957
L2  - http://www.redi-bw.de/links/unifm?url_ver=Z39.88-2004&rft_val_fmt=info:ofi/fmt:kev:mtx:dissertation&genre=dissertations&sid=ProQ:ProQuest+Dissertations+%26+Theses+Global&atitle=&title=Item+selection+methods+in+multidimensional+computerized+adaptive+testing+adopting+polytomously-scored+items+under+multidimensional+generalized+partial+credit+model&issn=&date=2012-01-01&volume=&issue=&spage=&au=Lin%2C+Haiyan&isbn=978-1-303-50911-7&jtitle=&btitle=&rft_id=info:eric/&rft_id=info:doi/
LA  - English
DB  - ProQuest Dissertations & Theses Global
N1  - Copyright - Database copyright ProQuest LLC; ProQuest does not claim copyright in the individual underlying works.
N1  - Zuletzt aktualisiert - 2022-10-31
M3  - Ph.D.
M1  - 3600698
ER  - 



TY  - JOUR
T1  - Evaluating Knowledge Structure-Based Adaptive Testing Algorithms and System Development
AN  - 1312423561; EJ988452
AB  - In recent years, many computerized test systems have been developed for diagnosing students' learning profiles. Nevertheless, it remains a challenging issue to find an adaptive testing algorithm to both shorten testing time and precisely diagnose the knowledge status of students. In order to find a suitable algorithm, four adaptive testing algorithms, based on ordering theory, item relational structure theory, Diagnosys, and domain experts, were evaluated based on the training sample size, prediction accuracy, and the use of test items by the simulation study with paper-based test data. Based on the results of simulation study, ordering theory has the best performance. An ordering-theory-based knowledge-structure-adaptive testing system was developed and evaluated. The results of this system showed that the two different interfaces, paper-based and computer-based, did not affect the examinees' performance. In addition, the effect of correct guessing was discussed, and two methods with adaptive testing algorithms were proposed to mitigate this effect. The experimental results showed that the proposed methods improve the effect of correct guessing. (Contains 6 tables and 18 figures.)
JF  - Educational Technology & Society
AU  - Wu, Huey-Min
AU  - Kuo, Bor-Chen
AU  - Yang, Jinn-Min
Y1  - 2012
PY  - 2012
DA  - 2012
SP  - 73
EP  - 88
PB  - International Forum of Educational Technology & Society
VL  - 15
IS  - 2
SN  - 1436-4522, 1436-4522
KW  - Taiwan
KW  - ERIC, Current Index to Journals in Education (CIJE)
KW  - Mathematics Instruction
KW  - Bayesian Statistics
KW  - Cognitive Structures
KW  - Mathematical Concepts
KW  - Mathematics
KW  - Foreign Countries
KW  - Experiments
KW  - Adaptive Testing
KW  - Sample Size
KW  - Simulation
KW  - Computer Assisted Testing
KW  - Educational Technology
KW  - Mathematics Education
KW  - Test Items
KW  - Expertise
UR  - https://www.proquest.com/scholarly-journals/evaluating-knowledge-structure-based-adaptive/docview/1312423561/se-2
L2  - http://www.redi-bw.de/links/unifm?url_ver=Z39.88-2004&rft_val_fmt=info:ofi/fmt:kev:mtx:journal&genre=article&sid=ProQ:ProQ%3Aeric&atitle=Evaluating+Knowledge+Structure-Based+Adaptive+Testing+Algorithms+and+System+Development&title=Educational+Technology+%26+Society&issn=14364522&date=2012-01-01&volume=15&issue=2&spage=73&au=Wu%2C+Huey-Min%3BKuo%2C+Bor-Chen%3BYang%2C+Jinn-Min&isbn=&jtitle=Educational+Technology+%26+Society&btitle=&rft_id=info:eric/EJ988452&rft_id=info:doi/
LA  - English
DB  - ERIC
N1  - Anzahl der Quellenangaben - 23
N1  - Zuletzt aktualisiert - 2021-02-11
N1  - SubjectsTermNotLitGenreText - 140ER1 10919ER1 6526ER1 6663ER1 138ER5 10827ER5 6467ER5 6603ER5; 10900ER1 10808ER5; 2043ER1 2099ER1 2098ER1 10811ER1 10919ER1 6526ER1 6663ER1 2029ER5 2085ER5 2084ER5 10719ER5 10827ER5 6467ER5 6603ER5; 6490ER1 6042ER1 6431ER5 5983ER5; 9769ER1 6663ER1 9687ER5 6603ER5; 1738ER1 1724ER5; 3760ER1 3724ER5; 3311ER1 10805ER1 3282ER5 10713ER5; 6497ER1 3192ER1 6438ER5 3164ER5; 6499ER1 5309ER1 6440ER5 5259ER5; 9210ER1 9130ER5; 6476ER1 6417ER5; 907ER1 10210ER1 2604ER1 3679ER1 6663ER1 10226ER1 6490ER1 6042ER1 898ER5 10140ER5 6431ER5 5983ER5 10125ER5 2586ER5 3643ER5 6603ER5; 3758ER1 3722ER5; 4166ER1 4394ER1 4124ER5 4350ER5; Taiwan
ER  - 



TY  - THES
T1  - On multistage adaptive biomarker-directed clinical trial design
AN  - 1287093791
AB  - Biomarker-directed targeted designs have been developed in recent years for pharmaceutical development for patient subpopulation with specific disease etiology. Multistage testing and adaption are desired for targeted designs to manage the risk of study population deviation, to conduct interim analysis and to make decisions on stochastic curtailment. We propose a multistage adaptive design for targeted trials that allows flexibility in multistage testing, stochastic curtailment decision-making, biomarker performance monitoring and biomarker performance-based sample size adaption.  The design is based on the theorem that the sequence of test statistics from multistage testings asymptotically satisfies Brownian motion in different targeted trial settings. The theorem is proven for both continuous and binary endpoints.      The simulation analysis has indicated that targeted multistage designs perform better than its untargeted counterpart in trial efficiency, effective information accumulation and stochastic curtailment decision-making by selecting targeted patients. However, this study has shown that biomarker performance influences heterogeneity of targeted study population and consequently affects planning and multistage testing of a targeted trial. The impact of biomarker performance has been extensively examined with regard to study efficiency, actual statistical power, type-I error, information accrument, and stochastic curtailment decision-making. To cope with the limited information of biomarker performance at study planning, we propose the biomarker performance-based sample size adaption for targeted trials for continuous and binary endpoints, respectively. The adaptive method can update actual biomarker performance at interim analysis and re-estimate sample size to achieve planned statistical power while preserving type-I error rate.  In summary, the multistage adaptive design improves targeted trials by providing great flexibility in multistage testing, early stopping, biomarker performance monitoring and biomarker performance-based sample size adaption.
JF  - ProQuest Dissertations and Theses
AU  - Gao, Zhong
A3  - Roy, Anindya
A3  - Tan, Ming
Y1  - 2012
PY  - 2012
DA  - 2012
SP  - 95
CY  - United States -- Maryland
PB  - University of Maryland, Baltimore County
PP  - United States -- Maryland
SN  - 978-1-267-88185-4
KW  - Pure sciences
KW  - Biological sciences
KW  - Adaptive designs
KW  - Biomarker-directed
KW  - Clinical trials
KW  - Multistage
KW  - Biostatistics
KW  - Statistics
KW  - 0308:Biostatistics
KW  - 0463:Statistics
UR  - https://www.proquest.com/dissertations-theses/on-multistage-adaptive-biomarker-directed/docview/1287093791/se-2?accountid=10957
L2  - http://www.redi-bw.de/links/unifm?url_ver=Z39.88-2004&rft_val_fmt=info:ofi/fmt:kev:mtx:dissertation&genre=dissertations&sid=ProQ:ProQuest+Dissertations+%26+Theses+Global&atitle=&title=On+multistage+adaptive+biomarker-directed+clinical+trial+design&issn=&date=2012-01-01&volume=&issue=&spage=&au=Gao%2C+Zhong&isbn=978-1-267-88185-4&jtitle=&btitle=&rft_id=info:eric/&rft_id=info:doi/
LA  - English
DB  - ProQuest Dissertations & Theses Global
N1  - Copyright - Database copyright ProQuest LLC; ProQuest does not claim copyright in the individual underlying works.
N1  - Zuletzt aktualisiert - 2022-10-31
M3  - Ph.D.
M1  - 3550810
ER  - 



TY  - THES
T1  - Item selection methods in polytomous computerized adaptive testing
AN  - 901887117
AB  - Given the rapid advancement of computer technology, the importance of administering adaptive tests with polytomous items is in great need. With regard to the applicability of adaptive testing using polytomous IRT models, adaptive testing can use polytomous items of either rating scales, or in some testing situations of multiple choice. Additionally, the availability of computerized polytomous scoring of open-ended items enhances such applicability. This need promotes the research in polytomous adaptive testing (PAT). This dissertation is an effort to focus on item selection methods, as a major component, in polytomous computerized adaptive testing. So, it consists of five chapters that cover the following:    Chapter 1 focuses on a thorough introduction to the item response theory (IRT) models and adaptive testing related to polytomous items. Such an important overview and introduction to basic concepts in test theory and mathematical models for polytomous items is needed for the flow of consequent chapters. Chapter 2 is devoted to the development of a central location index (LI) to uniquely represent the polytomous item with a scale value parameter using most commonly used polytomous models. The motivation and rationale to search for a central or an overall location parameter is twofold: (a) the confusion of multiple and different parameterizations for a polytomous item even for the same model, and (b) the unavailability of such single location parameter block the usage of certain item selection methods in adaptive testing. Two approaches are used to derive the proposed LIs, one is based on the item category response functions (ICRFs) and the other is based on the polytomous item response function (IRF). As a result, four LIs are proposed. Chapter 3 is particularly assigned to development of an item selection method based on the developed location index and primarily assess its performance in the PAT context relative to existing methods. This method belongs to the non-information based item selection methods and we referred it as Matching-LI method. The results support that this proposed method is promising and is capable to produce accurate ability estimates and successfully manage the item pool usage. Chapter 4 introduces new item selection methods taking in consideration the previous chapter's results. The new methods are the hybrid, stage-based information, polytomous  a-stratification methods. The first two methods try to merge more than one criterion for selecting items of each PAT (e.g., the hybrid method merges both the Matching-LI and maximum information (MI) methods). The last method uses Matching-LI method within each stratum. Chapter 5 provides discussion, conclusions, and limitations and future research directions with respect to important components of an adaptive testing program (i.e., item selection methods, item response models, item banks, and trait versus attribute estimation).
JF  - ProQuest Dissertations and Theses
AU  - Ali, Usama Sayed Ahmed
A3  - Anderson, Carolyn J.
Y1  - 2011
PY  - 2011
DA  - 2011
SP  - 88
CY  - United States -- Illinois
PB  - University of Illinois at Urbana-Champaign
PP  - United States -- Illinois
SN  - 978-1-124-96332-7
KW  - Education
KW  - Computerized adaptive testing
KW  - Item response theory
KW  - Item selection
KW  - Polytomous IRT models.
KW  - Polytomus items
KW  - Educational tests & measurements
KW  - Educational technology
KW  - Computers
KW  - Models
KW  - 0710:Educational technology
KW  - 0288:Educational tests & measurements
UR  - https://www.proquest.com/dissertations-theses/item-selection-methods-polytomous-computerized/docview/901887117/se-2?accountid=10957
L2  - http://www.redi-bw.de/links/unifm?url_ver=Z39.88-2004&rft_val_fmt=info:ofi/fmt:kev:mtx:dissertation&genre=dissertations&sid=ProQ:ProQuest+Dissertations+%26+Theses+Global&atitle=&title=Item+selection+methods+in+polytomous+computerized+adaptive+testing&issn=&date=2011-01-01&volume=&issue=&spage=&au=Ali%2C+Usama+Sayed+Ahmed&isbn=978-1-124-96332-7&jtitle=&btitle=&rft_id=info:eric/&rft_id=info:doi/
LA  - English
DB  - ProQuest Dissertations & Theses Global
N1  - Copyright - Database copyright ProQuest LLC; ProQuest does not claim copyright in the individual underlying works.
N1  - Zuletzt aktualisiert - 2022-11-01
M3  - Ph.D.
M1  - 3478592
ER  - 



TY  - JOUR
T1  - Adaptive Item-Based Learning Environments Based on the Item Response Theory: Possibilities and Challenges
AN  - 1140133682; EJ978605
AB  - The popularity of intelligent tutoring systems (ITSs) is increasing rapidly. In order to make learning environments more efficient, researchers have been exploring the possibility of an automatic adaptation of the learning environment to the learner or the context. One of the possible adaptation techniques is adaptive item sequencing by matching the difficulty of the items to the learner's knowledge level. This is already accomplished to a certain extent in adaptive testing environments, where the test is tailored to the person's ability level by means of the item response theory (IRT). Even though IRT has been a prevalent computerized adaptive test (CAT) approach for decades and applying IRT in item-based ITSs could lead to similar advantages as in CAT (e.g. higher motivation and more efficient learning), research on the application of IRT in such learning environments is highly restricted or absent. The purpose of this paper was to explore the feasibility of applying IRT in adaptive item-based ITSs. Therefore, we discussed the two main challenges associated with IRT application in such learning environments: the challenge of the data set and the challenge of the algorithm. We concluded that applying IRT seems to be a viable solution for adaptive item selection in item-based ITSs provided that some modifications are implemented. Further research should shed more light on the adequacy of the proposed solutions. (Contains 1 table and 3 figures.)
JF  - Journal of Computer Assisted Learning
AU  - Wauters, K.
AU  - Desmet, P.
AU  - Van den Noortgate, W.
Y1  - 2010/12//
PY  - 2010
DA  - Dec 2010
SP  - 549
EP  - 562
PB  - Wiley-Blackwell
VL  - 26
IS  - 6
SN  - 0266-4909, 0266-4909
KW  - ERIC, Current Index to Journals in Education (CIJE)
KW  - Intelligent Tutoring Systems
KW  - Programming
KW  - Computer Software
KW  - Second Language Learning
KW  - Data
KW  - Adaptive Testing
KW  - Problems
KW  - Knowledge Level
KW  - Computer Assisted Testing
KW  - Test Items
KW  - Item Analysis
KW  - Computer System Design
KW  - Difficulty Level
KW  - Item Response Theory
UR  - https://www.proquest.com/scholarly-journals/adaptive-item-based-learning-environments-on/docview/1140133682/se-2?accountid=10957
L2  - http://www.redi-bw.de/links/unifm?url_ver=Z39.88-2004&rft_val_fmt=info:ofi/fmt:kev:mtx:journal&genre=article&sid=ProQ:ProQ%3Aeric&atitle=Adaptive+Item-Based+Learning+Environments+Based+on+the+Item+Response+Theory%3A+Possibilities+and+Challenges&title=Journal+of+Computer+Assisted+Learning&issn=02664909&date=2010-12-01&volume=26&issue=6&spage=549&au=Wauters%2C+K.%3BDesmet%2C+P.%3BVan+den+Noortgate%2C+W.&isbn=&jtitle=Journal+of+Computer+Assisted+Learning&btitle=&rft_id=info:eric/EJ978605&rft_id=info:doi/10.1111%2Fj.1365-2729.2010.00368.x
LA  - English
DB  - ERIC
N1  - Anzahl der Quellenangaben - 73
N1  - Zuletzt aktualisiert - 2021-02-11
N1  - SubjectsTermNotLitGenreText - 5752ER1 98ER1 5697ER5 96ER5; 140ER1 10919ER1 6526ER1 6663ER1 138ER5 10827ER5 6467ER5 6603ER5; 10900ER1 10808ER5; 5598ER1 10914ER1 10966ER1 5545ER5 10822ER5 10874ER5; 5592ER1 10210ER1 2604ER1 3679ER1 6663ER1 5539ER5 10125ER5 2586ER5 3643ER5 6603ER5; 2602ER1 2584ER5; 8339ER1 8264ER5; 9519ER1 5956ER1 9438ER5 5901ER5; 2883ER1 2862ER5; 5410ER1 2040ER1 2099ER1 2098ER1 10811ER1 5360ER5 2026ER5 2085ER5 2084ER5 10719ER5; 8435ER1 2078ER1 5221ER1 9466ER1 6042ER1 8359ER5 2064ER5 5171ER5 9385ER5 5983ER5; 2084ER1 2070ER5; 2043ER1 2099ER1 2098ER1 10811ER1 10919ER1 6526ER1 6663ER1 2029ER5 2085ER5 2084ER5 10719ER5 10827ER5 6467ER5 6603ER5; 2093ER1 2799ER1 2079ER5 2780ER5
DO  - https://doi.org/10.1111/j.1365-2729.2010.00368.x
ER  - 



TY  - JOUR
T1  - A Comparison of Anchor-Item Designs for the Concurrent Calibration of Large Banks of Likert-Type Items
AN  - 815957844; EJ900846
AB  - Current interest in measuring quality of life is generating interest in the construction of computerized adaptive tests (CATs) with Likert-type items. Calibration of an item bank for use in CAT requires collecting responses to a large number of candidate items. However, the number is usually too large to administer to each subject in the calibration sample. The concurrent anchor-item design solves this problem by splitting the items into separate subtests, with some common items across subtests; then administering each subtest to a different sample; and finally running estimation algorithms once on the aggregated data array, from which a substantial number of responses are then missing. Although the use of anchor-item designs is widespread, the consequences of several configuration decisions on the accuracy of parameter estimates have never been studied in the polytomous case. The present study addresses this question by simulation, comparing the outcomes of several alternatives on the configuration of the anchor-item design. The factors defining variants of the anchor-item design are (a) subtest size, (b) balance of common and unique items per subtest, (c) characteristics of the common items, and (d) criteria for the distribution of unique items across subtests. The results of this study indicate that maximizing accuracy in item parameter recovery requires subtests of the largest possible number of items and the smallest possible number of common items; the characteristics of the common items and the criterion for distribution of unique items do not affect accuracy. (Contains 1 table, 10 figures, and 1 note.)
JF  - Applied Psychological Measurement
AU  - Garcia-Perez, Miguel A.
AU  - Alcala-Quintana, Rocio
AU  - Garcia-Cueto, Eduardo
Y1  - 2010/11//
PY  - 2010
DA  - Nov 2010
SP  - 580
EP  - 599
PB  - SAGE Publications
VL  - 34
IS  - 8
SN  - 0146-6216, 0146-6216
KW  - Subtests
KW  - ERIC, Current Index to Journals in Education (CIJE)
KW  - Adaptive Testing
KW  - Comparative Analysis
KW  - Equated Scores
KW  - Item Banks
KW  - Computer Assisted Testing
KW  - Test Items
KW  - Likert Scales
UR  - https://www.proquest.com/scholarly-journals/comparison-anchor-item-designs-concurrent/docview/815957844/se-2?accountid=10957
L2  - http://www.redi-bw.de/links/unifm?url_ver=Z39.88-2004&rft_val_fmt=info:ofi/fmt:kev:mtx:journal&genre=article&sid=ProQ:ProQ%3Aeric&atitle=A+Comparison+of+Anchor-Item+Designs+for+the+Concurrent+Calibration+of+Large+Banks+of+Likert-Type+Items&title=Applied+Psychological+Measurement&issn=01466216&date=2010-11-01&volume=34&issue=8&spage=580&au=Garcia-Perez%2C+Miguel+A.%3BAlcala-Quintana%2C+Rocio%3BGarcia-Cueto%2C+Eduardo&isbn=&jtitle=Applied+Psychological+Measurement&btitle=&rft_id=info:eric/EJ900846&rft_id=info:doi/10.1177%2F0146621609351259
LA  - English
DB  - ERIC
N1  - Anzahl der Quellenangaben - 34
N1  - Zuletzt aktualisiert - 2021-02-11
N1  - SubjectsTermNotLitGenreText - 1980ER1 3679ER1 6663ER1 1966ER5 3643ER5 6603ER5; 10900ER1 10808ER5; 3599ER1 9485ER1 2602ER1 3564ER5 9404ER5 2584ER5; 5593ER1 5540ER5; 2043ER1 2099ER1 2098ER1 10811ER1 10919ER1 6526ER1 6663ER1 2029ER5 2085ER5 2084ER5 10719ER5 10827ER5 6467ER5 6603ER5; 140ER1 10919ER1 6526ER1 6663ER1 138ER5 10827ER5 6467ER5 6603ER5; 6144ER1 738ER1 6527ER1 8711ER1 6085ER5 730ER5 6468ER5 8635ER5
DO  - https://doi.org/10.1177/0146621609351259
ER  - 



TY  - JOUR
T1  - Development and Evaluation of a Confidence-Weighting Computerized Adaptive Testing
AN  - 762468077; EJ899874
AB  - The purpose of this study was to examine whether the efficiency, precision, and validity of computerized adaptive testing (CAT) could be improved by assessing confidence differences in knowledge that examinees possessed. We proposed a novel polytomous CAT model called the confidence-weighting computerized adaptive testing (CWCAT), which combined a confidence-weighting scoring scheme with the graded response model (GRM). The CWCAT provided a more interactive testing environment by focusing on the examinees' confidence in their responses. An experiment was conducted to evaluate the comparison between the CWCAT and conventional CAT in terms of efficiency, precision, and validity. As expected, the polytomous method provided better discrimination among individual differences in the confidence in knowledge and required fewer items per examinee. Results also showed that CWCAT yielded ability estimates that were higher and better correlated to examinees' performance in English learning. Furthermore, the ability measured by CWCAT was not as likely to be affected by guessing as on conventional CAT, and, therefore, was more consistent with examinees' true ability. (Contains 9 tables and 9 figures.)
JF  - Educational Technology & Society
AU  - Yen, Yung-Chin
AU  - Ho, Rong-Guey
AU  - Chen, Li-Ju
AU  - Chou, Kun-Yi
AU  - Chen, Yan-Lin
Y1  - 2010
PY  - 2010
DA  - 2010
SP  - 163
EP  - 176
PB  - International Forum of Educational Technology & Society
VL  - 13
IS  - 3
SN  - 1436-4522, 1436-4522
KW  - Taiwan
KW  - Ability Estimates
KW  - ERIC, Current Index to Journals in Education (CIJE)
KW  - Elementary Secondary Education
KW  - High Schools
KW  - Secondary Education
KW  - Correlation
KW  - English (Second Language)
KW  - Student Evaluation
KW  - Computer Software
KW  - Programming
KW  - Second Language Learning
KW  - Foreign Countries
KW  - Evaluation Research
KW  - Computer Software Evaluation
KW  - Adaptive Testing
KW  - Validity
KW  - Comparative Analysis
KW  - Language Tests
KW  - Educational Technology
KW  - Computer Assisted Testing
KW  - Confidence Testing
KW  - High School Students
KW  - Item Response Theory
KW  - Guessing (Tests)
UR  - https://www.proquest.com/scholarly-journals/development-evaluation-confidence-weighting/docview/762468077/se-2
L2  - http://www.redi-bw.de/links/unifm?url_ver=Z39.88-2004&rft_val_fmt=info:ofi/fmt:kev:mtx:journal&genre=article&sid=ProQ:ProQ%3Aeric&atitle=Development+and+Evaluation+of+a+Confidence-Weighting+Computerized+Adaptive+Testing&title=Educational+Technology+%26+Society&issn=14364522&date=2010-01-01&volume=13&issue=3&spage=163&au=Yen%2C+Yung-Chin%3BHo%2C+Rong-Guey%3BChen%2C+Li-Ju%3BChou%2C+Kun-Yi%3BChen%2C+Yan-Lin&isbn=&jtitle=Educational+Technology+%26+Society&btitle=&rft_id=info:eric/EJ899874&rft_id=info:doi/
LA  - English
DB  - ERIC
N1  - Anzahl der Quellenangaben - 57
N1  - Zuletzt aktualisiert - 2021-02-11
N1  - SubjectsTermNotLitGenreText - 4166ER1 4394ER1 4124ER5 4350ER5; 3528ER1 3527ER1 5145ER1 5876ER1 9523ER1 5820ER1 3495ER5 9442ER5 5765ER5 3494ER5 5095ER5 5821ER5; 9519ER1 5956ER1 9438ER5 5901ER5; 5598ER1 10914ER1 10966ER1 5545ER5 10822ER5 10874ER5; 10337ER1 3676ER1 10247ER5 3640ER5; 5871ER1 11371ER1 10925ER1 6527ER1 5816ER5 11279ER5 10833ER5 6468ER5; 4803ER1 9534ER1 10411ER1 8114ER1 4605ER1 4755ER5 9453ER5 10321ER5 8043ER5 4558ER5; 3683ER1 6665ER1 8947ER1 3647ER5 6605ER5 8868ER5; 3311ER1 10805ER1 3282ER5 10713ER5; 2043ER1 2099ER1 2098ER1 10811ER1 10919ER1 6526ER1 6663ER1 2029ER5 2085ER5 2084ER5 10719ER5 10827ER5 6467ER5 6603ER5; 2084ER1 2070ER5; 2088ER1 3676ER1 2074ER5 3640ER5; 8435ER1 2078ER1 5221ER1 9466ER1 6042ER1 8359ER5 2064ER5 5171ER5 9385ER5 5983ER5; 2134ER1 10919ER1 6526ER1 6663ER1 2119ER5 10827ER5 6467ER5 6603ER5; 4615ER1 9039ER1 921ER1 4568ER5 8959ER5 912ER5; 140ER1 10919ER1 6526ER1 6663ER1 138ER5 10827ER5 6467ER5 6603ER5; 11346ER1 3677ER1 2445ER1 10154ER1 11254ER5 3641ER5 2427ER5 10069ER5; 1980ER1 3679ER1 6663ER1 1966ER5 3643ER5 6603ER5; 2295ER1 10210ER1 2604ER1 3679ER1 6663ER1 2278ER5 10125ER5 2586ER5 3643ER5 6603ER5; Taiwan
ER  - 



TY  - THES
T1  - Content clustering of a computerized-adaptive test
AN  - 822238427
AB  - School districts across the country are meeting the accountability challenges set forth by No Child Left Behind (NCLB) federal legislation, as well as challenges put forth at their state and local levels. Districts' accountability efforts have often brought them to contract with outside organizations offering standardized assessments for progress monitoring of students in intervention programs, and for on-going local assessments for program evaluation and planning. One such organization, Northwest Evaluation Association (NWEA), has developed several computer-based testing products to assist districts in their accountability and teaching/learning efforts. Their flagship product, Measures of Academic Progress (MAP), is a computerized-adaptive test (CAT), and is typically delivered on-line to districts.    Using Item Response Theory (IRT) selection algorithms, test questions are selected based on students' answers throughout the test experience. Students are presented with their first question (item) based on a predetermined value of their ability from previous testing and what is known about the item (Baker, 2001). Once they respond to the item, the software calibrates a temporary achievement level for that student, and selects their next question based on their estimated competency. Use of such powerful test assembly methods can enable a test to derive precise performance information about each student. Tests are assembled on demand, and results are immediately available to instructors and district personnel.    While precise information at a broader range of abilities is possible using such test assembly methods, item selection processes typically present items in a randomized order to maximize item bank usage, test security, and measurement properties. Little research has been done on item order with young test takers in the elementary grades, and the literature on item order with regard to computerized adaptive testing is silent. Of particular interest was whether a content-clustered version of a computerized adaptive test would result in stronger performance with young test takers, and whether the efficient testing processes already available via on-line testing could be improved. Data was reviewed in light of demographic sub-groups, including gender, ethnicity, and grade level.    The current study delivered both the standard version of a computerized adaptive test, MAP, along with a modified version, to a test population of 6589 students in a large southern U.S. school district of second, third, fourth and fifth graders. Using a double-blind selection process, half of the students received the standard MAP, and half received the modified version, which clustered items by goal strand across the five sub-tests of the assessment.    The study indicated that overall scores on MAP are not increased by content clustering, nor are there significant time gains on the overall test, for all students as a whole. However, results do indicate areas of significance with regard to ethnic sub-group differences. Overall test performance was stronger for African Americans on the modified version, while overall test duration was reduced for the three major ethnic sub-groups, particularly Hispanics. Boys improved their scores overall on the modified version, and gender differences on goal strands were present, which may indicate that certain goal strands scores increase under clustered conditions.
JF  - ProQuest Dissertations and Theses
AU  - Weber, Peggy Ann (Raethke)
A3  - Lindstrom, Michael R.
Y1  - 2009
PY  - 2009
DA  - 2009
SP  - 424
CY  - United States -- Arkansas
PB  - Bethel University (Arkansas)
PP  - United States -- Arkansas
SN  - 978-1-124-37648-6
KW  - Education
KW  - Adaptive assessment
KW  - Cognitive loading
KW  - Computerized adaptive testing
KW  - Content clustering
KW  - Item order
KW  - Working memory
KW  - Mathematics education
KW  - Educational tests & measurements
KW  - Elementary education
KW  - School districts
KW  - Achievement tests
KW  - No Child Left Behind Act 2001-US
KW  - Accountability
KW  - 0280:Mathematics education
KW  - 0288:Educational tests & measurements
KW  - 0524:Elementary education
UR  - https://www.proquest.com/dissertations-theses/content-clustering-computerized-adaptive-test/docview/822238427/se-2?accountid=10957
L2  - http://www.redi-bw.de/links/unifm?url_ver=Z39.88-2004&rft_val_fmt=info:ofi/fmt:kev:mtx:dissertation&genre=dissertations&sid=ProQ:ProQuest+Dissertations+%26+Theses+Global&atitle=&title=Content+clustering+of+a+computerized-adaptive+test&issn=&date=2009-01-01&volume=&issue=&spage=&au=Weber%2C+Peggy+Ann+%28Raethke%29&isbn=978-1-124-37648-6&jtitle=&btitle=&rft_id=info:eric/&rft_id=info:doi/
LA  - English
DB  - ProQuest Dissertations & Theses Global
N1  - Copyright - Database copyright ProQuest LLC; ProQuest does not claim copyright in the individual underlying works.
N1  - Zuletzt aktualisiert - 2022-11-17
M3  - Ed.D.
M1  - 3437448
ER  - 



TY  - JOUR
T1  - An Adaptive Testing System for Supporting Versatile Educational Assessment
AN  - 62002377; EJ819448
AB  - With the rapid growth of computer and mobile technology, it is a challenge to integrate computer based test (CBT) with mobile learning (m-learning) especially for formative assessment and self-assessment. In terms of self-assessment, computer adaptive test (CAT) is a proper way to enable students to evaluate themselves. In CAT, students are assessed through a process that uses item response theory (IRT), a well-founded psychometric theory. Furthermore, a large item bank is indispensable to a test, but when a CAT system has a large item bank, the test item selection of IRT becomes more tedious. Besides the large item bank, item exposure mechanism is also essential to a testing system. However, IRT all lack the above-mentioned points. These reasons have motivated the authors to carry out this study. This paper describes a design issue aimed at the development and implementation of an adaptive testing system. The system can support several assessment functions and different devices. Moreover, the researchers apply a novel approach, particle swarm optimization (PSO) to alleviate the computational complexity and resolve the problem of item exposure. Throughout the development of the system, a formative evaluation was embedded into an integral part of the design methodology that was used for improving the system. After the system was formally released onto the web, some questionnaires and experiments were conducted to evaluate the usability, precision, and efficiency of the system. The results of these evaluations indicated that the system provides an adaptive testing for different devices and supports versatile assessment functions. Moreover, the system can estimate students' ability reliably and validly and conduct an adaptive test efficiently. Furthermore, the computational complexity of the system was alleviated by the PSO approach. By the approach, the test item selection procedure becomes efficient and the average best fitness values are very close to the optimal solutions. (Contains 9 figures and 6 tables.)
JF  - Computers & Education
AU  - Huang, Yueh-Min
AU  - Lin, Yen-Ting
AU  - Cheng, Shu-Chen
Y1  - 2009/01//
PY  - 2009
DA  - Jan 2009
SP  - 53
EP  - 67
PB  - Elsevier
VL  - 52
IS  - 1
SN  - 0360-1315, 0360-1315
KW  - Item Selection
KW  - ERIC, Current Index to Journals in Education (CIJE)
KW  - Computer Software
KW  - Adaptive Testing
KW  - Computer Software Evaluation
KW  - Test Construction
KW  - Distance Education
KW  - Educational Assessment
KW  - Computer Assisted Testing
KW  - Educational Technology
KW  - Formative Evaluation
KW  - Self Evaluation (Individuals)
KW  - Handheld Devices
KW  - Test Items
KW  - Internet
KW  - Computer System Design
KW  - Item Response Theory
UR  - https://www.proquest.com/scholarly-journals/adaptive-testing-system-supporting-versatile/docview/62002377/se-2?accountid=10957
L2  - http://www.redi-bw.de/links/unifm?url_ver=Z39.88-2004&rft_val_fmt=info:ofi/fmt:kev:mtx:journal&genre=article&sid=ProQ:ProQ%3Aeric&atitle=An+Adaptive+Testing+System+for+Supporting+Versatile+Educational+Assessment&title=Computers+%26+Education&issn=03601315&date=2009-01-01&volume=52&issue=1&spage=53&au=Huang%2C+Yueh-Min%3BLin%2C+Yen-Ting%3BCheng%2C+Shu-Chen&isbn=&jtitle=Computers+%26+Education&btitle=&rft_id=info:eric/EJ819448&rft_id=info:doi/10.1016%2Fj.compedu.2008.06.007
LA  - English
DB  - ERIC
N1  - Zuletzt aktualisiert - 2021-02-11
N1  - SubjectsTermNotLitGenreText - 9587ER1 3676ER1 9506ER5 3640ER5; 10900ER1 10808ER5; 4201ER1 3676ER1 4159ER5 3640ER5; 3212ER1 3676ER1 3183ER5 3640ER5; 140ER1 10919ER1 6526ER1 6663ER1 138ER5 10827ER5 6467ER5 6603ER5; 5504ER1 2070ER1 7140ER1 5214ER1 5453ER5 2056ER5 7077ER5 5164ER5; 5598ER1 10914ER1 10966ER1 5545ER5 10822ER5 10874ER5; 2043ER1 2099ER1 2098ER1 10811ER1 10919ER1 6526ER1 6663ER1 2029ER5 2085ER5 2084ER5 10719ER5 10827ER5 6467ER5 6603ER5; 4659ER1 3381ER1 3602ER1 4612ER5 3351ER5 3567ER5; 10895ER1 6468ER1 2820ER1 10803ER5 6409ER5 2799ER5; 3311ER1 10805ER1 3282ER5 10713ER5; 2088ER1 3676ER1 2074ER5 3640ER5; 2093ER1 2799ER1 2079ER5 2780ER5; 2981ER1 3192ER1 2959ER5 3164ER5; 2084ER1 2070ER5
DO  - https://doi.org/10.1016/j.compedu.2008.06.007
ER  - 



TY  - JOUR
T1  - Adjusting to Test Takers
AN  - 61989401; EJ819164
AB  - This article discusses the growing interest in computer-adaptive testing, which supporters say can help guide instruction, increase student motivation, and determine the best use of resources for districts. This method of testing shortens the test by not asking high-achieving students questions that are too easy for them, and likewise not giving struggling students questions that are too hard. However, computer-adaptive assessments are not the best way to evaluate students in every situation, experts point out. Though, they recognize the potential of computer-adaptive testing, experts also voice caution in using this kind of method.
JF  - Education Week
AU  - Ash, Katie
Y1  - 2008/11//
PY  - 2008
DA  - Nov 2008
SP  - 19
EP  - 21
PB  - Editorial Projects in Education
VL  - 28
IS  - 13
SN  - 0277-4232, 0277-4232
KW  - California
KW  - Oregon
KW  - Utah
KW  - No Child Left Behind Act 2001
KW  - ERIC, Current Index to Journals in Education (CIJE)
KW  - Elementary Secondary Education
KW  - Adaptive Testing
KW  - Federal Legislation
KW  - Student Motivation
KW  - Student Evaluation
KW  - Testing Problems
KW  - Computer Assisted Testing
KW  - Educational Legislation
UR  - https://www.proquest.com/scholarly-journals/adjusting-test-takers/docview/61989401/se-2?accountid=10957
L2  - http://www.redi-bw.de/links/unifm?url_ver=Z39.88-2004&rft_val_fmt=info:ofi/fmt:kev:mtx:journal&genre=article&sid=ProQ:ProQ%3Aeric&atitle=Adjusting+to+Test+Takers&title=Education+Week&issn=02774232&date=2008-11-01&volume=28&issue=13&spage=19&au=Ash%2C+Katie&isbn=&jtitle=Education+Week&btitle=&rft_id=info:eric/EJ819164&rft_id=info:doi/
LA  - English
DB  - ERIC
N1  - Zuletzt aktualisiert - 2021-02-11
N1  - SubjectsTermNotLitGenreText - 140ER1 10919ER1 6526ER1 6663ER1 138ER5 10827ER5 6467ER5 6603ER5; 2043ER1 2099ER1 2098ER1 10811ER1 10919ER1 6526ER1 6663ER1 2029ER5 2085ER5 2084ER5 10719ER5 10827ER5 6467ER5 6603ER5; 10337ER1 3676ER1 10247ER5 3640ER5; 10358ER1 6915ER1 10268ER5 6852ER5; 3967ER1 6016ER1 3927ER5 5958ER5; 3261ER1 6016ER1 3232ER5 5958ER5; 10922ER1 8339ER1 10830ER5 8264ER5; California; Oregon; Utah
ER  - 



TY  - THES
T1  - Stability and sensitivity of a model-based person -fit index in detecting item pre -knowledge in computerized adaptive test
AN  - 304812844
AB  - Item response theory is a modern test theory. It focuses on the performance of each item. Under this framework, the performance of test takers on a test item can be predicted by a set of abilities. The relationship between the test takers' item performances and the set of abilities underlying item performances can be described by a monotonically increasing function called an item characteristic curve. Due to various personal reasons, the performances of the test takers may depart from the response patterns predicted by the underlying test model. In order to calculate the extent of departure of these aberrant response patterns, a number of methods have been developed under the theme "person-fit statistics". The degree of aberration is calculated as an index called person-fit index. Inside the computerized adaptive testing (CAT), test takers with different abilities will answer different numbers of questions and the difficulties of the items administered to them are usually clustered at the abilities of the test takers. Due to this reason, the application of person-fit indices in the computerized adaptive testing environment to measure misfit is difficult.    When the frequent accesses to the item bank has become feasible, test takers may memorize blocks of test items and share these items with future test takers. Individuals with prior knowledge of some items may use that information to get high scores, in the sense that their test scores have been artificially inflated. FLOR is an index of posterior log-odds ratio used for detecting the use of item pre-knowledge. It can be applied both in the fixed item, fixed length test and the CAT environment. It is a model-based index in which aberrant models are defined in the situation of item pre-knowledge. FLOR describes the likelihood that a response pattern arises from the aberrant models.    The present study used the hf plot to access the sensitivity of the person-fit indices. hf plot is a plot of hit rate against false alarm rate. For a higher hit rate, usually a higher false alarm rate is followed. hf plot provides a good tools for comparison between indices by inspection of the speed of rise of the curves. A sensitive index should give a faster rise of the curve. In this study, sensitivity of an index was defined as the speed of rise of the hf plot, which is represented by a parameter hfτ estimated from the data obtained from hf plot.    The present study assessed the stability of FLOR over other variables, which were unrelated to item pre-knowledge. It found that FLOR was stable over the discrimination and difficulty parameters of test items. It was also stable over positions of the exposed items in the test and the initial assignment of prior probability of item pre-knowledge. However, the asymptotes (guessing factor) and the probabilities of item exposure did affect the final values of FLOR seriously.    The present study also found that FLOR has a much superior sensitivity over other indices in detecting item pre-knowledge. Concerning about the sensitivity over different abilities of test takers, it was found that the sensitivity of FLOR was the highest among low ability test takers and the weakest among strong ability test takers in the fixed length and fixed items tests. However, the sensitivities of FLOR became the same among different abilities of test takers if items with difficulties matching their abilities were used in the tests. The number of beneficiaries among the test takers did not affect the sensitivity of FLOR. Moreover, in a simulation to test the differentiating power of FLOR, it was found that FLOR could differentiate item pre-knowledge from other reasons of personal misfits (test anxiety, player, random response and challenger) effectively.    After the stability and sensitivity of FLOR were investigated, the application of it in the CAT environment had become the main concern. The present studies found that both the test length and the number of exposed items affect the final value of FLOR. In the fixed length CAT, the FLOR has a much stronger sensitivity than lz and CUSUM in detecting item pre-knowledge. The sensitivity of FLOR in the fixed length CAT was the same as that in the fixed length fixed items test. If the test length could vary, the sensitivity of FLOR in CAT would be slightly weakened. The Adjusted FLOR index could increase the sensitivity. Concerning about the effect of ability on the sensitivity of FLOR in CAT, it was found that the abilities of the test takers in CAT did not affect the sensitivity of FLOR and Adjusted FLOR.
JF  - ProQuest Dissertations and Theses
AU  - Hui, Hing-fai
A3  - Hau, Kit-tai
Y1  - 2008
PY  - 2008
DA  - 2008
SP  - 118
CY  - Hong Kong
PB  - The Chinese University of Hong Kong (Hong Kong)
PP  - Hong Kong
SN  - 978-1-109-40180-6
KW  - Education
KW  - Computerized adaptive testing
KW  - FLOR
KW  - Item pre-knowledge
KW  - Model-based index
KW  - Person-fit index
KW  - Educational tests & measurements
KW  - 0288:Educational tests & measurements
UR  - https://www.proquest.com/dissertations-theses/stability-sensitivity-model-based-person-fit/docview/304812844/se-2?accountid=10957
L2  - http://www.redi-bw.de/links/unifm?url_ver=Z39.88-2004&rft_val_fmt=info:ofi/fmt:kev:mtx:dissertation&genre=dissertations&sid=ProQ:ProQuest+Dissertations+%26+Theses+Global&atitle=&title=Stability+and+sensitivity+of+a+model-based+person+-fit+index+in+detecting+item+pre+-knowledge+in+computerized+adaptive+test&issn=&date=2008-01-01&volume=&issue=&spage=&au=Hui%2C+Hing-fai&isbn=978-1-109-40180-6&jtitle=&btitle=&rft_id=info:eric/&rft_id=info:doi/
LA  - English
DB  - ProQuest Dissertations & Theses Global
N1  - Copyright - Database copyright ProQuest LLC; ProQuest does not claim copyright in the individual underlying works.
N1  - Zuletzt aktualisiert - 2022-10-29
M3  - Ed.D.
M1  - 3377981
ER  - 



TY  - THES
T1  - Effect of early misfit in computerized adaptive testing on the recovery of theta
AN  - 275653844
AB  - This study focused on how early misfit affected the recovery of &thetas; for a computerized adaptive test (CAT). Number of misfitting items, generating &thetas;, item selection method, and &thetas; estimation method were independent variables in this study. It was found that CAT could recover from misfit-as-correct-responses for low ability simulees given a sufficient number of items. CAT could not recover from misfit-as-incorrect-responses for high ability simulees. Implications of the study and suggestions for future research were provided.
JF  - ProQuest Dissertations and Theses
AU  - Guyer, Rick D.
A3  - Weiss, David J.
Y1  - 2008
PY  - 2008
DA  - 2008
SP  - 246
CY  - United States -- Minnesota
PB  - University of Minnesota
PP  - United States -- Minnesota
SN  - 978-0-549-94058-6
KW  - Psychology
KW  - Computerized adaptive testing
KW  - Misfit
KW  - Person fit
KW  - Trait estimation
KW  - Weighted likelihood estimation
KW  - Quantitative psychology
KW  - 0632:Quantitative psychology
UR  - https://www.proquest.com/dissertations-theses/effect-early-misfit-computerized-adaptive-testing/docview/275653844/se-2?accountid=10957
L2  - http://www.redi-bw.de/links/unifm?url_ver=Z39.88-2004&rft_val_fmt=info:ofi/fmt:kev:mtx:dissertation&genre=dissertations&sid=ProQ:ProQuest+Dissertations+%26+Theses+Global&atitle=&title=Effect+of+early+misfit+in+computerized+adaptive+testing+on+the+recovery+of+theta&issn=&date=2008-01-01&volume=&issue=&spage=&au=Guyer%2C+Rick+D.&isbn=978-0-549-94058-6&jtitle=&btitle=&rft_id=info:eric/&rft_id=info:doi/
LA  - English
DB  - ProQuest Dissertations & Theses Global
N1  - Copyright - Database copyright ProQuest LLC; ProQuest does not claim copyright in the individual underlying works.
N1  - Zuletzt aktualisiert - 2022-10-28
M3  - Ph.D.
M1  - 3338946
ER  - 



TY  - JOUR
T1  - Computerized Adaptive Testing for Polytomous Motivation Items: Administration Mode Effects and a Comparison with Short Forms
AN  - 62043948; EJ773650
AB  - In a randomized experiment (n = 515), a computerized and a computerized adaptive test (CAT) are compared. The item pool consists of 24 polytomous motivation items. Although items are carefully selected, calibration data show that Samejima's graded response model did not fit the data optimally. A simulation study is done to assess possible consequences of model misfit. CAT efficiency was studied by a systematic comparison of the CAT with two types of conventional fixed length short forms, which are created to be good CAT competitors. Results showed no essential administration mode effects. Efficiency analyses show that CAT outperformed the short forms in almost all aspects when results are aggregated along the latent trait scale. The real and the simulated data results are very similar, which indicate that the real data results are not affected by model misfit. (Contains 2 tables, 6 figures and 5 notes.)
JF  - Applied Psychological Measurement
AU  - Hol, A. Michiel
AU  - Vorst, Harrie C. M.
AU  - Mellenbergh, Gideon J.
Y1  - 2007
PY  - 2007
DA  - 2007
SP  - 412
EP  - 429
PB  - SAGE Publications
VL  - 31
IS  - 5
SN  - 0146-6216, 0146-6216
KW  - Netherlands
KW  - ERIC, Current Index to Journals in Education (CIJE)
KW  - Higher Education
KW  - Foreign Countries
KW  - Adaptive Testing
KW  - Student Motivation
KW  - Comparative Analysis
KW  - Test Format
KW  - Simulation
KW  - College Students
KW  - Computer Assisted Testing
KW  - Questionnaires
KW  - Test Items
KW  - Models
KW  - Item Response Theory
UR  - https://www.proquest.com/scholarly-journals/computerized-adaptive-testing-polytomous/docview/62043948/se-2?accountid=10957
L2  - http://www.redi-bw.de/links/unifm?url_ver=Z39.88-2004&rft_val_fmt=info:ofi/fmt:kev:mtx:journal&genre=article&sid=ProQ:ProQ%3Aeric&atitle=Computerized+Adaptive+Testing+for+Polytomous+Motivation+Items%3A+Administration+Mode+Effects+and+a+Comparison+with+Short+Forms&title=Applied+Psychological+Measurement&issn=01466216&date=2007-01-01&volume=31&issue=5&spage=412&au=Hol%2C+A.+Michiel%3BVorst%2C+Harrie+C.+M.%3BMellenbergh%2C+Gideon+J.&isbn=&jtitle=Applied+Psychological+Measurement&btitle=&rft_id=info:eric/EJ773650&rft_id=info:doi/10.1177%2F0146621606297314
LA  - English
DB  - ERIC
N1  - Anzahl der Quellenangaben - 39
N1  - Zuletzt aktualisiert - 2021-02-11
N1  - SubjectsTermNotLitGenreText - 10358ER1 6915ER1 10268ER5 6852ER5; 9769ER1 6663ER1 9687ER5 6603ER5; 140ER1 10919ER1 6526ER1 6663ER1 138ER5 10827ER5 6467ER5 6603ER5; 2043ER1 2099ER1 2098ER1 10811ER1 10919ER1 6526ER1 6663ER1 2029ER5 2085ER5 2084ER5 10719ER5 10827ER5 6467ER5 6603ER5; 1980ER1 3679ER1 6663ER1 1966ER5 3643ER5 6603ER5; 10900ER1 10808ER5; 6838ER1 9769ER1 6663ER1 6775ER5 9687ER5 6603ER5; 10898ER1 10806ER5; 1829ER1 10411ER1 8114ER1 4605ER1 1815ER5 10321ER5 8043ER5 4558ER5; 4166ER1 4394ER1 4124ER5 4350ER5; 5598ER1 10914ER1 10966ER1 5545ER5 10822ER5 10874ER5; 8642ER1 6527ER1 8566ER5 6468ER5; Netherlands
DO  - https://doi.org/10.1177/0146621606297314
ER  - 



TY  - THES
T1  - Development of Web-based intelligent tutoring system for SDView
AN  - 304819955
AB  - SouthDakotaView (SDView) is a program funded by the United States Geological Survey that provides satellite remote sensing imagery for a variety of applications, including education; however, an educational tool that utilizes the SDView imagery has yet not been developed. In particular, there is presently no easy way for students to make use of the SDView satellite imagery in learning elementary geography. An image map viewer could be useful for K-12 students to understand the relationship between image data and geographic information. A web-based intelligent tutoring system has been used in many educational systems as an efficient way to deliver educational material. In an intelligent tutoring system, an adaptive testing scheme can be employed to accurately evaluate a student's knowledge level by utilizing a test.    Therefore, we focus on developing a web-based intelligent tutoring system for elementary geography students using an adaptive testing scheme. In this thesis, a prototype system, called SD-ITS, has been developed for a web-based geographic learning system. The prototype system is focused on applying an adaptive testing scheme and making an interactive image map for the intelligent tutoring system. This thesis demonstrates that the prototype system, SD-ITS, controls the student's test progress and has the ability to provide appropriate level of tutoring information from SDView or other archives for the students. This system adaptively adjusts the geography testing for each student on the basis of his/her knowledge level. In addition, an interactive image map application provides tutoring information that helps the students comprehend the real geographic information. In this thesis, the prototype system aims to motivate elementary students to learn more geography information through adaptive testing and an interactive geography maps for tutoring purpose.
JF  - ProQuest Dissertations and Theses
AU  - Jeon, Hee Jung
A3  - Shin, Sung
Y1  - 2007
PY  - 2007
DA  - 2007
SP  - 101
CY  - United States -- South Dakota
PB  - South Dakota State University
PP  - United States -- South Dakota
SN  - 978-0-549-18021-0
KW  - Applied sciences
KW  - Computer science
KW  - 0984:Computer science
UR  - https://www.proquest.com/dissertations-theses/development-web-based-intelligent-tutoring-system/docview/304819955/se-2?accountid=10957
L2  - http://www.redi-bw.de/links/unifm?url_ver=Z39.88-2004&rft_val_fmt=info:ofi/fmt:kev:mtx:dissertation&genre=dissertations&sid=ProQ:ProQuest+Dissertations+%26+Theses+Global&atitle=&title=Development+of+Web-based+intelligent+tutoring+system+for+SDView&issn=&date=2007-01-01&volume=&issue=&spage=&au=Jeon%2C+Hee+Jung&isbn=978-0-549-18021-0&jtitle=&btitle=&rft_id=info:eric/&rft_id=info:doi/
LA  - English
DB  - ProQuest Dissertations & Theses Global
N1  - Copyright - Database copyright ProQuest LLC; ProQuest does not claim copyright in the individual underlying works.
N1  - Zuletzt aktualisiert - 2022-10-29
M3  - M.S.
M1  - 1446740
ER  - 



TY  - THES
T1  - An investigation of stratification exposure control procedures in CATs using the generalized partial credit model
AN  - 304782592
AB  - The a-stratification procedure of item exposure control was designed to stratify items by item discrimination to ensure that an adaptive test would administer items from the entire range of items, not just the most-informative ones. An improvement to the a-stratification method, the  a-stratification with b-blocking procedure added stratification according to item difficulty in order to take into account any correlation that might exist within the item pool between item discrimination and item difficulty. These procedures have been shown to work well using dichotomous items. This dissertation explored both stratification procedures using polytomous item pools to investigate whether or not an optimum number of strata could be implemented when administering polytomous computerized adaptive tests.    In addition to the stratification procedures, two other exposure control conditions were studied. The randomesque procedure was used in one condition while a no exposure control condition served as a baseline condition. Items calibrated according to the generalized partial credit model were used to construct two item pools. Since the items covered three areas of science, content balancing procedures were incorporated to ensure that each adaptive test provided the appropriate balance of content. Maximum likelihood estimation was used to estimate ability levels from simulated CATs. The number of strata used with both stratification procedures ranged from two to five, to ensure enough items per stratum.    Along with descriptive statistics and correlations, bias and root mean squared error helped portray the accuracy of the simulated tests. Item exposure and item pool usage rates were used to show how much of the item pools were being used across administrations of the tests. Finally, item overlap rates were calculated to show how many of the same items were being used among simulated examinees of similar and different abilities.    The results of this study did not reveal an optimum number of strata for the stratification procedures with either item pool. Furthermore, the randomesque procedure outperformed the stratification procedures in terms of item exposure and item overlap rates for both item pools. This surprising result was not affected by the number of strata used within the stratification procedures.
JF  - ProQuest Dissertations and Theses
AU  - Johnson, Marc Anthony
A3  - Dodd, Barbara G.
Y1  - 2007
PY  - 2007
DA  - 2007
SP  - 157
CY  - United States -- Texas
PB  - The University of Texas at Austin
PP  - United States -- Texas
SN  - 978-0-549-05809-0
KW  - Psychology
KW  - Computerized adaptive testing
KW  - Generalized partial credit
KW  - Polytomous item pools
KW  - Stratification exposure control
KW  - Psychological tests
KW  - Models
KW  - Quantitative psychology
KW  - 0632:Quantitative psychology
UR  - https://www.proquest.com/dissertations-theses/investigation-stratification-exposure-control/docview/304782592/se-2?accountid=10957
L2  - http://www.redi-bw.de/links/unifm?url_ver=Z39.88-2004&rft_val_fmt=info:ofi/fmt:kev:mtx:dissertation&genre=dissertations&sid=ProQ:ProQuest+Dissertations+%26+Theses+Global&atitle=&title=An+investigation+of+stratification+exposure+control+procedures+in+CATs+using+the+generalized+partial+credit+model&issn=&date=2007-01-01&volume=&issue=&spage=&au=Johnson%2C+Marc+Anthony&isbn=978-0-549-05809-0&jtitle=&btitle=&rft_id=info:eric/&rft_id=info:doi/
LA  - English
DB  - ProQuest Dissertations & Theses Global
N1  - Copyright - Database copyright ProQuest LLC; ProQuest does not claim copyright in the individual underlying works.
N1  - Zuletzt aktualisiert - 2022-08-30
M3  - Ph.D.
M1  - 3266891
ER  - 



TY  - THES
T1  - Quantifying the effects of partial saturation on sand production prediction and geomechanical responses of reservoirs
AN  - 301690149
AB  - The initiation of wellbore failure and subsequent production of sand to the surface in petroleum reservoirs has often been associated with an increase in water cut.  Published field-level studies and experimental results do not provide a definite correlation between these two events - sand production and water breakthrough.  The experimental work described in this thesis aims to quantify the changes in the strength of weakly cemented and unconsolidated sand samples due to changes in fluid saturations.  This thesis addresses the reasons for the observed strength differences using unconsolidated and weakly cemented samples made of pure quartz, and different pore fluids (air, water and oil).
  Uniaxial Compressive Strength (UCS) tests, Thick Wall Cylinder (TWC) strength tests and multistage tests were carried out at various saturation conditions.  A 3-D correlation between UCS, porosity and water saturation was recorded and the results were compared with theoretical predictions.  The initial failure and collapse strengths for partially saturated, unconsolidated TWC samples were approximately 7 and 37 times their UCS values of approximately 6 kPa.  The failure envelope for oil-saturated samples was weaker than water-saturated samples in multistage tests.  Suction controlled tests at avoids conditions did not show a systematic increase in shear strengths with increasing suction.
  The strength of weak and unconsolidated pure sand is shown to be not appreciably affected by changes in water saturation.  The loss of cementation and lubricating effect of pore fluids leads to reduced strength.  A common failure model based on suction alone is not recommended in predicting wellbore failure for all types of materials.
JF  - PQDT - UK & Ireland
AU  - Narayanasamy, Rajarajan
Y1  - 2007
PY  - 2007
DA  - 2007
SP  - 1
CY  - Scotland
PB  - Heriot-Watt University (United Kingdom)
PP  - Scotland
KW  - http://hdl.handle.net/10399/82
KW  - DXN115767
KW  - Applied sciences
KW  - Petroleum engineering
KW  - 0765:Petroleum engineering
UR  - https://www.proquest.com/dissertations-theses/quantifying-effects-partial-saturation-on-sand/docview/301690149/se-2?accountid=10957
L2  - http://www.redi-bw.de/links/unifm?url_ver=Z39.88-2004&rft_val_fmt=info:ofi/fmt:kev:mtx:dissertation&genre=dissertations&sid=ProQ:ProQuest+Dissertations+%26+Theses+Global&atitle=&title=Quantifying+the+effects+of+partial+saturation+on+sand+production+prediction+and+geomechanical+responses+of+reservoirs&issn=&date=2007-01-01&volume=&issue=&spage=&au=Narayanasamy%2C+Rajarajan&isbn=&jtitle=&btitle=&rft_id=info:eric/&rft_id=info:doi/
LA  - English
DB  - ProQuest Dissertations & Theses Global
N1  - Copyright - Database copyright ProQuest LLC; ProQuest does not claim copyright in the individual underlying works.
N1  - Analytdeskriptor - Bibliographic data provided by EThOS, the British Library’s UK thesis service: https://ethos.bl.uk/OrderDetails.do?uin=uk.bl.ethos.444910
N1  - Zuletzt aktualisiert - 2022-10-20
M3  - Ph.D.
M1  - U232292
ER  - 



TY  - JOUR
T1  - Reforming Federal Testing Policy to Support Teaching and Learning
AN  - 237003631
AB  - The No Child Left Behind Act (NCLB) assumes that state-mandated tests provide useful information to school administrators and teachers. However, interviews with administrators and teachers suggest that Minnesota's tests, which are representative of the current generation of state-mandated tests, fail to provide useful information to administrators and teachers about areas that need attention. The use of a single level of difficulty causes harmful stress and provides measurements of poor quality for many students. Computer-adaptive tests are a practical alternative, provide more useful information, reduce stress among students, and address four major concerns that teachers have about testing. This alternative could strengthen the link between testing and improved educational outcomes and improve the chances that NCLB will have a positive impact. However, current federal policy prohibits the use of computer-adaptive tests for accountability purposes. This article provides specific recommendations that potentially could address four critical concerns about current state-mandated tests. [PUBLICATION ABSTRACT]
JF  - Educational Policy
AU  - Yeh, Stuart S
Y1  - 2006/07//
PY  - 2006
DA  - Jul 2006
SP  - 495
EP  - 524
CY  - Los Altos
PB  - SAGE PUBLICATIONS, INC.
PP  - Los Altos
VL  - 20
IS  - 3
SN  - 08959048
KW  - Education--Special Education And Rehabilitation
KW  - No Child Left Behind Act 2001-US
KW  - Academic standards
KW  - Achievement tests
KW  - States
KW  - Education policy
KW  - Education reform
UR  - https://www.proquest.com/scholarly-journals/reforming-federal-testing-policy-support-teaching/docview/237003631/se-2?accountid=10957
L2  - http://www.redi-bw.de/links/unifm?url_ver=Z39.88-2004&rft_val_fmt=info:ofi/fmt:kev:mtx:journal&genre=article&sid=ProQ:ProQ%3Apais&atitle=Reforming+Federal+Testing+Policy+to+Support+Teaching+and+Learning&title=Educational+Policy&issn=08959048&date=2006-07-01&volume=20&issue=3&spage=495&au=Yeh%2C+Stuart+S&isbn=&jtitle=Educational+Policy&btitle=&rft_id=info:eric/&rft_id=info:doi/10.1177%2F0895904805284119
LA  - English
DB  - PAIS Index
N1  - Copyright - Copyright SAGE PUBLICATIONS, INC. Jul 2006
N1  - Dokumentbestandteil - References; Tables
N1  - Zuletzt aktualisiert - 2018-09-24
DO  - https://doi.org/10.1177/0895904805284119
ER  - 



TY  - JOUR
T1  - Reforming Federal Testing Policy to Support Teaching and Learning
AN  - 62100626; EJ737335
AB  - The No Child Left Behind Act (NCLB) assumes that state-mandated tests provide useful information to school administrators and teachers. However, interviews with administrators and teachers suggest that Minnesota's tests, which are representative of the current generation of state-mandated tests, fail to provide useful information to administrators and teachers about areas that need attention. The use of a single level of difficulty causes harmful stress and provides measurements of poor quality for many students. Computer-adaptive tests are a practical alternative, provide more useful information, reduce stress among students, and address four major concerns that teachers have about testing. This alternative could strengthen the link between testing and improved educational outcomes and improve the chances that NCLB will have a positive impact. However, current federal policy prohibits the use of computer-adaptive tests for accountability purposes. This article provides specific recommendations that potentially could address four critical concerns about current state-mandated tests. (Contains 1 figure, 1 table, and 6 notes.)
JF  - Educational Policy
AU  - Yeh, Stuart S.
Y1  - 2006
PY  - 2006
DA  - 2006
SP  - 495
EP  - 524
PB  - SAGE Publications
VL  - 20
IS  - 3
SN  - 0895-9048, 0895-9048
KW  - Minnesota
KW  - No Child Left Behind Act 2001
KW  - ERIC, Current Index to Journals in Education (CIJE)
KW  - Elementary Secondary Education
KW  - Teacher Attitudes
KW  - Administrator Attitudes
KW  - Educational Change
KW  - Educational Policy
KW  - Outcomes of Education
KW  - State Standards
KW  - Federal Legislation
KW  - Adaptive Testing
KW  - Test Construction
KW  - Test Format
KW  - Computer Assisted Testing
KW  - Accountability
KW  - Academic Achievement
UR  - https://www.proquest.com/scholarly-journals/reforming-federal-testing-policy-support-teaching/docview/62100626/se-2?accountid=10957
L2  - http://www.redi-bw.de/links/unifm?url_ver=Z39.88-2004&rft_val_fmt=info:ofi/fmt:kev:mtx:journal&genre=article&sid=ProQ:ProQ%3Aeric&atitle=Reforming+Federal+Testing+Policy+to+Support+Teaching+and+Learning&title=Educational+Policy&issn=08959048&date=2006-01-01&volume=20&issue=3&spage=495&au=Yeh%2C+Stuart+S.&isbn=&jtitle=Educational+Policy&btitle=&rft_id=info:eric/EJ737335&rft_id=info:doi/10.1177%2F0895904805284119
LA  - English
DB  - ERIC
N1  - Anzahl der Quellenangaben - 37
N1  - Zuletzt aktualisiert - 2021-02-11
N1  - SubjectsTermNotLitGenreText - 3967ER1 6016ER1 3927ER5 5958ER5; 3282ER1 8068ER1 3253ER5 7997ER5; 7549ER1 7480ER5; 89ER1 9044ER1 87ER5 8964ER5; 29ER1 98ER1 28ER5 96ER5; 140ER1 10919ER1 6526ER1 6663ER1 138ER5 10827ER5 6467ER5 6603ER5; 2043ER1 2099ER1 2098ER1 10811ER1 10919ER1 6526ER1 6663ER1 2029ER5 2085ER5 2084ER5 10719ER5 10827ER5 6467ER5 6603ER5; 10898ER1 10806ER5; 10617ER1 740ER1 10525ER5 732ER5; 169ER1 740ER1 167ER5 732ER5; 10895ER1 6468ER1 2820ER1 10803ER5 6409ER5 2799ER5; 3219ER1 1404ER1 3190ER5 1393ER5; 10196ER1 10154ER1 10111ER5 10069ER5; Minnesota
DO  - https://doi.org/10.1177/0895904805284119
ER  - 



TY  - THES
T1  - Measurement of Korean EFL college students' foreign language classroom speaking anxiety: Evidence of psychometric properties and accuracy of a computerized adaptive test (CAT) with dichotomously scored items using a CAT simulation
AN  - 305382849
AB  - Assessment of foreign language speaking anxiety is considered pertinent to assisting practitioners to reduce learners' speaking anxiety. Computerized adaptive testing (CAT) of severity of speaking anxiety has potential advantages over conventional paper-and-pencil (P&P) tests in terms of efficiency, precision, adaptiveness and real-time severity monitoring.    Item response theory (IRT) was applied to obtain item characteristics (i.e. anxiety severity and discrimination parameter) and develop an item pool that can be used for a computerized adaptive test to measure severity of speaking anxiety.    Based on two-parameter logistic IRT model, the study analyzed responses to a newly constructed English speaking anxiety inventory from a sample of 949 Korean EFL undergraduate students. The study used Principal Component Analysis and DIMTEST in a confirmatory mode to account for construct validity and conducted a computer simulation of CAT to investigate whether a CAT or a P&P test can more accurately estimate test-takers' severity levels of English speaking anxiety, conditional on their true severity of speaking anxiety.    Examining construct validity, the results indicated that a principal component analysis (PCA) of the data revealed that 23 percent of the total variation in the data was accounted for by the first component, which exceeds the 20 percent criterion established by Reckase (1979) for assuming unidimensionality and that Cognitive Speaking Anxiety (CSA) items were not dimensionally separable from the Psychosomatic Speaking Anxiety (PSA) items. The study established a pool of 142 items that can be used for a computerized adaptive speaking anxiety severity test. Results of a CAT simulation indicate that a 20-item simulated fixed-length CAT provides better accuracy than that of a P&P test.
JF  - ProQuest Dissertations and Theses
AU  - Yang, Tae-Kyoung
A3  - Chang, Hua-Hua
Y1  - 2005
PY  - 2005
DA  - 2005
SP  - 245
CY  - United States -- Texas
PB  - The University of Texas at Austin
PP  - United States -- Texas
SN  - 978-0-542-50049-7
KW  - Education
KW  - Accuracy
KW  - College students
KW  - Computerized adaptive test
KW  - EFL
KW  - Foreign language
KW  - Korean
KW  - Psychometric
KW  - Speaking anxiety
KW  - Educational evaluation
KW  - Educational psychology
KW  - Multicultural education
KW  - Higher education
KW  - Educational tests & measurements
KW  - 0455:Multicultural Education
KW  - 0443:Educational evaluation
KW  - 0525:Educational psychology
KW  - 0745:Higher education
KW  - 0288:Educational tests & measurements
UR  - https://www.proquest.com/dissertations-theses/measurement-korean-efl-college-students-foreign/docview/305382849/se-2?accountid=10957
L2  - http://www.redi-bw.de/links/unifm?url_ver=Z39.88-2004&rft_val_fmt=info:ofi/fmt:kev:mtx:dissertation&genre=dissertations&sid=ProQ:ProQuest+Dissertations+%26+Theses+Global&atitle=&title=Measurement+of+Korean+EFL+college+students%27+foreign+language+classroom+speaking+anxiety%3A+Evidence+of+psychometric+properties+and+accuracy+of+a+computerized+adaptive+test+%28CAT%29+with+dichotomously+scored+items+using+a+CAT+simulation&issn=&date=2005-01-01&volume=&issue=&spage=&au=Yang%2C+Tae-Kyoung&isbn=978-0-542-50049-7&jtitle=&btitle=&rft_id=info:eric/&rft_id=info:doi/
LA  - English
DB  - ProQuest Dissertations & Theses Global
N1  - Copyright - Database copyright ProQuest LLC; ProQuest does not claim copyright in the individual underlying works.
N1  - Zuletzt aktualisiert - 2022-10-31
M3  - Ph.D.
M1  - 3204228
ER  - 



TY  - JOUR
T1  - Constructing Rotating Item Pools for Constrained Adaptive Testing
AN  - 62079601; EJ739127
AB  - Preventing items in adaptive testing from being over- or underexposed is one of the main problems in computerized adaptive testing. Though the problem of overexposed items can be solved using a probabilistic item-exposure control method, such methods are unable to deal with the problem of underexposed items. Using a system of rotating item pools, on the other hand, is a method that potentially solves both problems. In this method, a master pool is divided into (possibly overlapping) smaller item pools, which are required to have similar distributions of content and statistical attributes. These pools are rotated among the testing sites to realize desirable exposure rates for the items. A test assembly model, motivated by Gulliksen's matched random subtests method, was explored to help solve the problem of dividing a master pool into a set of smaller pools. Different methods to solve the model are proposed. An item pool from the Law School Admission Test was used to evaluate the performances of computerized adaptive tests from systems of rotating item pools constructed using these methods.
JF  - Journal of Educational Measurement
AU  - Ariel, Adelaide
AU  - Veldkamp, Bernard P.
AU  - van der Linden, Wim J.
Y1  - 2004/12//
PY  - 2004
DA  - Dec 2004
SP  - 345
EP  - 359
PB  - Blackwell Publishing
VL  - 41
IS  - 4
SN  - 0022-0655, 0022-0655
KW  - ERIC, Current Index to Journals in Education (CIJE)
KW  - Adaptive Testing
KW  - Test Construction
KW  - Task Analysis
KW  - Computer Assisted Testing
KW  - Item Banks
KW  - Problem Solving
KW  - Methods Research
KW  - Item Analysis
UR  - https://www.proquest.com/scholarly-journals/constructing-rotating-item-pools-constrained/docview/62079601/se-2?accountid=10957
L2  - http://www.redi-bw.de/links/unifm?url_ver=Z39.88-2004&rft_val_fmt=info:ofi/fmt:kev:mtx:journal&genre=article&sid=ProQ:ProQ%3Aeric&atitle=Constructing+Rotating+Item+Pools+for+Constrained+Adaptive+Testing&title=Journal+of+Educational+Measurement&issn=00220655&date=2004-12-01&volume=41&issue=4&spage=345&au=Ariel%2C+Adelaide%3BVeldkamp%2C+Bernard+P.%3Bvan+der+Linden%2C+Wim+J.&isbn=&jtitle=Journal+of+Educational+Measurement&btitle=&rft_id=info:eric/EJ739127&rft_id=info:doi/10.1111%2Fj.1745-3984.2004.tb01170.x
LA  - English
DB  - ERIC
N1  - Zuletzt aktualisiert - 2021-02-11
N1  - SubjectsTermNotLitGenreText - 2043ER1 2099ER1 2098ER1 10811ER1 10919ER1 6526ER1 6663ER1 2029ER5 2085ER5 2084ER5 10719ER5 10827ER5 6467ER5 6603ER5; 140ER1 10919ER1 6526ER1 6663ER1 138ER5 10827ER5 6467ER5 6603ER5; 5593ER1 5540ER5; 10895ER1 6468ER1 2820ER1 10803ER5 6409ER5 2799ER5; 6665ER1 8947ER1 6605ER5 8868ER5; 8338ER1 1733ER1 8263ER5 1719ER5; 10594ER1 3679ER1 6663ER1 10503ER5 3643ER5 6603ER5; 5592ER1 10210ER1 2604ER1 3679ER1 6663ER1 5539ER5 10125ER5 2586ER5 3643ER5 6603ER5
DO  - https://doi.org/10.1111/j.1745-3984.2004.tb01170.x
ER  - 



TY  - THES
T1  - Application of the rule space model in computerized adaptive testing for diagnostic assessment
AN  - 305249080
AB  - With the development and maturation of computer technology and test theories, computerized adaptive testing (CAT) has become more widely used in educational assessment. Most CATS systems are based on item response theory (IRT), and as compared with traditional paper and pencil tests (P & P test), CAT selects the most suitable items according to individual examinee's ability. Thus, it provides a more accurate and efficient estimate of examinees' ability.    Along with the development of the society and the rising education for the general public, there is stronger demand that educational tests should not only be used to rank and screen examinees, but also to provide diagnostic information about students' misconceptions. Researchers have proposed different models to establish these diagnostic tests, through which we can estimate examinees' knowledge ability or psychological traits from their item responses. Among these models, the Rule Space Model (RSM) is an influential one and has been implemented in the analysis and reporting of the PSAT (Preliminary SAT Test) for high school leavers in the United States of America.    Incorporating RSM into CAT will lead to a totally new testing system—the diagnostic computerized adaptive test. However, we have little knowledge on the characteristics of the model in its ability estimation efficiency, its new item selection strategy, and stopping rule, among other things. The present research aimed to understand these characteristics.    Broadly differentiated, there were two main research purposes. Firstly, the research aimed to understand in greater depth the properties of the RSM, including, such as, how estimation accuracy would be affected by different factors (e.g. test length). Secondly, in the incorporation of RSM in diagnostic computerized adaptive tests, the performances of four different item selection methods were compared. These two purposes were achieved through two related sets of studies. The first set of experiments consisted of seven simulation experiments to investigate factors affecting the performance of the RSM. These factors being examined include: test length, number of attributes in the test, hierarchical relations among attributes, nature (simple versus complicated) of items used, item guessing or slipness parameter, and item response model (one- versus three-parameter model) used in item bank. The second set of experiments compared four item selection strategies in diagnostic CAT, with random selection method as borderline method. In order to calculate and compare the accuracy of examinees' attribute estimation, the Monte Carlo simulation method was used in both sets of studies. (Abstract shortened by UMI.)
JF  - ProQuest Dissertations and Theses
AU  - Wen, Jian-bing
A3  - Hau, Kit-Tai
Y1  - 2003
PY  - 2003
DA  - 2003
SP  - 155
CY  - Hong Kong
PB  - The Chinese University of Hong Kong (Hong Kong)
PP  - Hong Kong
SN  - 978-0-496-52429-7
KW  - Education
KW  - Computerized adaptive testing
KW  - Diagnostic assessment
KW  - Rule space
KW  - Educational evaluation
KW  - Educational technology
KW  - Educational tests & measurements
KW  - 0710:Educational technology
KW  - 0443:Educational evaluation
KW  - 0288:Educational tests & measurements
UR  - https://www.proquest.com/dissertations-theses/application-rule-space-model-computerized/docview/305249080/se-2?accountid=10957
L2  - http://www.redi-bw.de/links/unifm?url_ver=Z39.88-2004&rft_val_fmt=info:ofi/fmt:kev:mtx:dissertation&genre=dissertations&sid=ProQ:ProQuest+Dissertations+%26+Theses+Global&atitle=&title=Application+of+the+rule+space+model+in+computerized+adaptive+testing+for+diagnostic+assessment&issn=&date=2003-01-01&volume=&issue=&spage=&au=Wen%2C+Jian-bing&isbn=978-0-496-52429-7&jtitle=&btitle=&rft_id=info:eric/&rft_id=info:doi/
LA  - Chinese
DB  - ProQuest Dissertations & Theses Global
N1  - Copyright - Database copyright ProQuest LLC; ProQuest does not claim copyright in the individual underlying works.
N1  - Zuletzt aktualisiert - 2022-08-11
M3  - Ph.D.
M1  - 3104862
ER  - 



TY  - JOUR
T1  - Applicable Adaptive Testing Models for School Teachers
AN  - 62208269; EJ654148
AB  - Describes a study conducted in Taipei (Taiwan) that investigated the attitudinal effects of SPRT (Sequential Probability Ratio Test) adaptive testing environment on junior high school students. Discusses test anxiety; student preferences; test adaptability; acceptance of test results; number of items answered; and computer experience. (Author/LRW)
JF  - Educational Media International
AU  - Wang, Albert Chang-hwa
AU  - Chuang, Chi-lin
Y1  - 2002/03//
PY  - 2002
DA  - Mar 2002
SP  - 55
EP  - 59
VL  - 39
IS  - 1
SN  - 0952-3987, 0952-3987
KW  - Taiwan (Taipei)
KW  - Sequential Probability Ratio Test (Wald)
KW  - ERIC, Current Index to Journals in Education (CIJE)
KW  - Adaptive Testing
KW  - Junior High Schools
KW  - Student Attitudes
KW  - Foreign Countries
KW  - Junior High School Students
KW  - Test Anxiety
UR  - https://www.proquest.com/scholarly-journals/applicable-adaptive-testing-models-school/docview/62208269/se-2?accountid=10957
L2  - http://www.redi-bw.de/links/unifm?url_ver=Z39.88-2004&rft_val_fmt=info:ofi/fmt:kev:mtx:journal&genre=article&sid=ProQ:ProQ%3Aeric&atitle=Applicable+Adaptive+Testing+Models+for+School+Teachers&title=Educational+Media+International&issn=09523987&date=2002-03-01&volume=39&issue=1&spage=55&au=Wang%2C+Albert+Chang-hwa%3BChuang%2C+Chi-lin&isbn=&jtitle=Educational+Media+International&btitle=&rft_id=info:eric/EJ654148&rft_id=info:doi/
LA  - English
DB  - ERIC
N1  - Zuletzt aktualisiert - 2021-02-11
N1  - SubjectsTermNotLitGenreText - 140ER1 10919ER1 6526ER1 6663ER1 138ER5 10827ER5 6467ER5 6603ER5; 4166ER1 4394ER1 4124ER5 4350ER5; 5696ER1 9534ER1 10411ER1 8114ER1 4605ER1 5642ER5 9453ER5 10321ER5 8043ER5 4558ER5; 5698ER1 9536ER1 9420ER1 5308ER1 5644ER5 9455ER5 9339ER5 5258ER5; 10309ER1 740ER1 10219ER5 732ER5; 10890ER1 554ER1 8522ER1 10798ER5 549ER5 8446ER5; Taiwan (Taipei)
ER  - 



TY  - RPRT
T1  - Constructing Rotating Item Pools for Constrained Adaptive Testing. Research Report
AN  - 62197121; ED473533
AB  - Preventing items in adaptive testing from being over- or underexposed is one of the main problems in computerized adaptive testing. Though the problem of overexposed items can be solved using a probabilistic item-exposure control method, such methods are unable to deal with the problem of underexposed items. Using a system of rotating item pools, on the other hand, is a method that potentially solves both problems. In this method, a master pool is divided into (possibly overlapping) smaller item pools that are required to have similar distributions of content and statistical attributes. These pools are rotated among the testing sites to realize desirable exposure rates for the items. In this paper, a test assembly model for the problem of dividing a master pool into a set of smaller pools is presented. The model was motivated by Gullicksen's (1950) matched random subtests method. Different methods to solve the model are proposed. An item pool from the Law School Admission Test was used to evaluate the performances of computerized adaptive tests from systems of rotating item pools constructed using these methods. (Contains 6 figures and 14 references.) (Author/SLD)
AU  - Ariel, Adelaide
AU  - Veldkamp, Bernard P.
AU  - van der Linden, Wim J.
Y1  - 2002
PY  - 2002
DA  - 2002
SP  - 1
EP  - 30
PB  - Faculty of Educational Science and Technology
KW  - Item Exposure (Tests)
KW  - Constraints
KW  - ERIC, Resources in Education (RIE)
KW  - Adaptive Testing
KW  - Test Construction
KW  - Computer Assisted Testing
KW  - Item Banks
KW  - Test Items
UR  - https://www.proquest.com/reports/constructing-rotating-item-pools-constrained/docview/62197121/se-2?accountid=10957
L2  - http://www.redi-bw.de/links/unifm?url_ver=Z39.88-2004&rft_val_fmt=info:ofi/fmt:kev:mtx:book&genre=report&sid=ProQ:ProQ%3Aeric&atitle=&title=Constructing+Rotating+Item+Pools+for+Constrained+Adaptive+Testing.+Research+Report&issn=&date=2002-01-01&volume=&issue=&spage=1&au=Ariel%2C+Adelaide%3BVeldkamp%2C+Bernard+P.%3Bvan+der+Linden%2C+Wim+J.&isbn=&jtitle=&btitle=Constructing+Rotating+Item+Pools+for+Constrained+Adaptive+Testing.+Research+Report&rft_id=info:eric/ED473533&rft_id=info:doi/
LA  - English
DB  - ERIC
N1  - Zuletzt aktualisiert - 2021-02-11
ER  - 



TY  - JOUR
T1  - On Test and Computer Anxiety: Test Performance under CAT and SAT Conditions
AN  - 62369842; EJ626792
AB  - This study of undergraduates examined differences between computer adaptive testing (CAT) and self-adaptive testing (SAT), including feedback conditions and gender differences. Results of the Test Anxiety Inventory, Computer Anxiety Rating Scale, and a Student Attitude Questionnaire showed measurement efficiency is differentially affected by test condition and also showed significant gender effects. (Author/LRW)
JF  - Journal of Educational Computing Research
AU  - Shermis, Mark D.
AU  - Mzumara, Howard R.
AU  - Bublitz, Scott T.
Y1  - 2001
PY  - 2001
DA  - 2001
SP  - 57
EP  - 75
VL  - 24
IS  - 1
SN  - 0735-6331, 0735-6331
KW  - Computer Anxiety Scale
KW  - Test Anxiety Inventory
KW  - Self Adapted Testing
KW  - Test Anxiety Inventory (Spielberger)
KW  - ERIC, Current Index to Journals in Education (CIJE)
KW  - Adaptive Testing
KW  - Gender Issues
KW  - Higher Education
KW  - Sex Differences
KW  - Computer Anxiety
KW  - Undergraduate Students
KW  - Computer Assisted Testing
KW  - Questionnaires
KW  - Student Attitudes
KW  - Test Anxiety
UR  - https://www.proquest.com/scholarly-journals/on-test-computer-anxiety-performance-under-cat/docview/62369842/se-2?accountid=10957
L2  - http://www.redi-bw.de/links/unifm?url_ver=Z39.88-2004&rft_val_fmt=info:ofi/fmt:kev:mtx:journal&genre=article&sid=ProQ:ProQ%3Aeric&atitle=On+Test+and+Computer+Anxiety%3A+Test+Performance+under+CAT+and+SAT+Conditions&title=Journal+of+Educational+Computing+Research&issn=07356331&date=2001-01-01&volume=24&issue=1&spage=57&au=Shermis%2C+Mark+D.%3BMzumara%2C+Howard+R.%3BBublitz%2C+Scott+T.&isbn=&jtitle=Journal+of+Educational+Computing+Research&btitle=&rft_id=info:eric/EJ626792&rft_id=info:doi/
LA  - English
DB  - ERIC
N1  - Zuletzt aktualisiert - 2021-02-11
N1  - SubjectsTermNotLitGenreText - 140ER1 10919ER1 6526ER1 6663ER1 138ER5 10827ER5 6467ER5 6603ER5; Computer Anxiety; 2043ER1 2099ER1 2098ER1 10811ER1 10919ER1 6526ER1 6663ER1 2029ER5 2085ER5 2084ER5 10719ER5 10827ER5 6467ER5 6603ER5; 4353ER1 4310ER5; 4809ER1 8147ER1 3192ER1 4761ER5 8075ER5 3164ER5; 8642ER1 6527ER1 8566ER5 6468ER5; Sex Differences; 10309ER1 740ER1 10219ER5 732ER5; 10890ER1 554ER1 8522ER1 10798ER5 549ER5 8446ER5; 11231ER1 1829ER1 10411ER1 8114ER1 4605ER1 11139ER5 1815ER5 10321ER5 8043ER5 4558ER5
ER  - 



TY  - JOUR
T1  - Test Anxiety and Test Performance: Comparing Paper-based and Computer-Adaptive Versions of the Graduate Record Examinations (GRE) General Test
AN  - 62256890; EJ631333
AB  - Tests the hypothesis that the introduction of computer-adaptive testing may help to alleviate test anxiety and diminish the relationship between test anxiety and test performance. Compares a sample of Graduate Record Examinations (GRE) General Test takers who took the computer-adaptive version of the test with another sample who took the paper-based version. There was no support for the study's major hypothesis. (AEF)
JF  - Journal of Educational Computing Research
AU  - Powers, Donald E.
Y1  - 2001
PY  - 2001
DA  - 2001
SP  - 249
EP  - 73
VL  - 24
IS  - 3
SN  - 0735-6331, 0735-6331
KW  - Graduate Record Examinations
KW  - Paper and Pencil Tests
KW  - Technology Role
KW  - ERIC, Current Index to Journals in Education (CIJE)
KW  - Printed Materials
KW  - Comparative Analysis
KW  - Computer Assisted Testing
KW  - Nonprint Media
KW  - Performance
KW  - Test Anxiety
KW  - Tests
KW  - Testing
UR  - https://www.proquest.com/scholarly-journals/test-anxiety-performance-comparing-paper-based/docview/62256890/se-2?accountid=10957
L2  - http://www.redi-bw.de/links/unifm?url_ver=Z39.88-2004&rft_val_fmt=info:ofi/fmt:kev:mtx:journal&genre=article&sid=ProQ:ProQ%3Aeric&atitle=Test+Anxiety+and+Test+Performance%3A+Comparing+Paper-based+and+Computer-Adaptive+Versions+of+the+Graduate+Record+Examinations+%28GRE%29+General+Test&title=Journal+of+Educational+Computing+Research&issn=07356331&date=2001-01-01&volume=24&issue=3&spage=249&au=Powers%2C+Donald+E.&isbn=&jtitle=Journal+of+Educational+Computing+Research&btitle=&rft_id=info:eric/EJ631333&rft_id=info:doi/
LA  - English
DB  - ERIC
N1  - Zuletzt aktualisiert - 2021-02-11
N1  - SubjectsTermNotLitGenreText - 1980ER1 3679ER1 6663ER1 1966ER5 3643ER5 6603ER5; 2043ER1 2099ER1 2098ER1 10811ER1 10919ER1 6526ER1 6663ER1 2029ER5 2085ER5 2084ER5 10719ER5 10827ER5 6467ER5 6603ER5; 7209ER1 7145ER5; 7798ER1 921ER1 7728ER5 912ER5; 8298ER1 8223ER5; 10890ER1 554ER1 8522ER1 10798ER5 549ER5 8446ER5; 10919ER1 6526ER1 6663ER1 10827ER5 6467ER5 6603ER5; 10925ER1 6527ER1 10833ER5 6468ER5
ER  - 



TY  - THES
T1  - The effect of *test characteristics on aberrant response patterns in computer adaptive testing
AN  - 304699823
AB  - The advantages that computer adaptive testing offers over linear tests have been well documented. The Computer Adaptive Test (CAT) design is more efficient than the Linear test design as fewer items are needed to estimate an examinee's proficiency to a desired level of precision. In the ideal situation, a CAT will result in examinees answering different number of items according to the stopping rule employed. Unfortunately, the realities of testing conditions have necessitated the imposition of time and minimum test length limits on CATs. Such constraints might place a burden on the CAT test taker resulting in aberrant response behaviors by some examinees. Occurrence of such response patterns results in inaccurate estimation of examinee proficiency levels. This study examined the effects of test lengths, time limits and the interaction of these factors with the examinee proficiency levels on the occurrence of aberrant response patterns.    The focus of the study was on the aberrant behaviors caused by rushed guessing due to restrictive time limits. Four different testing scenarios were examined; fixed length performance tests with and without content constraints, fixed length mastery tests and variable length mastery tests without content constraints. For each of these testing scenarios, the effect of two test lengths, five different timing conditions and the interaction between these factors with three ability levels on ability estimation were examined. For fixed and variable length mastery tests, decision accuracy was also looked at in addition to the estimation accuracy. Several indices were used to evaluate the estimation and decision accuracy for different testing conditions.    The results showed that changing time limits had a significant impact on the occurrence of aberrant response patterns conditional on ability. Increasing test length had negligible if not negative effect on ability estimation when rushed guessing occured. In case of performance testing high ability examinees while in classification testing middle ability examinees suffered the most. The decision accuracy was considerably affected in case of variable length classification tests.
JF  - ProQuest Dissertations and Theses
AU  - Rizavi, Saba M.
A3  - Swaminathan, Hariharan
Y1  - 2001
PY  - 2001
DA  - 2001
SP  - 146
CY  - United States -- Massachusetts
PB  - University of Massachusetts Amherst
PP  - United States -- Massachusetts
SN  - 978-0-493-39353-7
KW  - Education
KW  - Psychology
KW  - Aberrant response patterns
KW  - Computer adaptive testing
KW  - Test
KW  - Educational evaluation
KW  - Psychological tests
KW  - Educational technology
KW  - 0632:Quantitative psychology
KW  - 0710:Educational technology
KW  - 0288:Educational tests & measurements
UR  - https://www.proquest.com/dissertations-theses/effect-test-characteristics-on-aberrant-response/docview/304699823/se-2?accountid=10957
L2  - http://www.redi-bw.de/links/unifm?url_ver=Z39.88-2004&rft_val_fmt=info:ofi/fmt:kev:mtx:dissertation&genre=dissertations&sid=ProQ:ProQuest+Dissertations+%26+Theses+Global&atitle=&title=The+effect+of+*test+characteristics+on+aberrant+response+patterns+in+computer+adaptive+testing&issn=&date=2001-01-01&volume=&issue=&spage=&au=Rizavi%2C+Saba+M.&isbn=978-0-493-39353-7&jtitle=&btitle=&rft_id=info:eric/&rft_id=info:doi/
LA  - English
DB  - ProQuest Dissertations & Theses Global
N1  - Copyright - Database copyright ProQuest LLC; ProQuest does not claim copyright in the individual underlying works.
N1  - Zuletzt aktualisiert - 2021-11-09
M3  - Ed.D.
M1  - 3027247
ER  - 



TY  - RPRT
T1  - Performance of Item Exposure Control Methods in Computerized Adaptive Testing: Further Explorations
AN  - 62436218; ED442837
AB  - This study examined the effectiveness of the Sympson and Hetter conditional procedure (SHC), a modification of the Sympson and Hetter (1985) algorithm, in controlling the exposure rates of items in a computerized adaptive testing (CAT) environment. The properties of the procedure were compared with those of the Davey and Parshall (1995) and the Stocking and Lewis (1995) (SLC) conditional multinomial procedures within the purview of estimating examinee's abilities. Each of the exposure control methods was incorporated into the item selection procedure and the adaptive testing progressed based on the CAT design established for this study. The advantages and disadvantages of these strategies were considered under four item pool sizes and two desired maximum exposure rates and were evaluated in light of test security, test overlap rates, utilization of the item pool, and conditional standard errors of measurement. Also, the issue of the appropriate conditional sample sizes in deriving the exposure control parameters was considered in the present study. Simulation results show no effect of using the four conditional sample sizes. The SHC produced the most satisfactory results in terms of item security and test overlap rates followed by the SLC method. Results also show that as long as the control for item exposure was not exercised, optimal items could be administered to almost every examinee under any of the four item pools. Findings of this study provide useful insights on how item pool sizes and maximum item exposure rates affect the performance of the exposure control methods. (Contains 3 tables, 13 figures, and 18 references.) (SLD)
AU  - Chang, Shun-Wen
AU  - Ansley, Timothy N.
AU  - Lin, Sieh-Hwa
Y1  - 2000/04//
PY  - 2000
DA  - Apr 2000
SP  - 1
EP  - 44
KW  - Item Exposure (Tests)
KW  - ERIC, Resources in Education (RIE)
KW  - Adaptive Testing
KW  - Sample Size
KW  - Algorithms
KW  - Computer Assisted Testing
KW  - Item Banks
KW  - Test Items
UR  - https://www.proquest.com/reports/performance-item-exposure-control-methods/docview/62436218/se-2?accountid=10957
L2  - http://www.redi-bw.de/links/unifm?url_ver=Z39.88-2004&rft_val_fmt=info:ofi/fmt:kev:mtx:book&genre=report&sid=ProQ:ProQ%3Aeric&atitle=&title=Performance+of+Item+Exposure+Control+Methods+in+Computerized+Adaptive+Testing%3A+Further+Explorations&issn=&date=2000-04-01&volume=&issue=&spage=1&au=Chang%2C+Shun-Wen%3BAnsley%2C+Timothy+N.%3BLin%2C+Sieh-Hwa&isbn=&jtitle=&btitle=Performance+of+Item+Exposure+Control+Methods+in+Computerized+Adaptive+Testing%3A+Further+Explorations&rft_id=info:eric/ED442837&rft_id=info:doi/
LA  - English
DB  - ERIC
N1  - Zuletzt aktualisiert - 2021-02-11
ER  - 



TY  - JOUR
T1  - Computerized Testing
AN  - 62433228; EJ601216
AB  - The Northwest Evaluation Association, serving over 300 U.S. school districts, is developing an Internet-enabled assessment system that adapts questions to each student's performance. Shorter, adaptive tests help students avoid frustrations or boredom caused by too-difficult or -easy questions. Scores are as valid as traditional test scores. (MLH)
JF  - American School Board Journal
AU  - Olson, Allan
Y1  - 2000/03//
PY  - 2000
DA  - Mar 2000
VL  - 187
IS  - 3
KW  - Northwest Evaluation Association
KW  - ERIC, Current Index to Journals in Education (CIJE)
KW  - Elementary Secondary Education
KW  - Student Evaluation
KW  - Achievement Tests
KW  - Computer Assisted Testing
KW  - Accountability
KW  - Scores
KW  - Test Validity
KW  - Internet
UR  - https://www.proquest.com/scholarly-journals/computerized-testing/docview/62433228/se-2?accountid=10957
L2  - http://www.redi-bw.de/links/unifm?url_ver=Z39.88-2004&rft_val_fmt=info:ofi/fmt:kev:mtx:journal&genre=article&sid=ProQ:ProQ%3Aeric&atitle=Computerized+Testing&title=American+School+Board+Journal&issn=&date=2000-03-01&volume=187&issue=3&spage=&au=Olson%2C+Allan&isbn=&jtitle=American+School+Board+Journal&btitle=&rft_id=info:eric/EJ601216&rft_id=info:doi/
LA  - English
DB  - ERIC
N1  - Zuletzt aktualisiert - 2021-02-11
N1  - SubjectsTermNotLitGenreText - 89ER1 9044ER1 87ER5 8964ER5; 109ER1 10925ER1 6527ER1 107ER5 10833ER5 6468ER5; 2043ER1 2099ER1 2098ER1 10811ER1 10919ER1 6526ER1 6663ER1 2029ER5 2085ER5 2084ER5 10719ER5 10827ER5 6467ER5 6603ER5; 3412ER1 3192ER1 3382ER5 3164ER5; 5504ER1 2070ER1 7140ER1 5214ER1 5453ER5 2056ER5 7077ER5 5164ER5; 9485ER1 2602ER1 9404ER5 2584ER5; 10337ER1 3676ER1 10247ER5 3640ER5; 10917ER1 11346ER1 3677ER1 2445ER1 10154ER1 10825ER5 11254ER5 3641ER5 2427ER5 10069ER5
ER  - 



TY  - THES
T1  - Multidimensional adaptive testing using the weighted likelihood estimation
AN  - 304637371
AB  - This study extended Warm's (1989) weighted likelihood estimation (WLE) to a multidimensional computerized adaptive test (MCAT) setting. WLE was compared with the maximum likelihood estimation (MLE), expected a posteriori (EAP), and maximum a posteriori (MAP) using a three-dimensional 3PL IRT model under a variety of computerized adaptive testing conditions. The dependent variables included bias, standard error of ability estimates (SE), square root of mean square error (RMSE), and test information. The independent variables were ability estimation methods, intercorrelation levels between dimensions, multidimensional structures, and ability combinations. Simulation results were presented in terms of descriptive statistics, such as figures and tables. In addition, inferential procedures were used to analyze bias by conceptualizing this Monte Carlo study as a statistical sampling experiment.    The results of this study indicate that WLE and the other three estimation methods yield significantly more accurate ability estimates under an approximate simple test structure with one dominant dimension and several secondary dimensions. All four estimation methods, especially WLE, yield very large SEs when a three equally dominant multidimensional structure was employed. Consistent with previous findings based on unidimensional IRT model, MLE and WLE are less biased in the extreme of the ability scale; MLE and WLE yield larger SEs than the Bayesian methods; test information-based SEs underestimate actual SEs for both MLE and WLE in MCAT situations, especially at shorter test lengths; WLE reduced the bias of MLE under the approximate simple structure; test information-based SEs underestimates the actual SEs of MLE and WLE estimators in the MCAT conditions, similar to the findings of Warm (1989) in the unidimensional case.    The results from the MCAT simulations did show some advantages of WLE in reducing the bias of MLE under the approximate simple structure with a fixed test length of 50 items, which was consistent with the previous research findings based on different unidimensional models. It is clear from the current results that all four methods perform very poorly when the multidimensional structures with multiple dominant factors were employed. More research efforts are urged to investigate systematically how different multidimensional structures affect the accuracy and reliability of ability estimation. Based on the simulated results in this study, there is no significant effect found on the ability estimation from the intercorrelation between dimensions.
JF  - ProQuest Dissertations and Theses
AU  - Tseng, Fen-Lan
A3  - Hsu, Tse-chi
Y1  - 2000
PY  - 2000
DA  - 2000
SP  - 156
CY  - United States -- Pennsylvania
PB  - University of Pittsburgh
PP  - United States -- Pennsylvania
SN  - 978-0-493-07733-8
KW  - Education
KW  - Psychology
KW  - Estimation
KW  - Item response theory
KW  - Multidimensional adaptive testing
KW  - Weighted likelihood estimation
KW  - Educational evaluation
KW  - Educational psychology
KW  - Psychological tests
KW  - Educational tests & measurements
KW  - Quantitative psychology
KW  - 0632:Quantitative psychology
KW  - 0443:Educational evaluation
KW  - 0525:Educational psychology
KW  - 0288:Educational tests & measurements
UR  - https://www.proquest.com/dissertations-theses/multidimensional-adaptive-testing-using-weighted/docview/304637371/se-2?accountid=10957
L2  - http://www.redi-bw.de/links/unifm?url_ver=Z39.88-2004&rft_val_fmt=info:ofi/fmt:kev:mtx:dissertation&genre=dissertations&sid=ProQ:ProQuest+Dissertations+%26+Theses+Global&atitle=&title=Multidimensional+adaptive+testing+using+the+weighted+likelihood+estimation&issn=&date=2000-01-01&volume=&issue=&spage=&au=Tseng%2C+Fen-Lan&isbn=978-0-493-07733-8&jtitle=&btitle=&rft_id=info:eric/&rft_id=info:doi/
LA  - English
DB  - ProQuest Dissertations & Theses Global
N1  - Copyright - Database copyright ProQuest LLC; ProQuest does not claim copyright in the individual underlying works.
N1  - Zuletzt aktualisiert - 2022-10-29
M3  - Ph.D.
M1  - 9998631
ER  - 



TY  - JOUR
T1  - Performing Automatic Exams
AN  - 62477475; EJ580072
AB  - Describes a tool for building software systems which replace the role of the examiner during a typical Italian academic exam in technical/scientific subjects. Such systems are designed to exploit the advantages of self-adapted testing for reducing effects of anxiety, and of computerized adaptive testing for increasing assessment efficiency. (Author/AEF)
JF  - Computers & Education
AU  - Frosini, G.
AU  - Lazzerini, B.
AU  - Marcelloni, F.
Y1  - 1998/11//
PY  - 1998
DA  - Nov 1998
SP  - 281
EP  - 300
VL  - 31
IS  - 3
SN  - 0360-1315, 0360-1315
KW  - Italy
KW  - ERIC, Current Index to Journals in Education (CIJE)
KW  - Automation
KW  - Teacher Role
KW  - Foreign Countries
KW  - Science Education
KW  - Adaptive Testing
KW  - Computer Assisted Testing
KW  - Anxiety
KW  - Design Preferences
KW  - Technology Education
KW  - Computer System Design
KW  - Evaluation Methods
KW  - Computer Software Development
KW  - Testing
UR  - https://www.proquest.com/scholarly-journals/performing-automatic-exams/docview/62477475/se-2?accountid=10957
L2  - http://www.redi-bw.de/links/unifm?url_ver=Z39.88-2004&rft_val_fmt=info:ofi/fmt:kev:mtx:journal&genre=article&sid=ProQ:ProQ%3Aeric&atitle=Performing+Automatic+Exams&title=Computers+%26+Education&issn=03601315&date=1998-11-01&volume=31&issue=3&spage=281&au=Frosini%2C+G.%3BLazzerini%2C+B.%3BMarcelloni%2C+F.&isbn=&jtitle=Computers+%26+Education&btitle=&rft_id=info:eric/EJ580072&rft_id=info:doi/
LA  - English
DB  - ERIC
N1  - Zuletzt aktualisiert - 2021-02-11
N1  - SubjectsTermNotLitGenreText - 140ER1 10919ER1 6526ER1 6663ER1 138ER5 10827ER5 6467ER5 6603ER5; 554ER1 8522ER1 549ER5 8446ER5; 839ER1 10805ER1 830ER5 10713ER5; 2043ER1 2099ER1 2098ER1 10811ER1 10919ER1 6526ER1 6663ER1 2029ER5 2085ER5 2084ER5 10719ER5 10827ER5 6467ER5 6603ER5; Computer Software Development; 2093ER1 2799ER1 2079ER5 2780ER5; 2804ER1 8204ER1 740ER1 2785ER5 8131ER5 732ER5; 3679ER1 6663ER1 3643ER5 6603ER5; 4166ER1 4394ER1 4124ER5 4350ER5; 9442ER1 3192ER1 9361ER5 3164ER5; 10700ER1 9128ER1 10608ER5 9048ER5; 10806ER1 3192ER1 10714ER5 3164ER5; 10919ER1 6526ER1 6663ER1 10827ER5 6467ER5 6603ER5; Italy
ER  - 



TY  - JOUR
T1  - Comparison of Test Administration Procedures for Placement Decisions in a Mathematics Course
AN  - 62447955; EJ576511
AB  - Three test administration procedures for making placement decisions in adult education were compared (paper-based, computer-based, and computerized-adaptive tests) with 90 adult-education students. Test performance was not differentially affected by the mode of administration, but the computerized adaptive test always yielded more precise ability estimates. (SLD)
JF  - Educational Research and Evaluation (An International Journal on Theory and Practice)
AU  - Straetmans, Gerard J. J. M.
AU  - Eggen, Theo J. H. M.
Y1  - 1998/09//
PY  - 1998
DA  - Sep 1998
SP  - 259
EP  - 75
VL  - 4
IS  - 3
SN  - 1380-3611, 1380-3611
KW  - Paper and Pencil Tests
KW  - ERIC, Current Index to Journals in Education (CIJE)
KW  - Ability
KW  - Adaptive Testing
KW  - Performance Factors
KW  - Adult Students
KW  - Comparative Analysis
KW  - Test Format
KW  - Computer Assisted Testing
KW  - Test Results
KW  - Adult Education
KW  - Student Placement
KW  - Test Use
UR  - https://www.proquest.com/scholarly-journals/comparison-test-administration-procedures/docview/62447955/se-2?accountid=10957
L2  - http://www.redi-bw.de/links/unifm?url_ver=Z39.88-2004&rft_val_fmt=info:ofi/fmt:kev:mtx:journal&genre=article&sid=ProQ:ProQ%3Aeric&atitle=Comparison+of+Test+Administration+Procedures+for+Placement+Decisions+in+a+Mathematics+Course&title=Educational+Research+and+Evaluation+%28An+International+Journal+on+Theory+and+Practice%29&issn=13803611&date=1998-09-01&volume=4&issue=3&spage=259&au=Straetmans%2C+Gerard+J.+J.+M.%3BEggen%2C+Theo+J.+H.+M.&isbn=&jtitle=Educational+Research+and+Evaluation+%28An+International+Journal+on+Theory+and+Practice%29&btitle=&rft_id=info:eric/EJ576511&rft_id=info:doi/
LA  - English
DB  - ERIC
N1  - Zuletzt aktualisiert - 2021-02-11
N1  - SubjectsTermNotLitGenreText - 2ER1 1ER5; 140ER1 10919ER1 6526ER1 6663ER1 138ER5 10827ER5 6467ER5 6603ER5; 213ER1 3192ER1 211ER5 3164ER5; 227ER1 229ER1 321ER1 8114ER1 4605ER1 10411ER1 225ER5 10321ER5 8043ER5 4558ER5 227ER5 318ER5; 1980ER1 3679ER1 6663ER1 1966ER5 3643ER5 6603ER5; 2043ER1 2099ER1 2098ER1 10811ER1 10919ER1 6526ER1 6663ER1 2029ER5 2085ER5 2084ER5 10719ER5 10827ER5 6467ER5 6603ER5; 7809ER1 5194ER1 7739ER5 5144ER5; 10370ER1 8009ER1 10280ER5 7939ER5; 10898ER1 10806ER5; 10906ER1 10814ER5; 10916ER1 10824ER5
ER  - 



TY  - JOUR
T1  - Psychometric Characteristics of Computer-Adaptive and Self-Adaptive Vocabulary Tests: The Role of Answer Feedback and Test Anxiety
AN  - 62381803; EJ590595
AB  - Studied effects of administration mode [computer adaptive test (CAT) versus self-adaptive test (SAT)], item-by-item answer feedback, and test anxiety on results from computerized vocabulary tests taken by 293 college students. CATs were more reliable than SATs, and administration time was less when feedback was provided. (SLD)
JF  - Journal of Educational Measurement
AU  - Vispoel, Walter P.
Y1  - 1998///Jul 1998 - Sep
PY  - 1998
DA  - Jul 1998 - Sep 1998
SP  - 155
EP  - 67
VL  - 35
IS  - 2
SN  - 0022-0655, 0022-0655
KW  - Self Adapted Testing
KW  - ERIC, Current Index to Journals in Education (CIJE)
KW  - Adaptive Testing
KW  - Vocabulary
KW  - Higher Education
KW  - College Students
KW  - Feedback
KW  - Computer Assisted Testing
KW  - Psychometrics
KW  - Test Items
KW  - Test Anxiety
UR  - https://www.proquest.com/scholarly-journals/psychometric-characteristics-computer-adaptive/docview/62381803/se-2?accountid=10957
L2  - http://www.redi-bw.de/links/unifm?url_ver=Z39.88-2004&rft_val_fmt=info:ofi/fmt:kev:mtx:journal&genre=article&sid=ProQ:ProQ%3Aeric&atitle=Psychometric+Characteristics+of+Computer-Adaptive+and+Self-Adaptive+Vocabulary+Tests%3A+The+Role+of+Answer+Feedback+and+Test+Anxiety&title=Journal+of+Educational+Measurement&issn=00220655&date=1998-07-01&volume=35&issue=2&spage=155&au=Vispoel%2C+Walter+P.&isbn=&jtitle=Journal+of+Educational+Measurement&btitle=&rft_id=info:eric/EJ590595&rft_id=info:doi/
LA  - English
DB  - ERIC
N1  - Zuletzt aktualisiert - 2021-02-11
N1  - SubjectsTermNotLitGenreText - 140ER1 10919ER1 6526ER1 6663ER1 138ER5 10827ER5 6467ER5 6603ER5; 1829ER1 10411ER1 8114ER1 4605ER1 1815ER5 10321ER5 8043ER5 4558ER5; 2043ER1 2099ER1 2098ER1 10811ER1 10919ER1 6526ER1 6663ER1 2029ER5 2085ER5 2084ER5 10719ER5 10827ER5 6467ER5 6603ER5; Feedback; 4809ER1 8147ER1 3192ER1 4761ER5 8075ER5 3164ER5; 8530ER1 8529ER1 938ER1 9466ER1 6042ER1 8454ER5 8453ER5 929ER5 9385ER5 5983ER5; 10890ER1 554ER1 8522ER1 10798ER5 549ER5 8446ER5; 10900ER1 10808ER5; 11467ER1 11371ER5
ER  - 



TY  - RPRT
T1  - Comparing Restricted and Unrestricted Self-Adapted Testing as Alternatives to Computerized Adaptive Testing
AN  - 62449718; ED423258
AB  - Previous studies have shown that, when administered a self-adapted test, a few examinees will choose item difficulty levels that are not well-matched to their proficiencies, resulting in high standard errors of proficiency estimation. This study investigated whether the previously observed effects of a self-adapted test--lower anxiety and higher test performance relative to a computerized adaptive test (CAT)--can be sustained while eliminating the high standard errors. A restricted self-adapted test (RS-AT) in which examinees were allowed to choose among a set of difficulty levels only in the region of their proficiency estimates was utilized in this study. Data were collected from 273 students in an introductory statistics class. The results show that while the RS-AT effectively controlled the standard errors of proficiency estimation, examinees receiving an RS-AT did not show higher mean proficiency or lower posttest state anxiety than examinees receiving a CAT. (Contains 3 tables and 15 references.) (SLD)
AU  - Roos, Linda L.
AU  - Wise, Steven L.
AU  - Finney, Sara J.
Y1  - 1998/04//
PY  - 1998
DA  - Apr 1998
SP  - 1
EP  - 22
KW  - Restricted Self Adapted Testing
KW  - ERIC, Resources in Education (RIE)
KW  - Adaptive Testing
KW  - Performance Factors
KW  - Comparative Analysis
KW  - Higher Education
KW  - College Students
KW  - Error of Measurement
KW  - Computer Assisted Testing
KW  - Selection
KW  - Test Items
KW  - Difficulty Level
KW  - Test Anxiety
UR  - https://www.proquest.com/reports/comparing-restricted-unrestricted-self-adapted/docview/62449718/se-2?accountid=10957
L2  - http://www.redi-bw.de/links/unifm?url_ver=Z39.88-2004&rft_val_fmt=info:ofi/fmt:kev:mtx:book&genre=report&sid=ProQ:ProQ%3Aeric&atitle=&title=Comparing+Restricted+and+Unrestricted+Self-Adapted+Testing+as+Alternatives+to+Computerized+Adaptive+Testing&issn=&date=1998-04-01&volume=&issue=&spage=1&au=Roos%2C+Linda+L.%3BWise%2C+Steven+L.%3BFinney%2C+Sara+J.&isbn=&jtitle=&btitle=Comparing+Restricted+and+Unrestricted+Self-Adapted+Testing+as+Alternatives+to+Computerized+Adaptive+Testing&rft_id=info:eric/ED423258&rft_id=info:doi/
LA  - English
DB  - ERIC
N1  - Zuletzt aktualisiert - 2021-02-11
ER  - 



TY  - THES
T1  - Item banking and test development. Psychometric matters and requirement analysis of a computerised system
AN  - 304465366
AB  - This dissertation is concerned with item banks, more specifically, with the construction of them. Item banks are an essential piece in adaptive strategies of testing, as they are the item source for this kind of tests, the place from where the item(s) to be administered at each moment are selected. However, item bank development appears as a complex task that could be the reason of a reduced number of available item banks and, consequently, a limited application of adaptive tests in practice.    These precedents have motivated the main goal of this dissertation: the development of a tool, that taking profit of computing power, supports the process of development and management of item banks. With it, it has been tried to do the development of item banks more efficient and, consequently, more accessible the renowned computerized adaptive tests.    The main contribution of this work to satisfy this goal has come through the in-depth review of the whole process of building an item bank and the analysis of requirements of a program oriented to give support to the item bank development process. This program is referred in the thesis as BANKIT.    With the former review emerged the meaning and implications of the different tasks involved in the building of item banks as well as the relationship between them, which has allowed to make clear what should be satisfied with BANKIT and set up the basis for the latter analysis. Available software, needs, tasks, attributes, data and functional analysis have made up the requirement analysis of this computerized system.
JF  - ProQuest Dissertations and Theses
AU  - Molina Ibanez, Jesus Gabriel
Y1  - 1997
PY  - 1997
DA  - 1997
SP  - 354
CY  - Spain
PB  - Universitat de Valencia (Spain)
PP  - Spain
SN  - 9788437030180
KW  - Psychology
KW  - Psychological tests
KW  - Quantitative psychology
KW  - 0632:Quantitative psychology
UR  - https://www.proquest.com/dissertations-theses/item-banking-test-development-psychometric/docview/304465366/se-2?accountid=10957
L2  - http://www.redi-bw.de/links/unifm?url_ver=Z39.88-2004&rft_val_fmt=info:ofi/fmt:kev:mtx:dissertation&genre=dissertations&sid=ProQ:ProQuest+Dissertations+%26+Theses+Global&atitle=&title=Item+banking+and+test+development.+Psychometric+matters+and+requirement+analysis+of+a+computerised+system&issn=&date=1997-01-01&volume=&issue=&spage=&au=Molina+Ibanez%2C+Jesus+Gabriel&isbn=9788437030180&jtitle=&btitle=&rft_id=info:eric/&rft_id=info:doi/
LA  - Spanish
DB  - ProQuest Dissertations & Theses Global
N1  - Copyright - Database copyright ProQuest LLC; ProQuest does not claim copyright in the individual underlying works.
N1  - Zuletzt aktualisiert - 2022-10-29
M3  - Ph.D.
M1  - C643124
ER  - 



TY  - THES
T1  - The effect of individual differences variables on the assessment of ability for computerized adaptive testing
AN  - 304278059
AB  - Computerized Adaptive Testing (CAT) continues to gain momentum as the accepted testing modality for a growing number of certification, licensure, education, government and human resource applications. However, the developers of these tests have for the most part failed to adequately explore the impact of individual differences such as test anxiety on the adaptive testing process. It is widely accepted that non-cognitive individual differences variables interact with the assessment of ability when using written examinations. Logic would dictate that individual differences variables would equally affect CAT. Two studies were used to explore this premise. In the first study, 507 examinees were given a test anxiety survey prior to taking a high stakes certification exam using CAT or using a written format. All examinees had already completed their course of study, and the examination would be their last hurdle prior to being awarded certification. High test anxious examinees performed worse than their low anxious counterparts on both testing formats. The second study replicated the finding that anxiety depresses performance in CAT. It also addressed the differential effect of anxiety on within test performance. Examinees were candidates taking their final certification examination following a four year college program. Ability measures were calculated for each successive part of the test for 923 subjects. Within subject performance varied depending upon test position. High anxious examinees performed poorly at all points in the test, while low and medium anxious examinee performance peaked in the middle of the test. If test anxiety and performance measures were actually the same trait, then low anxious individuals should have performed equally well throughout the test. The observed interaction of test anxiety and time on task serves as strong evidence that test anxiety has motivationally mediated as well as cognitively mediated effects. The results of the studies are discussed relative to the need for CAT developers to consider individual differences variables such as test anxiety when constructing their tests.
JF  - ProQuest Dissertations and Theses
AU  - Gershon, Richard Carl
Y1  - 1996
PY  - 1996
DA  - 1996
SP  - 137
CY  - United States -- Illinois
PB  - Northwestern University
PP  - United States -- Illinois
SN  - 979-8-209-37999-7
KW  - Education
KW  - Psychology
KW  - Psychological tests
KW  - Educational evaluation
KW  - Educational tests & measurements
KW  - Quantitative psychology
KW  - 0632:Quantitative psychology
KW  - 0443:Educational evaluation
KW  - 0288:Educational tests & measurements
UR  - https://www.proquest.com/dissertations-theses/effect-individual-differences-variables-on/docview/304278059/se-2?accountid=10957
L2  - http://www.redi-bw.de/links/unifm?url_ver=Z39.88-2004&rft_val_fmt=info:ofi/fmt:kev:mtx:dissertation&genre=dissertations&sid=ProQ:ProQuest+Dissertations+%26+Theses+Global&atitle=&title=The+effect+of+individual+differences+variables+on+the+assessment+of+ability+for+computerized+adaptive+testing&issn=&date=1996-01-01&volume=&issue=&spage=&au=Gershon%2C+Richard+Carl&isbn=979-8-209-37999-7&jtitle=&btitle=&rft_id=info:eric/&rft_id=info:doi/
LA  - English
DB  - ProQuest Dissertations & Theses Global
N1  - Copyright - Database copyright ProQuest LLC; ProQuest does not claim copyright in the individual underlying works.
N1  - Zuletzt aktualisiert - 2022-10-29
M3  - Ph.D.
M1  - 9632690
ER  - 



TY  - RPRT
T1  - Developing Computerized Tests for Classroom Teachers: A Pilot Study
AN  - 62673665; ED391471
AB  - Two types of computerized testing have been defined: (1) computer-based testing, using a computer to administer conventional tests in which all examinees take the same set of items; and (2) adaptive tests, in which items are selected for administration by the computer, based on examinee's previous responses. This paper discusses an option for classroom teachers that is easier to develop than a computerized adaptive test, but more secure and sophisticated than a computer-based test. The process of developing and pilot testing the computer-administered test and the results of a survey of student reactions are described. Subjects for the study consisted of 108 undergraduates taking summer educational technology courses in computer applications at a Southern university. Identical items were used for paper-and-pencil and computerized tests. No significant differences were found for either administration. Student responses indicated that: all of the students had familiarity with computers; 94% had no problems understanding the test directions; 53% initially experienced anxiety about taking the test on a computer; 89% indicated that the computer test was as fair as a paper test; and 61% indicated a preference for the computer test, while 19% indicated that both methods worked equally well. Four tables depict results for computerized tests versus paper-and-pencil tests; descriptive data for both kinds of tests; students' yes/no responses to the attitude survey; and examinee's comments regarding computerized testing. (AEF)
AU  - Glowacki, Margaret L.
AU  - And Others
Y1  - 1995/11/09/
PY  - 1995
DA  - 1995 Nov 09
SP  - 1
EP  - 13
KW  - Paper and Pencil Tests
KW  - ERIC, Resources in Education (RIE)
KW  - Tables (Data)
KW  - Higher Education
KW  - Computer Uses in Education
KW  - Courses
KW  - Student Attitudes
KW  - Student Surveys
KW  - Comparative Analysis
KW  - Pilot Projects
KW  - Test Format
KW  - Undergraduate Students
KW  - Computer Assisted Testing
KW  - Educational Technology
UR  - https://www.proquest.com/reports/developing-computerized-tests-classroom-teachers/docview/62673665/se-2?accountid=10957
L2  - http://www.redi-bw.de/links/unifm?url_ver=Z39.88-2004&rft_val_fmt=info:ofi/fmt:kev:mtx:book&genre=report&sid=ProQ:ProQ%3Aeric&atitle=&title=Developing+Computerized+Tests+for+Classroom+Teachers%3A+A+Pilot+Study&issn=&date=1995-11-09&volume=&issue=&spage=1&au=Glowacki%2C+Margaret+L.%3BAnd+Others&isbn=&jtitle=&btitle=Developing+Computerized+Tests+for+Classroom+Teachers%3A+A+Pilot+Study&rft_id=info:eric/ED391471&rft_id=info:doi/
LA  - English
DB  - ERIC
N1  - Zuletzt aktualisiert - 2021-02-11
ER  - 



TY  - RPRT
T1  - The Influence of Examinee Test-Taking Motivation in Computerized Adaptive Testing
AN  - 62656885; ED392839
AB  - The purpose of the study was to investigate the effects of test motivation on estimated ability, test anxiety, and attitudes toward computerized adaptive testing (CAT). Korean college students (n=208) were given the Math Aptitude Test, Math Self-Concept Scale, Math Test Anxiety Scale, Computer Competence Instrument, Computer Anxiety Scale, and Test Anxiety Inventory in the regular classroom. The two groups (motivated and non-motivated) were randomly assigned by each course section. The motivated group was given special test instructions. The paper-and-pencil test (PPT) and the CAT algebra tests were given to each group in random order (PPT-CAT or CAT-PPT) under the counterbalanced design at the computer laboratory. They were also given a 10-item paper test anxiety scale, a 10-item computer test anxiety scale, and a paper-and-pencil version of the Questionnaire on Computerized Adaptive Testing. A multivariate analysis of covariance, with the math aptitude and the test anxiety as covariates, demonstrated that test motivation influenced improvement in estimated ability and reduction in test anxiety, but did not affect CAT attitudes. (Contains 4 tables and 45 references.) (Author)
AU  - Kim, JinGyu
AU  - McLean, James E.
Y1  - 1995/04//
PY  - 1995
DA  - Apr 1995
SP  - 1
EP  - 22
KW  - South Korea
KW  - Koreans
KW  - Paper and Pencil Tests
KW  - Korea
KW  - ERIC, Resources in Education (RIE)
KW  - Ability
KW  - Multivariate Analysis
KW  - Higher Education
KW  - Student Attitudes
KW  - Foreign Countries
KW  - Adaptive Testing
KW  - Student Motivation
KW  - College Students
KW  - Self Concept
KW  - Aptitude Tests
KW  - Computer Assisted Testing
KW  - Mathematics Achievement
KW  - Test Anxiety
UR  - https://www.proquest.com/reports/influence-examinee-test-taking-motivation/docview/62656885/se-2?accountid=10957
L2  - http://www.redi-bw.de/links/unifm?url_ver=Z39.88-2004&rft_val_fmt=info:ofi/fmt:kev:mtx:book&genre=report&sid=ProQ:ProQ%3Aeric&atitle=&title=The+Influence+of+Examinee+Test-Taking+Motivation+in+Computerized+Adaptive+Testing&issn=&date=1995-04-01&volume=&issue=&spage=1&au=Kim%2C+JinGyu%3BMcLean%2C+James+E.&isbn=&jtitle=&btitle=The+Influence+of+Examinee+Test-Taking+Motivation+in+Computerized+Adaptive+Testing&rft_id=info:eric/ED392839&rft_id=info:doi/
LA  - English
DB  - ERIC
N1  - Zuletzt aktualisiert - 2021-02-11
ER  - 



TY  - THES
T1  - Content validity of a computerized adaptive licensing and certification examination: A comparison of content-balancing methods
AN  - 304232692
AB  - The benefits of adaptive testing, reduced test length and increased measurement precision, are in part achieved through the application of a maximum-information item selection algorithm. While such an algorithm is an effective means for tailoring the difficulty of adaptive tests to examinee ability, its use with multidimensional content domains might result in examinations that lack content validity.    Licensure and certification examinations typically assess knowledge of multidimensional content domains. As such, balancing test content is an important consideration for these types of tests.    This study investigated the effects of content balancing on an adaptive licensure and certification examination. A computer simulation was used with data distributions attained from previous administrations of a conventional test.    The performance of the adaptive test was examined under four content-balancing methods and fifteen stopping rules. The results showed that employing a rigorous content-balancing method had no significant adverse effect on the efficiency or accuracy of the adaptive test under study. Findings, also indicated that ability estimation bias contributed to slight inaccuracies in the determination of the passing status for examinees whose abilities were near the minimum passing ability.    The findings of this study support the use of adaptive testing for licensure and certification examinations under the following conditions: rigorous content balancing is employed, the item bank contains a sufficient number of items to optimally measure a range of abilities, and if estimation bias is found to affect the pass/fail decision, it is corrected. Implications of these findings are discussed for school certification testing.
JF  - ProQuest Dissertations and Theses
AU  - Newman, Larry S.
A3  - Snelbecker, Glenn
Y1  - 1995
PY  - 1995
DA  - 1995
SP  - 153
CY  - United States -- Pennsylvania
PB  - Temple University
PP  - United States -- Pennsylvania
SN  - 979-8-208-97876-4
KW  - Education
KW  - Psychology
KW  - Educational evaluation
KW  - Psychological tests
KW  - Educational technology
KW  - 0632:Quantitative psychology
KW  - 0710:Educational technology
KW  - 0288:Educational tests & measurements
UR  - https://www.proquest.com/dissertations-theses/content-validity-computerized-adaptive-licensing/docview/304232692/se-2?accountid=10957
L2  - http://www.redi-bw.de/links/unifm?url_ver=Z39.88-2004&rft_val_fmt=info:ofi/fmt:kev:mtx:dissertation&genre=dissertations&sid=ProQ:ProQuest+Dissertations+%26+Theses+Global&atitle=&title=Content+validity+of+a+computerized+adaptive+licensing+and+certification+examination%3A+A+comparison+of+content-balancing+methods&issn=&date=1995-01-01&volume=&issue=&spage=&au=Newman%2C+Larry+S.&isbn=979-8-208-97876-4&jtitle=&btitle=&rft_id=info:eric/&rft_id=info:doi/
LA  - English
DB  - ProQuest Dissertations & Theses Global
N1  - Copyright - Database copyright ProQuest LLC; ProQuest does not claim copyright in the individual underlying works.
N1  - Zuletzt aktualisiert - 2022-02-27
M3  - Ph.D.
M1  - 9527525
ER  - 



TY  - THES
T1  - The effects of computerized adaptive testing and examinee test motivation on algebra skills, test anxiety, and attitudes
AN  - 304160034
AB  - The major purpose of the study was to investigate the effects of test method and test motivation on estimated ability, test anxiety, and attitudes toward computerized adaptive testing (CAT). This research also examined the relationships between examinees' individual difference variables and computerized adaptive test performance.    Korean college students (n = 208) were given the Math Aptitude Test, Math Self-Concept Scale, Math Test Anxiety Scale, Computer Competence Instrument, Computer Anxiety Scale, and Test Anxiety Inventory in the regular classroom. The two groups (Motivated and Non-motivated) were randomly assigned by each course section. The motivated group was given special test instructions. The paper-and-pencil test (PPT) and the CAT algebra tests were given to each group in random order (PPT-CAT or CAT-PPT) at the computer laboratory. Students were given a 10-item paper test anxiety scale after taking the PPT, and a 10-item computer test anxiety scale after taking the CAT. Finally, they were given a paper-and-pencil version of the Questionnaire on Computerized Adaptive Testing and Computer Experience Questionnaire. The examinees' test motivation was the independent variable in a multivariate analysis of covariance (MANCOVA). The six dependent variables were two estimated algebra skills (PPT and CAT), two test anxiety scores (PPT and CAT), and two CAT attitudes scores. The covariates were the math aptitude and the test anxiety variables.    This study demonstrated the equivalence between the PPT and the CAT and found that test motivation influenced improvement in estimated ability and reduction in test anxiety, but did not affect CAT attitudes. It was found that math aptitude and test motivation variables could be the most significant predictors of computerized adaptive test performance.    The findings suggest that test motivation is an important factor to help explain students' achievement. Further research should focus on how to enhance students' motivation in classroom learning as well as on tests.
JF  - ProQuest Dissertations and Theses
AU  - Kim, Jingyu
A3  - McLean, James E.
Y1  - 1995
PY  - 1995
DA  - 1995
SP  - 122
CY  - United States -- Alabama
PB  - The University of Alabama
PP  - United States -- Alabama
SN  - 979-8-209-25055-5
KW  - Education
KW  - examinees
KW  - Educational evaluation
KW  - Educational technology
KW  - 0710:Educational technology
KW  - 0288:Educational tests & measurements
UR  - https://www.proquest.com/dissertations-theses/effects-computerized-adaptive-testing-examinee/docview/304160034/se-2?accountid=10957
L2  - http://www.redi-bw.de/links/unifm?url_ver=Z39.88-2004&rft_val_fmt=info:ofi/fmt:kev:mtx:dissertation&genre=dissertations&sid=ProQ:ProQuest+Dissertations+%26+Theses+Global&atitle=&title=The+effects+of+computerized+adaptive+testing+and+examinee+test+motivation+on+algebra+skills%2C+test+anxiety%2C+and+attitudes&issn=&date=1995-01-01&volume=&issue=&spage=&au=Kim%2C+Jingyu&isbn=979-8-209-25055-5&jtitle=&btitle=&rft_id=info:eric/&rft_id=info:doi/
LA  - English
DB  - ProQuest Dissertations & Theses Global
N1  - Copyright - Database copyright ProQuest LLC; ProQuest does not claim copyright in the individual underlying works.
N1  - Zuletzt aktualisiert - 2022-02-28
M3  - Ph.D.
M1  - 9535884
ER  - 



TY  - JOUR
T1  - The Psychological Impacts of Computerized Adaptive Testing Methods
AN  - 62724750; EJ491540
AB  - Discussion of computerized adaptive testing focuses on a study of graduate students at Indiana University at Bloomington that examined three kinds of computerized adaptive testing procedures to determine their psychological impact and how they may affect test performance. Previous research is reviewed, and further research is suggested. (28 references) (LRW)
JF  - Educational Technology
AU  - Powell, Zen-Hsiu Emily
Y1  - 1994/10//
PY  - 1994
DA  - Oct 1994
SP  - 41
EP  - 47
VL  - 34
IS  - 8
SN  - 0013-1962, 0013-1962
KW  - Psychological Influences
KW  - Indiana University Bloomington
KW  - ERIC, Current Index to Journals in Education (CIJE)
KW  - Correlation
KW  - Higher Education
KW  - Test Results
KW  - Hypothesis Testing
KW  - Adaptive Testing
KW  - Graduate Students
KW  - Analysis of Variance
KW  - Computer Assisted Testing
KW  - Academic Achievement
KW  - Literature Reviews
KW  - Research Needs
KW  - Test Anxiety
UR  - https://www.proquest.com/scholarly-journals/psychological-impacts-computerized-adaptive/docview/62724750/se-2?accountid=10957
L2  - http://www.redi-bw.de/links/unifm?url_ver=Z39.88-2004&rft_val_fmt=info:ofi/fmt:kev:mtx:journal&genre=article&sid=ProQ:ProQ%3Aeric&atitle=The+Psychological+Impacts+of+Computerized+Adaptive+Testing+Methods&title=Educational+Technology&issn=00131962&date=1994-10-01&volume=34&issue=8&spage=41&au=Powell%2C+Zen-Hsiu+Emily&isbn=&jtitle=Educational+Technology&btitle=&rft_id=info:eric/EJ491540&rft_id=info:doi/
LA  - English
DB  - ERIC
N1  - Zuletzt aktualisiert - 2021-02-11
N1  - SubjectsTermNotLitGenreText - 29ER1 98ER1 28ER5 96ER5; 140ER1 10919ER1 6526ER1 6663ER1 138ER5 10827ER5 6467ER5 6603ER5; Analysis of Variance; 2043ER1 2099ER1 2098ER1 10811ER1 10919ER1 6526ER1 6663ER1 2029ER5 2085ER5 2084ER5 10719ER5 10827ER5 6467ER5 6603ER5; 2295ER1 10210ER1 2604ER1 3679ER1 6663ER1 2278ER5 10125ER5 2586ER5 3643ER5 6603ER5; 4514ER1 1829ER1 10411ER1 8114ER1 4605ER1 4467ER5 1815ER5 10321ER5 8043ER5 4558ER5; 4809ER1 8147ER1 3192ER1 4761ER5 8075ER5 3164ER5; 5012ER1 3679ER1 6663ER1 4962ER5 3643ER5 6603ER5; 6202ER1 8584ER1 6143ER5 8508ER5; 8965ER1 7085ER1 8886ER5 7022ER5; 10890ER1 554ER1 8522ER1 10798ER5 549ER5 8446ER5; 10906ER1 10814ER5
ER  - 



TY  - RPRT
T1  - Comparing Computerized Adaptive and Self-Adapted Tests: The Influence of Examinee Achievement Locus of Control
AN  - 62798906; ED371007
AB  - This study investigated the relationship between examinee achievement-specific locus of control and the differences between self-adapted testing (SAT) and computerized adaptive testing (CAT) in terms of mean estimated proficiency and posttest state anxiety. Subjects were 379 college students. A disordinal interaction was found between test type and locus of control. Examinees with an internal locus of control were affected positively by the SAT (relative to the CAT). For examinees with an external locus of control, however, the SAT appeared to have a negative effect on both estimated proficiency and posttest state anxiety. There are four tables and two figures. (Contains 18 references.) (Author/SLD)
AU  - Wise, Steven L.
AU  - And Others
Y1  - 1994/04//
PY  - 1994
DA  - Apr 1994
SP  - 1
EP  - 22
KW  - Self Adapted Testing
KW  - ERIC, Resources in Education (RIE)
KW  - Higher Education
KW  - Personal Autonomy
KW  - Pretests Posttests
KW  - Adaptive Testing
KW  - Attribution Theory
KW  - Comparative Analysis
KW  - College Students
KW  - Self Concept
KW  - Computer Assisted Testing
KW  - Academic Achievement
KW  - Anxiety
KW  - Locus of Control
UR  - https://www.proquest.com/reports/comparing-computerized-adaptive-self-adapted/docview/62798906/se-2?accountid=10957
L2  - http://www.redi-bw.de/links/unifm?url_ver=Z39.88-2004&rft_val_fmt=info:ofi/fmt:kev:mtx:book&genre=report&sid=ProQ:ProQ%3Aeric&atitle=&title=Comparing+Computerized+Adaptive+and+Self-Adapted+Tests%3A+The+Influence+of+Examinee+Achievement+Locus+of+Control&issn=&date=1994-04-01&volume=&issue=&spage=1&au=Wise%2C+Steven+L.%3BAnd+Others&isbn=&jtitle=&btitle=Comparing+Computerized+Adaptive+and+Self-Adapted+Tests%3A+The+Influence+of+Examinee+Achievement+Locus+of+Control&rft_id=info:eric/ED371007&rft_id=info:doi/
LA  - English
DB  - ERIC
N1  - Zuletzt aktualisiert - 2021-02-11
ER  - 



TY  - THES
T1  - Computer-based selection tests: psychological and measurement implications of adaptive testing.
AN  - 900237843
AB  - The aim of this thesis is to develop realistic expectations about the psychological and psychometric implications of using computerized adaptive tests (CAT).  A review is carried out of literature on computerized-based testing (CBT) and CAT.  A field study as well as four laboratory experiments were conducted to achieve that goal.  The current research strongly suggested the equivalence between the P& P and CAT formats for the Abstract Reasoning (AR) and Mechanical Reasoning (MR) tests of the Differential Aptitude Tests (DAT), but failed to do so for the Numerical Ability (NA) test.  Also, the CAT version of DAT can predict a performance variable as accurately as can the P& P format.  Overall, testees' attitudes toward several aspects of computerized testing were positive.    The results confirmed the negative relationship between computer experience and computer anxiety.  Moreover, knowledge of CAT behaviour negatively affected subjects' performance, but did not increase the level of their state anxiety.  This suggested that a form of feedback acts during the adaptive test which has a negative effect on testees' performance and response time.  This assumption was confirmed.  Subjects spend a shorter time on the subsequent item after negative feedback (wrong) on the previous item than after positive feedback (right).  It has been found that although the response time for answering an individual item was higher for CAT format than for P& P format, the CAT version of DAT resulted in a 20% reduction in completion time of the test.  Also, the difficulty level of the initial items has a significant effect on testees' overall scores.    The findings of this thesis suggest that CAT has numerous advantages and potential for improving the efficiency and accuracy of testing, and has potential areas of future contribution within personnel selection and assessment.  This potential can be realized if proper consideration is made in designing, developing, and implementing these testing systems, and if professional standards are maintained by developers and users.
JF  - PQDT - UK & Ireland
AU  - Alkhadher, O.
Y1  - 1994
PY  - 1994
DA  - 1994
SP  - 1
CY  - England
PB  - The University of Nottingham (United Kingdom)
PP  - England
KW  - (UMI)AAIU552294
KW  - Psychology
KW  - Social psychology
KW  - 0451:Social psychology
UR  - https://www.proquest.com/dissertations-theses/computer-based-selection-tests-psychological/docview/900237843/se-2?accountid=10957
L2  - http://www.redi-bw.de/links/unifm?url_ver=Z39.88-2004&rft_val_fmt=info:ofi/fmt:kev:mtx:dissertation&genre=dissertations&sid=ProQ:ProQuest+Dissertations+%26+Theses+Global&atitle=&title=Computer-based+selection+tests%3A+psychological+and+measurement+implications+of+adaptive+testing.&issn=&date=1994-01-01&volume=&issue=&spage=&au=Alkhadher%2C+O.&isbn=&jtitle=&btitle=&rft_id=info:eric/&rft_id=info:doi/
LA  - English
DB  - ProQuest Dissertations & Theses Global
N1  - Copyright - Database copyright ProQuest LLC; ProQuest does not claim copyright in the individual underlying works.
N1  - Zuletzt aktualisiert - 2022-10-20
M3  - Ph.D.
M1  - U552294
ER  - 



TY  - RPRT
T1  - The Relationship between Examinee Anxiety and Preference for Self-Adapted Testing
AN  - 62817395; EJ484374
AB  - The hypothesis that previously found effects of self-adapted testing (SAT) are attributable to examinees' having an increased perception of control over a stressful testing situation was studied with 377 college students who took computerized adaptive tests or SAT. The strongest preference for SAT was seen in individuals with the highest mathematics anxiety. (SLD)
JF  - Applied Measurement in Education
AU  - Wise, Steven L.
AU  - And Others
Y1  - 1994
PY  - 1994
DA  - 1994
SP  - 81
EP  - 91
KW  - Perceived Control
KW  - Self Adapted Testing
KW  - Preference Data
KW  - Testing Effects
KW  - ERIC, Current Index to Journals in Education (CIJE)
KW  - Adaptive Testing
KW  - Mathematics Anxiety
KW  - Higher Education
KW  - College Students
KW  - Computer Assisted Testing
KW  - Student Attitudes
KW  - Test Anxiety
UR  - https://www.proquest.com/reports/relationship-between-examinee-anxiety-preference/docview/62817395/se-2?accountid=10957
L2  - http://www.redi-bw.de/links/unifm?url_ver=Z39.88-2004&rft_val_fmt=info:ofi/fmt:kev:mtx:journal&genre=article&sid=ProQ:ProQ%3Aeric&atitle=The+Relationship+between+Examinee+Anxiety+and+Preference+for+Self-Adapted+Testing&title=Applied+Measurement+in+Education&issn=08957347&date=1994-01-01&volume=7&issue=1&spage=81&au=Wise%2C+Steven+L.%3BAnd+Others&isbn=&jtitle=Applied+Measurement+in+Education&btitle=&rft_id=info:eric/EJ484374&rft_id=info:doi/
LA  - English
DB  - ERIC
N1  - Zuletzt aktualisiert - 2021-02-11
ER  - 



TY  - THES
T1  - Student and teacher attitudes concerning computer-adaptive testing methods in a middle school setting
AN  - 304096676
AB  - This study was designed to help educators evaluate the desirability of computer adaptive testing compared to nonadaptive paper/pencil testing for middle school students and teachers. Computer adaptive testing uses computers for test administration, selection of items, scoring, and reporting. The study examined three aspects of computer adaptive testing: (1) student attitudes concerning computer testing; (2) student factors (e.g., gender) affecting student attitudes; and (3) teacher perceptions about its strengths and weaknesses.    The study examined the relationships between student factors (gender, race/ethnicity, math ability level, math test performance, grade, and computer usage) and student attitudes concerning computer adaptive testing. Three aspects of student attitudes were examined: ease with the method, comfort with equipment, and preference for computer adaptive testing.    The sample consisted of 343 students and 6 teachers from one middle school in a mid-Atlantic school district. The students took two versions of a functional mathematics test--paper/pencil and computer adaptive--and a 14 item survey assessing their attitudes about the test. The teachers participated in a survey and interview.    The results of the study indicate that a majority of the students and teachers support computer adaptive testing. With regards to students, a majority reported positive reactions in areas such as preference for computer testing as opposed to paper/pencil testing, clarity of directions and graphics, and ease of reading from the screen. However, a majority of students also reported that they were bothered by their inability to review and change answers. In addition, accelerated and remedial ability level students were more concerned with some aspects of computer adaptive testing than average ability level students. Differential results for the factors of gender and math achievement were also found.    With regards to teachers, positive comments focused on the shorter time needed for testing, convenience of using computer technology for testing, and immediate scoring information.    Although the study shows strong student and teacher support for the use of this new testing methodology, several important issues should be addressed especially for remedial and accelerated ability level students.
JF  - ProQuest Dissertations and Theses
AU  - Ward, Bonnie C.
A3  - Conley, Sharon
Y1  - 1994
PY  - 1994
DA  - 1994
SP  - 223
CY  - United States -- Maryland
PB  - University of Maryland, College Park
PP  - United States -- Maryland
SN  - 979-8-209-21262-1
KW  - Education
KW  - School administration
KW  - Educational evaluation
KW  - Mathematics education
KW  - Educational technology
KW  - 0710:Educational technology
KW  - 0280:Mathematics education
KW  - 0514:Educational administration
KW  - 0288:Educational tests & measurements
UR  - https://www.proquest.com/dissertations-theses/student-teacher-attitudes-concerning-computer/docview/304096676/se-2?accountid=10957
L2  - http://www.redi-bw.de/links/unifm?url_ver=Z39.88-2004&rft_val_fmt=info:ofi/fmt:kev:mtx:dissertation&genre=dissertations&sid=ProQ:ProQuest+Dissertations+%26+Theses+Global&atitle=&title=Student+and+teacher+attitudes+concerning+computer-adaptive+testing+methods+in+a+middle+school+setting&issn=&date=1994-01-01&volume=&issue=&spage=&au=Ward%2C+Bonnie+C.&isbn=979-8-209-21262-1&jtitle=&btitle=&rft_id=info:eric/&rft_id=info:doi/
LA  - English
DB  - ProQuest Dissertations & Theses Global
N1  - Copyright - Database copyright ProQuest LLC; ProQuest does not claim copyright in the individual underlying works.
N1  - Zuletzt aktualisiert - 2022-02-28
M3  - Ph.D.
M1  - 9508075
ER  - 



TY  - JOUR
T1  - Individual Differences in Computerized Adaptive Testing
AN  - 62803573; ED365713
AB  - Research on the major computerized adaptive testing (CAT) strategies is reviewed, and some findings are reported that examine effects of examinee demographic and psychological characteristics on CAT strategies. In fixed branching strategies, all examinees respond to a common routing test, the score of which is used to assign examinees to a second-stage test. The currently popular statistically branched adaptive strategies are based on item-response theory, and include maximum likelihood strategy and Bayesian strategy. Two alternative strategies are the use of self-adapted testing and testlet strategies. Examinee characteristic variables are divided into: (1) demographic variables; (2) computer-use variables; (3) test-taking strategy variables; (4) cognitive characteristics; and (5) affective characteristics. Although research on the relationship between examinee psychological characteristics and CAT has been inconclusive, the basic findings are that examinees of different ethnic, gender, age, grade, ability, academic self-concept, test anxiety, computer anxiety, math anxiety, and computer experience groups are differentially affected by the adaptive testing strategies. Implications for research and practice are discussed. (Contains 67 references.) (SLD)
AU  - Kim, JinGyu
Y1  - 1993/11//
PY  - 1993
DA  - Nov 1993
SP  - 1
EP  - 22
KW  - Self Adapted Testing
KW  - Academic Self Concept
KW  - Testlets
KW  - ERIC, Resources in Education (RIE)
KW  - Ability
KW  - Cognitive Processes
KW  - Demography
KW  - Sex Differences
KW  - Student Characteristics
KW  - Age Differences
KW  - Bayesian Statistics
KW  - Affective Behavior
KW  - Individual Differences
KW  - Adaptive Testing
KW  - Mathematics Anxiety
KW  - Ethnic Groups
KW  - Test Wiseness
KW  - Computer Assisted Testing
KW  - Psychological Characteristics
KW  - Test Items
KW  - Computer Literacy
KW  - Maximum Likelihood Statistics
KW  - Item Response Theory
KW  - Test Anxiety
UR  - https://www.proquest.com/scholarly-journals/individual-differences-computerized-adaptive/docview/62803573/se-2?accountid=10957
L2  - http://www.redi-bw.de/links/unifm?url_ver=Z39.88-2004&rft_val_fmt=info:ofi/fmt:kev:mtx:journal&genre=unknown&sid=ProQ:ProQ%3Aeric&atitle=Individual+Differences+in+Computerized+Adaptive+Testing&title=Undefined&issn=&date=1993-11-01&volume=&issue=&spage=1&au=Kim%2C+JinGyu&isbn=&jtitle=Undefined&btitle=&rft_id=info:eric/ED365713&rft_id=info:doi/
LA  - English
DB  - ERIC
N1  - Zuletzt aktualisiert - 2021-02-11
N1  - SubjectsTermNotLitGenreText - 2ER1 1ER5; 140ER1 10919ER1 6526ER1 6663ER1 138ER5 10827ER5 6467ER5 6603ER5; 269ER1 921ER1 266ER5 912ER5; 317ER1 5121ER1 2875ER1 314ER5 5071ER5 2854ER5; 907ER1 10210ER1 2604ER1 3679ER1 6663ER1 10226ER1 6490ER1 6042ER1 898ER5 10140ER5 6431ER5 5983ER5 10125ER5 2586ER5 3643ER5 6603ER5; 1733ER1 1719ER5; 2043ER1 2099ER1 2098ER1 10811ER1 10919ER1 6526ER1 6663ER1 2029ER5 2085ER5 2084ER5 10719ER5 10827ER5 6467ER5 6603ER5; 2066ER1 10801ER1 6964ER1 2052ER5 10709ER5; 2731ER1 9926ER1 9466ER1 6042ER1 2712ER5 9842ER5 9385ER5 5983ER5; 3652ER1 8114ER1 4605ER1 3616ER5 8043ER5 4558ER5; 5121ER1 2875ER1 5071ER5 2854ER5; 5598ER1 10914ER1 10966ER1 5545ER5 10822ER5 10874ER5; 6493ER1 554ER1 8522ER1 6434ER5 549ER5 8446ER5; 6515ER1 10210ER1 2604ER1 3679ER1 6663ER1 10226ER1 6490ER1 6042ER1 6456ER5 10140ER5 6431ER5 5983ER5 10125ER5 2586ER5 3643ER5 6603ER5; 8516ER1 5118ER1 8440ER5 5068ER5; Sex Differences; 10319ER1 10229ER5; 10890ER1 554ER1 8522ER1 10798ER5 549ER5 8446ER5; 10900ER1 10808ER5; 10918ER1 10826ER5
ER  - 



TY  - RPRT
T1  - An Investigation of Restricted Self-Adapted Testing
AN  - 62872992; ED358153
AB  - A new testing strategy that provides protection against the problem of having examinees in adaptive testing choose difficulty levels that are not matched to their proficiency levels was introduced and evaluated. The method, termed restricted self-adapted testing (RSAT), still provides examinees with a degree of control over the difficulty levels of their test items. The range of item choice is restricted to a region around the examinee's current proficiency estimate. Participants in this study were 186 students in grades 3 through 8 in the Portland (Oregon) Public School system during the winter of 1992-93, who were tested as part of an ongoing computerized adaptive testing program. Students were randomly assigned to a computerized adaptive test (CAT), a self-adaptive test (SADT), or RSAT in mathematics. Results indicate no differences between CAT and SADT conditions in terms of mean proficiency and mean posttest state anxiety. The basic RSAT method appears to hold promise for providing examinees with control over the testing situation, while preventing large mismatches between item difficulty choice and proficiency level. The RSAT procedure should be evaluated empirically. (SLD)
AU  - Wise, Steven L.
AU  - And Others
Y1  - 1993/04//
PY  - 1993
DA  - Apr 1993
SP  - 1
EP  - 13
KW  - Portland School District OR
KW  - Restricted Self Adapted Testing
KW  - ERIC, Resources in Education (RIE)
KW  - Elementary School Students
KW  - Achievement Tests
KW  - Selection
KW  - Elementary Education
KW  - Pretests Posttests
KW  - Adaptive Testing
KW  - Comparative Testing
KW  - Mathematics Tests
KW  - Testing Problems
KW  - Computer Assisted Testing
KW  - Test Items
KW  - Difficulty Level
KW  - Mathematics Achievement
KW  - Test Anxiety
UR  - https://www.proquest.com/reports/investigation-restricted-self-adapted-testing/docview/62872992/se-2?accountid=10957
L2  - http://www.redi-bw.de/links/unifm?url_ver=Z39.88-2004&rft_val_fmt=info:ofi/fmt:kev:mtx:book&genre=report&sid=ProQ:ProQ%3Aeric&atitle=&title=An+Investigation+of+Restricted+Self-Adapted+Testing&issn=&date=1993-04-01&volume=&issue=&spage=1&au=Wise%2C+Steven+L.%3BAnd+Others&isbn=&jtitle=&btitle=An+Investigation+of+Restricted+Self-Adapted+Testing&rft_id=info:eric/ED358153&rft_id=info:doi/
LA  - English
DB  - ERIC
N1  - Zuletzt aktualisiert - 2021-02-11
ER  - 



TY  - RPRT
T1  - The Role of Anxiety in Examinee Preference for Self-Adapted Testing
AN  - 62867202; ED358154
AB  - This study assessed whether providing examinees with a choice between computerized adaptive testing (CAT) and self-adaptive testing (SAT) affects test performance in comparison with being assigned a CAT or SAT, and evaluated variables influencing examinee choice of either test form. The relative influences of test type and test choice on examinee anxiety were also examined. Subjects were 244 undergraduate and 133 graduate students from a large midwestern university. Students were randomly assigned to SAT, CAT, and choice conditions for an algebra test. Test-related anxiety was assessed with a paper-and-pencil measure in pretests and posttests. It was found that, for students with high mathematics anxiety, providing a choice between CAT and SAT led to significantly higher mean proficiency estimates, lending support to the hypothesis that examinees can cope with a stressful situation more effectively if they feel that they have some control over the source of the stress. Expected differences in estimated proficiency and posttest state anxiety between CAT and SAT conditions were not found, but a strong relationship was seen between examinee test type choice and mathematics anxiety level. Higher anxiety examinees have a greater preference for the control provided by SAT. Six tables and two graphs summarize findings. (SLD)
AU  - Wise, Steven L.
AU  - And Others
Y1  - 1993/04//
PY  - 1993
DA  - Apr 1993
SP  - 1
EP  - 18
KW  - Preference Patterns
KW  - Self Adapted Testing
KW  - ERIC, Resources in Education (RIE)
KW  - Ability
KW  - Test Selection
KW  - Higher Education
KW  - Student Attitudes
KW  - Pretests Posttests
KW  - Adaptive Testing
KW  - Comparative Testing
KW  - Mathematics Anxiety
KW  - Algebra
KW  - Test Format
KW  - College Students
KW  - Mathematics Tests
KW  - Computer Assisted Testing
KW  - Mathematics Achievement
KW  - Test Anxiety
UR  - https://www.proquest.com/reports/role-anxiety-examinee-preference-self-adapted/docview/62867202/se-2?accountid=10957
L2  - http://www.redi-bw.de/links/unifm?url_ver=Z39.88-2004&rft_val_fmt=info:ofi/fmt:kev:mtx:book&genre=report&sid=ProQ:ProQ%3Aeric&atitle=&title=The+Role+of+Anxiety+in+Examinee+Preference+for+Self-Adapted+Testing&issn=&date=1993-04-01&volume=&issue=&spage=1&au=Wise%2C+Steven+L.%3BAnd+Others&isbn=&jtitle=&btitle=The+Role+of+Anxiety+in+Examinee+Preference+for+Self-Adapted+Testing&rft_id=info:eric/ED358154&rft_id=info:doi/
LA  - English
DB  - ERIC
N1  - Zuletzt aktualisiert - 2021-02-11
ER  - 



TY  - THES
T1  - A comparison of computerized adaptive and paper-and-pencil tests on reliability and susceptibility to testwiseness
AN  - 304079877
AB  - Advances in the application of item response theory and the availability of powerful microcomputers have made computerized adaptive testing (CAT) a reality. Two issues that need to be considered before large-scale applications of computerized adaptive tests are (1) reliability of CAT scores, and (2) their susceptibility to factors that contaminate test scores.    Testwiseness has been shown to have a significant effect on P&P scores. The effects of TW on CAT scores have not been investigated. The purpose of this study was to compare the effects of TW on CAT and P&P tests. Reliability of a CAT and a P&P version of a test was also compared.    Subjects were 120 students at a large university. Half received TW training as part of their course content; half received no TW training. Subjects were randomly assigned to take a CAT or a P&P math test. They were tested again on the same test after three weeks. They also responded to a questionnaire about their attitudes toward the test and TW strategies they had used. It was hypothesized that TW training would be more effective in improving P&P test scores than CAT scores.    Contrary to expectations, results showed no significant effects for TW training. There is evidence to suggest that the group without TW training may have been more testwise than the TW training group initially. Mode effects were significant for test scores (higher on P&P than on CAT) and for attitudes toward the test (more positive toward CAT).    Correlation between the two sets of CAT scores was low (.29). Test-retest reliability was high (.81) for the P&P test. The low reliability of CAT in this study may be attributed to the inappropriateness of the difficulty level of the test for the subjects, inconsistencies in subjects' responses due to lack of motivation, and termination of the test at ability estimates greater than 4 or smaller than $-$4. It is suggested that with increased usage of CATs, new test-taking strategies may emerge that are only relevant to the CAT.
JF  - ProQuest Dissertations and Theses
AU  - Catterson, Shirin Khosropour
A3  - Shermis, Mark D.
A3  - Dodd, Barbara G.
Y1  - 1993
PY  - 1993
DA  - 1993
SP  - 144
CY  - United States -- Texas
PB  - The University of Texas at Austin
PP  - United States -- Texas
SN  - 979-8-209-11840-4
KW  - Education
KW  - computerized adaptive testing
KW  - item response theory
KW  - Educational evaluation
KW  - Educational technology
KW  - 0710:Educational technology
KW  - 0288:Educational tests & measurements
UR  - https://www.proquest.com/dissertations-theses/comparison-computerized-adaptive-paper-pencil/docview/304079877/se-2?accountid=10957
L2  - http://www.redi-bw.de/links/unifm?url_ver=Z39.88-2004&rft_val_fmt=info:ofi/fmt:kev:mtx:dissertation&genre=dissertations&sid=ProQ:ProQuest+Dissertations+%26+Theses+Global&atitle=&title=A+comparison+of+computerized+adaptive+and+paper-and-pencil+tests+on+reliability+and+susceptibility+to+testwiseness&issn=&date=1993-01-01&volume=&issue=&spage=&au=Catterson%2C+Shirin+Khosropour&isbn=979-8-209-11840-4&jtitle=&btitle=&rft_id=info:eric/&rft_id=info:doi/
LA  - English
DB  - ProQuest Dissertations & Theses Global
N1  - Copyright - Database copyright ProQuest LLC; ProQuest does not claim copyright in the individual underlying works.
N1  - Zuletzt aktualisiert - 2022-02-28
M3  - Ph.D.
M1  - 9323355
ER  - 



TY  - THES
T1  - Optimal test designs with content balancing and variable target information functions as constraints
AN  - 304058330
AB  - Optimal test design involves the application of an item selection heuristic to construct a test to fit the target information function in order that the standard error of the test can be controlled at different regions of the ability continuum. The real data simulation study assessed the efficiency of binary programming in optimal item selection by comparing the degree in which the obtained test information was approximated to different target information functions with a manual heuristic. The effects of imposing a content balancing constraint was studied in conventional, two-stage and adaptive tests designed using the automated procedure.    Results showed that the automated procedure improved upon the manual procedure significantly when a uniform target information function was used. However, when a peaked target information function was used, the improvement over the manual procedure was marginal. Both procedures were affected by the distribution of the item parameters in the item pool.    The degree in which the examinee empirical scores were recovered was lower when a content balancing constraint was imposed in the conventional test designs. The effect of uneven item parameter distribution in the item pool was shown by the poorer recovery of the empirical scores at the higher regions of the ability continuum. Two-stage tests were shown to limit the effects of content balancing. Content balanced adaptive tests using optimal item selection was shown to be efficient in empirical score recovery, especially in maintaining equiprecision in measurement over a wide ability range despite the imposition of content balancing constraint in the test design.    The study had implications for implementing automated test designs in the school systems supported by hardware and expertise in measurement theory and addresses the issue of content balancing using optimal test designs within an adaptive testing framework.
JF  - ProQuest Dissertations and Theses
AU  - Lam, Tit Loong
A3  - Swaminathan, Hariharan
Y1  - 1993
PY  - 1993
DA  - 1993
SP  - 116
CY  - United States -- Massachusetts
PB  - University of Massachusetts Amherst
PP  - United States -- Massachusetts
SN  - 979-8-208-35588-6
KW  - Education
KW  - design constraints
KW  - Educational evaluation
KW  - Educational tests & measurements
KW  - 0443:Educational evaluation
KW  - 0288:Educational tests & measurements
UR  - https://www.proquest.com/dissertations-theses/optimal-test-designs-with-content-balancing/docview/304058330/se-2?accountid=10957
L2  - http://www.redi-bw.de/links/unifm?url_ver=Z39.88-2004&rft_val_fmt=info:ofi/fmt:kev:mtx:dissertation&genre=dissertations&sid=ProQ:ProQuest+Dissertations+%26+Theses+Global&atitle=&title=Optimal+test+designs+with+content+balancing+and+variable+target+information+functions+as+constraints&issn=&date=1993-01-01&volume=&issue=&spage=&au=Lam%2C+Tit+Loong&isbn=979-8-208-35588-6&jtitle=&btitle=&rft_id=info:eric/&rft_id=info:doi/
LA  - English
DB  - ProQuest Dissertations & Theses Global
N1  - Copyright - Database copyright ProQuest LLC; ProQuest does not claim copyright in the individual underlying works.
N1  - Zuletzt aktualisiert - 2022-10-29
M3  - Ed.D.
M1  - 9316686
ER  - 



TY  - THES
T1  - Effects of computerized adaptive test anxiety on nursing licensure examination
AN  - 304041739
AB  - Purpose of study. This study was to determine if nursing candidates for licensure experienced an increase in anxiety due to the administration of an examination by computerized adaptive testing (CAT). A second purpose was to discover if certain groups were "at risk" of experiencing computer test anxiety.    Method. This study was a two-group, experimental design conducted concurrently with the CAT-NCLEX simulation. The National Council of State Boards of Nursing selected practical nursing graduates and repeat examinees to participate in a field test of the Computerized Adaptive Testing of the National Council Licensure Examination for Practical Nurses (NCLEX-PN). The experimental group for this study participated in the CAT simulation activity prior to taking NCLEX-PN and the control group did so after NCLEX-PN.    Both groups completed a 17 item Self-Report Anxiety Questionnaire. The responses were summed; the total score was considered to be a measure of anxiety.    Findings. Analysis of the data indicated that there was no statistically significant difference in computer test anxiety between the two groups. This is not to infer that there was no computer test anxiety expressed but rather that there was virtually no difference in the level of anxiety reported by the experimental group and the control group.    Further analysis of data indicated that persons with previous computer experience reported higher levels of computer test anxiety than those without previous computer experience and that females reported themselves as experiencing higher levels of computer test anxiety than did males. The results did not indicate that minorities experienced higher levels of computer test anxiety than non-minority groups. Lastly, there was no correlation between nursing grade point average or age and levels of computer test anxiety.    Conclusion. Participation in a computerized testing simulation did not affect the level of computer test anxiety. The only groups to experience higher levels of computer test anxiety are females and those with previous computer experience.
JF  - ProQuest Dissertations and Theses
AU  - Arrowood, Vada Ellis
A3  - Echternacht, Lonnie
Y1  - 1993
PY  - 1993
DA  - 1993
SP  - 127
CY  - United States -- Missouri
PB  - University of Missouri - Columbia
PP  - United States -- Missouri
SN  - 979-8-208-61100-5
KW  - Health and environmental sciences
KW  - Education
KW  - test anxiety
KW  - Educational evaluation
KW  - Educational psychology
KW  - Nursing licensure
KW  - Tests
KW  - Educational tests & measurements
KW  - Anxiety
KW  - Nursing
KW  - 0443:Educational evaluation
KW  - 0569:Nursing
KW  - 0525:Educational psychology
KW  - 0288:Educational tests & measurements
UR  - https://www.proquest.com/dissertations-theses/effects-computerized-adaptive-test-anxiety-on/docview/304041739/se-2?accountid=10957
L2  - http://www.redi-bw.de/links/unifm?url_ver=Z39.88-2004&rft_val_fmt=info:ofi/fmt:kev:mtx:dissertation&genre=dissertations&sid=ProQ:ProQuest+Dissertations+%26+Theses+Global&atitle=&title=Effects+of+computerized+adaptive+test+anxiety+on+nursing+licensure+examination&issn=&date=1993-01-01&volume=&issue=&spage=&au=Arrowood%2C+Vada+Ellis&isbn=979-8-208-61100-5&jtitle=&btitle=&rft_id=info:eric/&rft_id=info:doi/
LA  - English
DB  - ProQuest Dissertations & Theses Global
N1  - Copyright - Database copyright ProQuest LLC; ProQuest does not claim copyright in the individual underlying works.
N1  - Zuletzt aktualisiert - 2022-10-29
M3  - Ed.D.
M1  - 9404945
ER  - 



TY  - JOUR
T1  - Computerized Adaptive Testing with Different Groups
AN  - 62957492; EJ448041
AB  - Three computerized adaptive tests (CATs) in mathematics, reading, and writing were administered to 628 community college students to determine whether examinees of different ethnic, gender, ability, and age groups, and computer experience were differentially affected. Some differences exist; however, they do not preclude use of CATs. (SLD)
JF  - Educational Measurement: Issues and Practice
AU  - Legg, Sue M.
AU  - Buhr, Dianne C.
Y1  - 1992///Jul 1992 - Sep
PY  - 1992
DA  - Jul 1992 - Sep 1992
SP  - 23
EP  - 27
VL  - 11
IS  - 2
SN  - 0731-1745, 0731-1745
KW  - ERIC, Current Index to Journals in Education (CIJE)
KW  - Ability
KW  - Higher Education
KW  - Group Membership
KW  - Sex Differences
KW  - Age Differences
KW  - Reading Tests
KW  - Community Colleges
KW  - Test Bias
KW  - Writing Tests
KW  - Adaptive Testing
KW  - Comparative Testing
KW  - Ethnic Groups
KW  - College Students
KW  - Mathematics Tests
KW  - Computer Assisted Testing
KW  - Computer Literacy
UR  - https://www.proquest.com/scholarly-journals/computerized-adaptive-testing-with-different/docview/62957492/se-2?accountid=10957
L2  - http://www.redi-bw.de/links/unifm?url_ver=Z39.88-2004&rft_val_fmt=info:ofi/fmt:kev:mtx:journal&genre=article&sid=ProQ:ProQ%3Aeric&atitle=Computerized+Adaptive+Testing+with+Different+Groups&title=Educational+Measurement%3A+Issues+and+Practice&issn=07311745&date=1992-07-01&volume=11&issue=2&spage=23&au=Legg%2C+Sue+M.%3BBuhr%2C+Dianne+C.&isbn=&jtitle=Educational+Measurement%3A+Issues+and+Practice&btitle=&rft_id=info:eric/EJ448041&rft_id=info:doi/
LA  - English
DB  - ERIC
N1  - Zuletzt aktualisiert - 2021-02-11
N1  - SubjectsTermNotLitGenreText - 2ER1 1ER5; Adaptive Testing 140ER1 10919ER1 6526ER1 6663ER1 138ER5 10827ER5 6467ER5 6603ER5; 317ER1 5121ER1 2875ER1 314ER5 5071ER5 2854ER5; College Students 1829ER1 10411ER1 8114ER1 4605ER1 1815ER5 10321ER5 8043ER5 4558ER5; 1911ER1 8549ER1 1837ER1 9420ER1 5308ER1 11206ER1 1897ER5 8473ER5 1823ER5 9339ER5 5258ER5 11114ER5; Comparative Testing 1987ER1 10919ER1 6526ER1 6663ER1 1973ER5 10827ER5 6467ER5 6603ER5; Computer Assisted Testing 2043ER1 2099ER1 2098ER1 10811ER1 10919ER1 6526ER1 6663ER1 2029ER5 2085ER5 2084ER5 10719ER5 10827ER5 6467ER5 6603ER5; 2066ER1 10801ER1 6964ER1 2052ER5 10709ER5; 3652ER1 8114ER1 4605ER1 3616ER5 8043ER5 4558ER5; 4589ER1 4542ER5; 4809ER1 8147ER1 3192ER1 4761ER5 8075ER5 3164ER5; 6503ER1 10925ER1 6527ER1 6444ER5 10833ER5 6468ER5; 8774ER1 11371ER1 10925ER1 6527ER1 8698ER5 11279ER5 10833ER5 6468ER5; Sex Differences; Test Bias 10891ER1 955ER1 10799ER5 946ER5; 11782ER1 11371ER1 10925ER1 6527ER1 11685ER5 11279ER5 10833ER5 6468ER5
ER  - 



TY  - RPRT
T1  - Test Anxiety and Test Performance Under Computerized Adaptive Testing Methods
AN  - 62934912; ED344910
AB  - Little research exists on the psychological impacts of computerized adaptive testing (CAT) and how it may affect test performance. Three CAT procedures were examined, in which items were selected to match students' achievement levels, from the item pool at random, or according to student choice of item difficulty levels. Twenty-four graduate students (5 males and 19 females) at Indiana University (Richmond) were randomly assigned to one of six testing orders formed by the three mastery test approaches and by blocking on native and non-native speakers. While at a computer, students received a description of adaptive testing methods and then a 20-item pre-test anxiety measure. Right after completing each test, students responded to a 10-item in-test anxiety scale, ranked their preferences among tests, and evaluated their performance on each of the tests. No statistically significant mean differences were found among mean student achievement scores or among in-test anxiety means under the three adaptive testing methods. Students reporting higher anxiety scored significantly higher in the matched-selection test. Those preferring the matched-selection and self-selection tests the most were less anxious during those tests. Instead of actual performance, students' perceptions of how well they did were significantly correlated with preference rankings for the tests. The matched-selection tests required significantly fewer items to reach decisions than did the random-selection tests. Ten tables, 1 figure, and a 19-item list of references are included. (Author/SLD)
AU  - Powell, Z. Emily
Y1  - 1992/04//
PY  - 1992
DA  - Apr 1992
SP  - 1
EP  - 34
KW  - Psychological Influences
KW  - Testing Effects
KW  - ERIC, Resources in Education (RIE)
KW  - Higher Education
KW  - Mastery Tests
KW  - Test Results
KW  - Scores
KW  - Student Attitudes
KW  - Adaptive Testing
KW  - Comparative Testing
KW  - Graduate Students
KW  - Computer Assisted Testing
KW  - Academic Achievement
KW  - Test Items
KW  - Test Anxiety
UR  - https://www.proquest.com/reports/test-anxiety-performance-under-computerized/docview/62934912/se-2?accountid=10957
L2  - http://www.redi-bw.de/links/unifm?url_ver=Z39.88-2004&rft_val_fmt=info:ofi/fmt:kev:mtx:book&genre=report&sid=ProQ:ProQ%3Aeric&atitle=&title=Test+Anxiety+and+Test+Performance+Under+Computerized+Adaptive+Testing+Methods&issn=&date=1992-04-01&volume=&issue=&spage=1&au=Powell%2C+Z.+Emily&isbn=&jtitle=&btitle=Test+Anxiety+and+Test+Performance+Under+Computerized+Adaptive+Testing+Methods&rft_id=info:eric/ED344910&rft_id=info:doi/
LA  - English
DB  - ERIC
N1  - Zuletzt aktualisiert - 2021-02-11
ER  - 



TY  - RPRT
T1  - A Study of Black At-Risk Urban Youth Using Computer Assisted Testing
AN  - 62942037; ED348024
AB  - The objectives of this study were to develop, implement, and evaluate the year long project, Microcomputer Adaptive Testing High-Risk Urban Students (MATH-R-US). The project produced diagnostic software to meet the following criteria: (1) help students obtain high school mathematics credit needed for graduation; (2) motivate students to learn mathematics; (3) account for erratic student attendance; and (4) use computer adaptive testing as an integral part of the program. The project was used for an entire school year by a class in an urban high school with an at-risk predominantly black population and a high rate of absenteeism. The tests, which accept generative responses rather than multiple choice answers, were administered once a week in the school's computer lab. The results of each test were saved and practice sheets, with answer keys, were generated for the missed objectives. The program was evaluated to improve implementation and furnish descriptive data to the classroom teachers and school administrators. It was found that the program generated intense student competition to see who could get the most hamburger graphics--which appeared on the screen when students completed a test with 100% accuracy--in an hour. Both males and female students expressed positive attitudes about the course components, but female responses reflected more confidence in their own abilities. Computer math test scores indicated a consistent improvement on retesting of a topic, with 23 perfect scores on 43 retests. A discussion of the implications of this study concludes the paper. (2 tables, 22 references) (BBM)
AU  - Signer, Barbara R.
Y1  - 1992/02//
PY  - 1992
DA  - Feb 1992
SP  - 1
EP  - 11
KW  - ERIC, Resources in Education (RIE)
KW  - Diagnostic Teaching
KW  - Urban Areas
KW  - Diagnostic Tests
KW  - Mathematics Instruction
KW  - High Schools
KW  - Computer Assisted Testing
KW  - High Risk Students
KW  - Academic Achievement
KW  - Microcomputers
KW  - Student Attitudes
UR  - https://www.proquest.com/reports/study-black-at-risk-urban-youth-using-computer/docview/62942037/se-2?accountid=10957
L2  - http://www.redi-bw.de/links/unifm?url_ver=Z39.88-2004&rft_val_fmt=info:ofi/fmt:kev:mtx:book&genre=report&sid=ProQ:ProQ%3Aeric&atitle=&title=A+Study+of+Black+At-Risk+Urban+Youth+Using+Computer+Assisted+Testing&issn=&date=1992-02-01&volume=&issue=&spage=1&au=Signer%2C+Barbara+R.&isbn=&jtitle=&btitle=A+Study+of+Black+At-Risk+Urban+Youth+Using+Computer+Assisted+Testing&rft_id=info:eric/ED348024&rft_id=info:doi/
LA  - English
DB  - ERIC
N1  - SuppNotes - In: Proceedings of Selected Research and Development Presentations at the Convention of the Association for Educational Communications and Technology and Sponsored by the Research and Theory Division; see IR 015 706.
N1  - Zuletzt aktualisiert - 2021-02-11
ER  - 



TY  - RPRT
T1  - Correlates of Examinee Item Choice Behavior in Self-Adapted Testing
AN  - 63001252; ED331889
AB  - The strategies examinees employ when making item difficulty level choices in self-adapted computerized testing were investigated. Subjects were 148 college students (88 females and 60 males) in an introductory statistics course. The primary instrument was a self-adapted computerized algebra test used to measure student readiness for the statistics course. Each examinee was administered 20 items from a pool of 93. Students rated their self-efficacy before the test and were administered measures of mathematics anxiety and test anxiety. Inspection of each student's data file provided an indicator of selection strategy. Examinees who chose a more difficult first test item expressed greater capability and higher confidence, reported less anxiety just prior to testing, and less anxiety about mathematics in general. When selecting additional items, examinees tended toward what was termed a sluggishly flexible strategy; they chose more difficult items after passing an item or string of items, and chose less difficult items after failing a single item or string of items. The most frequent choice was to remain at the same level. Results indicate that self-adaptive testing may be a viable alternative to computerized adaptive testing. Two figures and two tables contain data from the study. (SLD)
AU  - Johnson, Phillip L.
AU  - And Others
Y1  - 1991/04//
PY  - 1991
DA  - Apr 1991
SP  - 1
EP  - 15
KW  - Self Adapted Testing
KW  - ERIC, Resources in Education (RIE)
KW  - Correlation
KW  - Higher Education
KW  - Self Efficacy
KW  - Statistics
KW  - Adaptive Testing
KW  - Mathematics Anxiety
KW  - Test Construction
KW  - Algebra
KW  - College Students
KW  - Mathematics Tests
KW  - Computer Assisted Testing
KW  - Item Banks
KW  - Test Items
KW  - Difficulty Level
KW  - Test Anxiety
UR  - https://www.proquest.com/reports/correlates-examinee-item-choice-behavior-self/docview/63001252/se-2?accountid=10957
L2  - http://www.redi-bw.de/links/unifm?url_ver=Z39.88-2004&rft_val_fmt=info:ofi/fmt:kev:mtx:book&genre=report&sid=ProQ:ProQ%3Aeric&atitle=&title=Correlates+of+Examinee+Item+Choice+Behavior+in+Self-Adapted+Testing&issn=&date=1991-04-01&volume=&issue=&spage=1&au=Johnson%2C+Phillip+L.%3BAnd+Others&isbn=&jtitle=&btitle=Correlates+of+Examinee+Item+Choice+Behavior+in+Self-Adapted+Testing&rft_id=info:eric/ED331889&rft_id=info:doi/
LA  - English
DB  - ERIC
N1  - Zuletzt aktualisiert - 2021-02-11
ER  - 



TY  - RPRT
T1  - Equivalence of Rasch Item Calibrations and Ability Estimates across Modes of Administration
AN  - 62571708; ED400281
AB  - The equivalence of pencil and paper Rasch item calibrations when used in a computer adaptive test administration was explored in this study. Items (n=726) were precalibarted with the pencil and paper test administrations. A computer adaptive test was administered to 321 medical technology students using the pencil and paper precalibrations in the item selection algorithms and in the computation of examinee ability estimates. The response data from the computer adaptive test administration were analyzed yielding recalibrated item difficulties and examinee ability estimates. Item precalibrations were compared with item recalibrations. Examinee ability estimates obtained using the item precalibrations on the computer adaptive administration were compared with the examinee ability estimates obtained from using the item recalibrations on the computer adaptive administration. The correlation for examinee ability estimates was 0.99 and for item correlations it was 0.90. Some item calibrations shifted but most remained consistent within the limits of error. Item shift, however, did not affect the ordering of examinee ability estimates. (Contains 1 table, 3 figures, and 23 references.) (Author/SLD)
AU  - Bergstrom, Betty A.
AU  - Lunz, Mary E.
Y1  - 1991/04//
PY  - 1991
DA  - Apr 1991
SP  - 1
EP  - 21
KW  - Item Selection
KW  - Rasch Model
KW  - Paper and Pencil Tests
KW  - Calibration
KW  - ERIC, Resources in Education (RIE)
KW  - Ability
KW  - Adaptive Testing
KW  - Algorithms
KW  - Test Format
KW  - Estimation (Mathematics)
KW  - Computer Assisted Testing
KW  - Medical Technologists
KW  - Item Response Theory
UR  - https://www.proquest.com/reports/equivalence-rasch-item-calibrations-ability/docview/62571708/se-2?accountid=10957
L2  - http://www.redi-bw.de/links/unifm?url_ver=Z39.88-2004&rft_val_fmt=info:ofi/fmt:kev:mtx:book&genre=report&sid=ProQ:ProQ%3Aeric&atitle=&title=Equivalence+of+Rasch+Item+Calibrations+and+Ability+Estimates+across+Modes+of+Administration&issn=&date=1991-04-01&volume=&issue=&spage=1&au=Bergstrom%2C+Betty+A.%3BLunz%2C+Mary+E.&isbn=&jtitle=&btitle=Equivalence+of+Rasch+Item+Calibrations+and+Ability+Estimates+across+Modes+of+Administration&rft_id=info:eric/ED400281&rft_id=info:doi/
LA  - English
DB  - ERIC
N1  - Zuletzt aktualisiert - 2021-02-11
ER  - 



TY  - RPRT
T1  - Individual Differences in Computer Adaptive Testing: Anxiety, Computer Literacy and Satisfaction
AN  - 62563471; ED400285
AB  - The relationship of several individual differences variables to Computer Adaptive Testing (CAT) as compared with traditional written tests are explored. Seven hundred sixty-five examinees took a Computer Adaptive Test and two fixed-length written tests. Each examinee also answered a computer literacy inventory, a satisfaction questionnaire, and a test anxiety survey. Test anxiety was found to be a significant factor in performance on both of the written tests, but not on the CAT test. Anxiety was also found to be a significant factor on several of the items on the satisfaction questionnaire. Overall, significant factors that predict satisfaction with CAT testing included level of test anxiety, computer literacy, and test length (the CAT test varied in terms of the number of items administered). Results are discussed in terms of the political and practical implications of administering CAT tests as compared to administering traditional written tests. The results also indicate that some of the individual differences variables that have been found to affect performance on written tests are not significant in CAT. (Contains two tables and six references.) (Author/SLD)
AU  - Gershon, Richard C.
AU  - Bergstrom, Betty
Y1  - 1991/04//
PY  - 1991
DA  - Apr 1991
SP  - 1
EP  - 18
KW  - ERIC, Resources in Education (RIE)
KW  - Adaptive Testing
KW  - Test Length
KW  - Computer Assisted Testing
KW  - Satisfaction
KW  - Adults
KW  - Computer Literacy
KW  - Individual Differences
KW  - Test Anxiety
UR  - https://www.proquest.com/reports/individual-differences-computer-adaptive-testing/docview/62563471/se-2?accountid=10957
L2  - http://www.redi-bw.de/links/unifm?url_ver=Z39.88-2004&rft_val_fmt=info:ofi/fmt:kev:mtx:book&genre=report&sid=ProQ:ProQ%3Aeric&atitle=&title=Individual+Differences+in+Computer+Adaptive+Testing%3A+Anxiety%2C+Computer+Literacy+and+Satisfaction&issn=&date=1991-04-01&volume=&issue=&spage=1&au=Gershon%2C+Richard+C.%3BBergstrom%2C+Betty&isbn=&jtitle=&btitle=Individual+Differences+in+Computer+Adaptive+Testing%3A+Anxiety%2C+Computer+Literacy+and+Satisfaction&rft_id=info:eric/ED400285&rft_id=info:doi/
LA  - English
DB  - ERIC
N1  - SuppNotes - Paper presented at the Annual Meeting of the National Council on Measurement in Education (San Francisco, CA, April 21-23, 1991).
N1  - Zuletzt aktualisiert - 2021-02-11
ER  - 



TY  - JOUR
T1  - CAI and At-Risk Minority Urban High School Students
AN  - 62920430; EJ443393
AB  - The Microcomputer Adaptive Testing High-Risk-Urban Students (MATH-R-US) project made computerized assessment an integral part of remedial high school mathematics at an inner-city school with predominantly African-American students. Results suggest that the girls exhibited greater self-esteem toward using computers than did boys and that student-created competition generated high levels of motivation. (29 references) (DB)
JF  - Journal of Research on Computing in Education
AU  - Signer, Barbara R.
Y1  - 1991///Jan 1991 - Mar
PY  - 1991
DA  - Jan 1991 - Mar 1991
SP  - 189
EP  - 203
VL  - 24
IS  - 2
SN  - 0888-6504, 0888-6504
KW  - Student Surveys
KW  - ERIC, Current Index to Journals in Education (CIJE)
KW  - Tables (Data)
KW  - Mathematics Instruction
KW  - Sex Differences
KW  - Attitude Measures
KW  - Student Attitudes
KW  - Student Motivation
KW  - Minority Group Children
KW  - Computer Assisted Testing
KW  - High Risk Students
KW  - Secondary Education
KW  - High School Students
KW  - Urban Schools
KW  - Self Esteem
UR  - https://www.proquest.com/scholarly-journals/cai-at-risk-minority-urban-high-school-students/docview/62920430/se-2?accountid=10957
L2  - http://www.redi-bw.de/links/unifm?url_ver=Z39.88-2004&rft_val_fmt=info:ofi/fmt:kev:mtx:journal&genre=article&sid=ProQ:ProQ%3Aeric&atitle=CAI+and+At-Risk+Minority+Urban+High+School+Students&title=Journal+of+Research+on+Computing+in+Education&issn=08886504&date=1991-01-01&volume=24&issue=2&spage=189&au=Signer%2C+Barbara+R.&isbn=&jtitle=Journal+of+Research+on+Computing+in+Education&btitle=&rft_id=info:eric/EJ443393&rft_id=info:doi/
LA  - English
DB  - ERIC
N1  - Zuletzt aktualisiert - 2021-02-11
N1  - SubjectsTermNotLitGenreText - 738ER1 6527ER1 730ER5 6468ER5; 2043ER1 2099ER1 2098ER1 10811ER1 10919ER1 6526ER1 6663ER1 2029ER5 2085ER5 2084ER5 10719ER5 10827ER5 6467ER5 6603ER5; 4788ER1 4740ER5; 4803ER1 9534ER1 10411ER1 8114ER1 4605ER1 4755ER5 9453ER5 10321ER5 8043ER5 4558ER5; 6499ER1 5309ER1 6440ER5 5259ER5; 6796ER1 1491ER1 321ER1 8114ER1 4605ER1 6734ER5 1480ER5 318ER5 8043ER5 4558ER5; 9524ER1 3412ER1 3192ER1 9443ER5 3382ER5 3164ER5; 9584ER1 9566ER1 9503ER5 9485ER5; Sex Differences; 10309ER1 740ER1 10219ER5 732ER5; 10358ER1 6915ER1 10268ER5 6852ER5; 10564ER1 11444ER1 10473ER5 11348ER5; 11318ER1 9420ER1 5308ER1 11226ER5 9339ER5 5258ER5
ER  - 



TY  - THES
T1  - Test anxiety and test performance under computerized adaptive testing methods
AN  - 303923651
AB  - Research on computerized adaptive testing (CAT) has shown its potential to estimate student achievement accurately and efficiently. However, there is a paucity of research on the psychological impacts of CAT and how it may, consequently, affect test performance. Three kinds of computerized adaptive testing procedures were examined, in which items were selected in one of three ways: (a) to match students' estimated achievement levels (matched-selection), (b) from the item pool at random (random-selection), and (c) according to student choice of item difficulty levels (self-selection).    Twenty-four graduate students were randomly assigned to one of six possible testing orders formed by the three adaptive tests while blocking on native and nonnative speakers. While at a computer, students first received a description of adaptive testing methods, followed by a 20-item pre-test anxiety measure. The three adaptive tests were then given in the assigned random order. Immediately after each adaptive test, students responded to a 10-item in-test anxiety scale. Finally, they ranked their preferences and evaluated their performance on each of the three tests. Written student comments and questions regarding the adaptive testing methods were also solicited.    No statistically significant differences were found among mean student achievement scores, nor among in-test anxiety means, under the three adaptive testing methods. Students who reported higher pre-test anxiety were found to score significantly higher in the matched-selection test. Students who preferred the matched-selection and self-selection tests the most tended to be significantly less anxious during those tests. However, instead of students' actual test performance, it was their perception of how well they did that was significantly correlated with their preference rankings for the three tests. Though not central to this study, the matched-selection tests required significantly fewer items to reach mastery decisions than did the random-selection tests.
JF  - ProQuest Dissertations and Theses
AU  - Powell, Zen-Hsiu Emily
Y1  - 1991
PY  - 1991
DA  - 1991
SP  - 179
CY  - United States -- Indiana
PB  - Indiana University
PP  - United States -- Indiana
SN  - 979-8-207-01213-1
KW  - Education
KW  - Educational evaluation
KW  - Curricula
KW  - Teaching
KW  - Educational technology
KW  - 0727:Curriculum development
KW  - 0710:Educational technology
KW  - 0288:Educational tests & measurements
UR  - https://www.proquest.com/dissertations-theses/test-anxiety-performance-under-computerized/docview/303923651/se-2?accountid=10957
L2  - http://www.redi-bw.de/links/unifm?url_ver=Z39.88-2004&rft_val_fmt=info:ofi/fmt:kev:mtx:dissertation&genre=dissertations&sid=ProQ:ProQuest+Dissertations+%26+Theses+Global&atitle=&title=Test+anxiety+and+test+performance+under+computerized+adaptive+testing+methods&issn=&date=1991-01-01&volume=&issue=&spage=&au=Powell%2C+Zen-Hsiu+Emily&isbn=979-8-207-01213-1&jtitle=&btitle=&rft_id=info:eric/&rft_id=info:doi/
LA  - English
DB  - ProQuest Dissertations & Theses Global
N1  - Copyright - Database copyright ProQuest LLC; ProQuest does not claim copyright in the individual underlying works.
N1  - Zuletzt aktualisiert - 2022-11-23
M3  - Ph.D.
M1  - 9134822
ER  - 



TY  - THES
T1  - Application of an IRT-based response aberrancy statistic to personality trait measurement
AN  - 303860707
AB  - The purpose of this research was to explore psychometric issues pertinent to application of an IRT-based response aberrancy detection statistic in the domain of personality measurement. Monte Carlo and real-data analyses were conducted to address several issues regarding the Z$\sb{\rm L}$ response aberrancy statistic. The major issues explored were characteristics of the null distribution of the Z$\sb{\rm L}$ statistic, and its power to identify aberrant response patterns. In addition, the consistency of examinee Z$\sb{\rm L}$ level across half-tests, and the recovery of Z$\sb{\rm L}$ at different adaptive test lengths was examined. There were four main results. First, the Z$\sb{\rm L}$ index null distribution was not well standaradized when item parameters of personality scales were used; the Z$\sb{\rm L}$ null distribution variance was significantly less than the hypothesized value of 1.0 under several conditions. Second, the power of the Z$\sb{\rm L}$ statistic to detect response aberrancy was affected by the scoring method. It appeared that detection power was optimal when a biweight estimator of $\theta$ was used. Third, examinee Z$\sb{\rm L}$ level was not consistent across half-tests. This result implied that response aberrancy is not a reliable person characteristic within a particular trait domain. Finally, identifying aberrant response patterns in an adaptive testing context was problematic. A major problem was that the null distribution of the Z$\sb{\rm L}$ statistic was not consistent across adaptive test lengths. In the discussion, several recommendations are made regarding proper implementation of response aberrancy statistics in personality measurement.
JF  - ProQuest Dissertations and Theses
AU  - Reise, Steven Paul
Y1  - 1990
PY  - 1990
DA  - 1990
SP  - 152
CY  - United States -- Minnesota
PB  - University of Minnesota
PP  - United States -- Minnesota
SN  - 979-8-209-10693-7
KW  - Psychology
KW  - Psychological tests
KW  - Personality
KW  - Personality psychology
KW  - Quantitative psychology
KW  - 0625:Personality psychology
KW  - 0632:Quantitative psychology
UR  - https://www.proquest.com/dissertations-theses/application-irt-based-response-aberrancy/docview/303860707/se-2?accountid=10957
L2  - http://www.redi-bw.de/links/unifm?url_ver=Z39.88-2004&rft_val_fmt=info:ofi/fmt:kev:mtx:dissertation&genre=dissertations&sid=ProQ:ProQuest+Dissertations+%26+Theses+Global&atitle=&title=Application+of+an+IRT-based+response+aberrancy+statistic+to+personality+trait+measurement&issn=&date=1990-01-01&volume=&issue=&spage=&au=Reise%2C+Steven+Paul&isbn=979-8-209-10693-7&jtitle=&btitle=&rft_id=info:eric/&rft_id=info:doi/
LA  - English
DB  - ProQuest Dissertations & Theses Global
N1  - Copyright - Database copyright ProQuest LLC; ProQuest does not claim copyright in the individual underlying works.
N1  - Zuletzt aktualisiert - 2022-10-29
M3  - Ph.D.
M1  - 9107458
ER  - 



TY  - JOUR
T1  - Self-Adapted Testing: A Performance-Improving Variant of Computerized Adaptive Testing
AN  - 63173886; EJ364450
AB  - An experiment was conducted that contrasted a variant of computerized adaptive testing, self-adapted testing, with two traditional tests. Participants completed a self-report of text anxiety and were randomly assigned to take one of the three tests of verbal ability. Subjects generally chose more difficult items as the test progressed. (Author/LMO)
JF  - Journal of Educational Psychology
AU  - Rocklin, Thomas
AU  - O'Donnell, Angela M.
Y1  - 1987/09//
PY  - 1987
DA  - Sep 1987
SP  - 315
EP  - 19
VL  - 79
IS  - 3
KW  - Test Anxiety Inventory
KW  - Self Adapted Tests
KW  - Test Anxiety Inventory (Spielberger)
KW  - ERIC, Current Index to Journals in Education (CIJE)
KW  - Adaptive Testing
KW  - Comparative Testing
KW  - Higher Education
KW  - Computer Assisted Testing
KW  - Verbal Tests
KW  - Test Items
KW  - Difficulty Level
KW  - Test Anxiety
UR  - https://www.proquest.com/scholarly-journals/self-adapted-testing-performance-improving/docview/63173886/se-2?accountid=10957
L2  - http://www.redi-bw.de/links/unifm?url_ver=Z39.88-2004&rft_val_fmt=info:ofi/fmt:kev:mtx:journal&genre=article&sid=ProQ:ProQ%3Aeric&atitle=Self-Adapted+Testing%3A+A+Performance-Improving+Variant+of+Computerized+Adaptive+Testing&title=Journal+of+Educational+Psychology&issn=&date=1987-09-01&volume=79&issue=3&spage=315&au=Rocklin%2C+Thomas%3BO%27Donnell%2C+Angela+M.&isbn=&jtitle=Journal+of+Educational+Psychology&btitle=&rft_id=info:eric/EJ364450&rft_id=info:doi/
LA  - English
DB  - ERIC
N1  - Zuletzt aktualisiert - 2021-02-11
N1  - SubjectsTermNotLitGenreText - 140ER1 10919ER1 6526ER1 6663ER1 138ER5 10827ER5 6467ER5 6603ER5; 1987ER1 10919ER1 6526ER1 6663ER1 1973ER5 10827ER5 6467ER5 6603ER5; 2043ER1 2099ER1 2098ER1 10811ER1 10919ER1 6526ER1 6663ER1 2029ER5 2085ER5 2084ER5 10719ER5 10827ER5 6467ER5 6603ER5; 2883ER1 2862ER5; 4809ER1 8147ER1 3192ER1 4761ER5 8075ER5 3164ER5; 10890ER1 554ER1 8522ER1 10798ER5 549ER5 8446ER5; 10900ER1 10808ER5; 11371ER1 10925ER1 6527ER1 11279ER5 10833ER5 6468ER5
ER  - 



TY  - RPRT
T1  - Computerized Adaptive Testing: A Comparison of the Nominal Response Model and the Three Parameter Logistic Model
AN  - 63257550; ED281872
AB  - A nominal response model-based computerized adaptive testing procedure (nominal CAT) was implemented using simulated data. Ability estimates from the nominal CAT were compared to those from a CAT based upon the three-parameter logistic model (3PL CAT). Furthermore, estimates from both CAT procedures were compared with the known true abilities used to generate the simulated data. Results showed that the nominal CAT's ability estimates were highly correlated with those of the 3PL CAT as well as with the true abilities. Furthermore, the nominal CAT had a significantly higher association with negative true thetas than did the 3PL CAT, and it also had significantly lower standard errors of estimate than did the 3PL CAT. However, the nominal model-based CAT had difficulty estimating positive thetas and had a poor convergence rate. In contrast, the 3PL CAT had a high convergence rate and its performance was not affected by whether the true abilities were positive or negative. Potential reasons for the nominal CAT's high nonconvergence rate as well as implications for computerized adaptive testing were discussed. (Author)
AU  - DeAyala, R. J.
AU  - Koch, William R.
Y1  - 1987/04//
PY  - 1987
DA  - Apr 1987
SP  - 1
EP  - 20
KW  - Three Parameter Model
KW  - Nominal Response Model
KW  - ERIC, Resources in Education (RIE)
KW  - Researchers
KW  - Adaptive Testing
KW  - Comparative Testing
KW  - Predictive Measurement
KW  - Estimation (Mathematics)
KW  - Latent Trait Theory
KW  - Mathematics Tests
KW  - Computer Assisted Testing
KW  - Computer Simulation
KW  - Test Items
KW  - Mathematical Models
UR  - https://www.proquest.com/reports/computerized-adaptive-testing-comparison-nominal/docview/63257550/se-2?accountid=10957
L2  - http://www.redi-bw.de/links/unifm?url_ver=Z39.88-2004&rft_val_fmt=info:ofi/fmt:kev:mtx:book&genre=report&sid=ProQ:ProQ%3Aeric&atitle=&title=Computerized+Adaptive+Testing%3A+A+Comparison+of+the+Nominal+Response+Model+and+the+Three+Parameter+Logistic+Model&issn=&date=1987-04-01&volume=&issue=&spage=1&au=DeAyala%2C+R.+J.%3BKoch%2C+William+R.&isbn=&jtitle=&btitle=Computerized+Adaptive+Testing%3A+A+Comparison+of+the+Nominal+Response+Model+and+the+Three+Parameter+Logistic+Model&rft_id=info:eric/ED281872&rft_id=info:doi/
LA  - English
DB  - ERIC
N1  - Zuletzt aktualisiert - 2021-02-11
ER  - 



TY  - THES
T1  - COMPUTERIZED ADAPTIVE TESTING: A COMPARISON OF THE NOMINAL RESPONSE MODEL AND THE THREE PARAMETER MODEL
AN  - 303612757
AB  - Item Response Theory (IRT) is a relatively recent development in psychometrics that has emerged as an attractive alternative to classical test theory. This is primarily a result of IRT's potential for solving many long standing measurement problems. One important and very promising application of IRT is Computerized Adaptive Testing (CAT). CAT is a procedure for administering tests which are individually tailored for each examinee. A CAT system uses a particular IRT model in combination with test item characteristics to estimate the examinee's ability.    To date, all implementations of CATs have been built on models which require dichotomous scoring of test items. For the purpose of ability estimation these systems do not differentiate between an examinee's incorrect answer and other incorrect alternatives.    In contrast to the IRT models presently used in CAT implementations, the Nominal Response Model utilizes the information in the pattern of wrong responses for ability estimation. No assumptions are made about a particular response being more correct than another.    A Nominal Response Model-based CAT (nominal CAT) was implemented using real and simulated data. Comparisons were made between the nominal CAT's ability estimates and those of a CAT based upon the three-parameter logistic model (3PL CAT). In addition, estimates from the nominal CAT were compared with the known true ability estimates used to generate the simulated data. Results showed that the nominal CAT's ability estimates were highly correlated with those of the 3PL CAT as well as with the true abilities. Furthermore, the nominal CAT had a significantly higher association with the negative true abilities than did the 3PL CAT and it also had significantly lower standard errors of estimate than did the 3PL CAT. However, the nominal CAT had difficulty estimating positive abilities and had a poor convergence rate. In contrast, the 3PL CAT has a high convergence rate and its performance was not affected by whether the true abilities were positive or negative. Implications for computerized adaptive testing were discussed.
JF  - ProQuest Dissertations and Theses
AU  - DE AYALA, RAFAEL JAIME
Y1  - 1987
PY  - 1987
DA  - 1987
SP  - 306
CY  - United States -- Texas
PB  - The University of Texas at Austin
PP  - United States -- Texas
SN  - 979-8-206-85918-8
KW  - Psychology
KW  - Psychological tests
KW  - Quantitative psychology
KW  - 0632:Quantitative psychology
UR  - https://www.proquest.com/dissertations-theses/computerized-adaptive-testing-comparison-nominal/docview/303612757/se-2?accountid=10957
L2  - http://www.redi-bw.de/links/unifm?url_ver=Z39.88-2004&rft_val_fmt=info:ofi/fmt:kev:mtx:dissertation&genre=dissertations&sid=ProQ:ProQuest+Dissertations+%26+Theses+Global&atitle=&title=COMPUTERIZED+ADAPTIVE+TESTING%3A+A+COMPARISON+OF+THE+NOMINAL+RESPONSE+MODEL+AND+THE+THREE+PARAMETER+MODEL&issn=&date=1987-01-01&volume=&issue=&spage=&au=DE+AYALA%2C+RAFAEL+JAIME&isbn=979-8-206-85918-8&jtitle=&btitle=&rft_id=info:eric/&rft_id=info:doi/
LA  - English
DB  - ProQuest Dissertations & Theses Global
N1  - Copyright - Database copyright ProQuest LLC; ProQuest does not claim copyright in the individual underlying works.
N1  - Zuletzt aktualisiert - 2022-10-29
M3  - Ph.D.
M1  - 8728539
ER  - 



TY  - THES
T1  - An adaptive test of musical memory: An application of item response theory to the assessment of musical ability
AN  - 303589997
AB  - The purpose of this study was to develop and evaluate an item response theory-based adaptive test of musical ability designed to overcome common shortcomings of commercially available musical ability tests. These shortcomings include test inefficiency, poor reliability and validity, ceiling effects, and examinee fatigue and boredom. Because of its suitability for item response theory-based techniques and its history in the music test field, tonal memory was chosen as the specific musical skill to be assessed.    The research was conducted in three phases. The goal of the first phase was to develop item-prototypes for the adaptive test pool by determining combinations of instrumental timbre, rhythmic complexity, and tonality that provide unidimensional, unbiased and reasonably valid ability estimates. Four forms of a tonal memory test were administered to 125 college students. Results from a split-plot ANOVA of total test scores, a factor analysis of item clusters, and comparisons of test validity coefficients indicated that tonal and atonal items with synthesized timbre and varied rhythms were the most appropriate items to include in the adaptive test pool.    The goal of the second phase was to select a final set of adaptive test items and assess their reliability and validity. One of three forms of a 104-item tonal memory test was administered to each of 336 high school and college students. Each form contained items matching the prototypes identified in the first phase. One hundred eighty items from phases one and two that provided good discrimination across all musical ability levels were chosen for the final adaptive test pool. Results indicated that scores based on the adaptive test items provided more reliable and valid ability estimates than two widely used conventional tests, the Seashore Tonal Memory Test and the Drake Musical Memory Test.    The goal of the third phase was to assess the efficiency of the actual adaptive test by administering it to 20 examinees. An average of 6, 9 and 12 items was needed to obtain reliabilities of.80,.85, and.90 respectively.    Taken as a whole these results provide compelling evidence that many shortcomings of conventional music tests can be overcome by adaptive tests. Adaptive testing appears to hold great promise for significantly improving the assessment of musical abilities. The development of adaptive tests of other musical skills is recommended.
JF  - ProQuest Dissertations and Theses
AU  - Vispoel, Walter Peter
Y1  - 1987
PY  - 1987
DA  - 1987
SP  - 277
CY  - United States -- Illinois
PB  - University of Illinois at Urbana-Champaign
PP  - United States -- Illinois
SN  - 979-8-206-75891-7
KW  - Education
KW  - Educational evaluation
KW  - Music education
KW  - Educational tests & measurements
KW  - 0443:Educational evaluation
KW  - 0288:Educational tests & measurements
KW  - 0522:Music education
UR  - https://www.proquest.com/dissertations-theses/adaptive-test-musical-memory-application-item/docview/303589997/se-2?accountid=10957
L2  - http://www.redi-bw.de/links/unifm?url_ver=Z39.88-2004&rft_val_fmt=info:ofi/fmt:kev:mtx:dissertation&genre=dissertations&sid=ProQ:ProQuest+Dissertations+%26+Theses+Global&atitle=&title=An+adaptive+test+of+musical+memory%3A+An+application+of+item+response+theory+to+the+assessment+of+musical+ability&issn=&date=1987-01-01&volume=&issue=&spage=&au=Vispoel%2C+Walter+Peter&isbn=979-8-206-75891-7&jtitle=&btitle=&rft_id=info:eric/&rft_id=info:doi/
LA  - English
DB  - ProQuest Dissertations & Theses Global
N1  - Copyright - Database copyright ProQuest LLC; ProQuest does not claim copyright in the individual underlying works.
N1  - Zuletzt aktualisiert - 2022-10-29
M3  - Ph.D.
M1  - 8803227
ER  - 



TY  - THES
T1  - PRESSUREMETER CREEP TESTING IN LABORATORY ICE
AN  - 303532124
AB  - Single stage and multistage pressuremeter creep tests have been conducted in large, laboratory-prepared samples of polycrystalline ice at temperatures of -2(DEGREES)C. One purpose of the experimental program was to investigate the validity and applicability of two creep theories, the widely used simple power law theory (strain-hardening formulation) and the recently proposed modified second-order fluid model. Another purpose was to investigate the relationship between single stage and multistage pressuremeter creep tests, in the same stress range, and thus to deduce the effect of loading history on the creep parameters.    For both models, the creep information obtained from the multistage pressuremeter tests was found to compare very well with the information derived from single stage pressuremeter creep tests, both in terms of creep parameters and predicted long-term behaviour. The modified second-order fluid model, however, produced less scatter in the stress exponent n derived from the multistage tests than the simple power law model.    Through analysis of the multistage tests, it appears that the past history of applied stresses in a pressuremeter creep test has little effect on the nature of the creep; rather, the amount of accumulated strain appears to be the controlling factor.    For pressuremeter testing in ice or ice-rich frozen soils, it should be assumed that a steady-state creep condition will eventually prevail with continued straining. Each stress increment in a pressuremeter creep test should be applied until at least the steady-state condition is approached, as evidenced by a b (time exponent) of at least 0.90 or an exponentially increasing cavity radius with time. In order to attain the steady-state condition in a reasonable amount of time, a field multistage pressuremeter creep test may be started at any stress level.    With carefully run calibrations both before and after each test, the OYO Elastmeter-100 pressuremeter with an electronic radius measuring device performed exceedingly well in this experimental program. Analysis of the results, for the time being, should be conducted in terms of the simple power law creep theory in its steady-state form. Because it can represent both primary and secondary creep in the same motion equation and is valid for large strains, the modified second-order fluid model is preferable to the simple power law model. Solutions to selected boundary-value problems, however, must be solved before it can be used in practice.
JF  - ProQuest Dissertations and Theses
AU  - KJARTANSON, BRUCE HENRY
Y1  - 1986
PY  - 1986
DA  - 1986
SP  - 1
CY  - Canada -- Manitoba, CA
PB  - University of Manitoba (Canada)
PP  - Canada -- Manitoba, CA
SN  - 978-0-315-34020-6
KW  - Applied sciences
KW  - Geotechnology
KW  - 0428:Geotechnology
UR  - https://www.proquest.com/dissertations-theses/pressuremeter-creep-testing-laboratory-ice/docview/303532124/se-2?accountid=10957
L2  - http://www.redi-bw.de/links/unifm?url_ver=Z39.88-2004&rft_val_fmt=info:ofi/fmt:kev:mtx:dissertation&genre=dissertations&sid=ProQ:ProQuest+Dissertations+%26+Theses+Global&atitle=&title=PRESSUREMETER+CREEP+TESTING+IN+LABORATORY+ICE&issn=&date=1986-01-01&volume=&issue=&spage=&au=KJARTANSON%2C+BRUCE+HENRY&isbn=978-0-315-34020-6&jtitle=&btitle=&rft_id=info:eric/&rft_id=info:doi/
LA  - English
DB  - ProQuest Dissertations & Theses Global
N1  - Copyright - Database copyright ProQuest LLC; ProQuest does not claim copyright in the individual underlying works.
N1  - Zuletzt aktualisiert - 2022-10-28
M3  - Ph.D.
M1  - NL34020
ER  - 



TY  - GEN
T1  - Computer Application Issues in Certification and Licensure Testing
AN  - 63291950; ED261079
AB  - Computer adaptive testing systems are feasible for certification and licensure testing. This is in part due to the availability of extensive yet inexpensive computers. Modern item response theory, combined with computerized adaptive testing, yields a powerful new method of testing which provides greater accuracy and efficiency and less boredom for the examinee. The computer presents each item, scores it, and then selects the next item which is appropriate for the individual examinee; thus, individual testing is much more feasible than it would be using human examiners. There are additional advantages: acceptable accuracy can be achieved with fewer items; more accurate estimates can be obtained at the extreme ends of the ability continuum; test security is improved; tests can be administered on demand when needed; and a greater variety of item types may be included. With licensing and certification tests, it is important to obtain a good pool of test items, to be accurate near the minimum cutting score, to have appropriate unidimensionality, and to use computers in a comfortable environment. It is also useful to have a large number of examinees, approximately 1000 or more. Cost estimates are encouraging, especially when the computer systems are used often. (GDC)
AU  - Harnisch, Delwyn L.
Y1  - 1985/03//
PY  - 1985
DA  - Mar 1985
SP  - 1
EP  - 12
KW  - ERIC, Resources in Education (RIE)
KW  - Researchers
KW  - Test Reliability
KW  - Mastery Tests
KW  - Test Validity
KW  - Occupational Tests
KW  - Certification
KW  - Adaptive Testing
KW  - Test Length
KW  - Individual Testing
KW  - Test Construction
KW  - Cost Effectiveness
KW  - Latent Trait Theory
KW  - Testing Problems
KW  - Computer Assisted Testing
KW  - Item Banks
KW  - Item Analysis
KW  - Test Items
KW  - Test Reliability
KW  - Mastery Tests
KW  - Test Validity
KW  - Occupational Tests
KW  - Certification
KW  - Adaptive Testing
KW  - Test Length
KW  - Individual Testing
KW  - Test Construction
KW  - Cost Effectiveness
KW  - Latent Trait Theory
KW  - Testing Problems
KW  - Computer Assisted Testing
KW  - Item Banks
KW  - Item Analysis
KW  - Test Items
UR  - https://www.proquest.com/speeches-presentations/computer-application-issues-certification/docview/63291950/se-2?accountid=10957
L2  - http://www.redi-bw.de/links/unifm?url_ver=Z39.88-2004&rft_val_fmt=info:ofi/fmt:kev:mtx:journal&genre=unknown&sid=ProQ:ProQ%3Aeric&atitle=Computer+Application+Issues+in+Certification+and+Licensure+Testing&title=Undefined&issn=&date=1985-03-01&volume=&issue=&spage=1&au=Harnisch%2C+Delwyn+L.&isbn=&jtitle=Undefined&btitle=&rft_id=info:eric/ED261079&rft_id=info:doi/
LA  - English
DB  - ERIC
N1  - SuppNotes - Paper presented at the Annual Meeting of the American Educational Research Association (69th, Chicago, IL, March 31-April 4, 1985).
N1  - Zuletzt aktualisiert - 2021-02-11
ER  - 



TY  - RPRT
T1  - Computer-Based Measurement of Intellectual Capabilities. Final Report
AN  - 63350164; ED248258
AB  - During 1975-1979 this research into the potential of computerized adaptive testing to reduce errors in the measurement of human capabilities used Marine recruits for a live-testing validity comparison of computerized adaptive and conventional tests. The program purposes were to: (1) identify the most useful computer-based adaptive testing strategies; (2) identify testing conditions that maximize the positive rather than negative psychological effects of computerized testing; (3) investigate intra-individual multidimensionality problems in ability testing; (4) examine probabilistic responding and free-response methods for computerized adaptive testing in order to extract maximum information from each test item response; and (5) develop, refine and evaluate new computer administered ability tests for spacial, perceptive, memory, and other abilities not now measurable using paper and pencil testing. Monte Carlo and Bayesian adaptive testing methods were used in these studies. Fifteen major findings, primarily on adaptive testing and test administration conditions, and implications for further research are given. Abstracts of the 16 research reports for studies for this program are given. (BS)
AU  - Weiss, David J.
Y1  - 1983/12//
PY  - 1983
DA  - Dec 1983
SP  - 1
EP  - 30
KW  - Marine Corps
KW  - ERIC, Resources in Education (RIE)
KW  - Researchers
KW  - Ability
KW  - Test Theory
KW  - Measurement Techniques
KW  - Bayesian Statistics
KW  - Response Style (Tests)
KW  - Psychometrics
KW  - Monte Carlo Methods
KW  - Adaptive Testing
KW  - Individual Testing
KW  - Test Construction
KW  - Latent Trait Theory
KW  - Testing Problems
KW  - Computer Assisted Testing
KW  - Adults
UR  - https://www.proquest.com/reports/computer-based-measurement-intellectual/docview/63350164/se-2?accountid=10957
L2  - http://www.redi-bw.de/links/unifm?url_ver=Z39.88-2004&rft_val_fmt=info:ofi/fmt:kev:mtx:book&genre=report&sid=ProQ:ProQ%3Aeric&atitle=&title=Computer-Based+Measurement+of+Intellectual+Capabilities.+Final+Report&issn=&date=1983-12-01&volume=&issue=&spage=1&au=Weiss%2C+David+J.&isbn=&jtitle=&btitle=Computer-Based+Measurement+of+Intellectual+Capabilities.+Final+Report&rft_id=info:eric/ED248258&rft_id=info:doi/
LA  - English
DB  - ERIC
N1  - Zuletzt aktualisiert - 2021-02-11
ER  - 



TY  - RPRT
T1  - Bias and Information of Bayesian Adaptive Testing. Research Report 83-2
AN  - 63511192; ED230615
AB  - Monte Carlo simulation was used to investigate score bias and information characteristics of Owen's Bayesian adaptive testing strategy, and to examine possible causes of score bias. Factors investigated in three related studies included effects of item discrimination, effects of fixed vs. variable test length, and effects of an accurate prior theta estimate. Data were generated from a three-parameter logistic model for 3,100 simulees in each of eight data sets; Bayesian adaptive tests were administered, drawing items from a "perfect" item pool. The results indicate that theta estimates from Owen's Bayesian adaptive testing method are affected by the prior theta estimate used and that the method does not provide measurements that are unbiased and equiprecise except under the unrealistic condition of an accurate prior theta estimate. (Author/PN)
AU  - Weiss, David J.
AU  - McBride, James R.
Y1  - 1983/03//
PY  - 1983
DA  - Mar 1983
SP  - 1
EP  - 32
PB  - Computerized Adpative Testing Laboratory
KW  - Item Discrimination (Tests)
KW  - Bayesian Tailored Testing
KW  - Monte Carlo Studies
KW  - Bayesian Adaptive Ability Testing
KW  - ERIC, Resources in Education (RIE)
KW  - Estimation (Mathematics)
KW  - Bayesian Statistics
KW  - Scores
KW  - Test Bias
KW  - Research Methodology
KW  - Adaptive Testing
KW  - Test Length
KW  - Ability Identification
KW  - Latent Trait Theory
KW  - Simulation
KW  - Computer Assisted Testing
KW  - Item Banks
KW  - Test Items
UR  - https://www.proquest.com/reports/bias-information-bayesian-adaptive-testing/docview/63511192/se-2?accountid=10957
L2  - http://www.redi-bw.de/links/unifm?url_ver=Z39.88-2004&rft_val_fmt=info:ofi/fmt:kev:mtx:book&genre=report&sid=ProQ:ProQ%3Aeric&atitle=&title=Bias+and+Information+of+Bayesian+Adaptive+Testing.+Research+Report+83-2&issn=&date=1983-03-01&volume=&issue=&spage=1&au=Weiss%2C+David+J.%3BMcBride%2C+James+R.&isbn=&jtitle=&btitle=Bias+and+Information+of+Bayesian+Adaptive+Testing.+Research+Report+83-2&rft_id=info:eric/ED230615&rft_id=info:doi/
LA  - English
DB  - ERIC
N1  - Zuletzt aktualisiert - 2021-02-11
ER  - 



TY  - THES
T1  - EFFECTS OF ITEM PARAMETER ERROR AND OTHER FACTORS ON TRAIT ESTIMATION IN LATENT TRAIT-BASED ADAPTIVE TESTING
AN  - 303161995
AB  - Latent trait estimates derived from adaptive testing were evaluated as a function of item response theory model, scoring method, level of a,b item pool correlation, test length, and degree of item parameter error. 1700 simulated subjects were "administered" items from a pool of 150 items using maximum information adaptive item selection. Each simulee was tested under 100 combinations of conditions, with trait estimation precision evaluated using inaccuracy, bias, root mean square error, information, and fidelity.    Results indicated little deterioration in trait estimates for realistic a, b, or c error. Very extreme b error alone or combined with other parameter error generated substantial deterioration for all models and test lengths. Shorter tests deteriorated most, and maximum likelihood (ML) scores deteriorated progressively more than Bayesian scores as item parameter error increased. r(a,b) level affected the results only at certain latent trait levels under very extreme b error.    Bayesian and ML scores had equal fidelities, while ML scores excelled using information. The other indices suggested ML superiority for lesser item parameter error and Bayesian superiority as error increased. Less error was needed for Bayesian superiority under simpler models.    The 2-parameter model was more precise that 1- or 3-parameter models for all but the bias index; its performance under considerable item parameter error equaled or exceeded the other two models with no item parameter error. Between-model differences were greater for Bayesian scoring, more extreme error conditions, shorter tests, and low or extreme trait values. Under extreme b error, differences varied widely but unsystematically with r(a,b).    Level of r(a,b) had little or no systematic effect on precision, although precision and test length increased concomitantly. Per-item precision increments were greatest for the 1- and 3-parameter models and, at certain trait levels, for more extreme error conditions.    Results suggest that latent trait estimates are robust to item parameter error, that interrelationships among variables are important considerations in adaptive testing design decisions, and that free response items should be more seriously considered.
JF  - ProQuest Dissertations and Theses
AU  - MATTSON, JOYCE DANN
Y1  - 1983
PY  - 1983
DA  - 1983
SP  - 351
CY  - United States -- Minnesota
PB  - University of Minnesota
PP  - United States -- Minnesota
SN  - 9798413121337
KW  - Psychology
KW  - Occupational psychology
KW  - 0624:Occupational psychology
UR  - https://www.proquest.com/dissertations-theses/effects-item-parameter-error-other-factors-on/docview/303161995/se-2?accountid=10957
L2  - http://www.redi-bw.de/links/unifm?url_ver=Z39.88-2004&rft_val_fmt=info:ofi/fmt:kev:mtx:dissertation&genre=dissertations&sid=ProQ:ProQuest+Dissertations+%26+Theses+Global&atitle=&title=EFFECTS+OF+ITEM+PARAMETER+ERROR+AND+OTHER+FACTORS+ON+TRAIT+ESTIMATION+IN+LATENT+TRAIT-BASED+ADAPTIVE+TESTING&issn=&date=1983-01-01&volume=&issue=&spage=&au=MATTSON%2C+JOYCE+DANN&isbn=9798413121337&jtitle=&btitle=&rft_id=info:eric/&rft_id=info:doi/
LA  - English
DB  - ProQuest Dissertations & Theses Global
N1  - Copyright - Database copyright ProQuest LLC; ProQuest does not claim copyright in the individual underlying works.
N1  - Zuletzt aktualisiert - 2022-10-22
M3  - Ph.D.
M1  - 8318100
ER  - 



TY  - RPRT
T1  - Robustness of Adaptive Testing to Multidimensionality
AN  - 63284410; ED264270
AB  - The present monte carlo simulation study was designed to examine the effects of multidimensionality during the administration of computerized adaptive testing (CAT). It was assumed that multidimensionality existed in the individuals to whom test items were being administered, i.e., that the correct or incorrect responses given by an individual were generated from a specified multidimensional structure, rather than the unidimensional item response theory (IRT) model normally assumed to have generated the observable dichotomous test item responses. The dichotomous response was then treated for CAT item selection and ability estimation purposes as if it had been generated by the unidimensional model. To the extent that the observed item response was affected by dimensions other than the first (which corresponded to the single dimension assumed to underlie the item selection and ability estimation process) errors should be introduced into the adaptive testing process. These errors should affect the ability estimates and the efficiency of CAT. The study focused on the nature and degree of these errors under a variety of multidimensional structures, to determine how robust CAT is to the effects of multidimensionality in examinees' responses to test items. (PN)
AU  - Weiss, David J.
AU  - Suhadolnik, Debra
Y1  - 1982/07//
PY  - 1982
DA  - Jul 1982
SP  - 1
EP  - 34
KW  - Armed Services Vocational Aptitude Battery
KW  - Robustness
KW  - ERIC, Resources in Education (RIE)
KW  - Researchers
KW  - Adaptive Testing
KW  - Tables (Data)
KW  - Postsecondary Education
KW  - Latent Trait Theory
KW  - Multidimensional Scaling
KW  - Simulation
KW  - Computer Assisted Testing
KW  - Factor Structure
KW  - Monte Carlo Methods
KW  - Test Items
UR  - https://www.proquest.com/reports/robustness-adaptive-testing-multidimensionality/docview/63284410/se-2?accountid=10957
L2  - http://www.redi-bw.de/links/unifm?url_ver=Z39.88-2004&rft_val_fmt=info:ofi/fmt:kev:mtx:book&genre=report&sid=ProQ:ProQ%3Aeric&atitle=&title=Robustness+of+Adaptive+Testing+to+Multidimensionality&issn=&date=1982-07-01&volume=&issue=&spage=1&au=Weiss%2C+David+J.%3BSuhadolnik%2C+Debra&isbn=&jtitle=&btitle=Robustness+of+Adaptive+Testing+to+Multidimensionality&rft_id=info:eric/ED264270&rft_id=info:doi/
LA  - English
DB  - ERIC
N1  - Zuletzt aktualisiert - 2021-02-11
ER  - 



TY  - RPRT
T1  - A Model for Incorporating Response-Time Data in Scoring Achievement Tests. Research Report No. 3
AN  - 63686872; ED183621
AB  - The differences in types of information-processing skills developed by different instructional backgrounds affect, negatively or positively, the learning of further advanced instructional materials. If prior and subsequent instructional methods are different, a proactive inhibition effect produces low achievement scores on a post test. This poses a serious problem for routing of students to an instructional level on the sole basis of performance on a diagnostic adaptive test and makes it essential to determine what information-processing strategy was used and consider this knowledge simultaneously. Response time often provides supplementary information which differentiates among individuals showing identical quality of performance. A model that reflects this kind of information, obtainable from response time scores, is formulated in a similar manner to latent trait theory. This model is useful in identifying discriminating items that are sensitive to differences in instructional method. It also is helpful in identifying an individual's instructional background to a certain extent. (Author/CTM)
AU  - Tatsuoka, Kikumi
AU  - Tatsuoka, Maurice
Y1  - 1979/07//
PY  - 1979
DA  - Jul 1979
SP  - 1
EP  - 46
KW  - Weibull Distributions
KW  - ERIC, Resources in Education (RIE)
KW  - Cognitive Processes
KW  - Diagnostic Tests
KW  - Achievement Tests
KW  - Scores
KW  - Reaction Time
KW  - Scoring
KW  - Mathematics
KW  - Junior High Schools
KW  - Latent Trait Theory
KW  - Computer Assisted Testing
KW  - Item Analysis
KW  - Models
KW  - Time Factors (Learning)
KW  - Teaching Methods
KW  - Statistical Analysis
UR  - https://www.proquest.com/reports/model-incorporating-response-time-data-scoring/docview/63686872/se-2?accountid=10957
L2  - http://www.redi-bw.de/links/unifm?url_ver=Z39.88-2004&rft_val_fmt=info:ofi/fmt:kev:mtx:book&genre=report&sid=ProQ:ProQ%3Aeric&atitle=&title=A+Model+for+Incorporating+Response-Time+Data+in+Scoring+Achievement+Tests.+Research+Report+No.+3&issn=&date=1979-07-01&volume=&issue=&spage=1&au=Tatsuoka%2C+Kikumi%3BTatsuoka%2C+Maurice&isbn=&jtitle=&btitle=A+Model+for+Incorporating+Response-Time+Data+in+Scoring+Achievement+Tests.+Research+Report+No.+3&rft_id=info:eric/ED183621&rft_id=info:doi/
LA  - English
DB  - ERIC
N1  - Zuletzt aktualisiert - 2021-02-11
ER  - 



TY  - RPRT
T1  - The Danger of Relying Solely on Diagnostic Adaptive Testing When Prior and Subsequent Instructional Methods Are Different. Research Report No. 2, October 1 through December 31, 1979
AN  - 63685650; ED183608
AB  - A situation in which computer-assisted instruction was combined with teacher-taught classes and computerized diagnostic adaptive testing was examined. The computerized diagnostic adaptive test for pre-algebra signed-number lessons was programmed along with a computer-managed routing system by which examinees were sent to the instructional unit corresponding to the skill level at which they stopped in the initial test. Upon completing the course a computerized conventional post test was administered. It was found that results differed markedly from results obtained in a previous study. A cluster analysis was performed on the response patterns of the skills, and four different groups were found. A discriminant analysis indicated significant differences among the four groups in response patterns of the skills in signed number operations. It was determined that differences between prior and current instructional methods confused students and caused confusion in the post-test data, and that the scoring procedure of the adaptive testing was not sensitive to individual differences in information processing skills which were affected by the instructional method used in previous teaching. It was concluded that reliance should not be placed solely on test results determined by performance scores on a diagnostic pretest. (Author/CTM)
AU  - Tatsuoka, Kikumi
AU  - Birenbaum, Menucha
Y1  - 1979/03//
PY  - 1979
DA  - Mar 1979
SP  - 1
EP  - 33
KW  - PLATO
KW  - ERIC, Resources in Education (RIE)
KW  - Diagnostic Tests
KW  - Educational Testing
KW  - Response Style (Tests)
KW  - Mathematics
KW  - Computer Assisted Instruction
KW  - Discriminant Analysis
KW  - Adaptive Testing
KW  - Instructional Materials
KW  - Junior High Schools
KW  - Latent Trait Theory
KW  - Testing Problems
KW  - Computer Assisted Testing
KW  - Item Analysis
KW  - Teaching Methods
KW  - Statistical Analysis
KW  - Arithmetic
KW  - Cluster Analysis
UR  - https://www.proquest.com/reports/danger-relying-solely-on-diagnostic-adaptive/docview/63685650/se-2?accountid=10957
L2  - http://www.redi-bw.de/links/unifm?url_ver=Z39.88-2004&rft_val_fmt=info:ofi/fmt:kev:mtx:book&genre=report&sid=ProQ:ProQ%3Aeric&atitle=&title=The+Danger+of+Relying+Solely+on+Diagnostic+Adaptive+Testing+When+Prior+and+Subsequent+Instructional+Methods+Are+Different.+Research+Report+No.+2%2C+October+1+through+December+31%2C+1979&issn=&date=1979-03-01&volume=&issue=&spage=1&au=Tatsuoka%2C+Kikumi%3BBirenbaum%2C+Menucha&isbn=&jtitle=&btitle=The+Danger+of+Relying+Solely+on+Diagnostic+Adaptive+Testing+When+Prior+and+Subsequent+Instructional+Methods+Are+Different.+Research+Report+No.+2%2C+October+1+through+December+31%2C+1979&rft_id=info:eric/ED183608&rft_id=info:doi/
LA  - English
DB  - ERIC
N1  - Zuletzt aktualisiert - 2021-02-11
ER  - 



TY  - RPRT
T1  - A Comparison of the Fairness of Adaptive and Conventional Testing Strategies. Research Report 78-1
AN  - 63740375; ED163036
AB  - This report examines how selection fairness is influenced by the characteristics of a selection instrument in terms of its distribution of item difficulties, level of item discrimination, degree of item bias, and testing strategy. Computer simulation was used in the administration of either a conventional or Bayesian adaptive ability test to a hypothetical target population consisting of a minority and majority subgroup. Fairness was evaluated by three indices which reflect the degree of differential validity, errors in prediction (Cleary's model), and proportion of applicants exceeding a selection cutoff (Thorndike's model). Major findings are: (1) when used in conjunction with either the Bayesian or conventional test, differential prediction increased fairness and facilitated the interpretation of the fairness indices; (2) the Bayesian adaptive tests were consistently fairer than the conventional tests for all item pools above the alpha=.7 discrimination level for tests of more than 30 items; (3) the differential prediction version of the Bayesian adaptive test produced almost perfectly fair performance on all fairness indices at high discrimination levels; and (4) the placement of subgroup prior distribution in the Bayesian adaptive testing procedure can affect test fairness. (Author/CTM)
AU  - Pine, Steven M.
AU  - Weiss, David J.
Y1  - 1978/08//
PY  - 1978
DA  - Aug 1978
SP  - 1
EP  - 42
PB  - Psychometric Methods Program
KW  - ERIC, Resources in Education (RIE)
KW  - Minority Groups
KW  - Bayesian Statistics
KW  - Prediction
KW  - Test Validity
KW  - Occupational Tests
KW  - Test Bias
KW  - Adaptive Testing
KW  - Comparative Testing
KW  - Test Construction
KW  - Personnel Selection
KW  - Simulation
KW  - Computer Assisted Testing
KW  - Item Analysis
KW  - Test Items
UR  - https://www.proquest.com/reports/comparison-fairness-adaptive-conventional-testing/docview/63740375/se-2?accountid=10957
L2  - http://www.redi-bw.de/links/unifm?url_ver=Z39.88-2004&rft_val_fmt=info:ofi/fmt:kev:mtx:book&genre=report&sid=ProQ:ProQ%3Aeric&atitle=&title=A+Comparison+of+the+Fairness+of+Adaptive+and+Conventional+Testing+Strategies.+Research+Report+78-1&issn=&date=1978-08-01&volume=&issue=&spage=1&au=Pine%2C+Steven+M.%3BWeiss%2C+David+J.&isbn=&jtitle=&btitle=A+Comparison+of+the+Fairness+of+Adaptive+and+Conventional+Testing+Strategies.+Research+Report+78-1&rft_id=info:eric/ED163036&rft_id=info:doi/
LA  - English
DB  - ERIC
N1  - Zuletzt aktualisiert - 2021-02-11
N1  - Vertragsnummer - N00014-76-C-0244
ER  - 



TY  - RPRT
T1  - Evaluations of Implied Orders as a Basis for Tailored Testing Using Simulations. Technical Report No. 4
AN  - 63821174; ED147369
AB  - TAILOR is a computer program that uses the implied orders concept as the basis for computerized adaptive testing. The basic characteristics of TAILOR, which does not involve pretesting, are reviewed here and two studies of it are reported. One is a Monte Carlo simulation based on the four-parameter Birnbaum model and the other uses a matrix of children's item responses to the Stanford-Binet Intelligence Scale. In the Birnbaum model study, a variety of conditions were simulated; it was found that TAILOR typically used responses to about half the items and achieved validities with the true score within a few points of the validity of the complete test. Item discrimination parameters affected the efficiency of TAILOR. The Binet study used correlations between scores based on one bank of tailored items and another independent, parallel set and found results similar to those in the Birnbaum simulation. TAILOR, like other adaptive testing systems, apparently can aid efficiency when item discriminations are high or when ability variance is large. (Author/MV)
AU  - Cliff, Norman
AU  - And Others
Y1  - 1977/09//
PY  - 1977
DA  - Sep 1977
SP  - 1
EP  - 70
KW  - Stanford Binet Intelligence Scale
KW  - ERIC, Resources in Education (RIE)
KW  - Computer Programs
KW  - Test Reliability
KW  - Measurement Techniques
KW  - Testing Programs
KW  - Test Validity
KW  - Adaptive Testing
KW  - Elementary Secondary Education
KW  - Test Length
KW  - Simulation
KW  - Computer Assisted Testing
KW  - Item Banks
KW  - Item Analysis
KW  - Matrices
KW  - Test Items
KW  - Difficulty Level
KW  - Statistical Analysis
KW  - Mathematical Models
UR  - https://www.proquest.com/reports/evaluations-implied-orders-as-basis-tailored/docview/63821174/se-2?accountid=10957
L2  - http://www.redi-bw.de/links/unifm?url_ver=Z39.88-2004&rft_val_fmt=info:ofi/fmt:kev:mtx:book&genre=report&sid=ProQ:ProQ%3Aeric&atitle=&title=Evaluations+of+Implied+Orders+as+a+Basis+for+Tailored+Testing+Using+Simulations.+Technical+Report+No.+4&issn=&date=1977-09-01&volume=&issue=&spage=1&au=Cliff%2C+Norman%3BAnd+Others&isbn=&jtitle=&btitle=Evaluations+of+Implied+Orders+as+a+Basis+for+Tailored+Testing+Using+Simulations.+Technical+Report+No.+4&rft_id=info:eric/ED147369&rft_id=info:doi/
LA  - Undefined
DB  - ERIC
N1  - Zuletzt aktualisiert - 2021-02-11
ER  - 



TY  - RPRT
T1  - Psychological Effects of Immediate Knowledge of Results and Adaptive Ability Testing. Research Report 76-4
AN  - 63908573; ED129863
AB  - The effects of providing immediate knowledge of results (KR) and adaptive testing on test anxiety and test-taking motivation were investigated. Also studied was the accuracy of student perceptions of the difficulty of adaptive and conventional tests administered with or without immediate knowledge of results. Testees were 350 college students divided into high- and low-ability groups and randomly assigned to one of four test strategies by KR conditions. The ability level of examinees was found to be related to their reported levels of motivation and to differences in reported motivation under the different testing conditions. These results suggest that adaptive testing creates a psychological environment for testing which is more equivalently motivating for examinees of all ability levels and results in a greater standardization of the test-taking environment, than does conventional testing. (Author)
AU  - Betz, Nancy E.
AU  - Weiss, David J.
Y1  - 1976/06//
PY  - 1976
DA  - Jun 1976
SP  - 1
EP  - 45
KW  - ERIC, Resources in Education (RIE)
KW  - Ability
KW  - Branching
KW  - Computer Oriented Programs
KW  - Achievement Tests
KW  - Test Results
KW  - Response Style (Tests)
KW  - Student Attitudes
KW  - College Students
KW  - Motivation
KW  - Feedback
KW  - Computer Assisted Testing
KW  - Anxiety
KW  - Difficulty Level
KW  - Statistical Analysis
KW  - Testing
UR  - https://www.proquest.com/reports/psychological-effects-immediate-knowledge-results/docview/63908573/se-2?accountid=10957
L2  - http://www.redi-bw.de/links/unifm?url_ver=Z39.88-2004&rft_val_fmt=info:ofi/fmt:kev:mtx:book&genre=report&sid=ProQ:ProQ%3Aeric&atitle=&title=Psychological+Effects+of+Immediate+Knowledge+of+Results+and+Adaptive+Ability+Testing.+Research+Report+76-4&issn=&date=1976-06-01&volume=&issue=&spage=1&au=Betz%2C+Nancy+E.%3BWeiss%2C+David+J.&isbn=&jtitle=&btitle=Psychological+Effects+of+Immediate+Knowledge+of+Results+and+Adaptive+Ability+Testing.+Research+Report+76-4&rft_id=info:eric/ED129863&rft_id=info:doi/
LA  - Undefined
DB  - ERIC
N1  - SuppNotes - For a related document, see TM 005 638
N1  - Zuletzt aktualisiert - 2021-02-11
N1  - Vertragsnummer - N00014-76-C-0243; NR150-382
ER  - 



TY  - RPRT
T1  - Computerized Adaptive Ability Measurement
AN  - 63914574; ED128404
AB  - The general objective of a research program on adaptive testing was to identify several sources of potential error in test scores, and to study adaptive testing as a means for reducing these errors. Errors can result from the mismatch of item difficulty to the individual's ability; the psychological effects of testing and the test environment; the inability to extract enough information from the testee's response; deviations from unidimensionality; and an oversimplistic conceptualization of ability. Several different strategies of adaptive testing are discussed, along with the information level they yield, and the bias that can result from various scoring methods. In a discussion of the unidimentionality of test items, the consistency of the testee's response is analyzed. Finally, group differences are examined in terms of the psychological effects of receiving immediate feedback, especially on low ability groups. The author concludes that adaptive testing and immediate knowledge of results may be able to provide testing conditions more conclusive to each person's ability to demonstrate his/her fullest capacities in test performance. (Author/BW)
AU  - Weiss, David J.
Y1  - 1975/09//
PY  - 1975
DA  - Sep 1975
SP  - 1
EP  - 39
KW  - ERIC, Resources in Education (RIE)
KW  - Ability
KW  - Computer Oriented Programs
KW  - Achievement Tests
KW  - Response Style (Tests)
KW  - Ability Grouping
KW  - Scores
KW  - Test Bias
KW  - Individual Differences
KW  - Adaptive Testing
KW  - Test Construction
KW  - Testing Problems
KW  - Feedback
KW  - Computer Assisted Testing
KW  - Test Interpretation
KW  - Error Patterns
KW  - Statistical Analysis
UR  - https://www.proquest.com/reports/computerized-adaptive-ability-measurement/docview/63914574/se-2?accountid=10957
L2  - http://www.redi-bw.de/links/unifm?url_ver=Z39.88-2004&rft_val_fmt=info:ofi/fmt:kev:mtx:book&genre=report&sid=ProQ:ProQ%3Aeric&atitle=&title=Computerized+Adaptive+Ability+Measurement&issn=&date=1975-09-01&volume=&issue=&spage=1&au=Weiss%2C+David+J.&isbn=&jtitle=&btitle=Computerized+Adaptive+Ability+Measurement&rft_id=info:eric/ED128404&rft_id=info:doi/
LA  - Undefined
DB  - ERIC
N1  - SuppNotes - Paper presented at the Annual Conference of the Military Testing Association (17th, Fort Benjamin Harrison, Indiana, September 15-19, 1975); Also included in TM 005 585
N1  - Zuletzt aktualisiert - 2021-02-11
N1  - Vertragsnummer - N00014-67-A-0113-0029
ER  - 



TY  - THES
T1  - ABILITY MEASUREMENT, TEST BIAS REDUCTION, AND PSYCHOLOGICAL REACTIONS TO TESTING AS A FUNCTION OF COMPUTER ADAPTIVE TESTING VERSUS CONVENTIONAL TESTING
AN  - 302020669
AB  - The effects of a test bias reduction technique on measurement precision in conventional and computer adaptive tests were investigated. Also, the relationships between test type, bias reduction procedures, and examinees' psychological reactions to testing were investigated. Latent trait theory was used in the construction and evaluation of the conventional and adaptive tests which were both administered on a computer terminal. The test items administered in this study were from the Scholastic Aptitude Test verbal section. The adaptive test used a Bayesian sequential item selection strategy to construct individualized 15-item tests for each examinee. The conventional test administered a fixed set of 15 items selected to measure equally well over the entire range of examinee ability.    The test bias reduction technique is a three step procedure for selecting items either interactively (adaptive) or a priori (conventional) to be in a test. First, the desired difficulty of the new item is specified. Second, either four or seven items with difficulty levels nearest the desired level are selected from the item pool. Third, the item with the lowest bias is selected to be in the test. The test bias reduction technique substantially reduced the bias index of the items administered to the examinees but also resulted in lower measurement precision of examinee ability. This effect on measurement precision was larger for the conventional test than the adaptive test. The levels of bias reduction, four items scanned for lowest bias compared to seven items scanned, made no difference on the measurement precision of the adaptive test. However, these levels of bias reduction made a difference on the measurement precision of the conventional test.    The adaptive test resulted in higher Bayesian modal estimates of ability than those from the conventional test. However, examinees perceived the adaptive test to be more difficult than the conventional test.    Examinees' psychological reactions of perceive guessing and perceived performance were affected by the type of test, adaptive or conventional, and were related to test performance independent of examinee ability. Also, the relationship between these psychological reactions and performance was generally greater for conventional than adaptive tests. Therefore, adaptive testing might affect performance scores differently than conventional testing through mediating psychological processes.    These findings are discussed in addition to possible directions for future research.
JF  - ProQuest Dissertations and Theses
AU  - ORBAN, JOSEPH ANDREW
Y1  - 1954
PY  - 1954
DA  - 1954
SP  - 162
CY  - United States -- Virginia
PB  - Virginia Polytechnic Institute and State University
PP  - United States -- Virginia
SN  - 9781084571198
KW  - Psychology
KW  - Occupational psychology
KW  - 0624:Occupational psychology
UR  - https://www.proquest.com/dissertations-theses/ability-measurement-test-bias-reduction/docview/302020669/se-2?accountid=10957
L2  - http://www.redi-bw.de/links/unifm?url_ver=Z39.88-2004&rft_val_fmt=info:ofi/fmt:kev:mtx:dissertation&genre=dissertations&sid=ProQ:ProQuest+Dissertations+%26+Theses+Global&atitle=&title=ABILITY+MEASUREMENT%2C+TEST+BIAS+REDUCTION%2C+AND+PSYCHOLOGICAL+REACTIONS+TO+TESTING+AS+A+FUNCTION+OF+COMPUTER+ADAPTIVE+TESTING+VERSUS+CONVENTIONAL+TESTING&issn=&date=1954-01-01&volume=&issue=&spage=&au=ORBAN%2C+JOSEPH+ANDREW&isbn=9781084571198&jtitle=&btitle=&rft_id=info:eric/&rft_id=info:doi/
LA  - English
DB  - ProQuest Dissertations & Theses Global
N1  - Copyright - Database copyright ProQuest LLC; ProQuest does not claim copyright in the individual underlying works.
N1  - Zuletzt aktualisiert - 2022-10-22
M3  - Ph.D.
M1  - 8206769
ER  - 



