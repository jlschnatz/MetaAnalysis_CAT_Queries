
OWN - ERIC
TI  - Exam Anxiety: Using Paired Adaptive Tests to Reduce Stress in Business Classes
AU  - Seeley, Eugene L.
AU  - Andrade, Maureen
AU  - Miller, Ronald Mellado
OT  - Test Anxiety
OT  - Adaptive Testing
OT  - Stress Management
OT  - College Students
OT  - Business Administration Education
OT  - Outcomes of Education
OT  - Scores
JT  - e-Journal of Business Education and Scholarship of Teaching
SO  - v12 n3 p1-13 Dec 2018
OID - EJ1203830
VI  - 12
IP  - 3
PG  - 1-13
DP  - Dec 2018
LID - http://eric.ed.gov/?id=EJ1203830
AB  - To reduce test-taking anxiety among businesses students, a Paired Adaptive Test (PAT) system was developed that allows students two chances to answer exam questions. In the study 46 students from three sections of Survey of International Business at Utah Valley University were given exams using the PAT. At the end of the semester, students were asked to complete a survey on test-taking anxiety for that class and other classes. The results indicated a twelve percent lower test-taking anxiety overall score for the PAT system and as much as 20.85% lower scores for questions key to taking specific exams. The implications of this research are that the PAT method could significantly reduce exam anxiety for students while providing a good assessment of their subject knowledge.
ISSN - EISSN-1835-9132
LA  - English
PT  - Journal Articles
PT  - Reports - Research

OWN - ERIC
TI  - Age-Related Changes in the Relation between Preschoolers' Anger and Persistence
AU  - Ramsook, K. Ashana
AU  - Benson, Lizbeth
AU  - Ram, Nilam
AU  - Cole, Pamela M.
OT  - Age Differences
OT  - Psychological Patterns
OT  - Preschool Children
OT  - Emotional Response
OT  - Self Control
OT  - Child Behavior
OT  - Emotional Development
OT  - Persistence
JT  - International Journal of Behavioral Development
SO  - v44 n3 p216-225 May 2020
AID - http://dx.doi.org/10.1177/0165025419866914
OID - EJ1253629
VI  - 44
IP  - 3
PG  - 216-225
DP  - May 2020
LID - http://eric.ed.gov/?id=EJ1253629
AB  - Although the functionalist perspective on emotional development posits that emotions serve adaptive functions, empirical tests of the role of anger mostly focus on how anger contributes to dysfunction. Developmentally, as children gain agency and skill at emotion regulation between the ages of 36 months and 48 months, their modulation of anger may facilitate its functional role for behavior. We examined this possibility through study of how 120 children's anger and sadness were related to persistence during the transparent locked box task at ages 36 and 48 months. Using survival analyses, we examined how children's anger and sadness were related to their giving up during the challenging task, and whether those relations were moderated by age. Using hidden Markov models (HMMs), we examined how children transitioned among anger, sadness, and on-task behavior states and whether those dynamics differed with age. Survival analysis revealed that age moderated the relation between anger and giving up. Greater anger was associated with greater likelihood of giving up earlier in the task at 36 months but with lower likelihood of giving up at 48 months. HMM analyses revealed that children were more likely to transition from a Calm/On-task to Calm/Off-task state at 36 months than at 48 months; that children were more likely to remain in an Anger/On-task state at 36 months than at 48 months; and that children were more likely to transition from Calm/On-task to Anger/On-task, and from Anger/On-task back to Calm/On-task at 48 months than at 36 months. Taken together, the findings suggest that anger appraisals may facilitate children in maintaining persistence, but that this functionality may develop with age.
ISSN - ISSN-0165-0254
GR  - R305B090007
GR  - R01HD076994
GR  - HD076994
LA  - English
PT  - Journal Articles
PT  - Reports - Research

OWN - ERIC
TI  - Computer-Adaptive Testing: Implications for Students' Achievement, Motivation, Engagement, and Subjective Test Experience
AU  - Martin, Andrew J.
AU  - Lazendic, Goran
OT  - Computer Assisted Testing
OT  - Elementary School Students
OT  - Secondary School Students
OT  - Gender Differences
OT  - Instructional Program Divisions
OT  - Socioeconomic Status
OT  - Geographic Location
OT  - Predictor Variables
OT  - Foreign Countries
OT  - Academic Achievement
OT  - Student Motivation
OT  - Learner Engagement
OT  - Numeracy
OT  - Student Experience
OT  - Achievement Tests
OT  - School Size
OT  - Statistical Analysis
OT  - Statistical Significance
JT  - Journal of Educational Psychology
SO  - v110 n1 p27-45 Jan 2018
AID - http://dx.doi.org/10.1037/edu0000205
OID - EJ1166166
VI  - 110
IP  - 1
PG  - 27-45
DP  - Jan 2018
LID - http://eric.ed.gov/?id=EJ1166166
AB  - The present study investigated the implications of computer-adaptive testing (operationalized by way of multistage adaptive testing; MAT) and "conventional" fixed order computer testing for various test-relevant outcomes in numeracy, including achievement, test-relevant motivation and engagement, and subjective test experience. It did so among N = 12,736 Australian elementary (years 3 and 5) and secondary (years 7 and 9) school students. Multilevel modeling assessed the extent to which Level 1 (student) test condition (fixed order vs. adaptive), gender, and year group factors and Level 2 (school) socioeducational advantage, location, structure, and size factors predicted students' test-relevant outcomes. In terms of statistically significant main effects, students in the computer-adaptive testing condition generated lower achievement error rates (i.e., higher measurement precision). Other statistically significant computer-adaptive test effects emerged as a function of year-level and gender, with positive effects of computer-adaptive testing being relatively greater for females and older students: these students achieved more highly (year 9 students), reported higher test-relevant motivation and engagement (year 9 students), and reported more positive subjective test experience (females and year 9 students). These findings (a) confirm that computer-adaptive testing yields greater achievement measurement precision, (b) suggest some positive test-relevant motivation and engagement effects from computer-adaptive testing, (c) counter claims that computer-adaptive testing reduces students' test-relevant motivation, engagement, and subjective experience, and (d) suggest positive computer-adaptive testing effects for older students at a developmental stage when they are typically less motivated and engaged.
ISSN - ISSN-0022-0663
LA  - English
PT  - Journal Articles
PT  - Reports - Research

OWN - ERIC
TI  - An Adaptive Test Analysis Based on Students' Motivation
AU  - Yoshioka, SÃ©rgio R. I.
AU  - Ishitani, Lucila
OT  - Student Motivation
OT  - Adaptive Testing
OT  - Computer Assisted Testing
OT  - Item Response Theory
OT  - Anxiety
OT  - Computer Attitudes
OT  - Test Theory
OT  - Test Reliability
OT  - Test Validity
OT  - Comparative Analysis
OT  - Item Banks
JT  - Informatics in Education
SO  - v17 n2 p381-404 2018
OID - EJ1195643
VI  - 17
IP  - 2
PG  - 381-404
DP  - 2018
LID - http://eric.ed.gov/?id=EJ1195643
AB  - Computerized Adaptive Testing (CAT) is now widely used. However, inserting new items into the question bank of a CAT requires a great effort that makes impractical the wide application of CAT in classroom teaching. One solution would be to use the tacit knowledge of the teachers or experts for a pre-classification and calibrate during the execution of tests with these items. Thus, this research consists of a comparative case study between a Stratified Adaptive Test (SAT), based on the tacit knowledge of a teacher, and a CAT based on Item Response Theory (IRT). The tests were applied in seven Computer Networks courses. The results indicate that levels of "anxiety" expressed in the use of the SAT were better than those using the CAT, in addition to being simpler to implement. In this way, it is recommended the implementation of a SAT, where the strata are initially based on the tacit knowledge of the teacher and later, as a result of an IRT calibration.
ISSN - ISSN-1648-5831
LA  - English
PT  - Journal Articles
PT  - Reports - Research

OWN - ERIC
TI  - Test Anxiety, Computer-Adaptive Testing and the Common Core
AU  - Colwell, Nicole Makas
OT  - Test Anxiety
OT  - Computer Assisted Testing
OT  - Evaluation Methods
OT  - Standardized Tests
OT  - Test Bias
OT  - Educational Technology
OT  - Test Items
OT  - Difficulty Level
OT  - Student Evaluation
OT  - Ability Identification
OT  - Elementary Secondary Education
OT  - Effect Size
OT  - Scores
OT  - Socioeconomic Status
OT  - High Stakes Tests
OT  - Summative Evaluation
JT  - Journal of Education and Training Studies
SO  - v1 n2 p50-60 Oct 2013
OID - EJ1054865
VI  - 1
IP  - 2
PG  - 50-60
DP  - Oct 2013
LID - http://eric.ed.gov/?id=EJ1054865
AB  - This paper highlights the current findings and issues regarding the role of computer-adaptive testing in test anxiety. The computer-adaptive test (CAT) proposed by one of the Common Core consortia brings these issues to the forefront. Research has long indicated that test anxiety impairs student performance. More recent research indicates that taking a test in a CAT format can affect the ability estimates of students with test anxiety. Inaccurate measures of ability are disconcerting because of the threat they pose to the validity of test score interpretation. This paper raises concerns regarding how the implementation of a computer-adaptive test for a large-scale common core assessment system could differentially affect students with test anxiety. Issues of fairness and score comparability are raised, and the implications of these issues are discussed.
ISSN - ISSN-2324-805X
LA  - English
PT  - Journal Articles
PT  - Reports - Evaluative

OWN - ERIC
TI  - A Correlational Analysis of Test Anxiety and Response Time on a Computerized Adaptive Math Test among Seventh Grade Students by Gender
AU  - Brom, Michael Wayde
OT  - Test Anxiety
OT  - Reaction Time
OT  - Responses
OT  - Correlation
OT  - Computer Assisted Testing
OT  - Mathematics Tests
OT  - Gender Differences
OT  - Statistical Analysis
OT  - Grade 7
JT  - ProQuest LLC
SO  - Ed.D. Dissertation, Liberty University
AID - http://gateway.proquest.com/openurl?url_ver=Z39.88-2004&rft_val_fmt=info:ofi/fmt:kev:mtx:dissertation&res_dat=xri:pqm&rft_dat=xri:pqdiss:10109978
OID - ED571741
DP  - 2016
LID - http://eric.ed.gov/?id=ED571741
AB  - Studies have shown that test anxiety has become more prevalent since the adoption of the No Child Left Behind Act in 2001 and that test anxiety negatively affects student achievement. Early research viewed test anxiety as being a unidimensional construct; however, recent research has purported that test anxiety is a multidimensional construct. Consequently, test anxiety being viewed as multidimensional has changed how test anxiety is measured as evidenced by the development of multidimensional test anxiety measurement scales. Literature indicates that there is a gap in the study of K-12 test anxiety and response time data on assessments. A nonexperimental correlational research design was used for this study. The purpose of this study was to examine the relationships between multidimensional test anxiety measures and the response time of students taking a computerized adaptive math assessment, the "Measures of Academic Progress"Â® (MAPÂ®) by the Northwest Evaluation Associationâ¢ (NWEAâ¢), while controlling for gender. Participants for the study were 215 seventh grade math students at a school in the West. The test anxiety of participants was measured using the Children's Test Anxiety Scale (CTAS). The response time data was captured by the NWEAâ¢ MAPÂ® math tests. Spearman correlations were used to examine the relationship between test anxiety and response time. No statistically significant relationships were found. Recommendations for future research are to examine relationships between test anxiety and RTE using different normative thresholds (NT) and utilizing other multidimensional test anxiety scales as they become commercially available. [The dissertation citations contained here are published with the permission of ProQuest LLC. Further reproduction is prohibited without permission. Copies of dissertations may be obtained by Telephone (800) 1-800-521-0600. Web page: http://www.proquest.com/en-US/products/dissertations/individuals.shtml.]
ISBN - 978-1-3397-3243-5
PT  - Dissertations/Theses - Doctoral Dissertations

OWN - ERIC
TI  - The Impact of Technology-Enhanced Items on Test-Taker Disengagement
AU  - Wise, Steven L.
AU  - Soland, James
AU  - Dupray, Laurence M.
OT  - Test Items
OT  - Guessing (Tests)
OT  - Multiple Choice Tests
OT  - Achievement Tests
OT  - Adaptive Testing
OT  - Computer Assisted Testing
OT  - Elementary Secondary Education
JT  - Journal of Applied Testing Technology
SO  - v22 n1 p28-36 2021
AID - http://jattjournal.net/index.php/atp/article/view/155941
OID - EJ1296001
VI  - 22
IP  - 1
PG  - 28-36
DP  - 2021
LID - http://eric.ed.gov/?id=EJ1296001
AB  - Technology-Enhanced Items (TEIs) have been purported to be more motivating and engaging to test takers than traditional multiple-choice items. The claim of enhanced engagement, however, has thus far received limited research attention. This study examined the rates of rapid-guessing behavior received by three types of items (multiple-choice, multiple select, and TEIs) on a commonly-used K-12 adaptive achievement test. Across three subject areas, the TEIs consistently showed the lowest rapid guessing rate, suggesting that their use may help mitigate the problem of disengaged test taking.
ISSN - EISSN-2375-5636
LA  - English
PT  - Journal Articles
PT  - Reports - Research

OWN - ERIC
TI  - Stopping Rules for Computer Adaptive Testing When Item Banks Have Nonuniform Information
AU  - Morris, Scott B.
AU  - Bass, Michael
AU  - Howard, Elizabeth
AU  - Neapolitan, Richard E.
OT  - Computer Assisted Testing
OT  - Adaptive Testing
OT  - Item Banks
OT  - Item Response Theory
OT  - Evaluation Methods
OT  - Psychometrics
OT  - Anxiety
JT  - International Journal of Testing
SO  - v20 n2 p146-168 2020
AID - http://dx.doi.org/10.1080/15305058.2019.1635604
OID - EJ1254419
VI  - 20
IP  - 2
PG  - 146-168
DP  - 2020
LID - http://eric.ed.gov/?id=EJ1254419
AB  - The standard error (SE) stopping rule, which terminates a computer adaptive test (CAT) when the "SE" is less than a threshold, is effective when there are informative questions for all trait levels. However, in domains such as patient-reported outcomes, the items in a bank might all target one end of the trait continuum (e.g., negative symptoms), and the bank may lack depth for many individuals. In such cases, the predicted standard error reduction (PSER) stopping rule will stop the CAT even if the "SE" threshold has not been reached and can avoid administering excessive questions that provide little additional information. By tuning the parameters of the PSER algorithm, a practitioner can specify a desired tradeoff between accuracy and efficiency. Using simulated data for the Patient-Reported Outcomes Measurement Information System Anxiety and Physical Function banks, we demonstrate that these parameters can substantially impact CAT performance. When the parameters were optimally tuned, the PSER stopping rule was found to outperform the "SE" stopping rule overall, particularly for individuals not targeted by the bank, and presented roughly the same number of items across the trait continuum. Therefore, the PSER stopping rule provides an effective method for balancing the precision and efficiency of a CAT.
ISSN - ISSN-1530-5058
GR  - R01LM011962
GR  - R01LM011663
LA  - English
PT  - Journal Articles
PT  - Reports - Research

OWN - ERIC
TI  - A Cluster Randomized Trial of the Social Skills Improvement System-Classwide Intervention Program (SSIS-CIP) in First Grade
AU  - DiPerna, James Clyde
AU  - Lei, Puiwa
AU  - Cheng, Weiyi
AU  - Hart, Susan Crandall
AU  - Bellinger, Jillian
OT  - Skill Development
OT  - Interpersonal Competence
OT  - Social Development
OT  - Elementary School Students
OT  - Elementary School Teachers
OT  - Randomized Controlled Trials
OT  - Comparative Analysis
OT  - Intervention
OT  - Behavior Problems
OT  - Student Behavior
OT  - Observation
OT  - Literacy
OT  - Numeracy
OT  - Empathy
OT  - Student Motivation
OT  - Learner Engagement
OT  - Academic Ability
OT  - Computer Assisted Testing
OT  - Rating Scales
OT  - Pretests Posttests
OT  - Program Effectiveness
OT  - Grade 1
OT  - Grade 2
OT  - Statistical Analysis
JT  - Journal of Educational Psychology
SO  - v110 n1 p1-16 Jan 2018
AID - http://dx.doi.org/10.1037/edu0000191
OID - EJ1166162
VI  - 110
IP  - 1
PG  - 1-16
DP  - Jan 2018
LID - http://eric.ed.gov/?id=EJ1166162
AB  - The purpose of this study was to evaluate the efficacy of a universal social skills program, the Social Skills Improvement System Classwide Intervention Program (SSIS-CIP; Elliott & Gresham, 2007), for students in first grade. Classrooms from 6 elementary schools were randomly assigned to treatment or business-as-usual control conditions. Teachers assigned to the treatment condition implemented the SSIS-CIP over a 12-week period. Students' social skills, problem behaviors, and approaches to learning were assessed via teacher ratings and direct observations of classroom behavior. In addition, their early literacy and numeracy skills were measured via computer-adaptive standardized tests. SSIS-CIP participation yielded small positive effects in students' social skills (particularly empathy and social engagement) and approaches to learning (academic motivation and engagement). Students' problem behaviors and academic skills, however, were unaffected by SSIS-CIP exposure.
ISSN - ISSN-0022-0663
GR  - R305A090438
LA  - English
PT  - Journal Articles
PT  - Reports - Research

OWN - ERIC
TI  - Examining the Link between Stress Events and Prosocial Behavior in Adolescents: More Ordinary Magic?
AU  - Larson, Andrea
AU  - Moses, Tally
OT  - Correlation
OT  - Prosocial Behavior
OT  - Adolescents
OT  - Resilience (Psychology)
OT  - Stress Variables
OT  - Coping
OT  - Social Support Groups
OT  - Social Status
OT  - Peer Relationship
OT  - Helping Relationship
OT  - Predictor Variables
OT  - Student Behavior
OT  - Hypothesis Testing
OT  - Secondary School Students
OT  - Student Surveys
OT  - Risk
OT  - Intervention
OT  - Individual Characteristics
OT  - Racial Differences
OT  - Minority Group Students
OT  - Socioeconomic Influences
OT  - Gender Differences
OT  - Homosexuality
OT  - Age Differences
OT  - Ethnic Groups
OT  - Multivariate Analysis
OT  - Regression (Statistics)
JT  - Youth & Society
SO  - v49 n6 p779-804 Sep 2017
AID - http://dx.doi.org/10.1177/0044118X14563049
OID - EJ1149835
VI  - 49
IP  - 6
PG  - 779-804
DP  - Sep 2017
LID - http://eric.ed.gov/?id=EJ1149835
AB  - Scholarship regarding adolescent resilience has typically defined resilience as the absence of negative outcomes rather than the existence of positive outcomes. This study drew on the challenge model of resilience, which anticipates a curvilinear relationship between stress exposure and adaptive functioning, to test whether adolescents reporting moderate levels of stress exposure were more likely to evidence prosocial behavior than youth exposed to more or less stress. Using data from approximately 13,000 adolescents, we tested three analytic models and investigated hypothesized moderation by coping, social resources, and markers of adolescent status. Our results did not align with the challenge model. Instead, we found that stress exposure was differentially associated with measures of prosocial behavior, that social resources supported volunteering but impeded helping a peer in some instances, and that markers of historically marginalized status were more predictive of stopping peer harassment than volunteering. Implications for future research are discussed.
ISSN - ISSN-0044-118X
LA  - English
PT  - Journal Articles
PT  - Reports - Research

OWN - ERIC
TI  - Identifying Differential Item Functioning in Multi-Stage Computer Adaptive Testing
AU  - Gierl, Mark J.
AU  - Lai, Hollis
AU  - Li, Johnson
OT  - Adaptive Testing
OT  - Test Bias
OT  - Computer Assisted Testing
OT  - Test Items
OT  - Error of Measurement
OT  - Psychometrics
JT  - Educational Research and Evaluation
SO  - v19 n2-3 p188-203 2013
AID - http://dx.doi.org/10.1080/13803611.2013.767622
OID - EJ996865
VI  - 19
PG  - 188-203
DP  - 2013
LID - http://eric.ed.gov/?id=EJ996865
AB  - The purpose of this study is to evaluate the performance of CATSIB (Computer Adaptive Testing-Simultaneous Item Bias Test) for detecting differential item functioning (DIF) when items in the matching and studied subtest are administered adaptively in the context of a realistic multi-stage adaptive test (MST). MST was simulated using a 4-item module in a 7-panel administration. Three independent variables, expected to affect DIF detection rates, were manipulated: item difficulty, sample size, and balanced/unbalanced design. CATSIB met the acceptable criteria, meaning that the Type I error and power rates met 5% and 80%, respectively, for the large reference/moderate focal sample and the large reference/large focal sample conditions. These results indicate that CATSIB can be used to consistently and accurately detect DIF on an MST, but only with moderate to large samples. (Contains 4 tables and 2 figures.)
ISSN - ISSN-1380-3611
LA  - English
PT  - Journal Articles
PT  - Reports - Research

OWN - ERIC
TI  - Comparing the Effect of Two Types of Computer Screen Background Lighting on Students' Reading Engagement and Achievement
AU  - Botello, Jennifer A.
OT  - Lighting
OT  - Computers
OT  - Computer Assisted Testing
OT  - Performance
OT  - Reading Tests
OT  - Summative Evaluation
OT  - Influences
OT  - Reading Skills
OT  - Visual Stimuli
OT  - Statistical Analysis
OT  - Elementary School Students
OT  - Grade 2
OT  - Grade 3
OT  - Grade 4
OT  - Grade 5
OT  - Grade 6
OT  - Learner Engagement
OT  - Student Behavior
OT  - Reading Comprehension
OT  - Hypothesis Testing
OT  - Educational Technology
JT  - ProQuest LLC
SO  - Ed.D. Dissertation, Lindenwood University
AID - http://gateway.proquest.com/openurl?url_ver=Z39.88-2004&rft_val_fmt=info:ofi/fmt:kev:mtx:dissertation&res_dat=xri:pqm&rft_dat=xri:pqdiss:3618650
OID - ED568528
DP  - 2014
LID - http://eric.ed.gov/?id=ED568528
AB  - With increased dependence on computer-based standardized tests to assess academic achievement, technological literacy has become an essential skill. Yet, because students have unequal access to technology, they may not have equal opportunities to perform well on these computer-based tests. The researcher had observed students taking the STAR Reading test (Renaissance Learning, 2009) and noticed a variance in scores in relation to classroom performance. The researcher intended, therefore, to explore variables that may affect the performance of students on a computer-based reading assessment. The researcher tested two different technology-related variables as students took a summative exam, the STAR Reading test. The purpose of this study was to explore how changes in visual stimuli affected the process of reading and student reading behavior. This quantitative study sought to ascertain whether changing the computer read-out to a black screen with white lettering made a difference in student engagement and comprehension among students in grades two through six during a computer-based adaptive test. The research site was one K-6 elementary school in a large suburban school district. The participants of the study were 316 children in grades two through six. One hundred and sixteen students were randomly sampled for student engagement data analysis. The researcher conducted a stratified random process to further select data for analysis. Students were exposed to both color display background variables throughout the study process. Teacher observers collected tallies on student engagement behaviors during the test-taking process. The researcher calculated the mean level of student engagement on each of five observed behaviors. The researcher also collected reading comprehension data for five subsequent benchmark sessions throughout the year. The engagement results of this study failed to support the hypothesis, which stated that elementary student behaviors during testing would verify a measureable difference in engagement when either a black or white display screen was presented. The results of the reading comprehension test also failed to support the hypothesis, which stated that there would be a measureable difference in elementary students' scores while taking computer-based tests when the computer screen was set to either black or white background. [The dissertation citations contained here are published with the permission of ProQuest LLC. Further reproduction is prohibited without permission. Copies of dissertations may be obtained by Telephone (800) 1-800-521-0600. Web page: http://www.proquest.com/en-US/products/dissertations/individuals.shtml.]
ISBN - 978-1-3038-7102-3
PT  - Dissertations/Theses - Doctoral Dissertations

OWN - ERIC
TI  - A Mixture Rasch Model-Based Computerized Adaptive Test for Latent Class Identification
AU  - Jiao, Hong
AU  - Macready, George
AU  - Liu, Junhui
AU  - Cho, Youngmi
OT  - Item Banks
OT  - Adaptive Testing
OT  - Computer Assisted Testing
OT  - Identification
OT  - Item Analysis
OT  - Ability
OT  - Measurement Techniques
OT  - Models
OT  - Computation
JT  - Applied Psychological Measurement
SO  - v36 n6 p469-493 Sep 2012
AID - http://dx.doi.org/10.1177/0146621612450068
OID - EJ974723
VI  - 36
IP  - 6
PG  - 469-493
DP  - Sep 2012
LID - http://eric.ed.gov/?id=EJ974723
AB  - This study explored a computerized adaptive test delivery algorithm for latent class identification based on the mixture Rasch model. Four item selection methods based on the Kullback-Leibler (KL) information were proposed and compared with the reversed and the adaptive KL information under simulated testing conditions. When item separation was large, all item selection methods did not differ evidently in terms of accuracy in classifying examinees into different latent classes and estimating latent ability. However, when item separation was small, two methods with class-specific ability estimates performed better than the other two methods based on a single latent ability estimate across all latent classes. The three types of KL information distributions were compared. The KL and the reversed KL information could be the same or different depending on the ability level and the item difficulty difference between latent classes. Although the KL information and the reversed KL information were different at some ability levels and item difficulty difference levels, the use of the KL, the reversed KL, or the adaptive KL information did not affect the results substantially due to the symmetric distribution of item difficulty differences between latent classes in the simulated item pools. Item pool usage and classification convergence points were examined as well. (Contains 6 tables and 4 figures.)
ISSN - ISSN-0146-6216
LA  - English
PT  - Journal Articles
PT  - Reports - Research

OWN - ERIC
TI  - Designing P-Optimal Item Pools in Computerized Adaptive Tests with Polytomous Items
AU  - Zhou, Xuechun
OT  - Item Banks
OT  - Computer Assisted Testing
OT  - Adaptive Testing
OT  - Test Items
OT  - Design
JT  - ProQuest LLC
SO  - Ph.D. Dissertation, Michigan State University
AID - http://gateway.proquest.com/openurl?url_ver=Z39.88-2004&rft_val_fmt=info:ofi/fmt:kev:mtx:dissertation&res_dat=xri:pqm&rft_dat=xri:pqdiss:3548769
OID - ED551381
DP  - 2012
LID - http://eric.ed.gov/?id=ED551381
AB  - Current CAT applications consist of predominantly dichotomous items, and CATs with polytomously scored items are limited. To ascertain the best approach to polytomous CAT, a significant amount of research has been conducted on item selection, ability estimation, and impact of termination rules based on polytomous IRT models. Few studies investigated the optimal pool characteristics for polytomous CAT implementation. Using the generalized partial credit model (GPCM) (Muraki, 1992), this study aims to identify an optimal item pool design for tests with polytomous items by extending the p-optimality method (Reckase, 2007). The extension includes the definition of atheta-bin to describe polytomous items succinctly for pool design, and item generation strategy using constrained nonlinear optimization method. Optimal item pools are generated using CAT simulations with and without practical constraints. The item pool characteristics under each condition are summarized and their performance is evaluated against an extended operational item pool. The results indicated that the practical constraints of the a-stratified exposure control and content balancing do not affect pool size to a large extent. However, the a-stratified control affects the pool characteristics greatly: the items included in the simulated pools with the control have larger a-parameter and provide higher maximum information on average. On the other hand, the content balancing applied in this study has little impact on pool design. The evaluation results of the pool performance are closely related to the pool characteristics. When the a-stratified exposure control applied, the consistent results include: 1) the average test information is lower than that without the constraint; 2) RMSE is higher and the correlation between the true and estimated abilities is lower; 3) the percentage of the correct classification for the highest achievement level is lower. However, for all the simulated optimal pools, while the test information is consistently above the target level 10.0, it is concluded that the a-stratified method resulted in an efficient use of less discriminative items with small decreases in measurement precision. With regard to the item pool usage, the percentage of items that are fully used, well used, rarely used, and never used are quite comparable for the pools designed with constraints. However, compared with the extended operational pool, when the a-stratified method applied, the conditional test overlap rate of the simulated pools is consistently lower. For the pool blueprint, because the normal ability distribution is assumed, more items are included in the middle of the ability scale for all simulated optimal pools. The distributions of the a- and b- parameters are similar under all conditions. When the a-stratified method applied, there are fewer items in the first stratum and the number of items in the second and third stratum is nearly identical. In addition, with the content balancing control, the number of items in the first content area is slightly less than the second one. [The dissertation citations contained here are published with the permission of ProQuest LLC. Further reproduction is prohibited without permission. Copies of dissertations may be obtained by Telephone (800) 1-800-521-0600. Web page: http://www.proquest.com/en-US/products/dissertations/individuals.shtml.]
ISBN - 978-1-2678-4484-2
PT  - Dissertations/Theses - Doctoral Dissertations

OWN - ERIC
TI  - Implementation Fidelity and Attainment in Computerized Practice of Mathematics
AU  - Topping, Keith J.
OT  - Foreign Countries
OT  - Computer Assisted Testing
OT  - Mathematics Tests
OT  - Mathematics Achievement
OT  - Acceleration (Education)
OT  - Norm Referenced Tests
OT  - Program Implementation
OT  - Fidelity
OT  - Test Reliability
OT  - Predictive Validity
JT  - Research Papers in Education
SO  - v35 n5 p529-547 2020
AID - http://dx.doi.org/10.1080/02671522.2019.1601759
OID - EJ1268159
VI  - 35
IP  - 5
PG  - 529-547
DP  - 2020
LID - http://eric.ed.gov/?id=EJ1268159
AB  - Measuring the implementation fidelity (IF) or integrity of interventions is crucial, otherwise a positive or negative outcome cannot be interpreted. Direct and indirect methods of IF measurement tend to over-emphasize teacher behaviour. This paper focuses on IF measured by student behaviour collected through computers. Attainment was measured by the STAR test of maths (a computerized item-banked adaptive norm-referenced test). Implementation quality (IF) was measured by Accelerated Maths (AM) (an instruction-free personalized practice and progress-monitoring system in mastery of mathematics skills). Attainment data was gathered in the UK on 20,103 students in 148 schools, and of these implementation data on n =Â 6,285. Only a small percentage of pupils scored on five AM implementation indices at or above the levels recommended. Correlations between attainment and implementation indices were modest, but high implementation was positively correlated with high attainment. Socio-economic status did not appear to affect implementation or attainment. Implementation quality of AM is clearly a problem in the UK, and needs improvement. However, overall students still scored above average on attainment.
ISSN - ISSN-0267-1522
LA  - English
PT  - Journal Articles
PT  - Reports - Research

OWN - ERIC
TI  - Computerized Adaptive Testing, Anxiety Levels, and Gender Differences
AU  - Fritts, Barbara E.
AU  - Marszalek, Jacob M.
OT  - Computer Attitudes
OT  - Adaptive Testing
OT  - Computer Assisted Testing
OT  - Achievement Tests
OT  - Path Analysis
OT  - Gender Differences
OT  - Test Anxiety
OT  - Comparative Analysis
OT  - Middle School Students
OT  - Test Format
OT  - Measures (Individuals)
OT  - Scores
OT  - Stereotypes
OT  - Racial Differences
OT  - Socioeconomic Status
JT  - Social Psychology of Education: An International Journal
SO  - v13 n3 p441-458 Sep 2010
AID - http://dx.doi.org/10.1007/s11218-010-9113-3
OID - EJ896744
VI  - 13
IP  - 3
PG  - 441-458
DP  - Sep 2010
LID - http://eric.ed.gov/?id=EJ896744
AB  - This study compares the amount of test anxiety experienced on a computerized adaptive test (CAT) to a paper-and-pencil test (P&P), as well as the state test anxiety experienced between males and females. Ninety-four middle school CAT examinees were compared to 65 middle school P&P examinees on their responses to the State-Trait Anxiety Inventory for Children (STAIC) after taking a standardized achievement test. Results of a multiple regression showed that P&P examinees had a higher mean STAIC score than CAT examinees after controlling for trait test anxiety and computer anxiety. Evidence of neither a main nor a moderator effect of gender was found. However, a subsequent path analysis gave evidence of an indirect effect of gender on STAIC score mediated by trait test anxiety. Results are discussed in the context of stereotype threat and the implications for the use of CAT in schools, given the digital divide between race and socioeconomic status. Recommendations for future research and practice are offered.
ISSN - ISSN-1381-2890
LA  - English
PT  - Journal Articles
PT  - Reports - Research

OWN - ERIC
TI  - A New Approach to the Measurement of Adaptive Behavior: Development of the PEDI-CAT for Children and Youth with Autism Spectrum Disorders
AU  - Kramer, Jessica M.
AU  - Coster, Wendy J.
AU  - Kao, Ying-Chia
AU  - Snow, Anne
AU  - Orsmond, Gael I.
OT  - Feedback (Response)
OT  - Autism
OT  - Focus Groups
OT  - Computer Assisted Testing
OT  - Pervasive Developmental Disorders
OT  - Behavior Modification
OT  - Evaluation Methods
OT  - Interviews
OT  - Test Items
OT  - Student Behavior
OT  - Parents
OT  - Test Validity
OT  - Adjustment (to Environment)
OT  - Social Cognition
OT  - Parent Responsibility
OT  - Rating Scales
OT  - Parent Attitudes
JT  - Physical & Occupational Therapy in Pediatrics
SO  - v32 n1 p34-47 Feb 2012
AID - http://dx.doi.org/10.3109/01942638.2011.606260
OID - EJ964543
VI  - 32
IP  - 1
PG  - 34-47
DP  - Feb 2012
LID - http://eric.ed.gov/?id=EJ964543
AB  - The use of current adaptive behavior measures in practice and research is limited by their length and need for a professional interviewer. There is a need for alternative measures that more efficiently assess adaptive behavior in children and youth with autism spectrum disorders (ASDs). The Pediatric Evaluation of Disability Inventory-Computer Adaptive Test (PEDI-CAT) is a computer-based assessment of a child's ability to perform activities required for personal self-sufficiency and engagement in the community. This study evaluated the applicability, representativeness, and comprehensiveness of the Daily Activity, Social/Cognitive, and Responsibility domains for children and youth with an ASD. Twenty professionals and 18 parents provided feedback via in-person or virtual focus groups and cognitive interviews. Items were perceived to represent relevant functional activities within each domain. Child factors and assessment characteristics influenced parents' ratings. In response to feedback, 15 items and additional directions were added to ensure the PEDI-CAT is a meaningful measure when used with this population. (Contains 3 tables.)
ISSN - ISSN-0194-2638
LA  - English
PT  - Journal Articles
PT  - Reports - Research

OWN - ERIC
TI  - Indicators of Adaptive Leadership for Teachers in Boromarajonani College of Nursing under Boromarachanok Institute: Developing and Testing the Structural Relationship Model
AU  - Potchana, Ratchanee
AU  - Sanrattana, Wirot
AU  - Suwannoi, Paisan
OT  - Leadership Styles
OT  - Teacher Leadership
OT  - Nursing Education
OT  - Foreign Countries
OT  - Evaluation Criteria
OT  - Teacher Effectiveness
OT  - Adjustment (to Environment)
OT  - Educational Improvement
OT  - College Faculty
JT  - Journal of Education and Learning
SO  - v9 n3 p92-98 2020
OID - EJ1255425
VI  - 9
IP  - 3
PG  - 92-98
DP  - 2020
LID - http://eric.ed.gov/?id=EJ1255425
AB  - Background of the study: Leadership is needed when one must deal with education management which means the relationship of influence between the leader and followers who have the same goal, to make a change. It can be used as a guideline in the planning and creating evaluation criteria to improve the adaptive leadership of teachers at Boromarajonani College of Nursing which consistent with the theory and the research result as well as the vision from scholars and organization. Objective: The objectives of this research were to (a) develop a theoretical model of structural relationship of major components, minor components and indicators of adaptive leadership, (b) test the fitness of the model was developed from theoretical model and empirical data, and (c) verify the factor loading value of the major components, minor components and indicators. Materials and Methods: The descriptive research collected by using the 5-level rating scale questionnaire. The population consisted of 1,740 teachers in Boromarajonani college of nursing under Boromarajchanok Institute. The data analysis using AMOS statistical program was done with the 617-return questionnaire. Results: The results showed that the theoretical model was consistent with empirical data with the following statistical metrics: relative Chi-square (CMIN/DF) < 3.00, root mean square error of approximation (RMSEA) < 0.05, goodness-of-fit index (GFI), adjusted goodness-of-fit index (AGFI), comparative fit index (CFI), and normed fit index (NFI) 0.90-1.00 were in accordance with the criteria from both the first and second order of confirmative factor analysis. The major components had factor loading ranging from 0.71 to 1.00, which is higher than the criterion as 0.70. The minor components and indicators had factor loading are higher than the criterion as 0.30. Conclusion: The finding can be used as a guideline in the planning and creation of evaluation criteria to improve the adaptive leadership of teachers at Boromarajonani College of Nursing which is consistent with the theory and research results as well as the vision from scholars and organization. This will help improve the performance efficiency of the teachers at Boromarajonani College of Nursing which will affect the education management and education improvement.
ISSN - ISSN-1927-5250
LA  - English
PT  - Journal Articles
PT  - Reports - Research

OWN - ERIC
TI  - Intergenerational Transmission of Adaptive Functioning: A Test of the Interactionist Model of SES and Human Development
AU  - Schofield, Thomas J
AU  - Martin, Monica J.
AU  - Conger, Katherine J.
AU  - Neppl, Tricia M.
AU  - Donnellan, M. Brent
AU  - Conger, Rand D.
OT  - Socioeconomic Status
OT  - Individual Development
OT  - Models
OT  - Individual Characteristics
OT  - Adolescents
OT  - Personality
OT  - Interpersonal Competence
OT  - Family Characteristics
OT  - Child Rearing
OT  - Goal Orientation
OT  - Psychological Patterns
OT  - Work Ethic
JT  - Child Development
SO  - v82 n1 p33-47 Jan-Feb 2011
AID - http://dx.doi.org/10.1111/j.1467-8624.2010.01539.x
OID - EJ927863
VI  - 82
IP  - 1
PG  - 33-47
DP  - Jan-Feb 2011
LID - http://eric.ed.gov/?id=EJ927863
AB  - The interactionist model (IM) of human development (R. D. Conger & M. B. Donellan, 2007) proposes that the association between socioeconomic status (SES) and human development involves a dynamic interplay that includes both social causation (SES influences human development) and social selection (individual characteristics affect SES). Using a multigenerational data set involving 271 families, the current study finds empirical support for the IM. Adolescent personality characteristics indicative of social competence, goal-setting, hard work, and emotional stability predicted later SES, parenting, and family characteristics that were related to the positive development of a third-generation child. Processes of both social selection and social causation appear to account for the association between SES and dimensions of human development indicative of healthy functioning across multiple generations. (Contains 3 tables and 1 figure.)
ISSN - ISSN-0009-3920
LA  - English
PT  - Journal Articles
PT  - Reports - Research

OWN - ERIC
TI  - Reforming Federal Testing Policy to Support Teaching and Learning
AU  - Yeh, Stuart S.
OT  - Federal Legislation
OT  - Educational Policy
OT  - Outcomes of Education
OT  - Accountability
OT  - Academic Achievement
OT  - Adaptive Testing
OT  - Computer Assisted Testing
OT  - Test Format
OT  - Teacher Attitudes
OT  - Administrator Attitudes
OT  - Test Construction
OT  - Educational Change
OT  - State Standards
JT  - Educational Policy
SO  - v20 n3 p495-524 2006
AID - http://dx.doi.org/10.1177/0895904805284119
OID - EJ737335
VI  - 20
IP  - 3
PG  - 495-524
DP  - 2006
LID - http://eric.ed.gov/?id=EJ737335
AB  - The No Child Left Behind Act (NCLB) assumes that state-mandated tests provide useful information to school administrators and teachers. However, interviews with administrators and teachers suggest that Minnesota's tests, which are representative of the current generation of state-mandated tests, fail to provide useful information to administrators and teachers about areas that need attention. The use of a single level of difficulty causes harmful stress and provides measurements of poor quality for many students. Computer-adaptive tests are a practical alternative, provide more useful information, reduce stress among students, and address four major concerns that teachers have about testing. This alternative could strengthen the link between testing and improved educational outcomes and improve the chances that NCLB will have a positive impact. However, current federal policy prohibits the use of computer-adaptive tests for accountability purposes. This article provides specific recommendations that potentially could address four critical concerns about current state-mandated tests. (Contains 1 figure, 1 table, and 6 notes.)
ISSN - ISSN-0895-9048
LA  - English
PT  - Journal Articles
PT  - Reports - Research

OWN - ERIC
TI  - A Comparison of Anchor-Item Designs for the Concurrent Calibration of Large Banks of Likert-Type Items
AU  - Garcia-Perez, Miguel A.
AU  - Alcala-Quintana, Rocio
AU  - Garcia-Cueto, Eduardo
OT  - Comparative Analysis
OT  - Test Items
OT  - Equated Scores
OT  - Item Banks
OT  - Computer Assisted Testing
OT  - Adaptive Testing
OT  - Likert Scales
JT  - Applied Psychological Measurement
SO  - v34 n8 p580-599 Nov 2010
AID - http://dx.doi.org/10.1177/0146621609351259
OID - EJ900846
VI  - 34
IP  - 8
PG  - 580-599
DP  - Nov 2010
LID - http://eric.ed.gov/?id=EJ900846
AB  - Current interest in measuring quality of life is generating interest in the construction of computerized adaptive tests (CATs) with Likert-type items. Calibration of an item bank for use in CAT requires collecting responses to a large number of candidate items. However, the number is usually too large to administer to each subject in the calibration sample. The concurrent anchor-item design solves this problem by splitting the items into separate subtests, with some common items across subtests; then administering each subtest to a different sample; and finally running estimation algorithms once on the aggregated data array, from which a substantial number of responses are then missing. Although the use of anchor-item designs is widespread, the consequences of several configuration decisions on the accuracy of parameter estimates have never been studied in the polytomous case. The present study addresses this question by simulation, comparing the outcomes of several alternatives on the configuration of the anchor-item design. The factors defining variants of the anchor-item design are (a) subtest size, (b) balance of common and unique items per subtest, (c) characteristics of the common items, and (d) criteria for the distribution of unique items across subtests. The results of this study indicate that maximizing accuracy in item parameter recovery requires subtests of the largest possible number of items and the smallest possible number of common items; the characteristics of the common items and the criterion for distribution of unique items do not affect accuracy. (Contains 1 table, 10 figures, and 1 note.)
ISSN - ISSN-0146-6216
LA  - English
PT  - Journal Articles
PT  - Reports - Research

OWN - ERIC
TI  - Accelerated Desensitization with Adaptive Attitudes and Test Gains with 5th Graders
AU  - Miller, Melanie
AU  - Morton, Jerome
AU  - Driscoll, Richard
AU  - Davis, Kai A.
OT  - Intervention
OT  - Desensitization
OT  - Relaxation Training
OT  - Stress Management
OT  - Test Anxiety
OT  - Grade 5
OT  - Program Effectiveness
OT  - Test Wiseness
JT  - Online Submission
SO  - Paper presented at the American Psychological Association Convention (Washington, DC, Aug 20, 2005)
OID - ED495137
DP  - 2006
LID - http://eric.ed.gov/?id=ED495137
AB  - The study evaluates an easily-administered test-anxiety reduction program. An entire fifth grade was screened, and 36 students identified as test-anxious were randomly assigned to an Intervention or a non-participant Control group. The intervention was an accelerated desensitization and adaptive attitudes (ADAA) treatment which involved stretch-tense, deep breath, release-relax and positive suggestion sequences to expedite anxiety reduction and also positive adaptive associations to each of eight learning, review, and testing scenes. The intervention was presented via a 31 minute recording, which students reviewed in five separate sessions over about half the school year. Test gains were calculated from the 2005 Tennessee Comprehensive Assessment tests (TCAPs) given after the intervention, adjusted for individual student 2004 TCAPs before the intervention. The Intervention group attained a significant seven percentile test gain over the Controls. The use of the recorded intervention is seen to vastly reduce the number of professional hours required for an anxiety-reduction program, and the program shows promise as a highly cost-effective means to benefit test-anxious students. (Contains 2 tables.)
PT  - Reports - Evaluative
PT  - Speeches/Meeting Papers

OWN - ERIC
TI  - Computerized Testing.
AU  - Olson, Allan
OT  - Accountability
OT  - Achievement Tests
OT  - Computer Assisted Testing
OT  - Elementary Secondary Education
OT  - Internet
OT  - Scores
OT  - Student Evaluation
OT  - Test Validity
JT  - American School Board Journal
SO  - v187 n3 suppl Mar 2000
OID - EJ601216
VI  - 187
IP  - 3
DP  - suppl Mar 2000
LID - http://eric.ed.gov/?id=EJ601216
AB  - The Northwest Evaluation Association, serving over 300 U.S. school districts, is developing an Internet-enabled assessment system that adapts questions to each student's performance. Shorter, adaptive tests help students avoid frustrations or boredom caused by too-difficult or -easy questions. Scores are as valid as traditional test scores. (MLH)
PT  - Journal Articles
PT  - Reports - Descriptive

OWN - ERIC
TI  - Proceedings of the Seventh International Conference on Educational Data Mining (EDM) (7th, London, United Kingdom, July 4-7, 2014)
AU  - Stamper, John, Ed.
AU  - Pardos, Zachary, Ed.
AU  - Mavrikis, Manolis, Ed.
AU  - McLaren, Bruce M., Ed.
OT  - Information Retrieval
OT  - Data Processing
OT  - Data Analysis
OT  - Data Collection
OT  - Educational Research
OT  - Online Courses
OT  - Prior Learning
OT  - Teaching Methods
OT  - Causal Models
OT  - Behavior
OT  - Algebra
OT  - Video Games
OT  - Scores
OT  - Grade 6
OT  - Mathematics Achievement
OT  - Standards
OT  - Evaluation Methods
OT  - Prediction
OT  - Gender Differences
OT  - Foreign Countries
OT  - Knowledge Level
OT  - Electronic Learning
OT  - Problem Solving
OT  - Item Response Theory
OT  - Programming
OT  - Individual Differences
OT  - Generalization
OT  - Models
OT  - Educational Games
OT  - Video Technology
OT  - Group Discussion
OT  - Computer Mediated Communication
OT  - Large Group Instruction
OT  - Verbal Ability
OT  - Eye Movements
OT  - Affective Behavior
OT  - Sequential Approach
OT  - Maps
OT  - Visualization
OT  - Simulation
OT  - Intelligent Tutoring Systems
OT  - Natural Language Processing
OT  - Textbooks
OT  - Risk
OT  - Learner Engagement
OT  - Mathematics
OT  - Computer Assisted Testing
OT  - Student Evaluation
OT  - Learning Theories
OT  - Classroom Communication
OT  - Scientific Concepts
OT  - Error Patterns
OT  - Learning Processes
OT  - Logical Thinking
OT  - Questioning Techniques
OT  - STEM Education
OT  - Majors (Students)
OT  - Nonmajors
OT  - College Attendance
OT  - Middle Schools
OT  - Computer Software
OT  - Mathematics Education
OT  - Matrices
OT  - Individualized Instruction
OT  - Case Studies
OT  - Science Instruction
OT  - Freehand Drawing
OT  - Topology
OT  - Computer Security
OT  - Feedback (Response)
OT  - Mathematics Instruction
OT  - Grammar
OT  - Writing Evaluation
OT  - Writing Instruction
OT  - Writing Skills
OT  - Interaction
OT  - Nonverbal Communication
OT  - Lecture Method
OT  - Grading
OT  - Self Evaluation (Individuals)
OT  - Student Behavior
OT  - Academic Persistence
OT  - Dropouts
OT  - Reading Comprehension
OT  - Statistical Analysis
OT  - Blended Learning
OT  - Comparative Analysis
OT  - Cost Effectiveness
OT  - Undergraduate Study
OT  - Course Evaluation
OT  - Cognitive Processes
OT  - Planning
OT  - Correlation
OT  - Learning Experience
OT  - Diagnostic Tests
OT  - Computer Peripherals
OT  - Asynchronous Communication
OT  - Standardized Tests
OT  - Coding
OT  - Student Characteristics
OT  - Classification
OT  - Educational Environment
OT  - Tests
OT  - Developmental Studies Programs
OT  - Remedial Mathematics
OT  - Hypothesis Testing
OT  - Essays
OT  - Visual Aids
OT  - Research Methodology
OT  - Educational Objectives
OT  - Cooperative Learning
OT  - Student Reaction
OT  - Group Dynamics
OT  - Peer Evaluation
OT  - Peer Influence
OT  - Student Attrition
OT  - Academic Achievement
OT  - Factor Analysis
OT  - Electronic Publishing
OT  - Measurement Techniques
OT  - Homework
OT  - Learning Modalities
OT  - Experiential Learning
OT  - Self Esteem
OT  - Emotional Response
OT  - Student Motivation
OT  - Incentives
OT  - Graphs
OT  - Integrated Learning Systems
JT  - International Educational Data Mining Society
OID - ED558339
DP  - 2014
LID - http://eric.ed.gov/?id=ED558339
AB  - The 7th International Conference on Education Data Mining held on July 4th-7th, 2014, at the Institute of Education, London, UK is the leading international forum for high-quality research that mines large data sets in order to answer educational research questions that shed light on the learning process. These data sets may come from the traces that students leave when they interact, either individually or collaboratively, with learning management systems, interactive learning environments, intelligent tutoring systems, educational games or when they participate in a data-rich learning context. The types of data therefore range from raw log files to eyetracking devices and other sensor data. Being hosted in London, UK the theme of the conference is "Big Data--Big Ben--Education Data Mining for Big Impact in Teaching and Learning". In this seventh year of EDM conferences, it is clear that the field is continuing to grow at a rapid pace. With renewed focus on education driven by big data learning analytics has put the EDM field in the center of growing interest. Traditional educational technologies, intelligent tutoring systems, educational games, and learning management systems all continue to generate growing amounts of data that are becoming available for analysis. The new interest in MOOCs and their promise to reach thousands or even hundreds of thousands of students per class requires techniques for feedback and grading that are being researched in the EDM domain. The conference submissions this year also continue to grow. A tremendous amount of work has gone into bringing this conference together, and the following are presented: (1) The Field of EDM: Where We Came from and Where We're Going (Joseph Beck); (2) Generative Adaptivity for Optimization of the Learning Ecosystem (Zoran Popovic; (3) 150K+ Online Students at a Time: How to Understand What's Happening in Online 4 Learning (Daniel Russell); (4) Adaptive Practice of Facts in Domains with Varied Prior Knowledge (Jan PapouÅ¡ek, Radek PelÃ¡nek and VÃ­t Stanislav); (5) Alternating Recursive Method for Q-Matrix Learning (Yuan Sun, Shiwei Ye, Shunya Inoue and Yi Sun); (6) Application of Time Decay Functions and the Elo System in Student Modeling (Radek PelÃ¡nek); (7) Causal Discovery with Models: Behavior, Affect, and Learning in Cognitive Tutor Algebra (Stephen Fancsali); (8) Choice-Based Assessment: Can Choices Made in Digital Games Predict 6th-Grade Students' Math Test Scores? (Min Chi, Daniel Schwartz, Kristen Pilner Blair and Doris B. Chin); (9) Comparing Expert and Metric-Based Assessments of Association Rule Interestingness (Diego Luna Bazaldua, Ryan Baker and Maria Ofelia San Pedro); (10) Different Parameters - Same Prediction: An Analysis of Learning Curves (Tanja KÃ¤ser, Kenneth Koedinger and Markus Gross); (11) Discovering Gender-Specific Knowledge from Finnish Basic Education Using PISA Scale Indices (Mirka Saarela and Tommi KÃ¤rkkÃ¤inen); (12) EduRank: A Collaborative Filtering Approach to Personalization in E-Learning (Avi Segal, Ziv Katzir, Kobi Gal, Guy Shani and Bracha Shapira); (13) Exploring Differences in Problem Solving with Data-Driven Approach Maps (Michael Eagle and Tiffany Barnes); (14) General Features in Knowledge Tracing: Applications to Model Multiple Subskills, Temporal Item Response Theory, and Expert Knowledge (JosÃ© GonzÃ¡lez-Brenes, Yun Huang and Peter Brusilovsky); (15) Generating Hints for Programming Problems Using Intermediate Output ( Barry Peddycord III, Andrew Hicks and Tiffany Barnes); (16) Integrating Latent-Factor and Knowledge-Tracing Models to Predict Individual Differences in Learning (Mohammad Khajah, Rowan Wing, Robert Lindsey and Michael Mozer); (17) Interpreting Model Discovery and Testing Generalization to a New Dataset (Ran Liu, Elizabeth A. McLaughlin and Kenneth R. Koedinger); (18) Learning Individual Behavior in an Educational Game: A Data-Driven Approach (Seong Jae Lee, Yun-En Liu and Zoran Popovic); (19) Predicting Learning and Affect from Multimodal Data Streams in Task-Oriented Tutorial Dialogue (Joseph Grafsgaard, Joseph Wiggins, Kristy Elizabeth Boyer, Eric Wiebe and James Lester); (20) Sentiment Analysis in MOOC Discussion Forums: What does It Tell Us? (Miaomiao Wen, Diyi Yang and Carolyn Rose); (21) The Effect of Mutual Gaze Perception on Students' Verbal Coordination (Bertrand Schneider and Roy Pea); (22) The Opportunities and Limitations of Scaling Up Sensor-Free Affect Detection (Michael Wixon, Ivon Arroyo, Kasia Muldner, Winslow Burleson, Cecil Lozano and Beverly Woolf); (23) The Problem Solving Genome: Analyzing Sequential Patterns of Student Work with Parameterized Exercises (Julio Guerra, Shaghayegh Sahebi, Peter Brusilovsky and Yu-Ru Lin); (24) Trading Off Scientific Knowledge and User Learning with Multi-Armed Bandits (Yun-En Liu, Travis Mandel, Emma Brunskill and Zoran Popovic); (25) Vertical and Stationary Scales for Progress Maps (Russell Almond, Ilya Goldin, Yuhua Guo and Nan Wang); (26) Visualization and Confirmatory Clustering of Sequence Data from a Simulation- Based Assessment Task (Yoav Bergner, Zhan Shu and Alina von Davier); (27) Who's in Control?: Categorizing Nuanced Patterns of Behaviors within a Game- Based Intelligent Tutoring System (Erica Snow, Laura Allen, Devin Russell and Danielle McNamara); (28) Acquisition of Triples of Knowledge from Lecture Notes: A Natural Language Processing Approach (Thushari Atapattu, Katrina Falkner and Nickolas Falkner); (29) Towards Assessing Students' Prior Knowledge from Tutorial Dialogues (Dan Stefanescu, Vasile Rus and Art Graesser); (30) Assigning Educational Videos at Appropriate Locations in Textbooks (Marios Kokkodis, Anitha Kannan and Krishnaram Kenthapadi) (31) Better Data Beats Big Data (Michael Yudelson, Stephen Fancsali, Steven Ritter, Susan Berman, Tristan Nixon and Ambarish Joshi); (32) Building a Student At-Risk Model: An End-to-End Perspective (Lalitha Agnihotri and Alexander Ott); (33) Can Engagement be Compared? Measuring Academic Engagement for Comparison (Ling Tan, Xiaoxun Sun and Siek Toon Khoo); (34) Comparison of Algorithms for Automatically Building Example-Tracing Tutor Models (Rohit Kumar, Matthew Roy, Bruce Roberts and John Makhoul); (35) Computer-Based Adaptive Speed Tests (Daniel Bengs and Ulf Brefeld); (36) Discovering Students' Complex Problem Solving Strategies in Educational Assessment (Krisztina TÃ³th, Heiko RÃ¶lke, Samuel Greiff and Sascha WÃ¼stenberg); (37) Discovering Theoretically Grounded Predictors of Shallow vs. Deep-Level Learning (Carol Forsyth, Arthur Graesser, Philip I. Pavlik Jr., Keith Millis and Borhan Samei); (38) Domain Independent Assessment of Dialogic Properties of Classroom Discourse (Borhan Samei, Andrew Olney, Sean Kelly, Martin Nystrand, Sidney D'Mello, Nathan Blanchard, Xiaoyi Sun, Marci Glaus and Art Graesser); (39) Empirically Valid Rules for Ill-Defined Domains (Collin Lynch and Kevin Ashley); (40) Entropy: A Stealth Measure of Agency in Learning Environments (Erica Snow, Matthew Jacovina, Laura Allen, Jianmin Dai and Danielle McNamara); (41) Error Analysis as a Validation of Learning Progressions (Brent Morgan, William Baggett and Vasile Rus); (42) Exploration of Student's Use of Rule Application References in a Propositional Logic Tutor (Michael Eagle, Vinaya Polamreddi, Behrooz Mostafavi and Tiffany Barnes); (43) Exploring Real-Time Student Models Based on Natural-Language Tutoring Sessions (Benjamin Nye, Mustafa Hajeer, Carolyn Forsyth, Borhan Samei, Xiangen Hu and Keith Millis); (44) Forum Thread Recommendation for Massive Open Online Courses (Diyi Yang, Mario Piergallini, Iris Howley and Carolyn Rose); (45) Investigating Automated Student Modeling in a Java MOOC (Michael Yudelson, Roya Hosseini, Arto Vihavainen and Peter Brusilovsky); (46) Mining Gap-Fill Questions from Tutorial Dialogues (Nobal B. Niraula, Vasile Rus, Dan Stefanescu and Arthur C. Graesser); (47) Online Optimization of Teaching Sequences with Multi-Armed Bandits (Benjamin Clement, Pierre-Yves Oudeyer, Didier Roy and Manuel Lopes); (48) Predicting MOOC Performance with Week 1 Behavior (Suhang Jiang, Adrienne Williams, Katerina Schenke, Mark Warschauer and Diane O'Dowd); (49) Predicting STEM and Non-STEM College Major Enrollment from Middle School Interaction with Mathematics Educational Software (Maria Ofelia San Pedro, Jaclyn Ocumpaugh, Ryan Baker and Neil Heffernan); (50) Quantized Matrix Completion for Personalized Learning (Andrew Lan, Christoph Studer and Richard Baraniuk); (51) Reengineering the Feature Distillation Process: A Case Study in Detection of Gaming the System (Luc Paquette, Adriana de Carvahlo, Ryan Baker and Jaclyn Ocumpaugh); (52) SKETCHMINER: Mining Learner-Generated Science Drawings with Topological Abstraction (Andy Smith, Eric N. Wiebe, Bradford W. Mott and James C. Lester); (53) Teachers and Students Learn Cyber Security: Comparing Software Quality, Security (Shlomi Boutnaru and Arnon Hershkovitz); (54) Testing the Multimedia Principle in the Real World: A Comparison of Video vs. Text Feedback in Authentic Middle School Math Assignments (Korinn Ostrow and Neil Heffernan); (55) The Importance of Grammar and Mechanics in Writing Assessment and Instruction: Evidence from Data Mining (Scott Crossley, Kris Kyle, Laura Allen and Danielle McNamara); (56) The Long and Winding Road: Investigating the Differential Writing Patterns of High and Low Skilled Writers (Laura Allen, Erica Snow and Danielle McNamara); (57) The Refinement of a Q-Matrix: Assessing Methods to Validate Tasks to Skills Mapping (Michel Desmarais, Behzad Beheshti and Peng Xu); (58) Tracing Knowledge and Engagement in Parallel in an Intelligent Tutoring System (Sarah Schultz and Ivon Arroyo); (59) Tracking Choices: Computational Analysis of Learning Trajectories (Erica Snow, Laura Allen, G.Tanner Jackson and Danielle McNamara); (60) Unraveling Students' Interaction Around a Tangible Interface Using Gesture Recognition (B
ISBN - 978-0-9839525-4-1
PT  - Collected Works - Proceedings

OWN - ERIC
TI  - Psychological Effects of Immediate Knowledge of Results and Adaptive Ability Testing. Research Report 76-4.
AU  - Betz, Nancy E.
AU  - Weiss, David J.
OT  - Ability
OT  - Achievement Tests
OT  - Anxiety
OT  - Branching
OT  - College Students
OT  - Computer Assisted Testing
OT  - Computer Oriented Programs
OT  - Difficulty Level
OT  - Feedback
OT  - Motivation
OT  - Response Style (Tests)
OT  - Statistical Analysis
OT  - Student Attitudes
OT  - Test Results
OT  - Testing
OID - ED129863
DP  - 1976
LID - http://eric.ed.gov/?id=ED129863
AB  - The effects of providing immediate knowledge of results (KR) and adaptive testing on test anxiety and test-taking motivation were investigated. Also studied was the accuracy of student perceptions of the difficulty of adaptive and conventional tests administered with or without immediate knowledge of results. Testees were 350 college students divided into high- and low-ability groups and randomly assigned to one of four test strategies by KR conditions. The ability level of examinees was found to be related to their reported levels of motivation and to differences in reported motivation under the different testing conditions. These results suggest that adaptive testing creates a psychological environment for testing which is more equivalently motivating for examinees of all  ability levels and results in a greater standardization of the test-taking environment, than does conventional testing. (Author)
PT  - Reports - Research

OWN - ERIC
TI  - Psychometric Characteristics of Computer-Adaptive and Self-Adaptive Vocabulary Tests: The Role of Answer Feedback and Test Anxiety.
AU  - Vispoel, Walter P.
OT  - Adaptive Testing
OT  - College Students
OT  - Computer Assisted Testing
OT  - Feedback
OT  - Higher Education
OT  - Psychometrics
OT  - Test Anxiety
OT  - Test Items
OT  - Vocabulary
JT  - Journal of Educational Measurement
SO  - v35 n2 p155-67 Sum 1998
OID - EJ590595
VI  - 35
IP  - 2
PG  - 155-67
DP  - Sum 1998
LID - http://eric.ed.gov/?id=EJ590595
AB  - Studied effects of administration mode [computer adaptive test (CAT) versus self-adaptive test (SAT)], item-by-item answer feedback, and test anxiety on results from computerized vocabulary tests taken by 293 college students. CATs were more reliable than SATs, and administration time was less when feedback was provided. (SLD)
ISSN - ISSN-0022-0655
LA  - English
PT  - Journal Articles
PT  - Reports - Research
PT  - Speeches/Meeting Papers

OWN - ERIC
TI  - A Comparison of Self-Adapted and Computerized Adaptive Tests.
AU  - Wise, Steven L.
AU  - And Others
OT  - Adaptive Testing
OT  - Comparative Testing
OT  - Computer Assisted Testing
OT  - Difficulty Level
OT  - Estimation (Mathematics)
OT  - Graduate Students
OT  - Higher Education
OT  - Item Response Theory
OT  - Mathematics Tests
OT  - Multiple Choice Tests
OT  - Scores
OT  - Test Anxiety
OT  - Test Construction
OT  - Test Format
OT  - Test Items
OT  - Undergraduate Students
JT  - Journal of Educational Measurement
SO  - v29 n4 p329-39 Win 1992
OID - EJ455219
VI  - 29
IP  - 4
PG  - 329-39
DP  - Win 1992
LID - http://eric.ed.gov/?id=EJ455219
AB  - Performance of 156 undergraduate and 48 graduate students on a self-adapted test (SFAT)--students choose the difficulty level of their test items--was compared with performance on a computer-adapted test (CAT). Those taking the SFAT obtained higher ability scores and reported lower posttest state anxiety than did CAT takers. (SLD)
ISSN - ISSN-0022-0655
LA  - English
PT  - Journal Articles
PT  - Reports - Research

OWN - ERIC
TI  - Individual Differences in Computer Adaptive Testing: Anxiety, Computer Literacy and Satisfaction.
AU  - Gershon, Richard C.
AU  - Bergstrom, Betty
OT  - Adaptive Testing
OT  - Adults
OT  - Computer Assisted Testing
OT  - Computer Literacy
OT  - Individual Differences
OT  - Satisfaction
OT  - Test Anxiety
OT  - Test Length
OID - ED400285
DP  - 1991
LID - http://eric.ed.gov/?id=ED400285
AB  - The relationship of several individual differences variables to Computer Adaptive Testing (CAT) as compared with traditional written tests are explored. Seven hundred sixty-five examinees took a Computer Adaptive Test and two fixed-length written tests. Each examinee also answered a computer literacy inventory, a satisfaction questionnaire, and a test anxiety survey. Test anxiety was found to be a significant factor in performance on both of the written tests, but not on the CAT test. Anxiety was also found to be a significant factor on several of the items on the satisfaction questionnaire. Overall, significant factors that predict satisfaction with CAT testing included level of test anxiety, computer literacy, and test length (the CAT test varied in terms of the number of items  administered). Results are discussed in terms of the political and practical implications of administering CAT tests as compared to administering traditional written tests. The results also indicate that some of the individual differences variables that have been found to affect performance on written tests are not significant in CAT. (Contains two tables and six references.) (Author/SLD)
PT  - Reports - Research
PT  - Speeches/Meeting Papers

OWN - ERIC
TI  - A Comparison of Self-Adapted and Computer-Adaptive Tests.
AU  - Wise, Steven L.
AU  - And Others
OT  - Ability Identification
OT  - Adaptive Testing
OT  - College Students
OT  - Comparative Testing
OT  - Computer Assisted Testing
OT  - Difficulty Level
OT  - Higher Education
OT  - Item Banks
OT  - Item Response Theory
OT  - Statistics
OT  - Test Anxiety
OID - ED331888
DP  - 1991
LID - http://eric.ed.gov/?id=ED331888
AB  - According to item response theory (IRT), examinee ability estimation is independent of the particular set of test items administered from a calibrated pool. Although the most popular application of this feature of IRT is computerized adaptive (CA) testing, a recently proposed alternative is self-adapted (SA) testing, in which examinees choose the difficulty level of each of their test items. Examinee performance was compared under CA and SA testing conditions for college students from an introductory statistics course. Three test forms were developed, testing mathematical knowledge necessary for the course. The final pool contained 93 items which were administered to 204 subjects. The SA test yielded significantly higher ability scores, and examinees taking the SA test reported  significantly lower posttest state anxiety. Implications of the differences between the two test types for measurement practice are discussed. Three tables present study data. (Author/SLD)
PT  - Reports - Research
PT  - Speeches/Meeting Papers

OWN - ERIC
TI  - Computerized Adaptive Tests. ERIC Digest No. 107.
AU  - Grist, Susan
AU  - And Others
OT  - Ability Identification
OT  - Adaptive Testing
OT  - Computer Assisted Testing
OT  - Computer Uses in Education
OT  - Elementary Secondary Education
OT  - Individual Testing
OT  - Test Construction
OT  - Test Use
OT  - Testing Problems
OID - ED315425
DP  - 1989
LID - http://eric.ed.gov/?id=ED315425
AB  - Computerized adaptive tests (CATs) make it possible to estimate the ability of each student during the testing process. The computer presents items to students at the appropriate level, and students take different versions of the same test. Computerized testing increases the flexibility of test management in that: (1) tests are given on demand and scores are immediately available; (2) differences among administrators cannot affect scores and trained administrators are not needed; (3) tests can be individually paced; and (4) test security is increased. Computerized testing also offers options for timing and formatting, increases efficiency, and can provide accurate scores over a wide range of abilities. Some limitations to CATs are considered. CATs are not appropriate for some  subjects and skills. Hardware limitations restrict the types of items that can be administered by computer, and many schools simply do not have the resources to administer CATs. A relatively large sample is needed to norm test items; comparable scores depend heavily on the quality of the estimates of item characteristics because each student answers a different set of items. The military has been among the pioneers in using CATs and at least two public school systems have begun to use them. A list of six organizations involved in computerized adaptive testing is included. (SLD)
PT  - ERIC Publications
PT  - Reports - Evaluative
PT  - ERIC Digests in Full Text

OWN - ERIC
TI  - The Relationship between Examinee Anxiety and Preference for Self-Adapted Testing.
AU  - Wise, Steven L.
AU  - And Others
OT  - Adaptive Testing
OT  - College Students
OT  - Computer Assisted Testing
OT  - Higher Education
OT  - Mathematics Anxiety
OT  - Student Attitudes
OT  - Test Anxiety
JT  - Applied Measurement in Education
SO  - v7 n1 p81-91 1994
OID - EJ484374
VI  - 7
IP  - 1
PG  - 81-91
DP  - 1994
LID - http://eric.ed.gov/?id=EJ484374
AB  - The hypothesis that previously found effects of self-adapted testing (SAT) are attributable to examinees' having an increased perception of control over a stressful testing situation was studied with 377 college students who took computerized adaptive tests or SAT. The strongest preference for SAT was seen in individuals with the highest mathematics anxiety. (SLD)
ISSN - ISSN-0895-7347
LA  - English
PT  - Reports - Research
PT  - Journal Articles

OWN - ERIC
TI  - An Investigation of Self-Adapted Testing in a Spanish High School Population.
AU  - Ponsoda, Vincente
AU  - And Others
OT  - Ability
OT  - Adaptive Testing
OT  - Anxiety
OT  - Comparative Analysis
OT  - Computer Assisted Testing
OT  - English (Second Language)
OT  - Foreign Countries
OT  - High School Students
OT  - High Schools
OT  - Test Format
JT  - Educational and Psychological Measurement
SO  - v57 n2 p210-21 Apr 1997
OID - EJ545463
VI  - 57
IP  - 2
PG  - 210-21
DP  - Apr 1997
LID - http://eric.ed.gov/?id=EJ545463
AB  - A study involving 209 Spanish high school students compared computer-based English vocabulary tests: (1) a self-adapted test (SAT); (2) a computerized adaptive test (CAT); (3) a conventional test; and (4) a test combining SAT and CAT. No statistically significant differences were found among test types for estimated ability or posttest anxiety. (SLD)
ISSN - ISSN-0013-1644
LA  - English
PT  - Reports - Research
PT  - Journal Articles

OWN - ERIC
TI  - Equivalence of Rasch Item Calibrations and Ability Estimates across Modes of Administration.
AU  - Bergstrom, Betty A.
AU  - Lunz, Mary E.
OT  - Ability
OT  - Adaptive Testing
OT  - Algorithms
OT  - Computer Assisted Testing
OT  - Estimation (Mathematics)
OT  - Item Response Theory
OT  - Medical Technologists
OT  - Test Format
OID - ED400281
DP  - 1991
LID - http://eric.ed.gov/?id=ED400281
AB  - The equivalence of pencil and paper Rasch item calibrations when used in a computer adaptive test administration was explored in this study. Items (n=726) were precalibarted with the pencil and paper test administrations. A computer adaptive test was administered to 321 medical technology students using the pencil and paper precalibrations in the item selection algorithms and in the computation of examinee ability estimates. The response data from the computer adaptive test administration were analyzed yielding recalibrated item difficulties and examinee ability estimates. Item precalibrations were compared with item recalibrations. Examinee ability estimates obtained using the item precalibrations on the computer adaptive administration were compared with the examinee ability  estimates obtained from using the item recalibrations on the computer adaptive administration. The correlation for examinee ability estimates was 0.99 and for item correlations it was 0.90. Some item calibrations shifted but most remained consistent within the limits of error. Item shift, however, did not affect the ordering of examinee ability estimates. (Contains 1 table, 3 figures, and 23 references.) (Author/SLD)
PT  - Reports - Research
PT  - Speeches/Meeting Papers

OWN - ERIC
TI  - An Investigation of Restricted Self-Adapted Testing.
AU  - Wise, Steven L.
AU  - And Others
OT  - Achievement Tests
OT  - Adaptive Testing
OT  - Comparative Testing
OT  - Computer Assisted Testing
OT  - Difficulty Level
OT  - Elementary Education
OT  - Elementary School Students
OT  - Mathematics Achievement
OT  - Mathematics Tests
OT  - Pretests Posttests
OT  - Selection
OT  - Test Anxiety
OT  - Test Items
OT  - Testing Problems
OID - ED358153
DP  - 1993
LID - http://eric.ed.gov/?id=ED358153
AB  - A new testing strategy that provides protection against the problem of having examinees in adaptive testing choose difficulty levels that are not matched to their proficiency levels was introduced and evaluated. The method, termed restricted self-adapted testing (RSAT), still provides examinees with a degree of control over the difficulty levels of their test items. The range of item choice is restricted to a region around the examinee's current proficiency estimate. Participants in this study were 186 students in grades 3 through 8 in the Portland (Oregon) Public School system during the winter of 1992-93, who were tested as part of an ongoing computerized adaptive testing program. Students were randomly assigned to a computerized adaptive test (CAT), a self-adaptive test  (SADT), or RSAT in mathematics. Results indicate no differences between CAT and SADT conditions in terms of mean proficiency and mean posttest state anxiety. The basic RSAT method appears to hold promise for providing examinees with control over the testing situation, while preventing large mismatches between item difficulty choice and proficiency level. The RSAT procedure should be evaluated empirically. (SLD)
PT  - Reports - Research
PT  - Speeches/Meeting Papers

OWN - ERIC
TI  - A Comparison of the Fairness of Adaptive and Conventional Testing Strategies. Research Report 78-1.
AU  - Pine, Steven M.
AU  - Weiss, David J.
OT  - Adaptive Testing
OT  - Bayesian Statistics
OT  - Comparative Testing
OT  - Computer Assisted Testing
OT  - Item Analysis
OT  - Minority Groups
OT  - Occupational Tests
OT  - Personnel Selection
OT  - Prediction
OT  - Simulation
OT  - Test Bias
OT  - Test Construction
OT  - Test Items
OT  - Test Validity
OID - ED163036
DP  - 1978
LID - http://eric.ed.gov/?id=ED163036
AB  - This report examines how selection fairness is influenced by the characteristics of a selection instrument in terms of its distribution of item difficulties, level of item discrimination, degree of item bias, and testing strategy. Computer simulation was used in the administration of either a conventional or Bayesian adaptive ability test to a hypothetical target population consisting of a minority and majority subgroup. Fairness was evaluated by three indices which reflect the degree of differential validity, errors in prediction (Cleary's model), and proportion of applicants exceeding a selection cutoff (Thorndike's model). Major findings are: (1) when used in conjunction with either the Bayesian or conventional test, differential prediction increased fairness and facilitated  the interpretation of the fairness indices; (2) the Bayesian adaptive tests were consistently fairer than the conventional tests for all item pools above the alpha=.7 discrimination level for tests of more than 30 items; (3) the differential prediction version of the Bayesian adaptive test produced almost perfectly fair performance on all fairness indices at high discrimination levels; and (4) the placement of subgroup prior distribution in the Bayesian adaptive testing procedure can affect test fairness. (Author/CTM)
PT  - Reports - Research

OWN - ERIC
TI  - Comparing Restricted and Unrestricted Self-Adapted Testing as Alternatives to Computerized Adaptive Testing.
AU  - Roos, Linda L.
AU  - Wise, Steven L.
AU  - Finney, Sara J.
OT  - Adaptive Testing
OT  - College Students
OT  - Comparative Analysis
OT  - Computer Assisted Testing
OT  - Difficulty Level
OT  - Error of Measurement
OT  - Higher Education
OT  - Performance Factors
OT  - Selection
OT  - Test Anxiety
OT  - Test Items
OID - ED423258
DP  - 1998
LID - http://eric.ed.gov/?id=ED423258
AB  - Previous studies have shown that, when administered a self-adapted test, a few examinees will choose item difficulty levels that are not well-matched to their proficiencies, resulting in high standard errors of proficiency estimation. This study investigated whether the previously observed effects of a self-adapted test--lower anxiety and higher test performance relative to a computerized adaptive test (CAT)--can be sustained while eliminating the high standard errors. A restricted self-adapted test (RS-AT) in which examinees were allowed to choose among a set of difficulty levels only in the region of their proficiency estimates was utilized in this study. Data were collected from 273 students in an introductory statistics class. The results show that while the RS-AT effectively  controlled the standard errors of proficiency estimation, examinees receiving an RS-AT did not show higher mean proficiency or lower posttest state anxiety than examinees receiving a CAT. (Contains 3 tables and 15 references.) (SLD)
PT  - Reports - Research
PT  - Speeches/Meeting Papers

OWN - ERIC
TI  - How Review Options and Administration Modes Influence Scores on Computerized Vocabulary Tests.
AU  - Vispoel, Walter P.
AU  - And Others
OT  - Adaptive Testing
OT  - College Students
OT  - Comparative Testing
OT  - Computer Assisted Testing
OT  - Concurrent Validity
OT  - Difficulty Level
OT  - Error Correction
OT  - Feedback
OT  - Higher Education
OT  - Review (Reexamination)
OT  - Scores
OT  - Test Anxiety
OT  - Test Reliability
OT  - Test Wiseness
OT  - Vocabulary Skills
OID - ED346161
DP  - 1992
LID - http://eric.ed.gov/?id=ED346161
AB  - The effects of review options (the opportunity for examinees to review and change answers) on the magnitude, reliability, efficiency, and concurrent validity of scores obtained from three types of computerized vocabulary tests (fixed item, adaptive, and self-adapted) were studied. Subjects were 97 college students at a large midwestern university who each completed one of the vocabulary tests and several measures of attitudes about review, item difficulty, and test anxiety. Review modestly enhanced test performance, slightly decreased measurement precision, moderately increased total testing time, affected concurrent validity, and was strongly favored by examinees. Computerized tests do not necessarily yield equivalent results, and such tests may have to be equated to ensure fair  use of test scores. Differences in performance favoring paper-and-pencil tests in some prior studies occurred because review options were excluded from the computerized tests. Results for administration mode are inconclusive. Compared to the other types, the fixed-item test yielded the least desirable results because scores were lowest, least reliable, and most susceptible to test anxiety effects. The choice between self-adapted and adaptive tests seems to depend on examinee anxiety level. Item difficulty suggestion, rather than answer feedback, is the predominant factor facilitating performance on self-adapted tests. Included are 3 tables, 4 graphs, and 38 references. (SLD)
PT  - Reports - Research
PT  - Speeches/Meeting Papers

OWN - ERIC
TI  - Examinee Issues in CAT.
AU  - Wise, Steven L.
OT  - Adaptive Testing
OT  - Computer Assisted Testing
OT  - Computer Attitudes
OT  - Equal Education
OT  - Item Banks
OT  - Review (Reexamination)
OT  - Student Attitudes
OT  - Student Motivation
OT  - Test Anxiety
OT  - Test Construction
OT  - Test Items
OT  - Testing Problems
OT  - Timed Tests
OID - ED408329
DP  - 1997
LID - http://eric.ed.gov/?id=ED408329
AB  - The perspective of the examinee during the administration of a computerized adaptive test (CAT) is discussed, focusing on issues of test development. Item review is the first issue discussed. Virtually no CATs provide the opportunity for the examinee to go back and review, and possibly change, answers. There are arguments on either side of the item review issue, and test givers should weigh them carefully, considering examinee anxiety and performance factors. Another issue is that of time limits, which have little benefit for test takers, but serve only the interests of test givers. CAT developers should consider very liberal time limits or none at all, especially since a CAT is shorter than its conventional testing counterparts. Test anxiety may be increased in a CAT  environment, and test developers should be aware of the potential for anxiety among examinees. Another issue is that of examinee motivation. CAT developers should be aware of the effects of test consequences on test performance to ensure that data used to calibrate item banks are collected under conditions that have the same consequences as the operational test. Equity is an important issue in CAT, since some examinees will have less computer experience than others. Each of these issues has implications for the validity of inferences made from CAT scores and should be considered when CATs are used. (Contains 27 references.) (SLD)
PT  - Reports - Evaluative
PT  - Speeches/Meeting Papers

OWN - ERIC
TI  - Developing Computerized Tests for Classroom Teachers: A Pilot Study.
AU  - Glowacki, Margaret L.
AU  - And Others
OT  - Comparative Analysis
OT  - Computer Assisted Testing
OT  - Computer Uses in Education
OT  - Courses
OT  - Educational Technology
OT  - Higher Education
OT  - Pilot Projects
OT  - Student Attitudes
OT  - Student Surveys
OT  - Tables (Data)
OT  - Test Format
OT  - Undergraduate Students
OID - ED391471
DP  - 1995
LID - http://eric.ed.gov/?id=ED391471
AB  - Two types of computerized testing have been defined: (1) computer-based testing, using a computer to administer conventional tests in which all examinees take the same set of items; and (2) adaptive tests, in which items are selected for administration by the computer, based on examinee's previous responses. This paper discusses an option for classroom teachers that is easier to develop than a computerized adaptive test, but more secure and sophisticated than a computer-based test. The process of developing and pilot testing the computer-administered test and the results of a survey of student reactions are described. Subjects for the study consisted of 108 undergraduates taking summer educational technology courses in computer applications at a Southern university. Identical  items were used for paper-and-pencil and computerized tests. No significant differences were found for either administration. Student responses indicated that: all of the students had familiarity with computers; 94% had no problems understanding the test directions; 53% initially experienced anxiety about taking the test on a computer; 89% indicated that the computer test was as fair as a paper test; and 61% indicated a preference for the computer test, while 19% indicated that both methods worked equally well. Four tables depict results for computerized tests versus paper-and-pencil tests; descriptive data for both kinds of tests; students' yes/no responses to the attitude survey; and examinee's comments regarding computerized testing. (AEF)
PT  - Reports - Research
PT  - Speeches/Meeting Papers

OWN - ERIC
TI  - Self-Regulation of Learning in the 21st Century: Understanding the Role of Academic Delay of Gratification.
AU  - Bembenutty, Hefer
OT  - Academic Achievement
OT  - College Students
OT  - Delay of Gratification
OT  - Goal Orientation
OT  - Higher Education
OT  - Self Efficacy
OT  - Student Motivation
OID - ED455204
DP  - 2001
LID - http://eric.ed.gov/?id=ED455204
AB  - This study examined college students' motivational tendencies as predictors of academic outcomes and tested how students' goal orientations and academic delay of gratification mediated these associations. The study used data, previously analyzed in 1999, on academic delay of gratification, personal achievement goal orientations, self-efficacy, test anxiety, demographics, time dedicated to studying, and college grade point average. The results show that students' task goal orientation and academic delay of gratification mediate the relationship between self-efficacy and the time students dedicate to study. These results are considered under the umbrella of Zimmerman's cyclical model of self-regulation, which posits that learners engage in sustaining cognition, behavior, and  emotions to pursue academic goals and intentions. The findings are also consistent with Mischel's self-regulatory approach, which assumes that effective delay of gratification is a function of motivation and voluntary postponement of immediate gratification in order to pursue later outcomes. The results demonstrate that students who have high self-efficacy are engaging in academic tasks for the sake of learning and mastering work, delay gratification and persist longer in goal directed study time. Implications for education and future research are discussed. Appended are: sample items from the Academic Volitional Strategy Scales and from the Patterns of Adaptive Learning Survey, Test Anxiety, Self-Efficacy and Reliability Cronbach Alphas. (Contains 39 references, 2 tables, and 2  figures.)(SM)
PT  - Reports - Research
PT  - Speeches/Meeting Papers

OWN - ERIC
TI  - Applicable Adaptive Testing Models for School Teachers.
AU  - Wang, Albert Chang-hwa
AU  - Chuang, Chi-lin
OT  - Adaptive Testing
OT  - Foreign Countries
OT  - Junior High School Students
OT  - Junior High Schools
OT  - Student Attitudes
OT  - Test Anxiety
JT  - Educational Media International
SO  - v39 n1 p55-59 Mar 2002
OID - EJ654148
VI  - 39
IP  - 1
PG  - 55-59
DP  - Mar 2002
LID - http://eric.ed.gov/?id=EJ654148
AB  - Describes a study conducted in Taipei (Taiwan) that investigated the attitudinal effects of SPRT (Sequential Probability Ratio Test) adaptive testing environment on junior high school students. Discusses test anxiety; student preferences; test adaptability; acceptance of test results; number of items answered; and computer experience. (Author/LRW)
ISSN - ISSN-0952-3987
LA  - English
PT  - Journal Articles
PT  - Reports - Research

OWN - ERIC
TI  - Computerized Adaptive Testing Exploring Examinee Response Time Using Hierarchical Linear Modeling.
AU  - Bergstrom, Betty
AU  - And Others
OT  - Adaptive Testing
OT  - Adults
OT  - Certification
OT  - Computer Assisted Testing
OT  - Difficulty Level
OT  - Reaction Time
OT  - Responses
OT  - Student Characteristics
OT  - Test Anxiety
OT  - Test Construction
OT  - Test Items
OT  - Test Length
OT  - Timed Tests
OID - ED400287
DP  - 1994
LID - http://eric.ed.gov/?id=ED400287
AB  - Examinee response times from a computerized adaptive test taken by 204 examinees taking a certification examination were analyzed using a hierarchical linear model. Two equations were posed: a within-person model and a between-person model. Variance within persons was eight times greater than variance between persons. Several variables significantly predicted within-person variance. Response time increased with increasing items, test length, and increasing relative item difficulty. Item sequence was negatively related to response time, and some content areas required more time than others. Examinees spent more time on items they got wrong than on items they got right, and they took longer to respond when the correct answer was A, B, or C than when the correct answer was D. Only  one variable, test anxiety, significantly predicted variance between examinees. Examinee age, sex, first language, and ethnicity did not predict between-person variance, and low-ability examinees did not take longer to respond to items than high-ability examinees. Understanding how item characteristics impact on response time may allow test developers to allot total test time based on the response time history of the individual test items. This study also suggests that examinee characteristics are generally not related to response time, but that more controllable factors such as item length, position of the keyed correct answer, and use of figures do contribute to response items. An appendix contains the anxiety survey. (Contains 1 figure, 5 tables, and 12 references.)  (Author/SLD)
PT  - Reports - Research
PT  - Speeches/Meeting Papers

OWN - ERIC
TI  - Computer-Based Measurement of Intellectual Capabilities. Final Report.
AU  - Weiss, David J.
OT  - Ability
OT  - Adaptive Testing
OT  - Adults
OT  - Bayesian Statistics
OT  - Computer Assisted Testing
OT  - Individual Testing
OT  - Latent Trait Theory
OT  - Measurement Techniques
OT  - Monte Carlo Methods
OT  - Psychometrics
OT  - Response Style (Tests)
OT  - Test Construction
OT  - Test Theory
OT  - Testing Problems
OID - ED248258
DP  - 1983
LID - http://eric.ed.gov/?id=ED248258
AB  - During 1975-1979 this research into the potential of computerized adaptive testing to reduce errors in the measurement of human capabilities used Marine recruits for a live-testing validity comparison of computerized adaptive and conventional tests. The program purposes were to: (1) identify the most useful computer-based adaptive testing strategies; (2) identify testing conditions that maximize the positive rather than negative psychological effects of computerized testing; (3) investigate intra-individual multidimensionality problems in ability testing; (4) examine probabilistic responding and free-response methods for computerized adaptive testing in order to extract maximum information from each test item response; and (5) develop, refine and evaluate new computer  administered ability tests for spacial, perceptive, memory, and other abilities not now measurable using paper and pencil testing. Monte Carlo and Bayesian adaptive testing methods were used in these studies. Fifteen major findings, primarily on adaptive testing and test administration conditions, and implications for further research are given. Abstracts of the 16 research reports for studies for this program are given. (BS)
PT  - Reports - Research

OWN - ERIC
TI  - A Model for Incorporating Response-Time Data in Scoring Achievement Tests. Research Report No. 3.
AU  - Tatsuoka, Kikumi
AU  - Tatsuoka, Maurice
OT  - Achievement Tests
OT  - Cognitive Processes
OT  - Computer Assisted Testing
OT  - Diagnostic Tests
OT  - Item Analysis
OT  - Junior High Schools
OT  - Latent Trait Theory
OT  - Mathematics
OT  - Models
OT  - Reaction Time
OT  - Scores
OT  - Scoring
OT  - Statistical Analysis
OT  - Teaching Methods
OT  - Time Factors (Learning)
OID - ED183621
DP  - 1979
LID - http://eric.ed.gov/?id=ED183621
AB  - The differences in types of information-processing skills developed by different instructional backgrounds affect, negatively or positively, the learning of further advanced instructional materials. If prior and subsequent instructional methods are different, a proactive inhibition effect produces low achievement scores on a post test. This poses a serious problem for routing of students to an instructional level on the sole basis of performance on a diagnostic adaptive test and makes it essential to determine what information-processing strategy was used and consider this knowledge simultaneously. Response time often provides supplementary information which differentiates among individuals showing identical quality of performance. A model that reflects this kind of information,  obtainable from response time scores, is formulated in a similar manner to latent trait theory. This model is useful in identifying discriminating items that are sensitive to differences in instructional method. It also is helpful in identifying an individual's instructional background to a certain extent. (Author/CTM)
PT  - Reports - Research

OWN - ERIC
TI  - A Critical Analysis of the Arguments for and against Item Review in Computerized Adaptive Testing.
AU  - Wise, Steven L.
OT  - Achievement Gains
OT  - Adaptive Testing
OT  - Computer Assisted Testing
OT  - Error Correction
OT  - Guessing (Tests)
OT  - Responses
OT  - Review (Reexamination)
OT  - Scores
OT  - Stakeholders
OT  - Test Construction
OT  - Test Items
OT  - Test Results
OT  - Test Wiseness
OT  - Testing Problems
OT  - Timed Tests
OID - ED400267
DP  - 1996
LID - http://eric.ed.gov/?id=ED400267
AB  - In recent years, a controversy has arisen about the advisability of allowing examinees to review their test items and possibly change answers. Arguments for and against allowing item review are discussed, and issues that a test designer should consider when designing a Computerized Adaptive Test (CAT) are identified. Most CATs do not allow examinees the opportunity to review their items. The reasons advanced for this position include: (1) the possibility of item dependence that might affect another answer; (2) a decrease in testing efficiency; (3) opening the test results to effects of test-taking strategies; (4) an increase in testing time; and (5) complications in test development. Arguments in favor of allowing review focus on legitimate score gain possibilities. The first  usually advanced is that examinees prefer to be able to review, and the second is that review yields legitimately improved scores. Consideration of arguments for and against item review is complicated by the presence of multiple stakeholders in the measurement process. The question of allowing item review is one without a clear answer, but the interests of test takers and test givers should be protected, perhaps by the development of new types of CAT. (Contains 1 table and 23 references.) (SLD)
PT  - Reports - Evaluative
PT  - Speeches/Meeting Papers

OWN - ERIC
TI  - Unintended Consequences or Testing the Integrity of Teachers and Students.
AU  - Kimmel, Ernest W.
OT  - Academic Achievement
OT  - Achievement Tests
OT  - Cheating
OT  - Computer Assisted Testing
OT  - Elementary Secondary Education
OT  - Standardized Tests
OT  - Teacher Education
OT  - Test Validity
OT  - Testing Problems
OID - ED414306
DP  - 1997
LID - http://eric.ed.gov/?id=ED414306
AB  - Large-scale testing programs are generally based on the assumptions that the test-takers experience standard conditions for taking the test and that everyone will do his or her own work without having prior knowledge of specific questions. These assumptions are not necessarily true. The ways students and educators use to get around standardizing conditions to gain an advantage are described, and ways to reduce these behaviors are presented. In the first place, there is traditional cheating, by copying, or describing answers, which is enhanced by electronic gadgets or international exchanges of information. Lax security can result in the theft of test booklets. Teachers and other educators can undermine the validity of test by ignoring evidence of student preknowledge of the test  or by tacitly colluding with students by allowing access to test materials. The measurement community needs to do a better job of educating educators about the importance of standard conditions. Testing agencies or programs should audit some testing sites to determine the existence of standard conditions, and new techniques of test administration, including computer adaptive tests, must be developed to improve test security. (SLD)
PT  - Reports - Evaluative
PT  - Speeches/Meeting Papers

OWN - ERIC
TI  - Evaluating a Computer-Adaptive ESL Placement Test.
AU  - Madsen, Harold
OT  - Anxiety
OT  - Computer Assisted Testing
OT  - English (Second Language)
OT  - Language Tests
OT  - Second Language Instruction
OT  - Student Attitudes
OT  - Test Reliability
JT  - CALICO Journal
SO  - v4 n2 p41-50 Dec 1986
OID - EJ369003
VI  - 4
IP  - 2
PG  - 41-50
DP  - Dec 1986
LID - http://eric.ed.gov/?id=EJ369003
AB  - Evaluates one of the first operational computerized-adaptive English-as-a-second-language tests in the United States, showing an overwhelmingly positive student reaction to the tests and higher effectiveness than conventional paper-and-pencil tests. (Author/CB)
PT  - Journal Articles
PT  - Reports - Research

