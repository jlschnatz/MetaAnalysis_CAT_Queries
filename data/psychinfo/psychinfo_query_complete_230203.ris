TY  - JOUR
ID  - 2019-77836-004
AN  - 2019-77836-004
AU  - Kaplan, Mehmet
AU  - de la Torre, Jimmy
T1  - A Blocked-CAT Procedure for CD-CAT
JF  - Applied Psychological Measurement
JO  - Applied Psychological Measurement
JA  - Appl Psychol Meas
Y1  - 2020/01//
VL  - 44
IS  - 1
SP  - 49
EP  - 64
PB  - Sage Publications
SN  - 0146-6216
SN  - 1552-3497
AD  - Kaplan, Mehmet, Faculty of Education, Artvin Coruh University, 08000, Artvin, Turkey
N1  - Accession Number: 2019-77836-004. PMID: 31853158 Partial author list: First Author & Affiliation: Kaplan, Mehmet; Artvin Coruh University, Turkey. Release Date: 20200109. Publication Type: Journal (0100), Peer Reviewed Journal (0110). Format Covered: Electronic. Document Type: Journal Article. Language: EnglishMajor Descriptor: Adaptive Testing; Diagnosis; Psychodiagnostic Typologies; Test Administration; Test Anxiety. Minor Descriptor: Test Construction. Classification: Research Methods & Experimental Design (2260). Population: Human (10). Methodology: Empirical Study; Mathematical Model; Quantitative Study; Scientific Simulation. Supplemental Data: Other Internet. Page Count: 16. Issue Publication Date: Jan, 2020. Copyright Statement: The Author(s). 2019. 
AB  - This article introduces a blocked-design procedure for cognitive diagnosis computerized adaptive testing (CD-CAT), which allows examinees to review items and change their answers during test administration. Four blocking versions of the new procedure were proposed. In addition, the impact of several factors, namely, item quality, generating model, block size, and test length, on the classification rates was investigated. Three popular item selection indices in CD-CAT were used and their efficiency compared using the new procedure. An additional study was carried out to examine the potential benefit of item review. The results showed that the new procedure is promising in that allowing item review resulted only in a small loss in attribute classification accuracy under some conditions. Moreover, using a blocked-design CD-CAT is beneficial to the extent that it alleviates the negative impact of test anxiety on examinees’ true performance. (PsycINFO Database Record (c) 2020 APA, all rights reserved)
KW  - item review
KW  - CD-CAT
KW  - block design
KW  - Adaptive Testing
KW  - Diagnosis
KW  - Psychodiagnostic Typologies
KW  - Test Administration
KW  - Test Anxiety
KW  - Test Construction
DO  - 10.1177/0146621619835500
UR  - https://search.ebscohost.com/login.aspx?direct=true&db=psyh&AN=2019-77836-004&lang=de&site=ehost-live
UR  - ORCID: 0000-0002-4175-3899
UR  - mehmet.kaplan2@gmail.com
DP  - EBSCOhost
DB  - APA PsycInfo
ER  - 

TY  - THES
ID  - 2004-99020-129
AN  - 2004-99020-129
AU  - Weinland, Stephan Raab
T1  - A calibration of the McGill Pain Questionnaire-Short Form: Using item response theory
JF  - Dissertation Abstracts International: Section B: The Sciences and Engineering
JO  - Dissertation Abstracts International: Section B: The Sciences and Engineering
Y1  - 2004///
VL  - 65
IS  - 4-B
SP  - 2118
EP  - 2118
PB  - ProQuest Information & Learning
SN  - 0419-4217
N1  - Accession Number: 2004-99020-129. Other Journal Title: Dissertation Abstracts International. Partial author list: First Author & Affiliation: Weinland, Stephan Raab; Illinois Inst Technology, US. Release Date: 20050321. Publication Type: Dissertation Abstract (0400). Format Covered: Electronic. Document Type: Dissertation. Dissertation Number: AAI3130102. Language: EnglishMajor Descriptor: Item Response Theory; Pain; Psychometrics; Questionnaires. Classification: Health & Mental Health Treatment & Prevention (3300); Psychometrics & Statistics & Methodology (2200). Population: Human (10). Methodology: Empirical Study. Page Count: 1. 
AB  - The McGill Pain Questionnaire - Short Form (MPQ - SF) is a measure of pain published in 1987. This dissertation conducts a confirmatory factor analysis of the measure's structure, as well as a calibration of the items on the measure, using Item Response Theory. A total of 1185 patients reporting to two chronic pain care treatment centers completed the MPQ - SF in the course of their admission to treatment. Factor analyses of their data suggests a three-factor solution of 'acute,' 'emotional,' and 'soreness' factors led to a higher order factor of 'pain.' This solution provides an adequate fit for the data. Previously published one and two factor structures did not demonstrate adequate fit in this data set. Unidimensionality in this sample was supported by the presence of a higher order factor of 'Pain.' Items on the measure are calibrated using the two-parameter logistic model. Specifically, item slope and location parameters are calculated for each of the items, and differential item functioning is investigated between four subgroups of data. Performance on the items was contrasted between subgroups differing on gender, pain diagnosis, workers' compensation status and pain coping status using Raju's DFIT methodology. No differential test functioning was found; however, some individual items did display differential item functioning by subgroup. Sample characteristics that contribute to differential item functioning are examined further. Additionally, implications for diagnostic considerations and computer adaptive testing are discussed. (PsycINFO Database Record (c) 2016 APA, all rights reserved)
KW  - McGill Pain Questionnaire
KW  - item response theory
KW  - factor analysis
KW  - pain
KW  - psychometrics
KW  - Item Response Theory
KW  - Pain
KW  - Psychometrics
KW  - Questionnaires
UR  - https://search.ebscohost.com/login.aspx?direct=true&db=psyh&AN=2004-99020-129&lang=de&site=ehost-live
DP  - EBSCOhost
DB  - APA PsycInfo
ER  - 

TY  - JOUR
ID  - 2010-21322-002
AN  - 2010-21322-002
AU  - García-Pérez, Miguel A.
AU  - Alcalá-Quintana, Rocío
AU  - García-Cueto, Eduardo
T1  - A comparison of anchor-item designs for the concurrent calibration of large banks of Likert-Type items
JF  - Applied Psychological Measurement
JO  - Applied Psychological Measurement
JA  - Appl Psychol Meas
Y1  - 2010/11//
VL  - 34
IS  - 8
SP  - 580
EP  - 599
PB  - Sage Publications
SN  - 0146-6216
SN  - 1552-3497
AD  - García-Pérez, Miguel A., Departamento de Metodologia, Facultad de Psicologia, Universidad Complutense, Campus de Somosaguas, 28223, Madrid, Spain
N1  - Accession Number: 2010-21322-002. Partial author list: First Author & Affiliation: García-Pérez, Miguel A.; Universidad Complutense, Madrid, Spain. Release Date: 20110103. Correction Date: 20121001. Publication Type: Journal (0100), Peer Reviewed Journal (0110). Format Covered: Electronic. Document Type: Journal Article. Language: EnglishMajor Descriptor: Algorithms; Statistics. Minor Descriptor: Likert Scales. Classification: Statistics & Mathematics (2240). Population: Human (10). Methodology: Empirical Study; Quantitative Study. References Available: Y. Page Count: 20. Issue Publication Date: Nov, 2010. Copyright Statement: The Author(s). 2010. 
AB  - Current interest in measuring quality of life is generating interest in the construction of computerized adaptive tests (CATs) with Likert-type items. Calibration of an item bank for use in CAT requires collecting responses to a large number of candidate items. However, the number is usually too large to administer to each subject in the calibration sample. The concurrent anchor-item design solves this problem by splitting the items into separate subtests, with some common items across subtests; then administering each subtest to a different sample; and finally running estimation algorithms once on the aggregated data array, from which a substantial number of responses are then missing. Although the use of anchor-item designs is widespread, the consequences of several configuration decisions on the accuracy of parameter estimates have never been studied in the polytomous case. The present study addresses this question by simulation, comparing the outcomes of several alternatives on the configuration of the anchor-item design. The factors defining variants of the anchor-item design are (a) subtest size, (b) balance of common and unique items per subtest, (c) characteristics of the common items, and (d) criteria for the distribution of unique items across subtests. The results of this study indicate that maximizing accuracy in item parameter recovery requires subtests of the largest possible number of items and the smallest possible number of common items; the characteristics of the common items and the criterion for distribution of unique items do not affect accuracy. (PsycINFO Database Record (c) 2016 APA, all rights reserved)
KW  - anchor–item designs
KW  - concurrent calibration
KW  - large banks
KW  - Likert–type items
KW  - algorithms
KW  - statistics
KW  - Algorithms
KW  - Statistics
KW  - Likert Scales
U1  - Sponsor: Ministerio de Educación y Ciencia, Spain. Grant: SEJ2005-00485. Recipients: No recipient indicated
DO  - 10.1177/0146621609351259
UR  - https://search.ebscohost.com/login.aspx?direct=true&db=psyh&AN=2010-21322-002&lang=de&site=ehost-live
UR  - miguel@psi.ucm.es
DP  - EBSCOhost
DB  - APA PsycInfo
ER  - 

TY  - JOUR
ID  - 2019-60957-001
AN  - 2019-60957-001
AU  - Segawa, Eisuke
AU  - Schalet, Benjamin
AU  - Cella, David
T1  - A comparison of computer adaptive tests (CATs) and short forms in terms of accuracy and number of items administrated using PROMIS profile
JF  - Quality of Life Research: An International Journal of Quality of Life Aspects of Treatment, Care & Rehabilitation
JO  - Quality of Life Research: An International Journal of Quality of Life Aspects of Treatment, Care & Rehabilitation
JA  - Qual Life Res
Y1  - 2020/01//
VL  - 29
IS  - 1
SP  - 213
EP  - 221
PB  - Springer
SN  - 0962-9343
SN  - 1573-2649
AD  - Segawa, Eisuke
N1  - Accession Number: 2019-60957-001. PMID: 31595451 Partial author list: First Author & Affiliation: Segawa, Eisuke; SK Data, Chicago, IL, US. Release Date: 20191010. Correction Date: 20211018. Publication Type: Journal (0100), Peer Reviewed Journal (0110). Format Covered: Electronic. Document Type: Journal Article. Language: EnglishGrant Information: Cella, David. Major Descriptor: Adaptive Testing; Item Response Theory; Pain; Profiles (Measurement); Health Related Quality of Life. Minor Descriptor: Test Forms; Test Items; Patient Reported Outcome Measures. Classification: Health Psychology & Medicine (3360). Population: Human (10). Tests & Measures: Patient-Reported Outcomes Measurement Information System DOI: 10.1037/t06780-000. Methodology: Empirical Study; Quantitative Study; Scientific Simulation. References Available: Y. Page Count: 9. Issue Publication Date: Jan, 2020. Publication History: First Posted Date: Oct 8, 2019; Accepted Date: Apr 30, 2019. Copyright Statement: Springer Nature Switzerland AG. 2019. 
AB  - Purpose: In the Patient-Reported Outcomes Measurement Information System (PROMIS), seven domains (Physical Function, Anxiety, Depression, Fatigue, Sleep Disturbance, Social Function, and Pain Interference) are packaged together as profiles. Each of these domains can also be assessed using computer adaptive tests (CATs) or short forms (SFs) of varying length (e.g., 4, 6, and 8 items). We compared the accuracy and number of items administrated of CAT versus each SF. Methods: PROMIS instruments are scored using item response theory (IRT) with graded response model and reported as T scores (mean = 50, SD = 10). We simulated 10,000 subjects from the normal distribution with mean 60 for symptom scales and 40 for function scales, and standard deviation 10 in each domain. We considered a subject’s score to be accurate when the standard error (SE) was less than 3.0. We recorded range of accurate scores (accurate range) and the number of items administrated. Results: The average number of items administrated in CAT was 4.7 across all domains. The accurate range was wider for CAT compared to all SFs in each domain. CAT was notably better at extending the accurate range into very poor health for Fatigue, Physical Function, and Pain Interference. Most SFs provided reasonably wide accurate range. Conclusions: Relative to SFs, CATs provided the widest accurate range, with slightly more items than SF4 and less than SF6 and SF8. Most SFs, especially longer ones, provided reasonably wide accurate range. (PsycInfo Database Record (c) 2021 APA, all rights reserved)
KW  - Computer adaptive testing (CAT)
KW  - Short form
KW  - PROMIS
KW  - Item response theory
KW  - Patient-Reported Outcomes Measurement Information System
KW  - Adult
KW  - Computer-Aided Design
KW  - Female
KW  - Humans
KW  - Male
KW  - Quality of Life
KW  - Adaptive Testing
KW  - Item Response Theory
KW  - Pain
KW  - Profiles (Measurement)
KW  - Health Related Quality of Life
KW  - Test Forms
KW  - Test Items
KW  - Patient Reported Outcome Measures
U1  - Sponsor: National Institutes of Health, US. Grant: U2CCA186878. Recipients: Cella, David
DO  - 10.1007/s11136-019-02312-8
UR  - https://search.ebscohost.com/login.aspx?direct=true&db=psyh&AN=2019-60957-001&lang=de&site=ehost-live
UR  - eisuke.segawa@gmail.com
DP  - EBSCOhost
DB  - APA PsycInfo
ER  - 

TY  - JOUR
ID  - 2016-53436-017
AN  - 2016-53436-017
AU  - Pedrosa, Ignacio
AU  - Suárez-Álvarez, Javier
AU  - García-Cueto, Eduardo
AU  - Muñiz, José
T1  - A computerized adaptive test for enterprising personality assessment in youth
JF  - Psicothema
JO  - Psicothema
JA  - Psicothema
Y1  - 2016/11//
VL  - 28
IS  - 4
SP  - 471
EP  - 478
PB  - Colegio Oficial de Psicólogos del Principado de Asturias
SN  - 0214-9915
SN  - 1886-144X
AD  - Pedrosa, Ignacio, Facultad Psicologia, Universidad de Oviedo, 33003, Oviedo, Spain
N1  - Accession Number: 2016-53436-017. PMID: 27776618 Partial author list: First Author & Affiliation: Pedrosa, Ignacio; Universidad de Oviedo, Oviedo, Spain. Release Date: 20170306. Publication Type: Journal (0100), Peer Reviewed Journal (0110). Format Covered: Print. Document Type: Journal Article. Language: EnglishMajor Descriptor: Adaptive Testing; Personality Measures; Psychometrics; Self-Efficacy. Classification: Personality Scales & Inventories (2223); Personality Traits & Processes (3120). Population: Human (10); Male (30); Female (40). Age Group: Adulthood (18 yrs & older) (300). Methodology: Empirical Study; Quantitative Study. References Available: Y. Page Count: 8. Issue Publication Date: Nov, 2016. Publication History: Accepted Date: Jun 10, 2016; First Submitted Date: Feb 22, 2016. Copyright Statement: Psicothema. 2016. 
AB  - Background: Assessing specific personality traits has shown better predictive power of enterprising personality than have broad personality traits. Hitherto, there have been no instruments that evaluate the combination of specific personality traits of enterprising personality in an adaptive format. So, the aim was to develop a Computerized Adaptive Test (CAT) to assess enterprising personality in young people. Methods: A pool of 161 items was developed and applied to two sets of participants (n₁ = 357 students, Mage = 17.89; SDage = 3.26; n₂ = 2,693 students; Mage = 16.52, SDage = 1.38) using a stratified sampling method. Results: 107 items that assess achievement motivation, risk-taking, innovativeness, autonomy, self-efficacy, stress tolerance, internal locus of control, and optimism were selected. The assumption of unidimensionality was tested. The CAT demonstrated high precision for a wide range of 8, using a mean of 10 items and demonstrating a relatively low Standard Error (0.378). Conclusions; A brief, valid, and precise instrument was obtained with relevant implications for educational and entrepreneurial contexts. (PsycINFO Database Record (c) 2017 APA, all rights reserved)
KW  - Entrepreneurship
KW  - Assessment
KW  - Computerized Adaptive Test
KW  - Youth
KW  - Adaptive Testing
KW  - Personality Measures
KW  - Psychometrics
KW  - Self-Efficacy
U1  - Sponsor: Spanish Ministry of Economy and Competitiveness, Spain. Recipients: No recipient indicated
U1  - Sponsor: University of Oviedo, Spain. Recipients: No recipient indicated
U1  - Sponsor: Spanish Ministry of Education, Culture and Sport, FPU program, Spain. Recipients: No recipient indicated
U1  - Sponsor: Asociación Española de Metodología de las Ciencias del Comportamiento, Spain. Grant: PSI2011-28638; IB-05-02; UNOV-11-BECDOC; AP2010-1999; PSI2014-56I14-P; AEMCCO-2015. Recipients: No recipient indicated
UR  - https://search.ebscohost.com/login.aspx?direct=true&db=psyh&AN=2016-53436-017&lang=de&site=ehost-live
UR  - npedrosa@cop.es
DP  - EBSCOhost
DB  - APA PsycInfo
ER  - 

TY  - JOUR
ID  - 2012-17891-010
AN  - 2012-17891-010
AU  - Turner-Bowker, Diane M.
AU  - Saris-Baglama, Renee N.
AU  - DeRosa, Michael A.
AU  - Giovannetti, Erin R.
AU  - Jensen, Roxanne E.
AU  - Wu, Albert W.
T1  - A computerized adaptive version of the SF-36 is feasible for clinic and internet administration in adults with HIV
JF  - AIDS Care
JO  - AIDS Care
JA  - AIDS Care
Y1  - 2012/07//
VL  - 24
IS  - 7
SP  - 886
EP  - 896
PB  - Taylor & Francis
SN  - 0954-0121
SN  - 1360-0451
AD  - Turner-Bowker, Diane M.
N1  - Accession Number: 2012-17891-010. PMID: 22348336 Partial author list: First Author & Affiliation: Turner-Bowker, Diane M.; Datacorp, Smithfield, RI, US. Release Date: 20120903. Correction Date: 20190211. Publication Type: Journal (0100), Peer Reviewed Journal (0110). Format Covered: Electronic. Document Type: Journal Article. Language: EnglishMajor Descriptor: HIV; Psychometrics; Test Reliability; Test Validity; Computerized Assessment. Minor Descriptor: Adaptive Testing; Internet; Pain. Classification: Clinical Psychological Testing (2224); Immunological Disorders (3291). Population: Human (10); Male (30); Female (40). Location: US. Age Group: Adulthood (18 yrs & older) (300). Tests & Measures: SF-36 Health Survey. Methodology: Empirical Study; Quantitative Study. References Available: Y. Page Count: 11. Issue Publication Date: Jul, 2012. Publication History: Revised Date: Jan 9, 2012; First Submitted Date: Sep 30, 2011. Copyright Statement: Taylor & Francis. 2012. 
AB  - DYNHA SF-36 is a computerized adaptive test version of the SF-36 Health Survey. The feasibility of administering a modified DYNHA SF-36 to adults with HIV was evaluated with Johns Hopkins University Moore (HIV) Clinic patients (N = 100) and Internet consumer health panel members (N = 101). Participants completed the DYNHA SF-36, modified to capture seven health domains [(physical function (PF), role function (RF, without physical or emotional attribution), bodily pain (BP), general health, vitality (VT), social function (SF), mental health (MH)], and scored to produce two summary components [Physical Component Summary (PCS), Mental Component Summary (MCS)]. Item-response theory-based response consistency, precision, mean scores, and discriminant validity were examined. A higher percentage of Internet participants responded consistently to the DYNHA SF-36. For each domain, three standard deviations were covered with five items (90% reliability); however, RF and SF scores were less precise at the upper end of measurement (better functioning). Mean scores were slightly higher for the Internet sample, with the exception of VT and MCS. Clinic and Internet participants reporting an AIDS diagnosis had significantly lower mean PCS and PF scores than those without a diagnosis. Additionally, significantly lower RF and BP scores were found for Internet participants reporting an AIDS diagnosis. The measure was well accepted by the majority of participants, although Internet respondents provided lower ratings for the tool’s usefulness. The DYNHA SF-36 has promise for measuring the impact of HIV and its treatment in both the clinic setting and through telemonitoring. (PsycINFO Database Record (c) 2019 APA, all rights reserved)
KW  - SF 36 Health Survey
KW  - internet administration
KW  - HIV
KW  - psychometrics
KW  - test validity
KW  - test reliability
KW  - bodily pain
KW  - Internet
KW  - Adult
KW  - Computer Systems
KW  - Feasibility Studies
KW  - Female
KW  - HIV Seropositivity
KW  - Health Status
KW  - Humans
KW  - Internet
KW  - Male
KW  - Psychometrics
KW  - Quality of Life
KW  - Reproducibility of Results
KW  - Sickness Impact Profile
KW  - Software
KW  - Surveys and Questionnaires
KW  - HIV
KW  - Psychometrics
KW  - Test Reliability
KW  - Test Validity
KW  - Computerized Assessment
KW  - Adaptive Testing
KW  - Internet
KW  - Pain
DO  - 10.1080/09540121.2012.656573
UR  - https://search.ebscohost.com/login.aspx?direct=true&db=psyh&AN=2012-17891-010&lang=de&site=ehost-live
UR  - dtbowker@mjdatacorp.com
DP  - EBSCOhost
DB  - APA PsycInfo
ER  - 

TY  - JOUR
ID  - 2018-34001-001
AN  - 2018-34001-001
AU  - Grieco, Joseph C.
AU  - Romero, Beverly
AU  - Flood, Emuella
AU  - Cabo, Raquel
AU  - Visootsak, Jeannie
T1  - A conceptual model of Angelman syndrome and review of relevant clinical outcomes assessments (COAs)
JF  - The Patient: Patient-Centered Outcomes Research
JO  - The Patient: Patient-Centered Outcomes Research
JA  - Patient
Y1  - 2019/02/04/
VL  - 12
IS  - 1
SP  - 97
EP  - 112
PB  - Springer
SN  - 1178-1653
SN  - 1178-1661
AD  - Grieco, Joseph C., Ovid Therapeutics, New York, NY, US
N1  - Accession Number: 2018-34001-001. PMID: 29987743 Partial author list: First Author & Affiliation: Grieco, Joseph C.; Ovid Therapeutics, New York, NY, US. Other Publishers: Adis International. Release Date: 20180712. Correction Date: 20221128. Publication Type: Journal (0100), Peer Reviewed Journal (0110). Format Covered: Electronic. Document Type: Journal Article. Language: EnglishMajor Descriptor: Content Validity; Genetic Disorders; Measurement; Symptoms; Clinical Models. Classification: Clinical Psychological Testing (2224); Physical & Somatic Disorders (3290). Population: Human (10); Female (40). Location: US. Age Group: Adulthood (18 yrs & older) (300); Young Adulthood (18-29 yrs) (320); Thirties (30-39 yrs) (340); Middle Age (40-64 yrs) (360); Aged (65 yrs & older) (380). Tests & Measures: Modified Performance-Oriented Mobility Assessment; Pediatric Evaluation of Disability Inventory Computer Adaptive Test; Aberrant Behavior Checklist–Community; Morning Diary; Semi-Structured Interview Guide; Anxiety, Depression and Mood Scale DOI: 10.1037/t10454-000. Methodology: Empirical Study; Interview; Qualitative Study; Quantitative Study. Supplemental Data: Appendixes Internet. References Available: Y. Page Count: 16. Issue Publication Date: Feb 4, 2019. Publication History: First Posted Date: Jul 10, 2018. Copyright Statement: The Author(s). 2018. 
AB  - Background: Angelman syndrome (AS) is a rare, neurological genetic disorder for which no clinical outcomes assessments (COAs) or conceptual models (CM) have been developed. Objective: This study aimed to identify symptoms and impacts relevant and important in this patient population and develop a conceptual model of AS, and to evaluate the content validity of selected COA instruments with potential for inclusion in clinical studies of AS to capture treatment benefit. Methods: For both concept elicitation (CE) and cognitive interviews (CI), caregivers of children, adolescents, and adults with AS and clinicians with AS experience were targeted. For CI, clinicians discussed the Modified Performance-Oriented Mobility Assessment (MPOMA-G) and ProtoKinetics Zeno Walkway™ and caregivers reviewed the Pediatric Evaluation of Disability Inventory Computer Adaptive Test (PEDI-CAT), the Anxiety, Depression and Mood Scale (ADAMS), the Aberrant Behavior Checklist–Community (ABC-C), and the Morning Diary. Results: Four clinicians and 34 caregivers participated in CE interviews; three clinicians and 36 caregivers participated in CI. A conceptual model, initially informed by literature, was refined based on interview data. Five domains of symptoms, signs, and characteristics of AS were identified: cognitive and executive functioning, social-emotional, emotional-expressive behavior, sensory-compulsive behavior, and physical. Patient impacts were identified in three domains: activities of daily living, school, and social/community. Caregiver impacts were identified in five domains: mental health, physical health, work, home, and social. While all instruments demonstrated the ability to provide relevant data for the AS population, each instrument either contained some items irrelevant to individuals with AS or was missing important concepts based on the interviews. No single instrument covered all relevant domains specific to AS. Conclusion: Future work should consider the adaptation of existing COAs and the development of a novel AS-specific instrument for use in clinical research to ensure outcomes important to this patient population are captured. (PsycInfo Database Record (c) 2022 APA, all rights reserved)
KW  - conceptual model
KW  - Angelman syndrome
KW  - clinical outcomes assessments
KW  - symptoms
KW  - content validity
KW  - Adolescent
KW  - Adult
KW  - Angelman Syndrome
KW  - Caregivers
KW  - Child
KW  - Concept Formation
KW  - Cost of Illness
KW  - Female
KW  - Humans
KW  - Interviews as Topic
KW  - Male
KW  - Middle Aged
KW  - Outcome Assessment, Health Care
KW  - Qualitative Research
KW  - Young Adult
KW  - Content Validity
KW  - Genetic Disorders
KW  - Measurement
KW  - Symptoms
KW  - Clinical Models
U1  - Sponsor: Ovid Therapeutics, US. Recipients: No recipient indicated
DO  - 10.1007/s40271-018-0323-7
UR  - https://search.ebscohost.com/login.aspx?direct=true&db=psyh&AN=2018-34001-001&lang=de&site=ehost-live
UR  - jgrieco@ovidrx.com
DP  - EBSCOhost
DB  - APA PsycInfo
ER  - 

TY  - JOUR
ID  - 2021-85582-001
AN  - 2021-85582-001
AU  - Lai, Cara H.
AU  - Shapiro, Lauren M.
AU  - Amanatullah, Derek F.
AU  - Chou, Loretta B.
AU  - Gardner, Michael J.
AU  - Hu, Serena S.
AU  - Safran, Marc R.
AU  - Kamal, Robin N.
T1  - A framework to make proms relevant to patients: Qualitative study of communication preferences of proms
JF  - Quality of Life Research: An International Journal of Quality of Life Aspects of Treatment, Care & Rehabilitation
JO  - Quality of Life Research: An International Journal of Quality of Life Aspects of Treatment, Care & Rehabilitation
JA  - Qual Life Res
Y1  - 2021/09/11/
PB  - Springer
SN  - 0962-9343
SN  - 1573-2649
AD  - Kamal, Robin N.
N1  - Accession Number: 2021-85582-001. PMID: 34510335 Partial author list: First Author & Affiliation: Lai, Cara H.; VOICES Health Policy Research Center, Stanford University School of Medicine, Palo Alto, CA, US. Release Date: 20210916. Publication Type: Journal (0100), Peer Reviewed Journal (0110). Format Covered: Electronic. Document Type: Journal. Language: EnglishMajor Descriptor: No terms assigned. Classification: Psychological & Physical Disorders (3200). References Available: Y. Publication History: Accepted Date: Aug 9, 2021. Copyright Statement: The Author(s), under exclusive licence to Springer Nature Switzerland AG. 2021. 
AB  - PurposePatient-reported outcome measures are tools for evaluating symptoms, magnitude of limitations, baseline health status, and outcomes from the patient’s perspective. Healthcare professional organizations and payers increasingly recommend PROMs for clinical care, but there lacks guidance regarding effective communication of PROMs with orthopedic surgery patients. This qualitative study aimed to identify (1) patient attitudes toward the use and communication of PROMs, and (2) what patients feel are the most relevant or important aspects of PROM results to discuss with their physicians.MethodsParticipants were recruited from a multispeciality orthopedic clinic. Three PROMs: the EuroQol-5 Dimension, the Patient-Specific Functional Scale, and the Patient-Reported Outcome Measurement Information System Physical Function Computer Adaptive Test were shown and a semi-structured interview was conducted to elicit PROMs attitudes and preferences. Interviews were transcribed and inductive-deductively coded. Coded excerpts were aggregated to (1) identify major themes and (2) analyze how themes interacted.ResultThree themes emerged: (1) Beliefs toward the purpose of PROMs, (2) PROMs as a reflection of self, and (3) PROMs to facilitate communication and guide healthcare decisions. These themes informed a framework outlining the patient perspective on communicating PROMs during clinical care.ConclusionPatient attitudes toward the use and communication of PROMs start with the incorporation of patient beliefs, which can facilitate or act as a barrier to engagement. Patients should ideally believe that PROMs are an accurate reflection of personal health state before incorporation into care. Clinicians should endeavor to communicate the purpose of a chosen PROM in line with a patient’s unique needs and what they feel is most relevant to their own care. Aspects of PROMs results which may be helpful to address include providing context for what scores mean and how they are calculated, and using scores as a way to weigh risks and benefits of treatment and tracking progress over time. Future research can focus on the effect of communication strategies on patient outcomes and engagement in care. (PsycInfo Database Record (c) 2021 APA, all rights reserved)
KW  - Patient-reported outcome measures
KW  - Qualitative research
KW  - Orthopedic surgery
KW  - Outcome measurement
KW  - No terms assigned
DO  - 10.1007/s11136-021-02972-5
UR  - https://search.ebscohost.com/login.aspx?direct=true&db=psyh&AN=2021-85582-001&lang=de&site=ehost-live
UR  - ORCID: 0000-0002-3011-6712
UR  - rnkamal@stanford.edu
DP  - EBSCOhost
DB  - APA PsycInfo
ER  - 

TY  - JOUR
ID  - 2022-71255-009
AN  - 2022-71255-009
AU  - Tan, Qingrong
AU  - Wang, Daxun
AU  - Luo, Fen
AU  - Cai, Yan
AU  - Tu, Dongbo
T1  - A high-efficiency and new online calibration method in CD-CAT based on information gain of entropy and EM algorithm
JF  - Acta Psychologica Sinica
JO  - Acta Psychologica Sinica
JA  - Xin Li Xue Bao
Y1  - 2021/11//
VL  - 53
IS  - 11
SP  - 1286
EP  - 1298
PB  - Science Press
SN  - 0439-755X
AD  - Tu, Dongbo, School of Psychology, Jiangxi Normal University, Nanchang, China, 330022
N1  - Accession Number: 2022-71255-009. Partial author list: First Author & Affiliation: Tan, Qingrong; School of Psychology, Jiangxi Normal University, Nanchang, China. Release Date: 20220811. Publication Type: Journal (0100), Peer Reviewed Journal (0110). Format Covered: Electronic. Document Type: Journal Article. Language: ChineseMajor Descriptor: Adaptive Testing; Algorithms; Diagnosis; Normal Distribution; Sample Size. Minor Descriptor: Discrimination; Estimation; Computerized Assessment. Classification: Tests & Testing (2220). Population: Human (10). Methodology: Empirical Study; Quantitative Study. References Available: Y. Page Count: 13. Issue Publication Date: Nov, 2021. 
AB  - Cognitive diagnostic computerized adaptive testing (CD-CAT) includes the advantages of both cognitive diagnosis (CD) and computerized adaptive testing (CAT), which can offer detailed diagnosis feedback for each examinee by applying fewer test items and time. It has been a promising field. An item bank is a prerequisite for the implementation of CD-CAT. However, its maintenance is a very challenging task. One of the effective ways to maintain the item bank is online calibration. Till now, there are only a few online calibration methods in the CD-CAT context that can calibrate Q-matrix and item parameters simultaneously. Moreover, the computational efficiency of these methods needs to be further improved. Therefore, it is crucial to find more online calibration methods that jointly calibrate the Q-matrix and item parameters. Inspired by the SIE (Single-Item Estimation) method proposed by Chen et al. (2015) and information gain feature selection criteria in feature selection, an information gain of entropy-based online calibration method (IGEOCM) was proposed in this study. The proposed method can jointly calibrate Q-matrix and item parameters in a sequential manner. The calibration process of the new items was described as follows: First, for the new item j, the q-vector can be calibrated by maximizing the information gain of entropy-based on the basis of the attribute patterns of examinees and the examinees’ responses to item j. Second, the item parameters of the new item j are estimated by the EM algorithm based on the posterior distribution of examinees’ attribute pattern, the examinees’ responses to item j, and the q-vector estimated in the first step. The first and second step are repeated for all other new items to obtain their estimated Q-matrix and item parameters item by item. Two simulation studies were conducted to examine whether the IGEOCM could accurately and efficiently calibrate the Q-matrix and item parameters of the new items under different calibration sample sizes (40, 80, 120, 160, and 200), different attribute pattern distributions (uniform distribution, higher-order distribution, and multivariate normal distribution), the different number of new items answered by examinee (4, 6, and 8), and different item selection algorithms (posterior-weighted Kullback-Leibler, PWKL; the modified PWKL, MPWKL; the generalized deterministic inputs, noisy and gate model discrimination index, GDI; and Shannon entropy, SHE). Furthermore, the performance of the proposed method was compared with the SIE, SIE-R-BIC, and RMSEA-N methods. The results indicated that (1) The IGEOCM worked well in terms of the calibration accuracy and estimation efficiency under all conditions, and outperformed the SIE, SIE-R-BIC, and RMSEA-N methods overall. (2) The accuracy of the item calibration increases as the sample size increases for all calibration methods under all conditions. (3) The SIE, SIE-R-BIC, RMSEA-N, and IGEOCM performed better under the uniform distribution and higher-order distribution than under the multivariate normal distribution. (4) The number of new items answered by the examinee had a negligible impact on the calibration accuracy and computation efficiency of the SIE, SIE-R-BIC, RMSEA-N, and IGEOCM. (5) The item selection algorithm in CD-CAT affects the Q-matrix calibration accuracy of the SIE and IGEOCM methods. Under the higher-order distribution and multivariate normal distribution, the SIE method and IGEOCM had higher Q-matrix calibration accuracy when the item selection algorithms were MPWKL and GDI. On the whole, although the proposed IGEOCM is competitive and outperforms the conventional method irrespective of the calibration precision or computational efficiency, the studies on the online calibration method in CD-CAT still need to be further deepened and expanded. (PsycInfo Database Record (c) 2022 APA, all rights reserved)
KW  - cognitive diagnostic computerized adaptive testing
KW  - item replenishing
KW  - online calibration
KW  - Q-matrix
KW  - information gain of entropy
KW  - Adaptive Testing
KW  - Algorithms
KW  - Diagnosis
KW  - Normal Distribution
KW  - Sample Size
KW  - Discrimination
KW  - Estimation
KW  - Computerized Assessment
DO  - 10.3724/SP.J.1041.2021.01286
UR  - https://search.ebscohost.com/login.aspx?direct=true&db=psyh&AN=2022-71255-009&lang=de&site=ehost-live
UR  - tudongbo@aliyun.com
UR  - cy1979123@aliyun.com
DP  - EBSCOhost
DB  - APA PsycInfo
ER  - 

TY  - JOUR
ID  - 2020-73351-058
AN  - 2020-73351-058
AU  - Tennekoon, Robin L.
AU  - Smith-Forbes, Enrique V.
AU  - Woods, Yvette
T1  - A mixed methods analysis of acute upper extremity pain as measured by the Patient Reported Outcomes Measurement Information System
JF  - Military Medicine
JO  - Military Medicine
JA  - Mil Med
Y1  - 2020/05//May-Jun, 2020
VL  - 185
IS  - 5-6
SP  - e870
EP  - e877
PB  - Oxford University Press
SN  - 0026-4075
SN  - 1930-613X
AD  - Tennekoon, Robin L., Blanchfield Army Community Hospital, Department of Occupational Therapy, 650 Joel Drive, Fort Campbell, KY, US, 42223
N1  - Accession Number: 2020-73351-058. Partial author list: First Author & Affiliation: Tennekoon, Robin L.; Blanchfield Army Community Hospital, Department of Occupational Therapy, Fort Campbell, KY, US. Other Publishers: Assn of Military Surgeons of the US. Release Date: 20210531. Publication Type: Journal (0100), Peer Reviewed Journal (0110). Format Covered: Electronic. Document Type: Journal Article. Language: EnglishConference Information: San Antonio Military Health System (SAMHS) and Universities Research Forum (SURF), Jun, 2018, San Antonio, TX, US. Conference Note: Poster presentation at the aforementioned conference and at the April 2019: Annual Conference of the American Occupational Therapy Association, New Orleans, LA; August 2019: Military Health System Research Symposium (MHSRS), Kissimmee, FL. Major Descriptor: Adaptive Testing; Occupational Therapy; Psychosocial Factors; Sociocultural Factors; Acute Pain. Minor Descriptor: Military Personnel. Classification: Occupational & Vocational Rehabilitation (3384); Military Psychology (3800). Population: Human (10); Male (30); Female (40). Location: US. Age Group: Adulthood (18 yrs & older) (300); Young Adulthood (18-29 yrs) (320); Thirties (30-39 yrs) (340); Middle Age (40-64 yrs) (360). Tests & Measures: Patient-Reported Outcomes Measurement Information System DOI: 10.1037/t06780-000. Methodology: Empirical Study; Followup Study; Field Study; Interview; Qualitative Study; Quantitative Study. Supplemental Data: Experimental Materials Internet. Page Count: 8. Issue Publication Date: May-Jun, 2020. 
AB  - Introduction: Chronic pain affects U.S. service member’s (SMs) more disproportionately than individuals in the general public. SMs have unique cultural pressures to ignore or deny acute pain; therefore, the beliefs and behaviors of this group may cause them to self-report their acute pain in a specific manner. This study evaluated the strength of the relationship of the patient reported outcomes measurement information system (PROMIS) upper extremity computer adaptive test (CAT) and assessed U.S. active duty SMs experience of acute pain and function compared to the quick disabilities of the arm, shoulder, and hand (QuickDASH). In addition, the PROMIS pain interference CAT, PROMIS pain behavior CAT, and PROMIS anxiety CAT were correlated to the PROMIS upper extremity CAT and QuickDASH questionnaires. Materials and Methods: This mixed methods, sequential, explanatory study included a convenience sample of 26 participants from two occupational therapy clinics. Participants were administered five self-report questionnaires at initial evaluation and at follow up (30–90 day). At follow up, 12 participants completed a semi-structured interview. Categorical variables were summarized using percentages and analyzed using a chi-square goodness of fit test. A Pearson correlation coefficient was used to analyze the linear relationship between the QuickDASH and specified PROMIS questionnaires. This study was approved by the Institutional Review Board at Brooke Army Medical Center, reference number C.2017.173d. Results: The initial and follow up scores from the self-report questionnaires demonstrated a good to excellent correlation between the PROMIS upper extremity CAT and the QuickDASH (r = –0.65; r = –0.81; p < 0.001). Qualitative data were gathered during a semi-structured interview of 12 participants after the follow up and were analyzed using thematic analysis. Three themes emerged from qualitative analysis of data: (1) impact of military culture, (2) psychosocial effects of acute pain, and (3) therapist contributions to disability awareness. Conclusions: This study demonstrates several cultural and psychosocial influences concerning the experience of SMs with acute UE disability and pain, which were not previously reported for this population. (PsycInfo Database Record (c) 2021 APA, all rights reserved)
KW  - chronic pain
KW  - psychosocial influences
KW  - cultural factors
KW  - occupational therapy
KW  - adaptive test
KW  - acute pain
KW  - Adaptive Testing
KW  - Occupational Therapy
KW  - Psychosocial Factors
KW  - Sociocultural Factors
KW  - Acute Pain
KW  - Military Personnel
DO  - 10.1093/milmed/usz396
UR  - https://search.ebscohost.com/login.aspx?direct=true&db=psyh&AN=2020-73351-058&lang=de&site=ehost-live
DP  - EBSCOhost
DB  - APA PsycInfo
ER  - 

TY  - JOUR
ID  - 2012-21009-002
AN  - 2012-21009-002
AU  - Jiao, Hong
AU  - Macready, George
AU  - Liu, Junhui
AU  - Cho, Youngmi
T1  - A mixture Rasch model–based computerized adaptive test for Latent Class Identification
JF  - Applied Psychological Measurement
JO  - Applied Psychological Measurement
JA  - Appl Psychol Meas
Y1  - 2012/09//
VL  - 36
IS  - 6
SP  - 469
EP  - 493
PB  - Sage Publications
SN  - 0146-6216
SN  - 1552-3497
AD  - Jiao, Hong, Department of Human Development and Quantitative Methodology, University of Maryland, 1230B Benjamin Building, College Park, MD, US, 20742
N1  - Accession Number: 2012-21009-002. Partial author list: First Author & Affiliation: Jiao, Hong; University of Maryland, College Park, MD, US. Release Date: 20121001. Correction Date: 20190211. Publication Type: Journal (0100), Peer Reviewed Journal (0110). Format Covered: Electronic. Document Type: Journal Article. Language: EnglishConference Information: Annual meeting of the National Council on Measurement in Education, 2011. Conference Note: An earlier version of the paper presented at the aforementioned conference. Major Descriptor: Adaptive Testing; Algorithms; Item Response Theory; Computerized Assessment. Minor Descriptor: Information. Classification: Statistics & Mathematics (2240). Population: Human (10). Methodology: Empirical Study; Quantitative Study; Scientific Simulation. References Available: Y. Page Count: 25. Issue Publication Date: Sep, 2012. Copyright Statement: The Author(s). 2012. 
AB  - This study explored a computerized adaptive test delivery algorithm for latent class identification based on the mixture Rasch model. Four item selection methods based on the Kullback–Leibler (KL) information were proposed and compared with the reversed and the adaptive KL information under simulated testing conditions. When item separation was large, all item selection methods did not differ evidently in terms of accuracy in classifying examinees into different latent classes and estimating latent ability. However, when item separation was small, two methods with class-specific ability estimates performed better than the other two methods based on a single latent ability estimate across all latent classes. The three types of KL information distributions were compared. The KL and the reversed KL information could be the same or different depending on the ability level and the item difficulty difference between latent classes. Although the KL information and the reversed KL information were different at some ability levels and item difficulty difference levels, the use of the KL, the reversed KL, or the adaptive KL information did not affect the results substantially due to the symmetric distribution of item difficulty differences between latent classes in the simulated item pools. Item pool usage and classification convergence points were examined as well. (PsycINFO Database Record (c) 2019 APA, all rights reserved)
KW  - Rasch model
KW  - computerized adaptive testing
KW  - latent class identification
KW  - algorithms
KW  - Kullback Leibler information
KW  - Adaptive Testing
KW  - Algorithms
KW  - Item Response Theory
KW  - Computerized Assessment
KW  - Information
DO  - 10.1177/0146621612450068
UR  - https://search.ebscohost.com/login.aspx?direct=true&db=psyh&AN=2012-21009-002&lang=de&site=ehost-live
UR  - hjiao@umd.edu
DP  - EBSCOhost
DB  - APA PsycInfo
ER  - 

TY  - JOUR
ID  - 2015-54088-001
AN  - 2015-54088-001
AU  - Chen, Ping
AU  - Wang, Chun
T1  - A new online calibration method for multidimensional computerized adaptive testing
JF  - Psychometrika
JO  - Psychometrika
JA  - Psychometrika
Y1  - 2016/09//
VL  - 81
IS  - 3
SP  - 674
EP  - 701
PB  - Springer
SN  - 0033-3123
SN  - 1860-0980
AD  - Chen, Ping, National Innovation Center for Assessment of Basic Education Quality, Beijing Normal University, No. 19, Xin Jie Kou Wai Street, Hai Dian District, Beijing, China, 100875
N1  - Accession Number: 2015-54088-001. PMID: 26608960 Partial author list: First Author & Affiliation: Chen, Ping; Beijing Normal University, Beijing, China. Release Date: 20151130. Correction Date: 20160905. Publication Type: Journal (0100), Peer Reviewed Journal (0110). Format Covered: Electronic. Document Type: Journal Article. Language: EnglishConference Information: Annual Meeting of the National Council on Measurement in Education, 2014, Philadelphia, PA, US. Conference Note: Part of the paper was originally presented at the aforementioned conference. Major Descriptor: Algorithms; Simulation; Statistical Regression. Minor Descriptor: Hypothesis Testing. Classification: Statistics & Mathematics (2240). Population: Human (10). Methodology: Empirical Study; Mathematical Model; Quantitative Study; Scientific Simulation. References Available: Y. Page Count: 28. Issue Publication Date: Sep, 2016. Publication History: First Posted Date: Nov 25, 2015; First Submitted Date: Nov 10, 2014. Copyright Statement: The Psychometric Society. 2015. 
AB  - Multidimensional-Method A (M-Method A) has been proposed as an efficient and effective online calibration method for multidimensional computerized adaptive testing (MCAT) (Chen & Xin, Paper presented at the 78th Meeting of the Psychometric Society, Arnhem, The Netherlands, 2013). However, a key assumption of M-Method A is that it treats person parameter estimates as their true values, thus this method might yield erroneous item calibration when person parameter estimates contain non-ignorable measurement errors. To improve the performance of M-Method A, this paper proposes a new MCAT online calibration method, namely, the full functional MLE-M-Method A (FFMLE-M-Method A). This new method combines the full functional MLE (Jones & Jin in Psychometrika 59:59–75, 1994; Stefanski & Carroll in Annals of Statistics 13:1335–1351, 1985) with the original M-Method A in an effort to correct for the estimation error of ability vector that might otherwise adversely affect the precision of item calibration. Two correction schemes are also proposed when implementing the new method. A simulation study was conducted to show that the new method generated more accurate item parameter estimation than the original M-Method A in almost all conditions. (PsycINFO Database Record (c) 2016 APA, all rights reserved)
KW  - online calibration
KW  - multidimensional computerized adaptive testing
KW  - operational item
KW  - new item
KW  - multidimensional two-parameter logistic model
KW  - full functional maximum likelihood estimator
KW  - Algorithms
KW  - Simulation
KW  - Statistical Regression
KW  - Hypothesis Testing
U1  - Sponsor: National Natural Science Foundation of China, China. Grant: 31300862. Recipients: No recipient indicated
U1  - Sponsor: Specialized Research Fund for the Doctoral Program of Higher Education. Grant: 20130003120002. Recipients: No recipient indicated
U1  - Sponsor: Fundamental Research Funds for the Central Universities of China, China. Grant: 2013YB26. Recipients: No recipient indicated
U1  - Sponsor: National Academy of Education. Grant: 792269. Other Details: Spencer Fellowship. Recipients: No recipient indicated
U1  - Sponsor: KLAS. Grant: 130026509. Recipients: No recipient indicated
DO  - 10.1007/s11336-015-9482-9
UR  - https://search.ebscohost.com/login.aspx?direct=true&db=psyh&AN=2015-54088-001&lang=de&site=ehost-live
UR  - ORCID: 0000-0002-2920-4205
UR  - pchen@bnu.edu.cn
DP  - EBSCOhost
DB  - APA PsycInfo
ER  - 

TY  - JOUR
ID  - 2021-62773-001
AN  - 2021-62773-001
AU  - Condy, Emma
AU  - Kaat, Aaron J.
AU  - Becker, Lindsey
AU  - Sullivan, Nancy
AU  - Soorya, Latha
AU  - Berger, Natalie
AU  - Berry-Kravis, Elizabeth
AU  - Michalak, Claire
AU  - Thurm, Audrey
T1  - A novel measure of matching categories for early development: Item creation and pilot feasibility study
JF  - Research in Developmental Disabilities
JO  - Research in Developmental Disabilities
JA  - Res Dev Disabil
Y1  - 2021/08//
VL  - 115
PB  - Elsevier Science
SN  - 0891-4222
SN  - 1873-3379
AD  - Thurm, Audrey, 10 Center Drive, Room 1C250, Bethesda, MD, US, 20892
N1  - Accession Number: 2021-62773-001. PMID: 34049209 Partial author list: First Author & Affiliation: Condy, Emma; National Institute of Mental Health, Bethesda, MD, US. Release Date: 20220414. Publication Type: Journal (0100), Peer Reviewed Journal (0110). Format Covered: Electronic. Document Type: Journal Article. Language: EnglishMajor Descriptor: Adaptive Testing; Concept Formation; Early Childhood Development; Mental Age; Psychometrics. Minor Descriptor: Difficulty Level (Test); Test Construction. Classification: Developmental Scales & Schedules (2222); Cognitive & Perceptual Development (2820). Population: Human (10); Male (30); Female (40). Location: US. Age Group: Childhood (birth-12 yrs) (100); Infancy (2-23 mo) (140); Preschool Age (2-5 yrs) (160); School Age (6-12 yrs) (180). Tests & Measures: Tablet-Based Concept Formation Test. Methodology: Empirical Study; Quantitative Study. References Available: Y. ArtID: 103993. Issue Publication Date: Aug, 2021. Publication History: First Posted Date: May 25, 2021; Accepted Date: May 12, 2021; Revised Date: Apr 27, 2021; First Submitted Date: Dec 11, 2020. 
AB  - Background: Many cognitive tests assess a limited developmental span, making longitudinal measurement for trials aimed at improving cognition challenging. Tests targeting transitional skills, which integrate foundational abilities into complex schemas, may be amenable to assessment across a wide developmental span. Furthermore, tablet-based tests permit computer adaptive testing (CAT), which is psychometrically more efficient and could increase testing motivation, especially for children with developmental delays. Such measures may be useful for research and clinical practice. Aims: Outline the creation of a novel, tablet-based concept formation test, and evaluate its feasibility in individuals with mental ages less than 24-months. Methods and procedures: Item generation, user interface construction, and pre-piloting were conducted in consultation with subject matter experts. Item content and interface parameters underwent iterative revisions, resulting in the pilot test. Outcomes and results: We created and piloted a tablet-based test of concept formation suitable for CAT-based administration with items of increasing difficulty based on target salience. We show feasibility in individuals with mental ages less than 24-months-old. Conclusions and implications: Tablet-based assessment of concept formation may be a useful outcome measure of an aspect of cognitive ability in young children. Future work will address optimizing the user interface and developing CAT administration. (PsycInfo Database Record (c) 2022 APA, all rights reserved)
KW  - Concept formation
KW  - Cognitive development
KW  - Early childhood
KW  - Child, Preschool
KW  - Cognition
KW  - Computers
KW  - Feasibility Studies
KW  - Humans
KW  - Neuropsychological Tests
KW  - Pilot Projects
KW  - Adaptive Testing
KW  - Concept Formation
KW  - Early Childhood Development
KW  - Mental Age
KW  - Psychometrics
KW  - Difficulty Level (Test)
KW  - Test Construction
U1  - Sponsor: National Institute of Mental Health, Intramural Research Program, US. Grant: 1ZICMH002961. Recipients: No recipient indicated
DO  - 10.1016/j.ridd.2021.103993
UR  - https://search.ebscohost.com/login.aspx?direct=true&db=psyh&AN=2021-62773-001&lang=de&site=ehost-live
UR  - ORCID: 0000-0003-3063-5157
UR  - ORCID: 0000-0001-8147-1899
UR  - ORCID: 0000-0003-2536-5930
UR  - athurm@mail.nih.gov
DP  - EBSCOhost
DB  - APA PsycInfo
ER  - 

TY  - JOUR
ID  - 2017-08224-001
AN  - 2017-08224-001
AU  - Oltmanns, Joshua R.
AU  - Widiger, Thomas A.
T1  - A self-report measure for the ICD-11 dimensional trait model proposal: The Personality Inventory for ICD-11
JF  - Psychological Assessment
JO  - Psychological Assessment
JA  - Psychol Assess
Y1  - 2018/02//
VL  - 30
IS  - 2
SP  - 154
EP  - 169
PB  - American Psychological Association
SN  - 1040-3590
SN  - 1939-134X
AD  - Oltmanns, Joshua R., Department of Psychology, University of Kentucky, 111-D Kastle Hall, Lexington, KY, US, 40506
N1  - Accession Number: 2017-08224-001. PMID: 28230410 Other Journal Title: Psychological Assessment: A Journal of Consulting and Clinical Psychology. Partial author list: First Author & Affiliation: Oltmanns, Joshua R.; Department of Psychology, University of Kentucky, Lexington, KY, US. Release Date: 20170223. Correction Date: 20180719. Publication Type: Journal (0100), Peer Reviewed Journal (0110). Format Covered: Electronic. Document Type: Journal Article. Language: EnglishMajor Descriptor: Models; Personality Disorders; Personality Traits; Taxonomies. Minor Descriptor: Self-Report; Test Construction; Test Validity. Classification: Clinical Psychological Testing (2224); Personality Disorders (3217). Population: Human (10); Male (30); Female (40). Age Group: Adulthood (18 yrs & older) (300). Tests & Measures: 5-Dimensional Personality Test; Noncontent Based Responding Scale; Computerized Adaptive Test for Personality Disorders-- Static Form; Personality Inventory for DSM-5 DOI: 10.1037/t30042-000; Personality Inventory for ICD-11 DOI: 10.1037/t66483-000; Eysenck Personality Questionnaire-Revised DOI: 10.1037/t05461-000. Methodology: Empirical Study; Quantitative Study. Supplemental Data: Tables and Figures Internet; Tests Internet. References Available: Y. Page Count: 16. Issue Publication Date: Feb, 2018. Publication History: First Posted Date: Feb 23, 2017; Accepted Date: Jan 19, 2017; Revised Date: Jan 12, 2017; First Submitted Date: May 31, 2016. Copyright Statement: American Psychological Association. 2017. 
AB  - Proposed for the 11th edition of the World Health Organization’s International Classification of Diseases (ICD-11) is a dimensional trait model for the classification of personality disorder (Tyrer, Reed, & Crawford, 2015). The ICD-11 proposal consists of 5 broad domains: negative affective, detachment, dissocial, disinhibition, and anankastic (Mulder, Horwood, Tyrer, Carter, & Joyce, 2016). Several field trials have examined this proposal, yet none has included a direct measure of the trait model. The purpose of the current study was to develop and provide initial validation for the Personality Inventory for ICD-11 (PiCD), a self-report measure of this proposed 5-domain maladaptive trait model. Item selection and scale construction proceeded through 3 initial data collections assessing potential item performance. Two subsequent studies were conducted for scale validation. In Study 1, the PiCD was evaluated in a sample of 259 MTurk participants (who were or had been receiving mental health treatment) with respect to 2 measures of general personality structure: The Eysenck Personality Questionnaire—Revised and the 5-Dimensional Personality Test. In Study 2, the PiCD was evaluated in an additional sample of 285 participants with respect to 2 measures of maladaptive personality traits: The Personality Inventory for DSM-5 and the Computerized Adaptive Test for Personality Disorders. Study 3 provides an item-level exploratory structural equation model with the combined samples from Studies 1 and 2. The results are discussed with respect to the validity of the measure and the potential benefits for future research in having a direct, self-report measure of the ICD-11 trait proposal. (PsycINFO Database Record (c) 2018 APA, all rights reserved)
AB  - Public Significance Statement—The purpose of the present study was to develop and validate a self-report measure of the personality disorder trait model proposed for the World Health Organization’s International Classification of Diseases (ICD-11). Three studies addressed the convergent and discriminant validity of the PiCD with respect to other normal and abnormal personality trait measures, as well as its factor structure. (PsycINFO Database Record (c) 2018 APA, all rights reserved)
KW  - ICD-11
KW  - self-report
KW  - personality disorders
KW  - personality traits
KW  - maladaptive personality
KW  - Models
KW  - Personality Disorders
KW  - Personality Traits
KW  - Taxonomies
KW  - Self-Report
KW  - Test Construction
KW  - Test Validity
DO  - 10.1037/pas0000459
L3  - 10.1037/pas0000459.supp (Supplemental)
UR  - https://search.ebscohost.com/login.aspx?direct=true&db=psyh&AN=2017-08224-001&lang=de&site=ehost-live
UR  - ORCID: 0000-0001-6670-6995
UR  - jroltmanns@uky.edu
DP  - EBSCOhost
DB  - APA PsycInfo
ER  - 

TY  - JOUR
ID  - 2006-07171-047
AN  - 2006-07171-047
AU  - Guorui, Li
AU  - Shengtao, Yu
T1  - A Study on the Self-report Inventory for Shanghai Middle School Students
JF  - Psychological Science (China)
JO  - Psychological Science (China)
Y1  - 2006/03//
VL  - 29
IS  - 2
SP  - 451
EP  - 453
PB  - Editorial Board of Psychological Science
SN  - 1671-6981
AD  - Guorui, Li, Department of Psychology, East China Normal University, Shanghai, China, 200062
N1  - Accession Number: 2006-07171-047. Other Journal Title: Information on Psychological Sciences. Partial author list: First Author & Affiliation: Guorui, Li; Department of Psychology, East China Normal University, Shanghai, China. Release Date: 20061127. Publication Type: Journal (0100), Peer Reviewed Journal (0110). Format Covered: Print. Document Type: Journal Article. Language: ChineseMajor Descriptor: Inventories; Mental Health; Middle School Students; Psychometrics; Self-Report. Minor Descriptor: Factor Analysis. Classification: Clinical Psychological Testing (2224); Psychological Disorders (3210). Population: Human (10). Location: China. Age Group: Adolescence (13-17 yrs) (200). Tests & Measures: Self-report Inventory for Shangliai Middle School Students. Methodology: Empirical Study; Quantitative Study. Page Count: 3. Issue Publication Date: Mar, 2006. 
AB  - On the basis of 8 standards of mental health, the researchers worked out one psychological scale Self-report Inventory for Shanghai Middle School Students (SISMSS). This scale consists of three parts, i.e. adaptive testing, illness diagnostic test and validity scale. Adaptive testing including 8 branch-tests, namely, active study, steady and optimistic emotion, self-cognition, sense of duty, achievement motivation, communication ability, sex consciousness and self-control ability. Illness diagnostic test including 7 branch-tests, namely, subschizophrenia, sub-depression, mania, obsessive compulsive disorder, anxiety, phobia and hysteria. The research results indicate that the coefficient of internal consistency of SISMSS is 0.9081 (p<0.01). In addition, coefficient of correlation with SCL-90 is 0.58 and 0.734 with MHT. The construct validity of adaptive testing is also verified by factor analysis. (PsycINFO Database Record (c) 2016 APA, all rights reserved)
KW  - Self-report Inventory for Shangliai Middle School Students
KW  - mental health
KW  - psychometrics
KW  - factor analysis
KW  - Inventories
KW  - Mental Health
KW  - Middle School Students
KW  - Psychometrics
KW  - Self-Report
KW  - Factor Analysis
UR  - https://search.ebscohost.com/login.aspx?direct=true&db=psyh&AN=2006-07171-047&lang=de&site=ehost-live
UR  - grli@psy.ecnu.edu.en
DP  - EBSCOhost
DB  - APA PsycInfo
ER  - 

TY  - THES
ID  - 2017-19723-022
AN  - 2017-19723-022
AU  - Shao, Can
T1  - Aberrant response detection using change-point analysis
JF  - Dissertation Abstracts International: Section B: The Sciences and Engineering
JO  - Dissertation Abstracts International: Section B: The Sciences and Engineering
Y1  - 2018///
VL  - 78
IS  - 7-B(E)
PB  - ProQuest Information & Learning
SN  - 0419-4217
SN  - 978-1369541755
N1  - Accession Number: 2017-19723-022. Other Journal Title: Dissertation Abstracts International. Partial author list: First Author & Affiliation: Shao, Can; University of Notre Dame, Psychology, US. Release Date: 20171005. Publication Type: Dissertation Abstract (0400). Format Covered: Electronic. Document Type: Dissertation. Dissertation Number: AAI10308190. ISBN: 978-1369541755. Language: EnglishMajor Descriptor: Estimation; Item Analysis (Test); Psychological Assessment; Reaction Time. Classification: Psychometrics & Statistics & Methodology (2200). Population: Human (10). Methodology: Empirical Study; Quantitative Study. 
AB  - Aberrant responses, which are caused by examinees' unusual behaviors (e.g., carelessness, speededness, item pre-knowledge, warm-up, copying answers from neighbors), are frequently seen in various testing programs. Having aberrant responses may bias item parameter estimates and threaten test validity. Thus proper modeling and detection of aberrant responses is crucial both theoretically and practically in educational and psychological measurement. Previous work has primarily approached the aberrant responses research in terms of developing person fit statistics and proposing models to capture the aberrant behaviors. This dissertation attempts to incorporate change-point analysis (CPA) to investigate two types of aberrant responses: speededness and warm-up effect; both of which can be viewed as having an abrupt change point. This is an extension work of Shao, Li, and Cheng (in press) which first introduced the CPA method to detect test speededness with known item parameters. Following this line of research, three studies are carried out. The first study investigates whether CPA can help improve item calibration in the presence of speededness. In this study, the CPA method is used to first detect speeded responses with unknown item parameters. After the detection of speeded examinees as well as their corresponding speeding point (the point at which an examinee starts to speed), a cleansing procedure based upon the detected speeding point is proposed to help improve item parameter estimation. The performance of speededness detection as well as the item parameter recovery before/after utilizing the cleansing procedure are evaluated. The second study applies the CPA method to warm-up effect detection. This study first compares the impact of warm-up effect on ability estimation and examinees' classification accuracy on two types of tests: Computerized Adaptive Testing and Paper and Pencil (linear) Testing. In the second step, the CPA based detection of warm-up effect is carried out. Two methods of obtaining the critical values are utilized in the study. Different from the first two studies which both conduct CPA using item response information, the third study applies CPA to response time data in speededness detection. A gradual change log-normal model is proposed to better model the real-life change of response time affected by speededness. Two test statistics are introduced to test for a change point using response time. Moreover, this study explores the possibility of using a fixed critical value across conditions at each nominal level so that practitioners do not have to obtain a critical value through simulation every time they run speededness detection. The dissertation thesis concludes with the final chapter including a discussion of the results, limitations, practical implications, and future directions. (PsycINFO Database Record (c) 2017 APA, all rights reserved)
KW  - psychological measurement
KW  - response time
KW  - item analysis
KW  - estimation
KW  - Estimation
KW  - Item Analysis (Test)
KW  - Psychological Assessment
KW  - Reaction Time
UR  - https://search.ebscohost.com/login.aspx?direct=true&db=psyh&AN=2017-19723-022&lang=de&site=ehost-live
DP  - EBSCOhost
DB  - APA PsycInfo
ER  - 

TY  - JOUR
ID  - 2022-80500-001
AN  - 2022-80500-001
AU  - Surahman, Ence
AU  - Wang, Tzu‐Hua
T1  - Academic dishonesty and trustworthy assessment in online learning: A systematic literature review
JF  - Journal of Computer Assisted Learning
JO  - Journal of Computer Assisted Learning
JA  - J Comput Assist Learn
Y1  - 2022/07/07/
PB  - Wiley-Blackwell Publishing Ltd.
SN  - 0266-4909
SN  - 1365-2729
N1  - Accession Number: 2022-80500-001. Partial author list: First Author & Affiliation: Surahman, Ence; Department of Education and Learning Technology, College of Education, National Tsing Hua University, Hsinchu, Taiwan. Other Publishers: Blackwell Publishing. Release Date: 20220711. Correction Date: 20221128. Publication Type: Journal (0100), Peer Reviewed Journal (0110). Format Covered: Electronic. Document Type: Journal. Language: EnglishMajor Descriptor: No terms assigned. Classification: Educational & School Psychology (3500). Publication History: Accepted Date: Jun 14, 2022; First Submitted Date: Jul 31, 2021. Copyright Statement: John Wiley & Sons Ltd. 2022. 
AB  - Background Academic dishonesty (AD) and trustworthy assessment (TA) are fundamental issues in the context of an online assessment. However, little systematic work currently exists on how researchers have explored AD and TA issues in online assessment practice. Objectives Hence, this research aimed at investigating the latest findings regarding AD forms, factors affecting AD and TA, and solutions to reduce AD and increase TA to maintain the quality of online assessment. Methods We reviewed 52 articles in Scopus and Web of Science databases from January 2017 to April 2021 using the Preferred Reporting Items for Systematic Reviews and Meta‐Analyses model as a guideline to perform a systematic literature review that included three stages, namely planning, conducting, and reporting. Results and conclusions Our review found that there were different forms of AD among students in online learning namely plagiarism, cheating, collusion, and using jockeys. Individual factors such as being lazy to learn, lack of ability, and poor awareness as well as situational factors including the influence of friends, the pressure of the courses, and ease of access to information were strongly associated with AD. A technology‐based approach such as using plagiarism‐checking software, multi‐artificial intelligence (AI) in a learning management system, computer adaptive tests, and online proctoring as well as pedagogical‐based approaches, such as implementing a research ethics course programme, and a re‐design assessment form such as oral‐based and dynamic assessment to reduce cheating behaviour and also sociocultural and sociotechnical adjustment related to the online assessment are reported to reduce AD and increase TA. Implications Educators should adjust the design of online learning and assessment methods as soon as possible. The identified gaps point towards unexplored study on AI, machine learning, learning analytics tools, and related issues of AD and TA in K12 education could motivated future work in the field. (PsycInfo Database Record (c) 2022 APA, all rights reserved)
KW  - academic dishonesty
KW  - academic integrity
KW  - online assessment
KW  - online learning
KW  - remote assessment
KW  - trustworthy assessment
KW  - No terms assigned
DO  - 10.1111/jcal.12708
UR  - https://search.ebscohost.com/login.aspx?direct=true&db=psyh&AN=2022-80500-001&lang=de&site=ehost-live
UR  - ORCID: 0000-0002-4085-9851
UR  - ORCID: 0000-0002-8850-4275
UR  - tzuhuawang@gmail.com
DP  - EBSCOhost
DB  - APA PsycInfo
ER  - 

TY  - THES
ID  - 1998-95005-133
AN  - 1998-95005-133
AU  - Park, Chung
T1  - Accuracy of parameter estimation on polytomous IRT models
JF  - Dissertation Abstracts International Section A: Humanities and Social Sciences
JO  - Dissertation Abstracts International Section A: Humanities and Social Sciences
Y1  - 1998/03//
VL  - 58
IS  - 9-A
SP  - 3481
EP  - 3481
PB  - ProQuest Information & Learning
SN  - 0419-4209
N1  - Accession Number: 1998-95005-133. Other Journal Title: Dissertation Abstracts International. Partial author list: First Author & Affiliation: Park, Chung; U Massachusetts, US. Release Date: 19990801. Publication Type: Dissertation Abstract (0400). Format Covered: Electronic. Document Type: Dissertation. Dissertation Number: AAM9809382. Language: EnglishMajor Descriptor: Item Response Theory; Measurement; Models; Statistical Estimation. Classification: Psychometrics & Statistics & Methodology (2200). Page Count: 1. 
AB  - Procedures based on item response theory (IRT) are widely accepted for solving various measurement problems which cannot be solved using classical test theory (CTT) procedures. The desirable features of dichotomous IRT models over CTT are well known and have been documented by Hambleton, Swaminathan, and Rogers (1991). However, dichotomous IRT models are inappropriate for situations where items need to be scored in more than two categories. For example, in performance assessments, most of the scoring rubrics for performance assessment require scoring of examinee's responses in ordered categories. In addition, polytomous IRT models are useful for assessing an examinee's partial knowledge or levels of mastery. However, the successful application of polytomous IRT models to practical situations depends on the availability of reasonable and well-behaved estimates of the parameters of the models. Therefore, in this study, the behavior of estimators of parameters in polytomous IRT models were examined. In the first study, factors that affected the accuracy, variance, and bias of the marginal maximum likelihood (MML) estimators in the generalized partial credit model (GPCM) were investigated. Overall, the results of the study showed that the MML estimators of the parameters of the GPCM, as obtained through the computer program, PARSCALE, performed well under various conditions. However, there was considerable bias in the estimates of the category parameters under all conditions investigated. The average bias did not decrease when sample size and test length increased. The bias contributed to large RMSE in the estimation of category parameters. Further studies need to be conducted to study the effect of bias in the estimates of parameters on the estimation of ability, the development of item banks, and on adaptive testing based on polytomous IRT models. In the second study, the effectiveness of Bayesian procedures for estimating parameters in the GPCM was examined. The results showed that Bayes procedures provided more accurate estimates of parameters with small data sets. Priors on the slope parameters, while having only a modest effect on the accuracy of estimation of slope parameters, had a very positive effect on the accuracy of estimation of the step difficulty parameters. (PsycINFO Database Record (c) 2016 APA, all rights reserved)
KW  - accuracy of parameter estimation on polytomous item response theory models
KW  - Item Response Theory
KW  - Measurement
KW  - Models
KW  - Statistical Estimation
UR  - https://search.ebscohost.com/login.aspx?direct=true&db=psyh&AN=1998-95005-133&lang=de&site=ehost-live
DP  - EBSCOhost
DB  - APA PsycInfo
ER  - 

TY  - JOUR
ID  - 2011-00853-009
AN  - 2011-00853-009
AU  - Wauters, K.
AU  - Desmet, P.
AU  - Van den Noortgate, W.
T1  - Adaptive item-based learning environments based on the item response theory: Possibilities and challenges
JF  - Journal of Computer Assisted Learning
JO  - Journal of Computer Assisted Learning
Y1  - 2010/12//
VL  - 26
IS  - 6
SP  - 549
EP  - 562
PB  - Wiley-Blackwell Publishing Ltd.
SN  - 0266-4909
SN  - 1365-2729
AD  - Wauters, K., Katholieke Universiteit Leuven, Campus Kortrijk, Etienne Sabbelaan 53, 8500, Kortrijk, Belgium
N1  - Accession Number: 2011-00853-009. Partial author list: First Author & Affiliation: Wauters, K.; iTEC, Interdisciplinary Research on Technology, Education and Communication, Katholieke Universiteit Leuven, Kortrijk, Belgium. Other Publishers: Blackwell Publishing. Release Date: 20110307. Publication Type: Journal (0100), Peer Reviewed Journal (0110). Format Covered: Electronic. Document Type: Journal Article. Language: EnglishMajor Descriptor: Adaptive Testing; Item Response Theory; Knowledge Level; Learning Environment; Motivation. Minor Descriptor: Learning Ability. Classification: Academic Learning & Achievement (3550). Population: Human (10). References Available: Y. Page Count: 14. Issue Publication Date: Dec, 2010. Publication History: Accepted Date: May 14, 2010. Copyright Statement: Blackwell Publishing Ltd. 2010. 
AB  - The popularity of intelligent tutoring systems (ITSs) is increasing rapidly. In order to make learning environments more efficient, researchers have been exploring the possibility of an automatic adaptation of the learning environment to the learner or the context. One of the possible adaptation techniques is adaptive item sequencing by matching the difficulty of the items to the learner’s knowledge level. This is already accomplished to a certain extent in adaptive testing environments, where the test is tailored to the person’s ability level by means of the item response theory (IRT). Even though IRT has been a prevalent computerized adaptive test (CAT) approach for decades and applying IRT in item-based ITSs could lead to similar advantages as in CAT (e.g. higher motivation and more efficient learning), research on the application of IRT in such learning environments is highly restricted or absent. The purpose of this paper was to explore the feasibility of applying IRT in adaptive item-based ITSs. Therefore, we discussed the two main challenges associated with IRT application in such learning environments: the challenge of the data set and the challenge of the algorithm. We concluded that applying IRT seems to be a viable solution for adaptive item selection in item-based ITSs provided that some modifications are implemented. Further research should shed more light on the adequacy of the proposed solutions. (PsycINFO Database Record (c) 2016 APA, all rights reserved)
KW  - knowledge level
KW  - computer adaptive test
KW  - item response theory
KW  - motivation
KW  - efficient learning
KW  - learning environment
KW  - ability level
KW  - Adaptive Testing
KW  - Item Response Theory
KW  - Knowledge Level
KW  - Learning Environment
KW  - Motivation
KW  - Learning Ability
DO  - 10.1111/j.1365-2729.2010.00368.x
UR  - https://search.ebscohost.com/login.aspx?direct=true&db=psyh&AN=2011-00853-009&lang=de&site=ehost-live
UR  - kelly.wauters@kuleuven-kortrijk.be
DP  - EBSCOhost
DB  - APA PsycInfo
ER  - 

TY  - JOUR
ID  - 2020-86853-001
AN  - 2020-86853-001
AU  - Van Bourg, Joshua
AU  - Gilchrist, Rachel
AU  - Wynne, Clive D. L.
T1  - Adaptive spatial working memory assessments for aging pet dogs
JF  - Animal Cognition
JO  - Animal Cognition
JA  - Anim Cogn
Y1  - 2021/05//
VL  - 24
IS  - 3
SP  - 511
EP  - 531
PB  - Springer
SN  - 1435-9448
SN  - 1435-9456
AD  - Van Bourg, Joshua
N1  - Accession Number: 2020-86853-001. PMID: 33185769 Partial author list: First Author & Affiliation: Van Bourg, Joshua; Department of Psychology, Arizona State University, Tempe, AZ, US. Release Date: 20201119. Correction Date: 20211216. Publication Type: Journal (0100), Peer Reviewed Journal (0110). Format Covered: Electronic. Document Type: Journal Article. Language: EnglishMajor Descriptor: Animal Development; Dogs; Short Term Memory; Spatial Memory; Animal Cognition. Minor Descriptor: Cognitive Development; Environmental Adaptation. Classification: Learning & Motivation (2420). Population: Animal (20); Male (30); Female (40). Methodology: Empirical Study; Quantitative Study. Supplemental Data: Other Internet. References Available: Y. Page Count: 21. Issue Publication Date: May, 2021. Publication History: First Posted Date: Nov 13, 2020; Accepted Date: Oct 30, 2020; Revised Date: Oct 27, 2020; First Submitted Date: Aug 4, 2020. Copyright Statement: Springer-Verlag GmbH Germany, part of Springer Nature. 2020. 
AB  - Assessments for spatial working memory (SWM) in pet dogs that can detect age-related cognitive deficits in a single session may aid in diagnosing canine dementia and may facilitate translational research on Alzheimer’s disease in humans. Adaptive testing procedures are widely used in single-session assessments for humans with diverse cognitive abilities. In this study, we designed and deployed two up-down staircase assessments for SWM in which 26 pet dogs were required to recall the location of a treat hidden behind one of two identical boxes following delays of variable length. In the first experiment, performance tended to decline with age but few dogs completed the test (n = 10). However, all of the dogs that participated in the second experiment (n = 24) completed the assessment and provided reliable evidence of learning and retaining the task. Delay length and age significantly predicted performance supporting the validity of this assessment. The relationships between age and performance were described by inverted U-shaped functions as both old and young dogs displayed deficits in weighted cumulative-scores and trial-by-trial performance. Thus, SWM in pet dogs may develop until midlife and decline thereafter. Exploratory analyses of non-mnemonic fixation strategies, sustained engagement, inhibitory control, and potential improvements for future SWM assessments which adopt this paradigm are also discussed. (PsycInfo Database Record (c) 2021 APA, all rights reserved)
KW  - Dog
KW  - Cognitive decline
KW  - Aging
KW  - Development
KW  - Spatial working memory
KW  - Staircase methods
KW  - Aging
KW  - Animals
KW  - Cognition Disorders
KW  - Dogs
KW  - Memory, Short-Term
KW  - Mental Recall
KW  - Spatial Memory
KW  - Animal Development
KW  - Dogs
KW  - Short Term Memory
KW  - Spatial Memory
KW  - Animal Cognition
KW  - Cognitive Development
KW  - Environmental Adaptation
U1  - Sponsor: Arizona Alzheimer’s Disease Center, US. Recipients: No recipient indicated
DO  - 10.1007/s10071-020-01447-3
UR  - https://search.ebscohost.com/login.aspx?direct=true&db=psyh&AN=2020-86853-001&lang=de&site=ehost-live
UR  - ORCID: 0000-0001-8408-1466
UR  - ORCID: 0000-0003-0599-9602
UR  - joshvanbourg@asu.edu
DP  - EBSCOhost
DB  - APA PsycInfo
ER  - 

TY  - JOUR
ID  - 1966-00032-001
AN  - 1966-00032-001
AU  - Greenwood, Dennis
AU  - Taylor, Charles
T1  - Adaptive testing in an older population
JF  - The Journal of Psychology: Interdisciplinary and Applied
JO  - The Journal of Psychology: Interdisciplinary and Applied
JA  - J Psychol
Y1  - 1965///
VL  - 60
IS  - 2
SP  - 193
EP  - 198
PB  - Heldref Publications
SN  - 0022-3980
SN  - 1940-1019
N1  - Accession Number: 1966-00032-001. Partial author list: First Author & Affiliation: Greenwood, Dennis. Other Publishers: Taylor & Francis. Release Date: 19660101. Correction Date: 20221128. Publication Type: Journal (0100), Peer Reviewed Journal (0110). Format Covered: Print. Document Type: Journal Article. Language: EnglishMajor Descriptor: Adaptive Testing; Aging; Anxiety; Intelligence Measures; Psychometrics. Classification: Tests & Testing (2220); Aging & Older Adult Development (2860). Population: Human (10). Age Group: Adulthood (18 yrs & older) (300); Middle Age (40-64 yrs) (360); Aged (65 yrs & older) (380). Methodology: Empirical Study; Quantitative Study. Page Count: 6. Issue Publication Date: 1965. 
AB  - To test whether allaying the anxiety of older Ss in taking intelligence tests significantly raises scores, 30 Ss matched for age, education, and IQ were retested by the WAIS: ½ by conventional methods and ½ by the 'adaptive' method. This technique involved beginning each test with an item below the S's anticipated mental level and alternating easy and hard items from the scale or a pool of similar items. The increase in scores from using the adaptive method was significantly greater than increase from retesting by the consecutive method. (PsycInfo Database Record (c) 2022 APA, all rights reserved)
KW  - intelligence tests
KW  - adaptive testing
KW  - older population
KW  - anxiety
KW  - Adaptive Testing
KW  - Aging
KW  - Anxiety
KW  - Intelligence Measures
KW  - Psychometrics
DO  - 10.1080/00223980.1965.10544767
UR  - https://search.ebscohost.com/login.aspx?direct=true&db=psyh&AN=1966-00032-001&lang=de&site=ehost-live
DP  - EBSCOhost
DB  - APA PsycInfo
ER  - 

TY  - JOUR
ID  - 1999-01506-004
AN  - 1999-01506-004
AU  - Bergstrom, Betty A.
AU  - Lunz, Mary E.
AU  - Gershon, Richard C.
T1  - Altering the level of difficulty in computer adaptive testing
JF  - Applied Measurement in Education
JO  - Applied Measurement in Education
Y1  - 1992///
VL  - 5
IS  - 2
SP  - 137
EP  - 149
PB  - Lawrence Erlbaum
SN  - 0895-7347
SN  - 1532-4818
N1  - Accession Number: 1999-01506-004. Partial author list: First Author & Affiliation: Bergstrom, Betty A.; Computer Adaptive Technologies, Inc., Chicago, IL, US. Other Publishers: Taylor & Francis. Release Date: 19991201. Correction Date: 20221128. Publication Type: Journal (0100), Peer Reviewed Journal (0110). Format Covered: Print. Document Type: Journal Article. Language: EnglishConference Information: Meeting of the National Council on Measurement in Education, Apr, 1991, Chicago, IL, US. Major Descriptor: Ability; Adaptive Testing; Difficulty Level (Test); Test Taking; Computerized Assessment. Minor Descriptor: Error of Measurement; Estimation; Test Items. Classification: Educational Measurement (2227); Educational & School Psychology (3500). Population: Human (10). Methodology: Empirical Study. References Available: Y. Page Count: 13. Issue Publication Date: 1992. 
AB  - Examines the effect of altering test difficulty on examinee ability measures and test length in a computer adaptive test. The 225 Ss were randomly assigned to 3 test difficulty conditions and given a variable length computer adaptive test. Examinees in the hard, medium, and easy test condition took a test targeted at the 50%, 60%, or 70% probability of correct response. The results show that altering the probability of a correct response does not affect estimation of examinee ability and that taking an easier computer adaptive test only slightly increases the number of items necessary to reach specified levels of precision. (PsycInfo Database Record (c) 2022 APA, all rights reserved)
KW  - level of test item difficulty on test length
KW  - estimation of ability
KW  - examinees
KW  - Ability
KW  - Adaptive Testing
KW  - Difficulty Level (Test)
KW  - Test Taking
KW  - Computerized Assessment
KW  - Error of Measurement
KW  - Estimation
KW  - Test Items
DO  - 10.1207/s15324818ame0502_4
UR  - https://search.ebscohost.com/login.aspx?direct=true&db=psyh&AN=1999-01506-004&lang=de&site=ehost-live
DP  - EBSCOhost
DB  - APA PsycInfo
ER  - 

TY  - JOUR
ID  - 2008-17099-007
AN  - 2008-17099-007
AU  - Huang, Yueh-Min
AU  - Lin, Yen-Ting
AU  - Cheng, Shu-Chen
T1  - An adaptive testing system for supporting versatile educational assessment
JF  - Computers & Education
JO  - Computers & Education
JA  - Comput Educ
Y1  - 2009/01//
VL  - 52
IS  - 1
SP  - 53
EP  - 67
PB  - Elsevier Science
SN  - 0360-1315
SN  - 1873-782X
AD  - Huang, Yueh-Min, Department of Engineering Science, National Cheng Kug University, No. 1, Ta-Hsueh Road, Tainan, Taiwan, 701
N1  - Accession Number: 2008-17099-007. Partial author list: First Author & Affiliation: Huang, Yueh-Min; Department of Engineering Science, National Cheng Kug University, Tainan, Taiwan. Release Date: 20090202. Correction Date: 20190211. Publication Type: Journal (0100), Peer Reviewed Journal (0110). Format Covered: Electronic. Document Type: Journal Article. Language: EnglishMajor Descriptor: Academic Achievement; Adaptive Testing; Distance Education; Self-Evaluation; Technology. Minor Descriptor: Education; Computerized Assessment. Classification: Academic Learning & Achievement (3550). Population: Human (10). Methodology: Empirical Study; Quantitative Study. References Available: Y. Page Count: 15. Issue Publication Date: Jan, 2009. 
AB  - With the rapid growth of computer and mobile technology, it is a challenge to integrate computer based test (CBT) with mobile learning (m-learning) especially for formative assessment and self-assessment. In terms of self-assessment, computer adaptive test (CAT) is a proper way to enable students to evaluate themselves. In CAT, students are assessed through a process that uses item response theory (IRT), a well-founded psychometric theory. Furthermore, a large item bank is indispensable to a test, but when a CAT system has a large item bank, the test item selection of IRT becomes more tedious. Besides the large item bank, item exposure mechanism is also essential to a testing system. However, IRT all lack the above-mentioned points. These reasons have motivated the authors to carry out this study. This paper describes a design issue aimed at the development and implementation of an adaptive testing system. The system can support several assessment functions and different devices. Moreover, the researchers apply a novel approach, particle swarm optimization (PSO) to alleviate the computational complexity and resolve the problem of item exposure. Throughout the development of the system, a formative evaluation was embedded into an integral part of the design methodology that was used for improving the system. After the system was formally released onto the web, some questionnaires and experiments were conducted to evaluate the usability, precision, and efficiency of the system. The results of these evaluations indicated that the system provides an adaptive testing for different devices and supports versatile assessment functions. Moreover, the system can estimate students' ability reliably and validly and conduct an adaptive test efficiently. Furthermore, the computational complexity of the system was alleviated by the PSO approach. By the approach, the test item selection procedure becomes efficient and the average best fitness values are very close to the optimal solutions. (PsycINFO Database Record (c) 2019 APA, all rights reserved)
KW  - adaptive testing system
KW  - versatile educational assessment
KW  - technology growth
KW  - self-assessment
KW  - distance education
KW  - academic performance
KW  - Academic Achievement
KW  - Adaptive Testing
KW  - Distance Education
KW  - Self-Evaluation
KW  - Technology
KW  - Education
KW  - Computerized Assessment
U1  - Sponsor: National Science Council of Taiwan, Taiwan. Grant: NSC 95-2221-E-006-307-MY3; NSC 95-2221-E-006-306-MY3; NSC 96-2524-S-032-001. Recipients: No recipient indicated
DO  - 10.1016/j.compedu.2008.06.007
UR  - https://search.ebscohost.com/login.aspx?direct=true&db=psyh&AN=2008-17099-007&lang=de&site=ehost-live
UR  - kittyc@mail.stut.edu.tw
UR  - rickylin@easyLearn.org
UR  - huang@mail.ncku.edu.tw
DP  - EBSCOhost
DB  - APA PsycInfo
ER  - 

TY  - JOUR
ID  - 2022-88568-001
AN  - 2022-88568-001
AU  - Petersen, Morten Aa.
AU  - Gamper, Eva-Maria
AU  - Costantini, Anna
AU  - Giesinger, Johannes M.
AU  - Holzner, Bernhard
AU  - Johnson, Colin
AU  - Sztankay, Monika
AU  - Young, Teresa
AU  - Groenvold, Mogens
T1  - An emotional functioning item bank of 24 items for computerized adaptive testing (CAT) was established
JF  - Journal of Clinical Epidemiology
JO  - Journal of Clinical Epidemiology
JA  - J Clin Epidemiol
Y1  - 2016/02//
VL  - 70
SP  - 90
EP  - 100
PB  - Elsevier Science
SN  - 0895-4356
SN  - 1878-5921
AD  - Petersen, Morten Aa., Research Unit, Department of Palliative Medicine, Bispebjerg Hospital, University of Copenhagen, Bispebjerg Bakke 23, 2400, Copenhagen, Denmark
N1  - Accession Number: 2022-88568-001. PMID: 26363341 Other Journal Title: Journal of Chronic Diseases. Partial author list: First Author & Affiliation: Petersen, Morten Aa.; Research Unit, Department of Palliative Medicine, Bispebjerg Hospital, University of Copenhagen, Copenhagen, Denmark. Institutional Authors: EORTC Quality of Life Group. Other Publishers: Pergamon Press. Release Date: 20220901. Publication Type: Journal (0100), Peer Reviewed Journal (0110). Format Covered: Electronic. Document Type: Journal Article. Language: EnglishGrant Information: Gamper, Eva-Maria. Major Descriptor: Adaptive Testing; Neoplasms; Psychometrics; Test Items; Quality of Life Measures. Minor Descriptor: Test Construction. Classification: Clinical Psychological Testing (2224); Cancer (3293). Population: Human (10); Male (30); Female (40). Location: Austria; Denmark; Italy; United Kingdom. Age Group: Adulthood (18 yrs & older) (300); Young Adulthood (18-29 yrs) (320); Thirties (30-39 yrs) (340); Middle Age (40-64 yrs) (360); Aged (65 yrs & older) (380); Very Old (85 yrs & older) (390). Tests & Measures: EORTC-QLQ-CAT-Emotional Function Item Bank. Methodology: Empirical Study; Quantitative Study. References Available: Y. Page Count: 11. Issue Publication Date: Feb, 2016. Publication History: First Posted Date: Sep 9, 2015; Accepted Date: Sep 7, 2015. Copyright Statement: All rights reserved. Elsevier Inc. 2016. 
AB  - Objective: To improve measurement precision, the European Organisation for Research and Treatment of Cancer (EORTC) Quality of Life Group is developing an item bank for computerized adaptive testing (CAT) of emotional functioning (EF). The item bank will be within the conceptual framework of the widely used EORTC Quality of Life questionnaire (QLQ-C30). Study Design and Setting: On the basis of literature search and evaluations by international samples of experts and cancer patients, 38 candidate items were developed. The psychometric properties of the items were evaluated in a large international sample of cancer patients. This included evaluations of dimensionality, item response theory (IRT) model fit, differential item functioning (DIF), and of measurement precision/statistical power. Results: Responses were obtained from 1,023 cancer patients from four countries. The evaluations showed that 24 items could be included in a unidimensional IRT model. DIF did not seem to have any significant impact on the estimation of EF. Evaluations indicated that the CAT measure may reduce sample size requirements by up to 50% compared to the QLQ-C30 EF scale without reducing power. Conclusion: On the basis of thorough psychometric evaluations, we have established an EF item bank of 24 items. This will allow for more precise and flexible measurement of EF, while maintaining backward compatibility with the QLQ-C30 EF scale. (PsycInfo Database Record (c) 2022 APA, all rights reserved)
KW  - computerized adaptive testing
KW  - EORTC QLQ-C30
KW  - emotional functioning
KW  - item response theory
KW  - oncology
KW  - patient-reported outcome
KW  - Adult
KW  - Aged
KW  - Aged, 80 and over
KW  - Emotions
KW  - Female
KW  - Humans
KW  - Male
KW  - Middle Aged
KW  - Neoplasms
KW  - Psychometrics
KW  - Quality of Life
KW  - Surveys and Questionnaires
KW  - Adaptive Testing
KW  - Neoplasms
KW  - Psychometrics
KW  - Test Items
KW  - Quality of Life Measures
KW  - Test Construction
U1  - Sponsor: EORTC Quality of Life Group. Recipients: No recipient indicated
U1  - Sponsor: Austrian Science Fund, Austria. Grant: FWF L502; FWF J3353. Recipients: Gamper, Eva-Maria; Giesinger, Johannes M.
DO  - 10.1016/j.jclinepi.2015.09.002
UR  - https://search.ebscohost.com/login.aspx?direct=true&db=psyh&AN=2022-88568-001&lang=de&site=ehost-live
UR  - ORCID: 0000-0002-3389-3621
UR  - Morten.Aagaard.Petersen@regionh.dk
DP  - EBSCOhost
DB  - APA PsycInfo
ER  - 

TY  - THES
ID  - 2013-99010-576
AN  - 2013-99010-576
AU  - Lam, Wai Yan Wendy
T1  - An empirical examination of the impact of item parameters on IRT information functions in mixed format tests
JF  - Dissertation Abstracts International Section A: Humanities and Social Sciences
JO  - Dissertation Abstracts International Section A: Humanities and Social Sciences
Y1  - 2013///
VL  - 73
IS  - 7-A(E)
PB  - ProQuest Information & Learning
SN  - 0419-4209
SN  - 978-1-267-20953-5
N1  - Accession Number: 2013-99010-576. Other Journal Title: Dissertation Abstracts International. Partial author list: First Author & Affiliation: Lam, Wai Yan Wendy; U Massachusetts Amherst, US. Release Date: 20130401. Correction Date: 20221128. Publication Type: Dissertation Abstract (0400). Format Covered: Electronic. Document Type: Dissertation. Dissertation Number: AAI3498358. ISBN: 978-1-267-20953-5. Language: EnglishMajor Descriptor: Empirical Methods; Information; Item Response Theory; Test Construction. Minor Descriptor: Theories. Classification: Educational & School Psychology (3500). Population: Human (10). Methodology: Empirical Study; Quantitative Study. 
AB  - IRT, also referred as 'modern test theory', offers many advantages over CTT-based methods in test development. Specifically, an IRT information function has the capability to build a test that has the desired precision of measurement for any defined proficiency scale when a sufficient number of test items are available. This feature is extremely useful when the information is used for decision making, for instance, whether an examinee attain certain mastery level. Computerized adaptive testing (CAT) is one of the many examples using IRT information functions in test construction. The purposes of this study were as follows: (1) to examine the consequences of improving the test quality through the addition of more discriminating items with different item formats; (2) to examine the effect of having a test where its difficulty does not align with the ability level of the intended population; (3) to investigate the change in decision consistency and decision accuracy; and (4) to understand changes in expected information when test quality is either improved or degraded, using both empirical and simulated data. Main findings from the study were as follows: (1) increasing the discriminating power of any types of items generally increased the level of information; however, sometimes it could bring adverse effect to the extreme ends of the ability continuum; (2) it was important to have more items that were targeted at the population of interest, otherwise, no matter how good the quality of the items may be, they were of less value in test development when they were not targeted to the distribution of candidate ability or at the cutscores; (3) decision consistency (DC), Kappa statistic, and decision accuracy (DA) increased with better quality items; (4) DC and  Kappa were negatively affected when difficulty of the test did not match with the ability of the intended population; however, the effect was less severe if the test was easier than needed; (5) tests with more better quality items lowered false positive (FP) and false negative (FN) rate at the cutscores; (6) when test difficulty did not match with the ability of the target examinees, in general, both FP and FN rates increased; (7) polytomous items tended to yield more information than dichotomously scored items, regardless of the discriminating parameter and difficulty of the item; and (8) the more score categories an item had, the more information it could provide. Findings from this thesis should help testing agencies and practitioners to have better understanding of the item parameters on item and test information functions. This understanding is crucial for the improvement of the item bank quality and ultimately on how to build better tests that could provide more accurate proficiency classifications. However, at the same time, item writers should be conscientious about the fact that the item information function is merely a statistical tool for building a good test, other criteria should also be considered, for example, content balancing and content validity. (PsycInfo Database Record (c) 2022 APA, all rights reserved)
KW  - item response theory
KW  - mixed format tests
KW  - test construction
KW  - theory
KW  - Empirical Methods
KW  - Information
KW  - Item Response Theory
KW  - Test Construction
KW  - Theories
UR  - https://search.ebscohost.com/login.aspx?direct=true&db=psyh&AN=2013-99010-576&lang=de&site=ehost-live
DP  - EBSCOhost
DB  - APA PsycInfo
ER  - 

TY  - JOUR
ID  - 2019-05830-008
AN  - 2019-05830-008
AU  - Keaton, Sarah A.
AU  - Madaj, Zachary B.
AU  - Heilman, Patrick
AU  - Smart, LeAnn
AU  - Grit, Jamie
AU  - Gibbons, Robert
AU  - Postolache, Teodor T.
AU  - Roaten, Kimberly
AU  - Achtyes, Eric D.
AU  - Brundin, Lena
T1  - An inflammatory profile linked to increased suicide risk
JF  - Journal of Affective Disorders
JO  - Journal of Affective Disorders
JA  - J Affect Disord
Y1  - 2019/03/15/
VL  - 247
SP  - 57
EP  - 65
PB  - Elsevier Science
SN  - 0165-0327
SN  - 1573-2517
AD  - Brundin, Lena
N1  - Accession Number: 2019-05830-008. PMID: 30654266 Partial author list: First Author & Affiliation: Keaton, Sarah A.; Department of Physiology, Michigan State University, East Lansing, MI, US. Release Date: 20190415. Publication Type: Journal (0100), Peer Reviewed Journal (0110). Format Covered: Electronic. Document Type: Journal Article. Language: EnglishGrant Information: Brundin, Lena. Major Descriptor: Cytokines; Inflammation; Major Depression; Risk Factors; Suicide. Minor Descriptor: Interleukins; Leucocytes. Classification: Psychological Disorders (3210). Population: Human (10); Female (40). Location: US. Age Group: Adolescence (13-17 yrs) (200); Adulthood (18 yrs & older) (300); Young Adulthood (18-29 yrs) (320); Thirties (30-39 yrs) (340); Middle Age (40-64 yrs) (360); Aged (65 yrs & older) (380). Tests & Measures: Columbia-Suicide Severity Rating Scale DOI: 10.1037/t52667-000; Hamilton Rating Scale for Depression DOI: 10.1037/t04100-000; Structured Clinical Interview for DSM-IV Axis I Disorders; Center for Epidemiologic Studies Depression Scale; Patient Health Questionnaire-9 DOI: 10.1037/t06165-000. Methodology: Empirical Study; Quantitative Study. Supplemental Data: Tables and Figures Internet. References Available: Y. Page Count: 9. Issue Publication Date: Mar 15, 2019. Publication History: First Posted Date: Jan 3, 2019; Accepted Date: Dec 24, 2018; Revised Date: Nov 25, 2018; First Submitted Date: Sep 14, 2018. Copyright Statement: All rights reserved. Elsevier B.V. 2019. 
AB  - Background: Suicide risk assessments are often challenging for clinicians, and therefore, biological markers are warranted as guiding tools in these assessments. Suicidal patients display increased cytokine levels in peripheral blood, although the composite inflammatory profile in the subjects is still unknown. It is also not yet established whether certain inflammatory changes are specific to suicidal subjects. To address this, we measured 45 immunobiological factors in peripheral blood and identified the biological profiles associated with cross-diagnostic suicide risk and depression, respectively. Methods: Sixty-six women with mood and anxiety disorders underwent computerized adaptive testing for mental health, assessing depression and suicide risk. Weighted correlation network analysis was used to uncover system level associations between suicide risk, depression, and the immunobiological factors in plasma. Secondary regression models were used to establish the sensitivity of the results to potential confounders, including age, body mass index (BMI), treatment and symptoms of depression and anxiety. Results: The biological profile of patients assessed to be at increased suicide risk differed from that associated with depression. At the system level, a biological cluster containing increased levels of interleukin-6, lymphocytes, monocytes, white blood cell count and polymorphonuclear leukocyte count significantly impacted suicide risk, with the latter two inferring the strongest influence. The cytokine interleukin-8 was independently and negatively associated with increased suicide risk. The results remained after adjusting for confounders. Limitations: This study is cross-sectional and not designed to prove causality. Discussion: A unique immunobiological profile was linked to increased suicide risk. The profile was different from that observed in patients with depressive symptoms, and indicates that granulocyte mediated biological mechanisms could be activated in patients at risk for suicide. (PsycINFO Database Record (c) 2019 APA, all rights reserved)
KW  - Depression
KW  - Suicide
KW  - White blood cells
KW  - Network analysis
KW  - Cytokine
KW  - Granolucyte
KW  - Cytokines
KW  - Inflammation
KW  - Major Depression
KW  - Risk Factors
KW  - Suicide
KW  - Interleukins
KW  - Leucocytes
U1  - Sponsor: National Institutes of Health, US. Grant: R01-MH104622. Recipients: Brundin, Lena; Achtyes, Eric D.; Postolache, Teodor T.
U1  - Sponsor: National Institutes of Health, US. Grant: R01-MH66302. Recipients: Gibbons, Robert
U1  - Sponsor: Pine Rest Foundation. Recipients: Achtyes, Eric D.; Smart, LeAnn
U1  - Sponsor: American Foundation for Suicide Prevention, US. Grant: DIG 1-162-12. Other Details: Distinguished Investigator Award. Recipients: Postolache, Teodor T.
U1  - Sponsor: National Institute of Diabetes and Digestive and Kidney Diseases, US. Grant: P30 DK072488. Other Details: NORC pilot/developmental grant. Recipients: Postolache, Teodor T.
U1  - Sponsor: Joint Institute for Food Safety and Applied Nutrition. Recipients: No recipient indicated
U1  - Sponsor: Food and Drug Administration, US. Grant: Cooperative Agreement FDU.001418. Recipients: Postolache, Teodor T.
U1  - Sponsor: US Department of Veterans Affairs, CSR&D, US. Grant: 1I01CX001310-01. Other Details: Merit Award. Recipients: Postolache, Teodor T.; Brundin, Lena
DO  - 10.1016/j.jad.2018.12.100
UR  - https://search.ebscohost.com/login.aspx?direct=true&db=psyh&AN=2019-05830-008&lang=de&site=ehost-live
UR  - ORCID: 0000-0001-8939-1535
UR  - ORCID: 0000-0002-7453-0448
UR  - Lena.Brundin@vai.org
UR  - Eric.Achtyes@PineRest.org
DP  - EBSCOhost
DB  - APA PsycInfo
ER  - 

TY  - JOUR
ID  - 1997-03451-002
AN  - 1997-03451-002
AU  - Ponsoda, Vicente
AU  - Wise, Steven L.
AU  - Olea, Julio
AU  - Revuelta, Javier
T1  - An investigation of self-adapted testing in a Spanish high school population
JF  - Educational and Psychological Measurement
JO  - Educational and Psychological Measurement
JA  - Educ Psychol Meas
Y1  - 1997/04//
VL  - 57
IS  - 2
SP  - 210
EP  - 221
PB  - Sage Publications
SN  - 0013-1644
SN  - 1552-3888
N1  - Accession Number: 1997-03451-002. Partial author list: First Author & Affiliation: Ponsoda, Vicente; U Autónoma, Madrid, Spain. Release Date: 19970101. Correction Date: 20190211. Publication Type: Journal (0100), Peer Reviewed Journal (0110). Format Covered: Print. Document Type: Journal Article. Language: EnglishMajor Descriptor: Ability Level; Adaptive Testing; Test Administration; Test Anxiety; Computerized Assessment. Minor Descriptor: High School Students; Test Scores; Vocabulary. Classification: Curriculum & Programs & Teaching Methods (3530); Educational Measurement (2227). Population: Human (10). Age Group: Adolescence (13-17 yrs) (200). Methodology: Empirical Study. Page Count: 12. Issue Publication Date: Apr, 1997. 
AB  - This study, using 209 Spanish high school students, compared 4 types of a computer-based English vocabulary test: (1) a self-adapted test (SAT), (2) a computerized adaptive test (CAT), (3) a conventional test of randomly selected items, and (4) a test that combined SAT and CAT. No statistically significant differences were found among the test types for either estimated ability or posttest anxiety. Statistically significant differences were found for the number of correct responses and testing time. The results suggest caution in generalizations made by researchers and practitioners regarding the effects of a SAT on examinees. (PsycINFO Database Record (c) 2019 APA, all rights reserved)
KW  - self adapted &/vs computerized adaptive vs conventional version of English vocabulary test
KW  - estimated ability & posttest anxiety & correct responses & testing time
KW  - high school students
KW  - Spain
KW  - Ability Level
KW  - Adaptive Testing
KW  - Test Administration
KW  - Test Anxiety
KW  - Computerized Assessment
KW  - High School Students
KW  - Test Scores
KW  - Vocabulary
DO  - 10.1177/0013164497057002002
UR  - https://search.ebscohost.com/login.aspx?direct=true&db=psyh&AN=1997-03451-002&lang=de&site=ehost-live
UR  - ORCID: 0000-0003-4705-6282
DP  - EBSCOhost
DB  - APA PsycInfo
ER  - 

TY  - THES
ID  - 2007-99220-063
AN  - 2007-99220-063
AU  - Johnson, Marc Anthony
T1  - An investigation of stratification exposure control procedures in CATS using the generalized partial credit model
JF  - Dissertation Abstracts International: Section B: The Sciences and Engineering
JO  - Dissertation Abstracts International: Section B: The Sciences and Engineering
Y1  - 2007///
VL  - 68
IS  - 5-B
SP  - 3444
EP  - 3444
PB  - ProQuest Information & Learning
SN  - 0419-4217
N1  - Accession Number: 2007-99220-063. Other Journal Title: Dissertation Abstracts International. Partial author list: First Author & Affiliation: Johnson, Marc Anthony; U Texas At Austin, US. Release Date: 20080107. Correction Date: 20190211. Publication Type: Dissertation Abstract (0400). Format Covered: Electronic. Document Type: Dissertation. Dissertation Number: AAI3266891. Language: EnglishMajor Descriptor: Discrimination; Psychometrics; Computerized Assessment. Classification: Psychometrics & Statistics & Methodology (2200). Population: Human (10). Methodology: Empirical Study; Quantitative Study. Page Count: 1. 
AB  - The a-stratification procedure of item exposure control was designed to stratify items by item discrimination to ensure that an adaptive test would administer items from the entire range of items, not just the most-informative ones. An improvement to the a-stratification method, the a-stratification with b-blocking procedure added stratification according to item difficulty in order to take into account any correlation that might exist within the item pool between item discrimination and item difficulty. These procedures have been shown to work well using dichotomous items. This dissertation explored both stratification procedures using polytomous item pools to investigate whether or not an optimum number of strata could be implemented when administering polytomous computerized adaptive tests. In addition to the stratification procedures, two other exposure control conditions were studied. The randomesque procedure was used in one condition while a no exposure control condition served as a baseline condition. Items calibrated according to the generalized partial credit model were used to construct two item pools. Since the items covered three areas of science, content balancing procedures were incorporated to ensure that each adaptive test provided the appropriate balance of content. Maximum likelihood estimation was used to estimate ability levels from simulated CATs. The number of strata used with both stratification procedures ranged from two to five, to ensure enough items per stratum. Along with descriptive statistics and correlations, bias and root mean squared error helped portray the accuracy of the simulated tests. Item exposure and item pool usage rates were used to show how much of the item pools were being used across administrations of the tests. Finally, item overlap rates were calculated to show how many of the same items were being used among simulated examinees of similar and different abilities. The results of this study did not reveal an optimum number of strata for the stratification procedures with either item pool. Furthermore, the randomesque procedure outperformed the stratification procedures in terms of item exposure and item overlap rates for both item pools. This surprising result was not affected by the number of strata used within the stratification procedures. (PsycINFO Database Record (c) 2019 APA, all rights reserved)
KW  - stratification exposure
KW  - CATS
KW  - generalized partial credit model
KW  - item discrimination
KW  - Discrimination
KW  - Psychometrics
KW  - Computerized Assessment
UR  - https://search.ebscohost.com/login.aspx?direct=true&db=psyh&AN=2007-99220-063&lang=de&site=ehost-live
DP  - EBSCOhost
DB  - APA PsycInfo
ER  - 

TY  - THES
ID  - 2014-99210-510
AN  - 2014-99210-510
AU  - Wang, Xinrui
T1  - An investigation on computer-adaptive multistage testing panels for multidimensional assessment
JF  - Dissertation Abstracts International Section A: Humanities and Social Sciences
JO  - Dissertation Abstracts International Section A: Humanities and Social Sciences
Y1  - 2014///
VL  - 75
IS  - 5-A(E)
PB  - ProQuest Information & Learning
SN  - 0419-4209
SN  - 978-1-303-68583-5
N1  - Accession Number: 2014-99210-510. Other Journal Title: Dissertation Abstracts International. Partial author list: First Author & Affiliation: Wang, Xinrui; U North Carolina at Greensboro, US. Release Date: 20141124. Correction Date: 20221128. Publication Type: Dissertation Abstract (0400). Format Covered: Electronic. Document Type: Dissertation. Dissertation Number: AAI3609605. ISBN: 978-1-303-68583-5. Language: EnglishMajor Descriptor: Adaptive Testing; Test Construction. Minor Descriptor: Computers; Transection; Computerized Assessment. Classification: Educational & School Psychology (3500). Population: Human (10). Age Group: Adulthood (18 yrs & older) (300). Methodology: Empirical Study; Quantitative Study. 
AB  - The computer-adaptive multistage testing (ca-MST) has been developed as an alternative to computerized adaptive testing (CAT), and been increasingly adopted in large-scale assessments. Current research and practice only focus on ca-MST panels for credentialing purposes. The ca-MST test mode, therefore, is designed to gauge a single scale. The present study is the first step to investigate ca-MST panels for diagnostic purposes, which involve the assessment of multiple attributes in the same test. This study employed computer simulation to compare multidimensional ca-MST panels and their unidimensional counterparts, and to explore the factors that affect the accuracy and efficiency of multidimensional ca-MST. Nine multidimensional ca-MST panel designs—which differed in configuration and test length—were simulated under varied attribute correlation scenarios. In addition, item pools with different qualities were studied to suggest appropriate item bank design. The comparison between the multidimensional ca-MST and a sequential of unidimensional ca-MST suggested that when attributes correlated moderate to high, employing a multidimensional ca-MST provided more accurate and efficient scoring decisions than several unidimensional ca-MST with IRT scoring. However, a multidimensional ca-MST did not perform better than its unidimensional counterpart with MIRT scoring. Nevertheless, multidimensional panels are still promising for diagnostic purposes given practical considerations. The investigation on multidimensional ca-MST design indicated the following: Higher attribute correlation was associated with better scoring decision because more information carried by a correlation matrix was available for estimation. This held true across all item pool conditions. An optimal item pool would be the one that was discriminative, appropriately located and specifically designed for a configuration. The accuracy and efficiency of a multidimensional ca-MST panel would be diminished if its item pool was too easy, or not informative. According to the results, the 1-2-3 configuration design was most promising. In terms of test length, an appropriate decision would largely depend on the attribute correlation and the item pool characteristics. (PsycInfo Database Record (c) 2022 APA, all rights reserved)
KW  - correlated moderate
KW  - practical considerations
KW  - Adaptive Testing
KW  - Test Construction
KW  - Computers
KW  - Transection
KW  - Computerized Assessment
UR  - https://search.ebscohost.com/login.aspx?direct=true&db=psyh&AN=2014-99210-510&lang=de&site=ehost-live
DP  - EBSCOhost
DB  - APA PsycInfo
ER  - 

TY  - JOUR
ID  - 2010-07212-018
AN  - 2010-07212-018
AU  - Irwin, Debra E.
AU  - Stucky, Brian
AU  - Langer, Michelle M.
AU  - Thissen, David
AU  - DeWitt, Esi Morgan
AU  - Lai, Jin-Shei
AU  - Varni, James W.
AU  - Yeatts, Karin
AU  - DeWalt, Darren A.
T1  - An item response analysis of the pediatric PROMIS anxiety and depressive symptoms scales
JF  - Quality of Life Research: An International Journal of Quality of Life Aspects of Treatment, Care & Rehabilitation
JO  - Quality of Life Research: An International Journal of Quality of Life Aspects of Treatment, Care & Rehabilitation
JA  - Qual Life Res
Y1  - 2010/05//
VL  - 19
IS  - 4
SP  - 595
EP  - 607
PB  - Springer
SN  - 0962-9343
SN  - 1573-2649
AD  - Irwin, Debra E., Department of Epidemiology, University of North Carolina at Chapel Hill, Chapel Hill, NC, US, 27599
N1  - Accession Number: 2010-07212-018. PMID: 20213516 Partial author list: First Author & Affiliation: Irwin, Debra E.; Department of Epidemiology, University of North Carolina at Chapel Hill, Chapel Hill, NC, US. Release Date: 20100607. Publication Type: Journal (0100), Peer Reviewed Journal (0110). Format Covered: Electronic. Document Type: Journal Article. Language: EnglishMajor Descriptor: Anxiety; Client Attitudes; Major Depression; Psychometrics; Treatment Outcomes. Minor Descriptor: Pediatrics. Classification: Clinical Psychological Testing (2224); Psychological Disorders (3210). Population: Human (10); Male (30); Female (40); Outpatient (60). Location: US. Age Group: Childhood (birth-12 yrs) (100); School Age (6-12 yrs) (180); Adolescence (13-17 yrs) (200). Tests & Measures: Pediatric Quality of Life Inventory. Methodology: Empirical Study; Quantitative Study. References Available: Y. Page Count: 13. Issue Publication Date: May, 2010. Publication History: First Posted Date: Mar 7, 2010; Accepted Date: Feb 15, 2010. Copyright Statement: Springer Science+Business Media B.V. 2010. 
AB  - Purpose: The Patient-Reported Outcomes Measurement Information System (PROMIS) aims to develop self-reported item banks for clinical research. The PROMIS pediatrics (aged 8–17) project focuses on the development of item banks across several health domains (physical function, pain, fatigue, emotional distress, social role relationships, and asthma symptoms). The psychometric properties of the anxiety and depressive symptom item banks are described. Methods: Participants (n = 1,529) were recruited in public school settings, hospital-based outpatient and subspecialty pediatrics clinics. The anxiety (k = 18) and depressive symptoms (k = 21) items were split between two test administration forms. Hierarchical confirmatory factor-analytic models (CFA) were conducted to evaluate scale dimensionality and local dependence. IRT analyses were then used to finalize item banks and short forms. Results: CFA results confirmed that anxiety and depressive symptoms are separate constructs and indicative of negative affect. Items with local dependence and DIF were removed resulting in 15 anxiety and 14 depressive symptoms items. The psychometric differences between short forms and simulated computer adaptive tests are presented. Conclusions: PROMIS pediatric item banks were developed to provide efficient assessment of health-related quality of life domains. This sample provides initial calibrations of anxiety and depressive symptoms item banks and creates PROMIS pediatric instruments, version 1.0. (PsycINFO Database Record (c) 2016 APA, all rights reserved)
KW  - Patient-Reported Outcomes Measurement Information System
KW  - anxiety symptoms
KW  - depression symptoms
KW  - psychometrics
KW  - pediatrics
KW  - Adaptation, Psychological
KW  - Adolescent
KW  - Anxiety
KW  - Child
KW  - Depression
KW  - Female
KW  - Humans
KW  - Male
KW  - North Carolina
KW  - Outpatients
KW  - Pediatrics
KW  - Psychometrics
KW  - Self Concept
KW  - Stress, Psychological
KW  - Texas
KW  - Anxiety
KW  - Client Attitudes
KW  - Major Depression
KW  - Psychometrics
KW  - Treatment Outcomes
KW  - Pediatrics
DO  - 10.1007/s11136-010-9619-3
UR  - https://search.ebscohost.com/login.aspx?direct=true&db=psyh&AN=2010-07212-018&lang=de&site=ehost-live
UR  - dirwin@email.unc.edu
DP  - EBSCOhost
DB  - APA PsycInfo
ER  - 

TY  - JOUR
ID  - 2018-22203-001
AN  - 2018-22203-001
AU  - Beleckas, Casey M.
AU  - Prather, Heidi
AU  - Guattery, Jason
AU  - Wright, Melissa
AU  - Kelly, Michael
AU  - Calfee, Ryan P.
T1  - Anxiety in the orthopedic patient: Using PROMIS to assess mental health
JF  - Quality of Life Research: An International Journal of Quality of Life Aspects of Treatment, Care & Rehabilitation
JO  - Quality of Life Research: An International Journal of Quality of Life Aspects of Treatment, Care & Rehabilitation
JA  - Qual Life Res
Y1  - 2018/09//
VL  - 27
IS  - 9
SP  - 2275
EP  - 2282
PB  - Springer
SN  - 0962-9343
SN  - 1573-2649
AD  - Calfee, Ryan P.
N1  - Accession Number: 2018-22203-001. PMID: 29740783 Partial author list: First Author & Affiliation: Beleckas, Casey M.; Washington University School of Medicine, St. Louis, MO, US. Release Date: 20180510. Correction Date: 20221128. Publication Type: Journal (0100), Peer Reviewed Journal (0110). Format Covered: Electronic. Document Type: Journal Article. Language: EnglishMajor Descriptor: Anxiety; Mental Health; Physical Disorders; Treatment Effectiveness Evaluation. Classification: Clinical Psychological Testing (2224); Physical & Somatic Disorders (3290). Population: Human (10); Male (30); Female (40); Outpatient (60). Location: US. Age Group: Adulthood (18 yrs & older) (300). Tests & Measures: Generalized Anxiety Disorder 7 DOI: 10.1037/t02591-000; Patient-Reported Outcomes Measurement Information System DOI: 10.1037/t06780-000. Methodology: Empirical Study; Quantitative Study. References Available: Y. Page Count: 8. Issue Publication Date: Sep, 2018. Publication History: First Posted Date: May 8, 2018; Accepted Date: Apr 26, 2018. Copyright Statement: Springer International Publishing AG, part of Springer Nature. 2018. 
AB  - Purpose: This study explored the performance of the Patient-Reported Outcomes Measurement Information System (PROMIS) Anxiety assessment relative to the Depression assessment in orthopedic patients, the relationship between Anxiety with self-reported Physical Function and Pain Interference, and to determine if Anxiety levels varied according to the location of orthopedic conditions. Methods: This cross-sectional evaluation analyzed 14,962 consecutive adult new-patient visits to a tertiary orthopedic practice between 4/1/2016 and 12/31/2016. All patients completed PROMIS Anxiety, Depression, Physical Function, and Pain Interference computer adaptive tests (CATs) as routine clinical intake. Patients were grouped by the orthopedic service providing care and categorized as either affected with Anxiety if scoring > 62 based on linkage to the Generalized Anxiety Disorder-7 survey. Spearman correlations between the PROMIS scores were calculated. Bivariate statistics assessed differences in Anxiety and Depression scores between patients of different orthopedic services. Results: 20% of patients scored above the threshold to be considered affected by Anxiety. PROMIS Anxiety scores demonstrated a stronger correlation than Depression scores with Physical Function and Pain Interference scores. Patients with spine conditions reported the highest median Anxiety scores and were more likely to exceed the Anxiety threshold than patients presenting to sports or upper extremity surgeons. Conclusions: One in five new orthopedic patients reports Anxiety levels that may warrant intervention. This rate is heightened in patients needing spine care. Patient-reported Physical Function more strongly correlates with PROMIS Anxiety than Depression suggesting that the Anxiety CAT is a valuable addition to assess mental health among orthopedic patients. Level of Evidence: Diagnostic level III. (PsycInfo Database Record (c) 2022 APA, all rights reserved)
KW  - Anxiety
KW  - Mental health
KW  - Orthopedic
KW  - Patient-reported
KW  - Depression
KW  - Anxiety
KW  - Mental Health
KW  - Physical Disorders
KW  - Treatment Effectiveness Evaluation
U1  - Sponsor: National Institutes of Health, National Center for Advancing Translational Sciences (NCATS), US. Grant: UL1TR000448; TL1TR000449. Other Details: Washington University Institute of Clinical and Translational Sciences. Recipients: No recipient indicated
U1  - Sponsor: Siteman Comprehensive Cancer Center. Recipients: No recipient indicated
U1  - Sponsor: National Cancer Institute, US. Grant: P30 CA091842. Other Details: Cancer Center Support Grant. Recipients: No recipient indicated
DO  - 10.1007/s11136-018-1867-7
UR  - https://search.ebscohost.com/login.aspx?direct=true&db=psyh&AN=2018-22203-001&lang=de&site=ehost-live
UR  - ORCID: 0000-0003-1875-9018
UR  - calfeer@wudosis.wustl.edu
DP  - EBSCOhost
DB  - APA PsycInfo
ER  - 

TY  - JOUR
ID  - 2019-28166-001
AN  - 2019-28166-001
AU  - Lee, Yi-Lien
AU  - Lin, Kao-Chang
AU  - Chien, Tsair-Wei
T1  - Application of a multidimensional computerized adaptive test for a Clinical Dementia Rating Scale through computer-aided techniques
JF  - Annals of General Psychiatry
JO  - Annals of General Psychiatry
JA  - Ann Gen Psychiatry
Y1  - 2019/05/17/
VL  - 18
PB  - BioMed Central Limited
SN  - 1744-859X
AD  - Chien, Tsair-Wei, Department of Medical Research, Chi-Mei Medical Center, 901 Chung Hwa Road, Yung Kung Dist, Tainan, Taiwan, 710
N1  - Accession Number: 2019-28166-001. PMID: 31131014 Other Journal Title: Annals of General Hospital Psychiatry. Partial author list: First Author & Affiliation: Lee, Yi-Lien; Department of Medical Affairs, Chi-Mei Medical Center, Tainan, Taiwan. Release Date: 20200514. Correction Date: 20220714. Publication Type: Journal (0100), Peer Reviewed Journal (0110). Format Covered: Electronic. Document Type: Journal Article. Language: EnglishMajor Descriptor: Adaptive Testing; Dementia; Psychometrics; Test Administration. Minor Descriptor: Computers; Family Members; Microcomputers; Test Construction. Classification: Neuropsychological Assessment (2225); Neurological Disorders & Brain Damage (3297). Population: Human (10); Male (30); Female (40); Outpatient (60). Location: Taiwan. Age Group: Adulthood (18 yrs & older) (300); Middle Age (40-64 yrs) (360); Aged (65 yrs & older) (380); Very Old (85 yrs & older) (390). Tests & Measures: Clinical Dementia Rating Scale. Methodology: Empirical Study; Quantitative Study. Supplemental Data: Experimental Materials Internet; Tables and Figures Internet. References Available: Y. ArtID: 5. Issue Publication Date: May 17, 2019. Publication History: First Posted Date: May 17, 2019; Accepted Date: Apr 29, 2019; First Submitted Date: Oct 9, 2017. Copyright Statement: This article is distributed under the terms of the Creative Commons Attribution 4.0 International License (http://creativecommons.org/licenses/by/4.0/), which permits unrestricted use, distribution, and reproduction in any medium, provided you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons license, and indicate if changes were made. The Creative Commons Public Domain Dedication waiver (http://creativecommons.org/publicdomain/zero/1.0/) applies to the data made available in this article, unless otherwise stated. The Author(s). 2019. 
AB  - Background: With the increasingly rapid growth of the elderly population, individuals aged 65 years and above now compose 14% of Taiwanese citizens, thereby making Taiwanese society an aged society. A leading factor that affects the elderly population is dementia. A method of precisely and efficiently examining patients with dementia through multidimensional computer adaptive testing (MCAT) to accurately determine the patients’ stage of dementia needs to be developed. This study aimed to develop online MCAT that family members can use on their own computers, tablets, or smartphones to predict the extent of dementia for patients responding to the Clinical Dementia Rating (CDR) instrument. Methods: The CDR was applied to 366 outpatients in a hospital in Taiwan. MCAT was employed with parameters for items across eight dimensions, and responses were simulated to compare the efficiency and precision between MCAT and non-adaptive testing (NAT). The number of items saved and the estimated person measures was compared between the results of MCAT and NAT, respectively. Results: MCAT yielded substantially more precise measurements and was considerably more efficient than NAT. MCAT achieved 20.19% (= [53 − 42.3]/53) saving in item length when the measurement differences were less than 5%. Pearson correlation coefficients were highly consistent among the eight domains. The cut-off points for the overall measures were − 1.4, − 0.4, 0.4, and 1.4 logits, which was equivalent to 20% for each portion in percentile scores. Substantially fewer items were answered through MCAT than through NAT without compromising the precision of MCAT. Conclusions: Developing a website that family members can use on their own computers, tablets, and smartphones to help them perform online screening and prediction of dementia in older adults is useful and manageable. (PsycInfo Database Record (c) 2022 APA, all rights reserved)
KW  - Dementia
KW  - Clinical Dementia Rating
KW  - multidimensional Computer adaptive testing
KW  - Cut-off point
KW  - test development
KW  - Adaptive Testing
KW  - Dementia
KW  - Psychometrics
KW  - Test Administration
KW  - Computers
KW  - Family Members
KW  - Microcomputers
KW  - Test Construction
DO  - 10.1186/s12991-019-0228-4
UR  - https://search.ebscohost.com/login.aspx?direct=true&db=psyh&AN=2019-28166-001&lang=de&site=ehost-live
UR  - ORCID: 0000-0003-1329-0679
UR  - smile@mail.chimei.org.tw
DP  - EBSCOhost
DB  - APA PsycInfo
ER  - 

TY  - BOOK
ID  - 2020-10240-000
AN  - 2020-10240-000
AU  - Jiao, Hong
AU  - Lissitz, Robert W.
ED  - Jiao, Hong
ED  - Lissitz, Robert W.
T1  - Application of artificial intelligence to assessment
T3  - MARCES book series
Y1  - 2020///
CY  - Waxhaw, NC
PB  - Information Age Publishing, Inc.
SN  - 978-1-64113-951-9
SN  - 978-1-64113-952-6
SN  - 978-1-64113-953-3
N1  - Accession Number: 2020-10240-000. Partial author list: First Author & Affiliation: Jiao, Hong; University of Maryland, MD, US. Release Date: 20210621. Correction Date: 20221128. Publication Type: Book (0200), Edited Book (0280). Format Covered: Print. Document Type: Book. ISBN: 978-1-64113-951-9, ISBN Paperback; 978-1-64113-952-6, ISBN Hardcover; 978-1-64113-953-3, ISBN Digital (undefined format). Language: EnglishMajor Descriptor: Adaptive Testing; Artificial Intelligence; Automation; Scoring (Testing). Minor Descriptor: Classical Test Theory; Item Response Theory; Machine Learning; Psychometrics; Test Construction. Classification: Tests & Testing (2220); Artificial Intelligence & Expert Systems (4120). Intended Audience: Psychology: Professional & Research (PS). Page Count: 209. 
AB  - This book presents the applications of artificial intelligence (AI) in test development. In particular, it includes research and successful examples of using AI technology in automated item generation, automated test assembly, automated scoring, and computerized adaptive testing (CAT). By utilizing artificial intelligence, the efficiency of item development, test form construction, test delivery, and scoring could be dramatically increased. Chapters on automated item generation offer different perspectives related to generating a large number of items with controlled psychometric properties including the latest development of using machine learning methods. The book illustrates automated scoring for different types of assessments such as speaking and writing from both methodological aspects and practical considerations. Further, it elaborates automated test assembly for the conventional linear tests from both classical test theory and item response theory perspectives. Item pool design and assembly for the linear-on-the-fly tests elaborates more complications in practice when test security is a big concern. Finally, several chapters focus on CAT at either item or module levels. CAT is further illustrated as an effective approach to increasing test-takers' engagement in testing. In summary, the book includes both theoretical, methodological, and applied research and practices that serve as the foundation for future development. These chapters provide illustrations of efforts to automate the process of test development. While some of these automation processes have become common practices such as automated test assembly, automated scoring, and computerized adaptive testing, some others such as automated item generation calls for more research and exploration. When new AI methods are emerging and evolving, it is expected that researchers can expand and improve the methods for automating different steps in test development to enhance the automation features and practitioners can adopt quality automation procedures to improve assessment practices. (PsycInfo Database Record (c) 2022 APA, all rights reserved)
KW  - automated item generation
KW  - automated scoring
KW  - automated test assembly
KW  - classical test theory
KW  - computerized adaptive testing
KW  - item response theory
KW  - machine learning
KW  - psychometric properties
KW  - test development
KW  - artificial intelligence
KW  - Adaptive Testing
KW  - Artificial Intelligence
KW  - Automation
KW  - Scoring (Testing)
KW  - Classical Test Theory
KW  - Item Response Theory
KW  - Machine Learning
KW  - Psychometrics
KW  - Test Construction
UR  - https://search.ebscohost.com/login.aspx?direct=true&db=psyh&AN=2020-10240-000&lang=de&site=ehost-live
DP  - EBSCOhost
DB  - APA PsycInfo
ER  - 

TY  - JOUR
ID  - 2019-48651-001
AN  - 2019-48651-001
AU  - Mao, Xiuzhen
AU  - Zhang, Jiahui
AU  - Xin, Tao
T1  - Application of dimension reduction to CAT item selection under the bifactor model
JF  - Applied Psychological Measurement
JO  - Applied Psychological Measurement
JA  - Appl Psychol Meas
Y1  - 2019/09//
VL  - 43
IS  - 6
SP  - 419
EP  - 434
PB  - Sage Publications
SN  - 0146-6216
SN  - 1552-3497
AD  - Zhang, Jiahui, Michigan State University, 233 Erickson Hall, East Lansing, MI, US, 48824-1034
N1  - Accession Number: 2019-48651-001. PMID: 31452552 Partial author list: First Author & Affiliation: Mao, Xiuzhen; Sichuan Normal University, Chengdu, China. Release Date: 20190905. Publication Type: Journal (0100), Peer Reviewed Journal (0110). Format Covered: Electronic. Document Type: Journal Article. Language: EnglishGrant Information: Mao, Xiuzhen. Major Descriptor: Adaptive Testing; Estimation; Measurement; Statistical Probability. Minor Descriptor: Item Response Theory; Test Items. Classification: Statistics & Mathematics (2240). Methodology: Empirical Study; Mathematical Model; Quantitative Study. Supplemental Data: Appendixes Internet; Other Internet. References Available: Y. Page Count: 16. Issue Publication Date: Sep, 2019. Copyright Statement: SAGE Publications. 2018. 
AB  - Multidimensional computerized adaptive testing (MCAT) based on the bifactor model is suitable for tests with multidimensional bifactor measurement structures. Several item selection methods that proved to be more advantageous than the maximum Fisher information method are not practical for bifactor MCAT due to time-consuming computations resulting from high dimensionality. To make them applicable in bifactor MCAT, dimension reduction is applied to four item selection methods, which are the posterior-weighted Fisher D-optimality (PDO) and three non-Fisher information-based methods—posterior expected Kullback–Leibler information (PKL), continuous entropy (CE), and mutual information (MI). They were compared with the Bayesian D-optimality (BDO) method in terms of estimation precision. When both the general and group factors are the measurement objectives, BDO, PDO, CE, and MI perform equally well and better than PKL. When the group factors represent nuisance dimensions, MI and CE perform the best in estimating the general factor, followed by the BDO, PDO, and PKL. How the bifactor pattern and test length affect estimation accuracy was also discussed. (PsycINFO Database Record (c) 2019 APA, all rights reserved)
KW  - multidimensional item response theory
KW  - bifactor model
KW  - multidimensional computerized adaptive testing
KW  - item selection method
KW  - measurement precision
KW  - Adaptive Testing
KW  - Estimation
KW  - Measurement
KW  - Statistical Probability
KW  - Item Response Theory
KW  - Test Items
U1  - Sponsor: National Natural Science Foundation of China, China. Grant: 31400897. Recipients: Mao, Xiuzhen
DO  - 10.1177/0146621618813086
UR  - https://search.ebscohost.com/login.aspx?direct=true&db=psyh&AN=2019-48651-001&lang=de&site=ehost-live
UR  - nellykimzhang@gmail.com
DP  - EBSCOhost
DB  - APA PsycInfo
ER  - 

TY  - JOUR
ID  - 2012-00303-005
AN  - 2012-00303-005
AU  - Riley, William T.
AU  - Pilkonis, Paul
AU  - Cella, David
T1  - Application of the National Institutes of Health Patient-Reported Outcome Measurement Information System (PROMIS) to mental health research
JF  - Journal of Mental Health Policy and Economics
JO  - Journal of Mental Health Policy and Economics
JA  - J Ment Health Policy Econ
Y1  - 2011/12//
VL  - 14
IS  - 4
SP  - 201
EP  - 208
PB  - ICMPE
SN  - 1091-4358
SN  - 1099-176X
AD  - Riley, William T., National Heart, Lung, and Blood Institute, 6701 Rockledge Dr., MSC 7936, Bethesda, MD, US, 20892
N1  - Accession Number: 2012-00303-005. PMID: 22345362 Partial author list: First Author & Affiliation: Riley, William T.; National Heart, Lung, and Blood Institute, National Institutes of Health, Bethesda, MD, US. Other Publishers: Wiley-Blackwell Publishing Ltd. Release Date: 20120213. Correction Date: 20190211. Publication Type: Journal (0100), Peer Reviewed Journal (0110). Format Covered: Print. Document Type: Journal Article. Language: EnglishMajor Descriptor: Mental Health Services; Self-Report; Treatment Effectiveness Evaluation; Treatment Outcomes; Computerized Assessment. Minor Descriptor: Experimentation; Client Records; Patient Reported Outcome Measures. Classification: Health & Mental Health Services (3370). Population: Human (10). Tests & Measures: Hamilton Rating Scale for Depression DOI: 10.1037/t04100-000; Patient Health Questionnaire-9 DOI: 10.1037/t06165-000. References Available: Y. Page Count: 8. Issue Publication Date: Dec, 2011. Publication History: Accepted Date: Nov 28, 2011; First Submitted Date: Nov 9, 2011. Copyright Statement: ICMPE. 2011. 
AB  - Background: The Patient-Reported Outcomes Measurement Information System (PROMIS®) is a National Institutes of Health initiative to develop item banks measuring patient-reported outcomes (PROs) and to create and make available a computerized adaptive testing system (CAT) that allows for efficient and precise assessment of PROs in clinical research and practice. Aims of the Study: This paper provides an overview of PROMIS and its application to mental health research. Methods: The PROMIS methodology for item bank development and testing is described, with a focus on the implications of this work for mental health research. Results: Utilizing qualitative item review and state-of-the-art applications of item response theory (IRT), PROMIS investigators have developed, tested, and released item banks measuring physical, mental, and social health components. Ongoing efforts continue to add new item banks and further validate existing banks. Discussion: PROMIS provides item banks measuring several domains of interest to mental health researchers including emotional distress, social function, and sleep. PROMIS methodology also provides a rigorous standard for the development of new mental health measures. Implications for Health Care Provision: Web-based CAT or administration of short forms derived from PROMIS item banks provide efficient and precise dimensional estimates of clinical outcomes that can be utilized to monitor patient progress and assess quality improvement. Implications for Future Research: Use of the dimensional PROMIS metrics (and co-calibration of the PROMIS item banks with existing PROs) will allow comparisons of mental health and related health outcomes across disorders and studies. (PsycINFO Database Record (c) 2019 APA, all rights reserved)
KW  - Patient–Reported Outcome Measurement Information System
KW  - mental health services
KW  - mental health research
KW  - Adolescent
KW  - Adult
KW  - Child
KW  - Data Collection
KW  - Diagnostic Self Evaluation
KW  - Humans
KW  - Information Systems
KW  - Mental Disorders
KW  - National Institutes of Health (U.S.)
KW  - Outcome Assessment (Health Care)
KW  - Psychometrics
KW  - United States
KW  - Mental Health Services
KW  - Self-Report
KW  - Treatment Effectiveness Evaluation
KW  - Treatment Outcomes
KW  - Computerized Assessment
KW  - Experimentation
KW  - Client Records
KW  - Patient Reported Outcome Measures
UR  - https://search.ebscohost.com/login.aspx?direct=true&db=psyh&AN=2012-00303-005&lang=de&site=ehost-live
UR  - winley@mail.nih.gov
DP  - EBSCOhost
DB  - APA PsycInfo
ER  - 

TY  - JOUR
ID  - 2018-47935-001
AN  - 2018-47935-001
AU  - van Bebber, Jan
AU  - Flens, Gerard
AU  - Wigman, Johanna T. W.
AU  - de Beurs, Edwin
AU  - Sytema, Sjoerd
AU  - Wunderink, Lex
AU  - Meijer, Rob R.
T1  - Application of the Patient‐Reported Outcomes Measurement Information System (PROMIS) item parameters for anxiety and depression in the Netherlands
JF  - International Journal of Methods in Psychiatric Research
JO  - International Journal of Methods in Psychiatric Research
JA  - Int J Methods Psychiatr Res
Y1  - 2018/12//
VL  - 27
IS  - 4
SP  - 1
EP  - 8
PB  - John Wiley & Sons
SN  - 1049-8931
SN  - 1557-0657
AD  - van Bebber, Jan, ICPE (UMCG), PO Box 30001 (internal CC72), NL 9700RB, Groningen, Netherlands
N1  - Accession Number: 2018-47935-001. Partial author list: First Author & Affiliation: van Bebber, Jan; Interdisciplinary Ctr Psychopathol and Emot Regulat, University Medical Center Groningen, University of Groningen, Groningen, Netherlands. Release Date: 20180927. Correction Date: 20200227. Publication Type: Journal (0100), Peer Reviewed Journal (0110). Format Covered: Electronic. Document Type: Journal Article. Language: EnglishGrant Information: Wigman, Johanna T. W. Major Descriptor: Adaptive Testing; Anxiety; Major Depression; Psychometrics. Minor Descriptor: Item Analysis (Test). Classification: Clinical Psychological Testing (2224); Psychological Disorders (3210). Population: Human (10). Location: Netherlands; US. Tests & Measures: Patient-Reported Outcomes Measurement Information System DOI: 10.1037/t06780-000. Methodology: Empirical Study; Quantitative Study. Supplemental Data: Tables and Figures Internet. Page Count: 8. Issue Publication Date: Dec, 2018. Publication History: Accepted Date: Aug 15, 2018; Revised Date: Jun 22, 2018; First Submitted Date: Dec 11, 2017. Copyright Statement: John Wiley & Sons, Ltd. 2018. 
AB  - Objectives: The Patient‐Reported Outcomes Measurement Information System (PROMIS) Health Organization has compiled and calibrated item banks for various domains in the United States, and these item banks have been translated into Dutch language. Methods: The item banks for Anxiety and Depression have been administered in two samples, one drawn from the Dutch general and one drawn from the Dutch clinical population. The aim of this study was to investigate the appropriateness of the official PROMIS item parameters for these item banks that have been estimated based on data collected in the United States for use in the Netherlands. For both domains, we determined the fit of U.S. item parameters, the effect on individual domain scores and levels, the effect on correlations with full item bank totals, and the effect on classification accuracies of adaptive test scores for diagnoses of anxiety and mood disorders. Results: The results showed that especially in the clinical population sample, fit appeared to be problematic for many items. However, simulations revealed that both sets of item parameters (official PROMIS vs. unique Dutch) perform nearly equally well in practice. Conclusion: We tentatively conclude that the official PROMIS item parameters can be used for scaling respondents in the Netherlands. (PsycINFO Database Record (c) 2020 APA, all rights reserved)
KW  - anxiety
KW  - computerized adaptive tests
KW  - depression
KW  - Patient‐Reported Outcomes Measurement Information System
KW  - Real Data Simulation
KW  - Adaptive Testing
KW  - Anxiety
KW  - Major Depression
KW  - Psychometrics
KW  - Item Analysis (Test)
U1  - Sponsor: Nederlandse Organisatie voor Wetenschappelijk Onderzoek, Netherlands. Grant: 016.156.019. Recipients: No recipient indicated
U1  - Sponsor: Veni. Grant: 016.156.019. Recipients: Wigman, Johanna T. W.
U1  - Sponsor: Mental Health Care Center Friesland, Netherlands. Recipients: No recipient indicated
DO  - 10.1002/mpr.1744
UR  - https://search.ebscohost.com/login.aspx?direct=true&db=psyh&AN=2018-47935-001&lang=de&site=ehost-live
UR  - ORCID: 0000-0003-3832-8477
UR  - ORCID: 0000-0001-9453-1862
UR  - j.van.bebber@umcg.nl
DP  - EBSCOhost
DB  - APA PsycInfo
ER  - 

TY  - JOUR
ID  - 2021-80584-001
AN  - 2021-80584-001
AU  - Wang, Wenhao
AU  - Kingston, Neal M.
AU  - Davis, Marcia H.
AU  - Tiemann, Gail C.
AU  - Tonks, Stephen
AU  - Hock, Michael
T1  - Applying evidence‐centered design in the development of a multidimensional adaptive reading motivation measure
JF  - Educational Measurement: Issues and Practice
JO  - Educational Measurement: Issues and Practice
Y1  - 2021///Win 2021
VL  - 40
IS  - 4
SP  - 91
EP  - 100
PB  - Wiley-Blackwell Publishing Ltd.
SN  - 0731-1745
SN  - 1745-3992
N1  - Accession Number: 2021-80584-001. Partial author list: First Author & Affiliation: Wang, Wenhao; University of Kansas, Lawrence, KS, US. Other Publishers: Blackwell Publishing. Release Date: 20210830. Correction Date: 20220124. Publication Type: Journal (0100), Peer Reviewed Journal (0110). Format Covered: Electronic. Document Type: Journal Article. Language: EnglishMajor Descriptor: Adaptive Testing; Educational Measurement; High School Students; Psychometrics; Motivation Measures. Minor Descriptor: Item Response Theory; Reading; School Adjustment; Test Construction. Classification: Educational Measurement (2227); Curriculum & Programs & Teaching Methods (3530). Population: Human (10); Male (30); Female (40). Location: US. Age Group: Childhood (birth-12 yrs) (100); School Age (6-12 yrs) (180); Adolescence (13-17 yrs) (200). Methodology: Empirical Study; Quantitative Study; Scientific Simulation. References Available: Y. Page Count: 10. Issue Publication Date: Win 2021. Publication History: First Posted Date: Sep 15, 2021. Copyright Statement: The National Council on Measurement in Education. 2021. 
AB  - Adaptive tests are more efficient than fixed‐length tests through the use of item response theory; adaptive tests also present students questions that are tailored to their proficiency level. Although the adaptive algorithm is straightforward, developing a multidimensional computer adaptive test (MCAT) measure is complex. Evidence‐centered design (ECD) can provide a highly structured framework to guide the development of a MCAT. In this paper, the development of the adaptive reading motivation measure (ARMM) demonstrates the process of applying ECD to the development of a MCAT measure. This paper focuses on the conceptual assessment framework layer of the ECD that guides the technical aspects of MCAT development. The five models of conceptual assessment framework layer are student model, task model, evidence model, assembly model, and presentation model. How each model guided the development of the ARMM is described in detail. In this paper, we demonstrate how the conceptual assessment framework in ECD can provide a useful structure for developing MCAT measures. (PsycInfo Database Record (c) 2022 APA, all rights reserved)
KW  - conceptual assessment framework
KW  - evidence‐centered design
KW  - multidimensional computer adaptive testing
KW  - Adaptive Testing
KW  - Educational Measurement
KW  - High School Students
KW  - Psychometrics
KW  - Motivation Measures
KW  - Item Response Theory
KW  - Reading
KW  - School Adjustment
KW  - Test Construction
U1  - Sponsor: US Department of Education, Institute for Education Sciences (IES), US. Grant: R305A110148. Other Details: “Development and Validation of Online Adaptive Reading Motivation Measures.”. Recipients: No recipient indicated
DO  - 10.1111/emip.12468
UR  - https://search.ebscohost.com/login.aspx?direct=true&db=psyh&AN=2021-80584-001&lang=de&site=ehost-live
UR  - ORCID: 0000-0001-6698-4338
DP  - EBSCOhost
DB  - APA PsycInfo
ER  - 

TY  - JOUR
ID  - 2008-06905-016
AN  - 2008-06905-016
AU  - Fayers, Peter M.
T1  - Applying item response theory and computer adaptive testing: The challenges for health outcomes assessment
JF  - Quality of Life Research: An International Journal of Quality of Life Aspects of Treatment, Care & Rehabilitation
JO  - Quality of Life Research: An International Journal of Quality of Life Aspects of Treatment, Care & Rehabilitation
JA  - Qual Life Res
Y1  - 2007/08//
VL  - 16
IS  - Suppl1
SP  - 187
EP  - 194
PB  - Springer
SN  - 0962-9343
SN  - 1573-2649
AD  - Fayers, Peter M., Department of Public Health, Institute of Applied Health Sciences, University of Aberdeen Medical School, Polwarth Building, Foresterhill, Aberdeen, United Kingdom, AB25 2ZD
N1  - Accession Number: 2008-06905-016. Partial author list: First Author & Affiliation: Fayers, Peter M.; Faculty of Medicine, Pain and Palliation Research Group, Norwegian University of Science and Technology, Trondheim, Norway. Release Date: 20080609. Correction Date: 20190211. Publication Type: Journal (0100), Peer Reviewed Journal (0110). Format Covered: Electronic. Document Type: Journal Article. Language: EnglishMajor Descriptor: Adaptive Testing; Client Attitudes; Health; Item Response Theory; Health Outcomes. Classification: Statistics & Mathematics (2240); Health & Mental Health Treatment & Prevention (3300). Population: Human (10). References Available: Y. Page Count: 8. Issue Publication Date: Aug, 2007. 
AB  - Objectives: We review the papers presented at the NCI/DIA conference, to identify areas of controversy and uncertainty, and to highlight those aspects of item response theory (IRT) and computer adaptive testing (CAT) that require theoretical or empirical research in order to justify their application to patient reported outcomes (PROs). Background: IRT and CAT offer exciting potential for the development of a new generation of PRO instruments. However, most of the research into these techniques has been in non-healthcare settings, notably in education. Educational tests are very different from PRO instruments, and consequently problematic issues arise when adapting IRT and CAT to healthcare research. Results: Clinical scales differ appreciably from educational tests, and symptoms have characteristics distinctly different from examination questions. This affects the transferring of IRT technology. Particular areas of concern when applying IRT to PROs include inadequate software, difficulties in selecting models and communicating results, insufficient testing of local independence and other assumptions, and a need of guidelines for estimating sample size requirements. Similar concerns apply to differential item functioning (DIF), which is an important application of IRT. Multidimensional IRT is likely to be advantageous only for closely related PRO dimensions. Conclusions: Although IRT and CAT provide appreciable potential benefits, there is a need for circumspection. Not all PRO scales are necessarily appropriate targets for this methodology. Traditional psychometric methods, and especially qualitative methods, continue to have an important role alongside IRT. Research should be funded to address the specific concerns that have been identified. (PsycINFO Database Record (c) 2019 APA, all rights reserved)
KW  - item response theory
KW  - computer adaptive testing
KW  - health outcomes assessment
KW  - patient reported outcomes
KW  - health care research
KW  - Adaptive Testing
KW  - Client Attitudes
KW  - Health
KW  - Item Response Theory
KW  - Health Outcomes
DO  - 10.1007/s11136-007-9197-1
UR  - https://search.ebscohost.com/login.aspx?direct=true&db=psyh&AN=2008-06905-016&lang=de&site=ehost-live
UR  - ORCID: 0000-0003-4778-1513
UR  - p.fayers@abdn.ac.uk
DP  - EBSCOhost
DB  - APA PsycInfo
ER  - 

TY  - JOUR
ID  - 2020-78388-001
AN  - 2020-78388-001
AU  - Obbarius, Alexander
AU  - Ehrenthal, Johannes C.
AU  - Fischer, Felix
AU  - Liegl, Gregor
AU  - Obbarius, Nina
AU  - Sarrar, Lea
AU  - Rose, Matthias
T1  - Applying item response theory to the OPD Structure Questionnaire: Identification of a unidimensional core construct and feasibility of computer adaptive testing
JF  - Journal of Personality Assessment
JO  - Journal of Personality Assessment
JA  - J Pers Assess
Y1  - 2021/09//Sep-Oct, 2021
VL  - 103
IS  - 5
SP  - 645
EP  - 658
PB  - Taylor & Francis
SN  - 0022-3891
SN  - 1532-7752
AD  - Obbarius, Alexander, Department of Psychosomatic Medicine, Charite – Universitatsmedizin Berlin, Hindenburgdamm 30, Berlin, 12203, Germany
N1  - Accession Number: 2020-78388-001. PMID: 33052064 Other Journal Title: Journal of Projective Techniques & Personality Assessment. Partial author list: First Author & Affiliation: Obbarius, Alexander; Department of Psychosomatic Medicine, Center for Internal Medicine and Dermatology, Charite – Universitatsmedizin Berlin, Berlin, Germany. Other Publishers: Lawrence Erlbaum. Release Date: 20201019. Correction Date: 20210920. Publication Type: Journal (0100), Peer Reviewed Journal (0110). Format Covered: Electronic. Document Type: Journal Article. Language: EnglishGrant Information: Obbarius, Alexander. Major Descriptor: Adaptive Testing; Item Response Theory; Personality Disorders; Questionnaires; Exploratory Factor Analysis. Classification: Clinical Psychological Testing (2224); Personality Disorders (3217). Population: Human (10); Male (30); Female (40). Location: Germany. Age Group: Adolescence (13-17 yrs) (200); Adulthood (18 yrs & older) (300); Young Adulthood (18-29 yrs) (320); Thirties (30-39 yrs) (340); Middle Age (40-64 yrs) (360); Aged (65 yrs & older) (380); Very Old (85 yrs & older) (390). Tests & Measures: Structured Clinical Interview for DSM-IV Axis II Disorders; Patient Health Questionnaire-15; Narcissism Inventory 90-Item Version; Computer Adaptive Test; OPD Structure Questionnaire. Methodology: Empirical Study; Interview; Quantitative Study. Supplemental Data: Other Internet. References Available: Y. Page Count: 14. Issue Publication Date: Sep-Oct, 2021. Publication History: Accepted Date: Sep 14, 2020; First Submitted Date: Aug 13, 2019. Copyright Statement: Taylor & Francis Group, LLC. 2020. 
AB  - Recent developments in the dimensional assessment of personality functioning have made the implementation of latent measurement models increasingly attractive. In this study, we applied item response theory (IRT) to a well-established personality functioning instrument (the OPD Structure Questionnaire) to identify a unidimensional latent trait and to evaluate the feasibility of computer adaptive testing (CAT). We hypothesized that the use of IRT could reduce the test burden—compared to a fixed short form—while maintaining high precision over a wide range of the latent trait. The OPD-SQ was collected from 1235 patients in a psychosomatic clinic. IRT assumptions were fulfilled. A 9-factor model yielded sufficient fit and unidimensionality in exploratory factor analysis with bifactor rotation. Items were iteratively reduced, and a graded-response IRT model was fitted to the data. Simulations showed that a CAT with approximately 7 items was able to capture an OPD-SQ global severity score with an accuracy similar to that of a fixed 12-item short form. The final item bank and CAT yielded satisfactory content validity. Strong correlations with depression and anxiety replicated previous results on the OPD-SQ. We concluded that IRT applications could be useful to reduce the test burden of personality functioning instruments. (PsycInfo Database Record (c) 2021 APA, all rights reserved)
KW  - personality disorders
KW  - item response theory
KW  - OPD Structure Questionnaire
KW  - unidimensional core construct
KW  - computer adaptive testing
KW  - exploratory factor analysis
KW  - Adaptive Testing
KW  - Item Response Theory
KW  - Personality Disorders
KW  - Questionnaires
KW  - Exploratory Factor Analysis
U1  - Sponsor: German Research Foundation, Germany. Grant: OB 437/2-1. Other Details: Research fellowship. Recipients: Obbarius, Alexander
DO  - 10.1080/00223891.2020.1828435
UR  - https://search.ebscohost.com/login.aspx?direct=true&db=psyh&AN=2020-78388-001&lang=de&site=ehost-live
UR  - ORCID: 0000-0002-1681-3597
UR  - ORCID: 0000-0001-9073-817X
UR  - ORCID: 0000-0002-9693-6676
UR  - ORCID: 0000-0003-2348-1348
UR  - alexander.obbarius@charite.de
DP  - EBSCOhost
DB  - APA PsycInfo
ER  - 

TY  - BOOK
ID  - 1988-97352-000
AN  - 1988-97352-000
AU  - Drummond, Robert J.
T1  - Appraisal procedures for counselors and helping professionals
Y1  - 1988///
CY  - Columbus, OH
PB  - Merrill Publishing Co
SN  - 0-675-20526-3
N1  - Accession Number: 1988-97352-000. Partial author list: First Author & Affiliation: Drummond, Robert J.; U of North Florida, Jacksonville, FL, US. Release Date: 19880101. Publication Type: Book (0200), Authored Book (0240). Format Covered: Print. Document Type: Book. ISBN: 0-675-20526-3, ISBN Hardcover. Language: EnglishMajor Descriptor: Health Care Delivery; Measurement; Psychometrics. Classification: Tests & Testing (2220). Population: Human (10). Intended Audience: Psychology: Professional & Research (PS). Page Count: 400. 
AB  - The purpose of this text is to help current and future workers in the helping professions become better consumers of psychological and educational tests and assessment procedures.  The book focuses on seven components. [1] 'Models for test use and selection.' The first chapter presents several models to provide a framework for conceptualizing when to use tests and why.  [2] 'Basic competencies needed.' Chapters 2 through 5 identify the basic measurement, statistical, and research skills needed to select, administer, and interpret tests and assessment information. [3] 'Types of tests and assessment procedures.' Chapters 6 through 11 introduce the different types of test and assessment techniques that are commonly used in the helping professions.  [4] 'Test takers and their needs.' Chapters 12 through 15 concern test takers and their needs. These chapters address text anxiety, test-taking skills, assessment of individuals with special needs, and computer-assisted and computer-adaptive testing.  [5] 'Test administrators and their roles.' Chapters 16 through 19 look at the roles and problems of the test administrator. The validity and reliability of test results depend in part upon the skill of the examiner, who must not only administer and score the test but also interpret and disseminate the results. [6] 'Test utilization in different settings.' Chapters 20 to 22 look at how tests and assessment procedures are used in different contexts, specifically education, industry and mental health. [7] 'Current issues and trends.' The last chapter recapitulates some of the major trends and issues introduced in the text and predicts some future directions. (PsycINFO Database Record (c) 2016 APA, all rights reserved)
KW  - Health Care Delivery
KW  - Measurement
KW  - Psychometrics
UR  - https://search.ebscohost.com/login.aspx?direct=true&db=psyh&AN=1988-97352-000&lang=de&site=ehost-live
DP  - EBSCOhost
DB  - APA PsycInfo
ER  - 

TY  - JOUR
ID  - 2008-17448-006
AN  - 2008-17448-006
AU  - Unick, George J.
AU  - Shumway, Martha
AU  - Hargreaves, William
T1  - Are we ready for computerized adaptive testing?
JF  - Psychiatric Services
JO  - Psychiatric Services
JA  - Psychiatr Serv
Y1  - 2008/04//
VL  - 59
IS  - 4
SP  - 369
EP  - 369
PB  - American Psychiatric Assn
SN  - 1075-2730
SN  - 1557-9700
AD  - Unick, George J., UCSF/SFGH Department of Psychiatry, 2727 Mariposa St., Suite 100, San Francisco, CA, US, 94110
N1  - Accession Number: 2008-17448-006. PMID: 18378833 Other Journal Title: Hospital & Community Psychiatry. Partial author list: First Author & Affiliation: Unick, George J.; Department of Psychiatry, University of California, San Francisco, San Francisco, CA, US. Release Date: 20090216. Correction Date: 20160428. Publication Type: Journal (0100), Peer Reviewed Journal (0110). Format Covered: Electronic. Document Type: Comment/Reply. Language: EnglishMajor Descriptor: Adaptive Testing; Item Response Theory; Measurement; Mental Health; Outpatient Treatment. Minor Descriptor: Affective Disorders; Anxiety Disorders. Classification: Clinical Psychological Testing (2224); Outpatient Services (3371). Population: Human (10). References Available: Y. Page Count: 1. Issue Publication Date: Apr, 2008. 
AB  - Comments on an article by Robert D. Gibbons et al. (see record [rid]2008-17448-005[/rid]). Gibbons and colleagues' article highlights an emerging methodology of considerable value to psychiatric services. Computerized adaptive testing (CAT) has multiple advantages over standard paper-and-pencil assessment tools. CAT uses existing data to streamline and individualize the measurement process. By selecting items of particular relevance to an individual respondent, CAT applications simultaneously reduce the number of needed questions, increase measurement precision, and decrease respondent burden. CAT's applicability has been demonstrated, but its full potential for psychiatric services has yet to be realized. Unfortunately, the technology and infrastructure needed to implement CAT are currently beyond the reach of most clinicians and researchers. (PsycINFO Database Record (c) 2016 APA, all rights reserved)
KW  - item response theory
KW  - computerized adaptive testing
KW  - psychiatric measurement
KW  - mental health assessment
KW  - outpatient treatment
KW  - mood disorder
KW  - anxiety disorders
KW  - Attitude of Health Personnel
KW  - Diagnosis, Computer-Assisted
KW  - Humans
KW  - Mental Disorders
KW  - Software
KW  - Adaptive Testing
KW  - Item Response Theory
KW  - Measurement
KW  - Mental Health
KW  - Outpatient Treatment
KW  - Affective Disorders
KW  - Anxiety Disorders
DO  - 10.1176/appi.ps.59.4.369
UR  - https://search.ebscohost.com/login.aspx?direct=true&db=psyh&AN=2008-17448-006&lang=de&site=ehost-live
UR  - jayunick@yahoo.com
DP  - EBSCOhost
DB  - APA PsycInfo
ER  - 

TY  - JOUR
ID  - 2020-03359-001
AN  - 2020-03359-001
AU  - Carlozzi, Noelle E.
AU  - Lange, Rael T.
AU  - Kallen, Michael A.
AU  - Boileau, Nicholas R.
AU  - Sander, Angelle M.
AU  - Massengale, Jill P.
AU  - Nakase-Richardson, Risa
AU  - Tulsky, David S.
AU  - French, Louis M.
AU  - Hahn, Elizabeth A.
AU  - Ianni, Phillip A.
AU  - Miner, Jennifer A.
AU  - Hanks, Robin
AU  - Brickell, Tracey A.
T1  - Assessing vigilance in caregivers after traumatic brain injury: TBI-CareQOL Caregiver Vigilance
T3  - Caregivers of Service Members/Veterans and Civilians With Traumatic Brain Injury
JF  - Rehabilitation Psychology
JO  - Rehabilitation Psychology
JA  - Rehabil Psychol
Y1  - 2020/11//
VL  - 65
IS  - 4
SP  - 418
EP  - 431
PB  - American Psychological Association
SN  - 0090-5550
SN  - 1939-1544
SN  - 978-1-4338-9417-6
AD  - Carlozzi, Noelle E., Department of Physical Medicine and Rehabilitation, University of Michigan, North Campus Research Complex, 2800 Plymouth Road, Building NCRC B14, Room G216, Ann Arbor, MI, US, 48109-2800
N1  - Accession Number: 2020-03359-001. PMID: 31971432 Other Journal Title: Psychological Aspects of Disability. Partial author list: First Author & Affiliation: Carlozzi, Noelle E.; Department of Physical Medicine and Rehabilitation, University of Michigan, Ann Arbor, MI, US. Other Publishers: Division 22 of the American Psychological Association; Educational Publishing Foundation; Springer Publishing. Release Date: 20200123. Correction Date: 20201231. Publication Type: Journal (0100), Peer Reviewed Journal (0110). Format Covered: Electronic. Document Type: Journal Article. ISBN: 978-1-4338-9417-6. Language: EnglishMajor Descriptor: Adaptive Testing; Caregivers; Test Construction; Traumatic Brain Injury; Vigilance. Minor Descriptor: Caregiver Burden; Measurement; Self-Report; Test Items; Test Reliability; Test Validity; Treatment Outcomes; Health Related Quality of Life; Test-Retest Reliability. Classification: Health Psychology Testing (2226); Home Care & Hospice (3375). Population: Human (10); Male (30); Female (40). Location: US. Age Group: Adulthood (18 yrs & older) (300). Tests & Measures: TBI-Caregiver Quality of Life-Caregiver Vigilance; RAND-12-Item Short Form Health Survey; Caregiver Appraisal Scale; Patient-Reported Outcomes Measurement Information System-Anxiety; Patient-Reported Outcomes Measurement Information System-Social Isolation; Mayo-Portland Adaptability Inventory-4 DOI: 10.1037/t29117-000. Methodology: Empirical Study; Quantitative Study. Page Count: 14. Issue Publication Date: Nov, 2020. Publication History: First Posted Date: Jan 23, 2020; Accepted Date: Nov 10, 2019; Revised Date: Sep 25, 2019; First Submitted Date: Mar 13, 2019. Copyright Statement: American Psychological Association. 2020. 
AB  - Objective: Caregivers of individuals with traumatic brain injury (TBI) frequently experience anxiety related to the caregiver role. Often this is due to a caregiver’s perceived need to avoid people and situations that might upset or 'trigger' the care recipient. There are currently no self-report measures that capture these feelings; thus, this article describes the development and preliminary validation efforts for the TBI-Caregiver Quality of Life (CareQOL) Caregiver Vigilance item bank. Design: A sample of 532 caregivers of civilians (n = 218) or service members/veterans (SMVs; n = 314) with TBI completed 32 caregiver vigilance items, other measures of health-related quality of life (RAND-12, Patient-Reported Outcomes Measurement Information System [PROMIS] Depression, PROMIS Social Isolation, Caregiver Appraisal Scale), and the Mayo–Portland Adaptability Inventory-4. Results: The final item bank contains 18 items, as supported by exploratory and confirmatory factor analysis, item response theory graded response modeling (GRM), and differential item functioning investigations. Expert review and GRM calibration data informed the selection of a 6-item short form and programming of a computer adaptive test. Internal consistency reliability for the different administration formats were excellent (reliability coefficients ≥ .90). Three-week test–retest stability was supported (i.e., r ≥ .78). Correlations between vigilance and other self-report measures supported convergent and discriminant validity (0.01 ≤ r ≤ .69). Known-groups validity was also supported. Conclusions: The new TBI-CareQOL Caregiver Vigilance computer adaptive test and corresponding 6-item short form were developed using established rigorous measurement development standards, providing the first self-report measure to evaluate caregiver vigilance. This development work indicates that this measure exhibits strong psychometric properties. (PsycInfo Database Record (c) 2020 APA, all rights reserved)
KW  - caregiver
KW  - health-related quality of life
KW  - traumatic brain injury
KW  - caregiver burden
KW  - patient-reported outcomes
KW  - Adaptive Testing
KW  - Caregivers
KW  - Test Construction
KW  - Traumatic Brain Injury
KW  - Vigilance
KW  - Caregiver Burden
KW  - Measurement
KW  - Self-Report
KW  - Test Items
KW  - Test Reliability
KW  - Test Validity
KW  - Treatment Outcomes
KW  - Health Related Quality of Life
KW  - Test-Retest Reliability
U1  - Sponsor: National Institutes of Health, National Institute of Nursing Research, US. Grant: R01NR013658. Recipients: No recipient indicated
U1  - Sponsor: General Dynamics Information Technology. Grant: DVBIC-SC-14-003; W91YTZ-13-C-0015. Recipients: No recipient indicated
U1  - Sponsor: National Center for Advancing Translational Sciences, US. Grant: UL1TR000433. Recipients: No recipient indicated
DO  - 10.1037/rep0000302
UR  - https://search.ebscohost.com/login.aspx?direct=true&db=psyh&AN=2020-03359-001&lang=de&site=ehost-live
UR  - ORCID: 0000-0002-9451-0604
UR  - ORCID: 0000-0002-4056-4404
UR  - ORCID: 0000-0003-0439-9429
UR  - carlozzi@med.umich.edu
DP  - EBSCOhost
DB  - APA PsycInfo
ER  - 

TY  - JOUR
ID  - 2013-07923-004
AN  - 2013-07923-004
AU  - Pilkonis, Paul A.
AU  - Choi, Seung W.
AU  - Salsman, John M.
AU  - Butt, Zeeshan
AU  - Moore, Tara L.
AU  - Lawrence, Suzanne M.
AU  - Zill, Nicholas
AU  - Cyranowski, Jill M.
AU  - Kelly, Morgen A. R.
AU  - Knox, Sarah S.
AU  - Cella, David
T1  - Assessment of self-reported negative affect in the NIH Toolbox
JF  - Psychiatry Research
JO  - Psychiatry Research
JA  - Psychiatry Res
Y1  - 2013/03/30/
VL  - 206
IS  - 1
SP  - 88
EP  - 97
PB  - Elsevier Science
SN  - 0165-1781
SN  - 1872-7123
AD  - Pilkonis, Paul A., Department of Psychiatry, University of Pittsburgh Medical Center, Western Psychiatric Institute and Clinic, 3811 O’Hara Street, Pittsburgh, PA, US
N1  - Accession Number: 2013-07923-004. PMID: 23083918 Partial author list: First Author & Affiliation: Pilkonis, Paul A.; Department of Psychiatry, University of Pittsburgh Medical Center, Western Psychiatric Institute and Clinic, Pittsburgh, PA, US. Release Date: 20130624. Correction Date: 20210812. Publication Type: Journal (0100), Peer Reviewed Journal (0110). Format Covered: Electronic. Document Type: Journal Article. Language: EnglishMajor Descriptor: Emotional Regulation; Measurement; Self-Report. Minor Descriptor: Anger; Cognition; Fear; Item Response Theory; Sadness. Classification: Personality Scales & Inventories (2223). Population: Human (10); Male (30); Female (40). Location: US. Age Group: Childhood (birth-12 yrs) (100); School Age (6-12 yrs) (180); Adolescence (13-17 yrs) (200); Adulthood (18 yrs & older) (300); Thirties (30-39 yrs) (340); Middle Age (40-64 yrs) (360); Aged (65 yrs & older) (380); Very Old (85 yrs & older) (390). Tests & Measures: Buss–Perry Aggression Questionnaire; PROMIS Depression Item Bank; PROMIS Pediatric Depression Item Bank; Center for Epidemiologic Studies Depression Scale Child Form; PROMIS Anxiety Item Bank; PROMIS Pediatric Anxiety Item Bank; PROMIS Anger Item Bank; PROMIS Pediatric Anger Scale; Generalized Anxiety Disorder 7 Item Scale; Mood and Anxiety Symptom Questionnaire DOI: 10.1037/t13679-000; Short Mood and Feelings Questionnaire DOI: 10.1037/t15197-000; Anger Expression Scale for Children DOI: 10.1037/t81079-000; Beck Depression Inventory DOI: 10.1037/t00741-000; Geriatric Depression Scale DOI: 10.1037/t00930-000; Center for Epidemiologic Studies Depression Scale; Patient Health Questionnaire DOI: 10.1037/t02598-000; Screen for Child Anxiety Related Emotional Disorders DOI: 10.1037/t03542-000. Methodology: Empirical Study; Quantitative Study. References Available: Y. Page Count: 10. Issue Publication Date: Mar 30, 2013. Publication History: Accepted Date: Sep 20, 2012; Revised Date: Jun 26, 2012; First Submitted Date: Jan 31, 2012. Copyright Statement: All rights reserved. Elsevier Ireland Ltd. 2012. 
AB  - We report on the selection of self-report measures for inclusion in the NIH Toolbox that are suitable for assessing the full range of negative affect including sadness, fear, and anger. The Toolbox is intended to serve as a 'core battery' of assessment tools for cognition, sensation, motor function, and emotional health that will help to overcome the lack of consistency in measures used across epidemiological, observational, and intervention studies. A secondary goal of the NIH Toolbox is the identification of measures that are flexible, efficient, and precise, an agenda best fulfilled by the use of item banks calibrated with models from item response theory (IRT) and suitable for adaptive testing. Results from a sample of 1763 respondents supported use of the adult and pediatric item banks for emotional distress from the Patient-Reported Outcomes Measurement Information System (PROMIS®) as a starting point for capturing the full range of negative affect in healthy individuals. Content coverage for the adult Toolbox was also enhanced by the development of a scale for somatic arousal using items from the Mood and Anxiety Symptom Questionnaire (MASQ) and scales for hostility and physical aggression using items from the Buss–Perry Aggression Questionnaire (BPAQ). (PsycInfo Database Record (c) 2021 APA, all rights reserved)
KW  - anger
KW  - fear
KW  - sadness
KW  - self reporting
KW  - negative affect
KW  - cognition
KW  - assessment tools
KW  - item response theory
KW  - Adolescent
KW  - Adult
KW  - Age Factors
KW  - Child
KW  - Databases, Factual
KW  - Factor Analysis, Statistical
KW  - Female
KW  - Humans
KW  - Male
KW  - Middle Aged
KW  - Mood Disorders
KW  - National Institutes of Health (U.S.)
KW  - Psychometrics
KW  - Quality of Life
KW  - Self Report
KW  - Surveys and Questionnaires
KW  - United States
KW  - Young Adult
KW  - Emotional Regulation
KW  - Measurement
KW  - Self-Report
KW  - Anger
KW  - Cognition
KW  - Fear
KW  - Item Response Theory
KW  - Sadness
U1  - Sponsor: National Institutes of Health, Office of Behavioral and Social Sciences Research, Blueprint for Neuroscience Research, US. Grant: HHS-N-260–2006-00007-C. Other Details: Federal funds. Recipients: No recipient indicated
DO  - 10.1016/j.psychres.2012.09.034
UR  - https://search.ebscohost.com/login.aspx?direct=true&db=psyh&AN=2013-07923-004&lang=de&site=ehost-live
UR  - ORCID: 0000-0001-7872-7267
UR  - pilkonispa@upmc.edu
DP  - EBSCOhost
DB  - APA PsycInfo
ER  - 

TY  - JOUR
ID  - 2008-10224-009
AN  - 2008-10224-009
AU  - Soli, Sigfrid D.
AU  - Wong, Lena L. N.
T1  - Assessment of speech intelligibility in noise with the Hearing in Noise Test
JF  - International Journal of Audiology
JO  - International Journal of Audiology
JA  - Int J Audiol
Y1  - 2008/06//
VL  - 47
IS  - 6
SP  - 356
EP  - 361
PB  - Informa Healthcare
SN  - 1499-2027
SN  - 1708-8186
AD  - Soli, Sigfrid D., House Ear Institute, Human Communicating Sciences and Devices, 2100 W. 3rd Street, Los Angeles, CA, US, 90057
N1  - Accession Number: 2008-10224-009. PMID: 18569108 Partial author list: First Author & Affiliation: Soli, Sigfrid D.; House Ear Institute, Los Angeles, CA, US. Other Publishers: Taylor & Francis. Release Date: 20080825. Correction Date: 20200116. Publication Type: Journal (0100), Peer Reviewed Journal (0110). Format Covered: Print. Document Type: Journal Article. Language: EnglishMajor Descriptor: Noise Effects; Sentence Comprehension; Speech and Hearing Measures; Speech Perception; Test Construction. Minor Descriptor: Hearing Disorders; Language; Psychometrics; Speech Characteristics. Classification: Sensory & Motor Testing (2221); Vision & Hearing & Sensory Disorders (3299). Population: Human (10). Tests & Measures: Hearing in Noise Test DOI: 10.1037/t71961-000. Methodology: Empirical Study; Quantitative Study. References Available: Y. Page Count: 6. Issue Publication Date: Jun, 2008. 
AB  - The speech materials themselves can have a large effect on intelligibility. The lexical status of the words, the grammatical complexity of the utterances, and the length of the utterances all affect intelligibility. The characteristics of the materials when they are spoken can also affect intelligibility. These characteristics include the phonetic similarity of the words, the speaking rate and clarity of the speaker, the naturalness of the speaker's voice, the speaker's gender, and the speaker's dialect. The spectral and temporal energetic properties of non-speech maskers can have a large effect on intelligibility. If the masker is other speech, these energetic properties, as well as the informational properties of the masker, also come into play. A solution to the problems is found in the use of a different stationary masking noise for each language. The spectrum of this masker is matched to the long term average spectrum (LTAS) of the talker's voice for each language. The reflection of sound from the surfaces of an enclosed space, or reverberation, can produce measurable degradation of speech intelligibility in noise via two mechanisms. Acoustic effects of the test environment on sound field HINT scores are most problematic when an individual's scores are to be placed on the absolute scale of speech intelligibility by reference to the norms for the relevant test condition. One of the major advantages of adaptive tests is that they can measure an attribute of interest over a range that includes both normal and impaired function. In the following short reports, relevant details about the development and normalization of the HINT materials in each language are provided. (PsycINFO Database Record (c) 2020 APA, all rights reserved)
KW  - speech intelligibility
KW  - noise effects
KW  - Hearing in Noise Test
KW  - hearing impairment
KW  - psychometrics
KW  - Audiometry, Speech
KW  - Hearing Loss
KW  - Humans
KW  - Noise
KW  - Perceptual Distortion
KW  - Perceptual Masking
KW  - Psychoacoustics
KW  - Reference Values
KW  - Speech Intelligibility
KW  - Speech Perception
KW  - Noise Effects
KW  - Sentence Comprehension
KW  - Speech and Hearing Measures
KW  - Speech Perception
KW  - Test Construction
KW  - Hearing Disorders
KW  - Language
KW  - Psychometrics
KW  - Speech Characteristics
U1  - Sponsor: Cochlear. Recipients: No recipient indicated
U1  - Sponsor: Advanced Bionics. Recipients: No recipient indicated
U1  - Sponsor: MedEl. Recipients: No recipient indicated
U1  - Sponsor: Otologics. Recipients: No recipient indicated
U1  - Sponsor: Starkey. Recipients: No recipient indicated
U1  - Sponsor: Biologic. Recipients: No recipient indicated
U1  - Sponsor: Natus. Recipients: No recipient indicated
U1  - Sponsor: Maico. Other Details: HINT. Recipients: No recipient indicated
DO  - 10.1080/14992020801895136
UR  - https://search.ebscohost.com/login.aspx?direct=true&db=psyh&AN=2008-10224-009&lang=de&site=ehost-live
UR  - ssoli@hei.org
DP  - EBSCOhost
DB  - APA PsycInfo
ER  - 

TY  - JOUR
ID  - 2004-11706-002
AN  - 2004-11706-002
AU  - Hontangas, Pedro
AU  - Olea, Julio
AU  - Ponsoda, Vicente
AU  - Revuelta, Javier
AU  - Wise, Steven L.
T1  - Assisted self-adapted testing: A comparative study
JF  - European Journal of Psychological Assessment
JO  - European Journal of Psychological Assessment
JA  - Eur J Psychol Assess
Y1  - 2004///
VL  - 20
IS  - 1
SP  - 2
EP  - 9
PB  - Hogrefe & Huber Publishers
SN  - 1015-5759
SN  - 2151-2426
AD  - Ponsoda, Vicente, Facultad de Psicologia, Universidad Autonoma de Madrid, Canto Blanco, E-28049, Madrid, Spain
N1  - Accession Number: 2004-11706-002. Other Journal Title: Evaluación Psicológica. Partial author list: First Author & Affiliation: Hontangas, Pedro; Universidad de Valencia, Valencia, Spain. Other Publishers: Hogrefe Publishing. Release Date: 20040322. Correction Date: 20190211. Publication Type: Journal (0100), Peer Reviewed Journal (0110). Format Covered: Print. Document Type: Journal Article. Language: EnglishMajor Descriptor: Adaptive Testing; Psychometrics; Test Anxiety; Computerized Assessment. Classification: Educational Measurement (2227). Population: Human (10); Male (30); Female (40). Location: Spain. Age Group: Adolescence (13-17 yrs) (200); Adulthood (18 yrs & older) (300); Young Adulthood (18-29 yrs) (320). Tests & Measures: State Trait Anxiety Inventory. Methodology: Empirical Study; Quantitative Study. References Available: Y. Page Count: 8. Issue Publication Date: 2004. Copyright Statement: Hogrefe & Huber Publishers. 2004. 
AB  - A new type of self-adapted test (S-AT), called Assisted Self-Adapted Test (AS-AT), is presented. It differs from an ordinary S-AT in that prior to selecting the difficulty category, the computer advises examinees on their best difficulty category choice, based on their previous performance. Three tests (computerized adaptive test, AS-AT, and S-AT) were compared regarding both their psychometric (precision and efficiency) and psychological (anxiety) characteristics. Tests were applied in an actual assessment situation, in which test scores determined 20% of term grades. A sample of 173 high school students participated. Neither differences in posttest anxiety nor ability were obtained. Concerning precision, AS-AT was as precise as CAT, and both revealed more precision than S-AT. It was concluded that AS-AT acted as a CAT concerning precision. Some hints, but not conclusive support, of the psychological similarity between AS-AT and S-AT was also found. (PsycINFO Database Record (c) 2019 APA, all rights reserved)
KW  - assisted self-adapted test
KW  - computerized adaptive test
KW  - psychometrics
KW  - psychological characteristics
KW  - anxiety
KW  - Adaptive Testing
KW  - Psychometrics
KW  - Test Anxiety
KW  - Computerized Assessment
DO  - 10.1027/1015-5759.20.1.2
UR  - https://search.ebscohost.com/login.aspx?direct=true&db=psyh&AN=2004-11706-002&lang=de&site=ehost-live
UR  - ORCID: 0000-0003-4705-6282
UR  - Vicente.Ponsoda@uam.es
DP  - EBSCOhost
DB  - APA PsycInfo
ER  - 

TY  - JOUR
ID  - 1993-24348-001
AN  - 1993-24348-001
AU  - Rosen, Stuart
AU  - Stock, Daphne
T1  - 'Auditory filter bandwidths as a function of level at low frequencies (125 Hz–2 kHz)': Erratum
JF  - Journal of the Acoustical Society of America
JO  - Journal of the Acoustical Society of America
JA  - J Acoust Soc Am
Y1  - 1992/12//
VL  - 92
IS  - 6
SP  - 3438
EP  - 3438
PB  - Acoustical Society of American
SN  - 0001-4966
N1  - Accession Number: 1993-24348-001. Partial author list: First Author & Affiliation: Rosen, Stuart; U London, University Coll London, England. Release Date: 19930701. Publication Type: Journal (0100), Peer Reviewed Journal (0110). Format Covered: Print. Document Type: Erratum/Correction. Language: EnglishMajor Descriptor: Auditory Masking; Auditory Perception; Pitch (Frequency). Classification: Auditory & Speech Perception (2326). Population: Human (10). Age Group: Adulthood (18 yrs & older) (300). Methodology: Empirical Study. Page Count: 1. Issue Publication Date: Dec, 1992. 
AB  - Reports an error in the original article by S. Rosen and D. Stock (Journal of the Acoustical Society of America, 1992[Aug], Vol 92[2, Pt 1], 773–781). The figures in this article were reproduced from copies instead of from the originals, and although all figures were affected to some degree, Figure 1 as printed may be misleading. A reprint of Figure 1 from the original is presented. (The following abstract of this article originally appeared in PA, Vol 80:12194.) Determined the auditory filtering capabilities of 4 normal listeners between 125 Hz and 1 kHz, with particular emphasis on changes of bandwidth level. Masked thresholds were determined for 4 probe frequencies at 4 levels of masking noise, each combination of probe frequency and masker level being tested at 5 notch widths. Thresholds were determined using a 2-down, 1-up adaptive testing procedure and a 2-interval, 2-alternative forced-choice task. Auditory filter bandwidths always decreased with decreasing frequency and generally increased with increasing level, although the size of the effect was not uniform across frequency. In general, the effect of level increased with frequency, with little or no effect on bandwidths at 125 Hz.… (PsycINFO Database Record (c) 2016 APA, all rights reserved)
KW  - changes in bandwidth level at low frequencies
KW  - auditory filtering capability
KW  - adult normal listeners
KW  - erratum
KW  - Auditory Masking
KW  - Auditory Perception
KW  - Pitch (Frequency)
DO  - 10.1121/1.405326
UR  - https://search.ebscohost.com/login.aspx?direct=true&db=psyh&AN=1993-24348-001&lang=de&site=ehost-live
DP  - EBSCOhost
DB  - APA PsycInfo
ER  - 

TY  - JOUR
ID  - 1985-08384-001
AN  - 1985-08384-001
AU  - Weiss, David J.
AU  - McBride, James R.
T1  - Bias and information of Bayesian adaptive testing
JF  - Applied Psychological Measurement
JO  - Applied Psychological Measurement
JA  - Appl Psychol Meas
Y1  - 1984///Sum 1984
VL  - 8
IS  - 3
SP  - 273
EP  - 285
PB  - Sage Publications
SN  - 0146-6216
SN  - 1552-3497
N1  - Accession Number: 1985-08384-001. Partial author list: First Author & Affiliation: Weiss, David J.; U Minnesota, Minneapolis. Release Date: 19850401. Correction Date: 20121001. Publication Type: Journal (0100), Peer Reviewed Journal (0110). Format Covered: Print. Document Type: Journal Article. Language: EnglishMajor Descriptor: Adaptive Testing; Item Analysis (Statistical); Test Bias; Test Construction; Test Scores. Minor Descriptor: Difficulty Level (Test). Classification: Psychometrics & Statistics & Methodology (2200). Population: Human (10). Page Count: 13. Issue Publication Date: Sum 1984. 
AB  - Since test scores are typically used to differentiate among persons, one highly desirable property of a test is that it measures equally well at all levels of a trait. The present study used simulated data and a Monte Carlo technique to investigate score bias and information characteristics of R. J. Owen's (1969) Bayesian adaptive testing strategy and to examine possible causes of test bias. Factors investigated included effects of an accurate prior θ estimate, of item discriminations, and of fixed vs variable test length. Results indicate that estimates from Owen's method are affected by the prior θ estimate used and that the method does not provide measurements that are unbiased and equiprecise unless an accurate prior θ estimate is used. (14 ref) (PsycINFO Database Record (c) 2016 APA, all rights reserved)
KW  - Bayesian adaptive testing strategy of R. J. Owen
KW  - ability estimate & score bias & item information functions
KW  - Adaptive Testing
KW  - Item Analysis (Statistical)
KW  - Test Bias
KW  - Test Construction
KW  - Test Scores
KW  - Difficulty Level (Test)
DO  - 10.1177/014662168400800303
UR  - https://search.ebscohost.com/login.aspx?direct=true&db=psyh&AN=1985-08384-001&lang=de&site=ehost-live
DP  - EBSCOhost
DB  - APA PsycInfo
ER  - 

TY  - JOUR
ID  - 2013-25195-001
AN  - 2013-25195-001
AU  - Yang, Yebing
AU  - Sun, Yunfeng
AU  - Zhang, Ying
AU  - Jiang, Yuan
AU  - Tang, Jingjing
AU  - Zhu, Xia
AU  - Miao, Danmin
T1  - Bifactor item response theory model of acute stress response
JF  - PLoS ONE
JO  - PLoS ONE
JA  - PLoS One
Y1  - 2013/06/07/
VL  - 8
IS  - 6
PB  - Public Library of Science
SN  - 1932-6203
AD  - Zhu, Xia
N1  - Accession Number: 2013-25195-001. PMID: 23762336 Partial author list: First Author & Affiliation: Yang, Yebing; Department of Psychology, Fourth Military Medical University, Xi’an City, China. Release Date: 20140303. Correction Date: 20210712. Publication Type: Journal (0100), Peer Reviewed Journal (0110). Format Covered: Electronic. Document Type: Journal Article. Language: EnglishMajor Descriptor: Item Response Theory; Stress Reactions; Acute Stress. Minor Descriptor: Anxiety; Natural Disasters; Statistical Data. Classification: Psychophysiology (2560). Population: Human (10); Male (30). Location: China. Age Group: Adolescence (13-17 yrs) (200); Adulthood (18 yrs & older) (300); Young Adulthood (18-29 yrs) (320). Tests & Measures: Acute Stress Response Scale; Psychiatric Diagnostic Screening Questionnaires; State Trait Anxiety Inventory. Methodology: Empirical Study; Mathematical Model; Quantitative Study. References Available: Y. ArtID: e65291. Issue Publication Date: Jun 7, 2013. Publication History: First Posted Date: Jun 7, 2013; Accepted Date: Apr 23, 2013; First Submitted Date: Dec 27, 2012. Copyright Statement: This is an open-access article distributed under the terms of the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited. Yang et al. 2013. 
AB  - Background: Better understanding of acute stress responses is important for revision of DSM-5. However, the latent structure and relationship between different aspects of acute stress responses haven’t been clarified comprehensively. Bifactor item response model may help resolve this problem. Objective: The purpose of this study is to develop a statistical model of acute stress responses, based on data from earthquake rescuers using Acute Stress Response Scale (ASRS). Through this model, we could better understand acute stress responses comprehensively, and provide preliminary information for computerized adaptive testing of stress responses. Methods: Acute stress responses of earthquake rescuers were evaluated using ASRS, and state/trait anxiety were assessed using State-trait Anxiety Inventory (STAI). A hierarchical item response model (bifactor model) was used to analyze the data. Additionally, we tested this hierarchical model with model fit comparisons with one-dimensional and five-dimensional models. The correlations among acute stress responses and state/trait anxiety were compared, based on both the five-dimensional and bifactor models. Results: Model fit comparisons showed bifactor model fit the data best. Item loadings on general and specific factors varied greatly between different aspects of stress responses. Many symptoms (40%) of physiological responses had positive loadings on general factor, and negative loadings on specific factor of physiological responses, while other stress responses had positive loadings on both general and specific factors. After extracting general factor of stress responses using bifactor analysis, significant positive correlations between physiological responses and state/trait anxiety (r = 0.185/0.112, p < 0.01) changed into negative ones (r = −0.177/−0.38, p < 0.01). Conclusion: Our results demonstrated bifactor structure of acute stress responses, and positive and negative correlations between physiological responses and stress responses suggested physiological responses could have negative feedback on severity of stress responses. This finding has not been convincingly demonstrated in previous research. (PsycInfo Database Record (c) 2021 APA, all rights reserved)
KW  - item response theory
KW  - acute stress response
KW  - statistical model
KW  - earthquake rescuers
KW  - anxiety
KW  - Adult
KW  - Anxiety
KW  - Humans
KW  - Models, Theoretical
KW  - Psychometrics
KW  - Stress Disorders, Traumatic, Acute
KW  - Surveys and Questionnaires
KW  - Item Response Theory
KW  - Stress Reactions
KW  - Acute Stress
KW  - Anxiety
KW  - Natural Disasters
KW  - Statistical Data
U1  - Sponsor: Fourth Military Medical University, Department of Psychology, China. Recipients: No recipient indicated
DO  - 10.1371/journal.pone.0065291
UR  - https://search.ebscohost.com/login.aspx?direct=true&db=psyh&AN=2013-25195-001&lang=de&site=ehost-live
UR  - miaodanmin@hotmail.com
UR  - zhuxia@fmmu.edu.cn
DP  - EBSCOhost
DB  - APA PsycInfo
ER  - 

TY  - JOUR
ID  - 2009-08749-004
AN  - 2009-08749-004
AU  - Yost, Kathleen J.
AU  - Webster, Kimberly
AU  - Baker, David W.
AU  - Choi, Seung W.
AU  - Bode, Rita K.
AU  - Hahn, Elizabeth A.
T1  - Bilingual health literacy assessment using the Talking Touchscreen/la pantalla parlanchina: Development and pilot testing
JF  - Patient Education and Counseling
JO  - Patient Education and Counseling
JA  - Patient Educ Couns
Y1  - 2009/06//
VL  - 75
IS  - 3
SP  - 295
EP  - 301
PB  - Elsevier Science
SN  - 0738-3991
SN  - 1873-5134
AD  - Hahn, Elizabeth A., Center on Outcomes, Research and Education (CORE), 1001 University Place, Suite 100, Evanston, IL, US, 60201
N1  - Accession Number: 2009-08749-004. PMID: 19386462 Partial author list: First Author & Affiliation: Yost, Kathleen J.; Center on Outcomes, Research and Education (CORE), NorthShore University Health System, Evanston, IL, US. Release Date: 20090803. Correction Date: 20201005. Publication Type: Journal (0100), Peer Reviewed Journal (0110). Format Covered: Electronic. Document Type: Journal Article. Language: EnglishMajor Descriptor: Health Care Utilization; Health Education; Literacy; Psychometrics; Test Construction. Minor Descriptor: Bilingualism; Health. Classification: Tests & Testing (2220); Promotion & Maintenance of Health & Wellness (3365). Population: Human (10); Male (30); Female (40). Age Group: Adulthood (18 yrs & older) (300); Young Adulthood (18-29 yrs) (320); Thirties (30-39 yrs) (340); Middle Age (40-64 yrs) (360). Tests & Measures: Test of Functional Health Literacy in Adults DOI: 10.1037/t10545-000. Methodology: Empirical Study; Qualitative Study; Quantitative Study. References Available: Y. Page Count: 7. Issue Publication Date: Jun, 2009. 
AB  - Objective: Current health literacy measures are too long, imprecise, or have questionable equivalence of English and Spanish versions. The purpose of this paper is to describe the development and pilot testing of a new bilingual computer-based health literacy assessment tool. Methods: We analyzed literacy data from three large studies. Using a working definition of health literacy, we developed new prose, document and quantitative items in English and Spanish. Items were pilot tested on 97 English- and 134 Spanish-speaking participants to assess item difficulty. Results: Items covered topics relevant to primary care patients and providers. English- and Spanish-speaking participants understood the tasks involved in answering each type of question. The English Talking Touchscreen was easy to use and the English and Spanish items provided good coverage of the difficulty continuum. Conclusion: Qualitative and quantitative results provided useful information on computer acceptability and initial item difficulty. After the items have been administered on the Talking Touchscreen (la Pantalla Parlanchina) to 600 English-speaking (and 600 Spanish-speaking) primary care patients, we will develop a computer adaptive test. Practice implications: This health literacy tool will enable clinicians and researchers to more precisely determine the level at which low health literacy adversely affects health and healthcare utilization. (PsycInfo Database Record (c) 2020 APA, all rights reserved)
KW  - bilingualism
KW  - health literacy assessment
KW  - tool development
KW  - healthcare utilization
KW  - Adult
KW  - Educational Status
KW  - Ethnic Groups
KW  - Female
KW  - Humans
KW  - Male
KW  - Middle Aged
KW  - Multilingualism
KW  - Multimedia
KW  - Patient Education as Topic
KW  - Pilot Projects
KW  - Psychometrics
KW  - Qualitative Research
KW  - User-Computer Interface
KW  - Health Care Utilization
KW  - Health Education
KW  - Literacy
KW  - Psychometrics
KW  - Test Construction
KW  - Bilingualism
KW  - Health
U1  - Sponsor: National Heart, Lung, and Blood Institute, US. Grant: R01-HL081485. Recipients: No recipient indicated
DO  - 10.1016/j.pec.2009.02.020
UR  - https://search.ebscohost.com/login.aspx?direct=true&db=psyh&AN=2009-08749-004&lang=de&site=ehost-live
UR  - e-hahn@northwestern.edu
DP  - EBSCOhost
DB  - APA PsycInfo
ER  - 

TY  - JOUR
ID  - 2022-31969-017
AN  - 2022-31969-017
AU  - Hanley, Adam W.
AU  - Gililland, Jeremy
AU  - Erickson, Jill
AU  - Pelt, Christopher
AU  - Peters, Christopher
AU  - Rojas, Jamie
AU  - Garland, Eric L.
T1  - Brief preoperative mind–body therapies for total joint arthroplasty patients: A randomized controlled trial
JF  - Pain
JO  - Pain
JA  - Pain
Y1  - 2021/06//
VL  - 162
IS  - 6
SP  - 1749
EP  - 1757
PB  - Lippincott Williams & Wilkins
SN  - 0304-3959
SN  - 1872-6623
AD  - Garland, Eric L., College of Social Work, University of Utah, 395 South, 1500 East, Salt Lake City, UT, US, 84112
N1  - Accession Number: 2022-31969-017. PMID: 33449510 Partial author list: First Author & Affiliation: Hanley, Adam W.; Center on Mindfulness and Integrative Health Intervention Development (C-MIIND), University of Utah, College of Social, Salt Lake City, UT, US. Other Publishers: Elsevier Science. Release Date: 20220609. Publication Type: Journal (0100), Peer Reviewed Journal (0110). Format Covered: Electronic. Document Type: Journal Article. Language: EnglishGrant Information: Garland, Eric L. Major Descriptor: Surgical Patients; Mind Body Therapy. Minor Descriptor: Hips; Joint Disorders; Knee. Classification: Medical Treatment of Physical Illness (3363). Population: Human (10); Male (30); Female (40). Location: US. Age Group: Adulthood (18 yrs & older) (300); Aged (65 yrs & older) (380). Methodology: Clinical Trial; Empirical Study; Followup Study; Quantitative Study. Supplemental Data: Experimental Materials Internet. References Available: Y. Page Count: 9. Issue Publication Date: Jun, 2021. Publication History: First Posted Date: Jan 12, 2021; Accepted Date: Dec 8, 2020; Revised Date: Dec 2, 2020; First Submitted Date: Aug 17, 2020. Copyright Statement: International Association for the Study of Pain. 2021. 
AB  - :Although knee and hip replacements are intended to relieve pain and improve function, up to 44% of knee replacement patients and 27% of hip replacement patients report persistent postoperative joint pain. Improving surgical pain management is essential. We conducted a single-site, 3-arm, parallel-group randomized clinical trial conducted at an orthopedic clinic, among patients undergoing total joint arthroplasty (TJA) of the hip or knee. Mindfulness meditation (MM), hypnotic suggestion (HS), and cognitive-behavioral pain psychoeducation (cognitive-behavioral pain psychoeducation) were each delivered in a single, 15-minute group session as part of a 2-hour, preoperative education program. Preoperative outcomes—pain intensity, pain unpleasantness, pain medication desire, and anxiety—were measured with numeric rating scales. Postoperative physical functioning at 6-week follow-up was assessed with the Patient-Reported Outcomes Measurement Information System Physical Function computer adaptive test. Total joint arthroplasty patients were randomized to preoperative MM, HS, or cognitive-behavioral pain psychoeducation (n = 285). Mindfulness meditation and HS led to significantly less preoperative pain intensity, pain unpleasantness, and anxiety. Mindfulness meditation also decreased preoperative pain medication desire relative to cognitive-behavioral pain psychoeducation and increased postoperative physical functioning at 6-week follow-up relative to HS and cognitive-behavioral pain psychoeducation. Moderation analysis revealed the surgery type did not differentially impact the 3 interventions. Thus, a single session of a simple, scripted MM intervention may be able to immediately decrease TJA patients' preoperative clinical symptomology and improve postoperative physical function. As such, embedding brief MM interventions in surgical care pathways has the potential to improve surgical outcomes for the millions of patients receiving TJA each year. (PsycInfo Database Record (c) 2022 APA, all rights reserved)
KW  - Mind-body intervention
KW  - Mindfulness
KW  - Hypnosis
KW  - Cognitive-behavioral therapy
KW  - Orthopedic surgery
KW  - Knee replacement
KW  - Hip replacement
KW  - Pain
KW  - Anxiety
KW  - Arthroplasty, Replacement, Hip
KW  - Arthroplasty, Replacement, Knee
KW  - Humans
KW  - Knee Joint
KW  - Mind-Body Therapies
KW  - Pain, Postoperative
KW  - Surgical Patients
KW  - Mind Body Therapy
KW  - Hips
KW  - Joint Disorders
KW  - Knee
U1  - Sponsor: National Institutes of Health, US. Grant: R01DA042033. Recipients: Garland, Eric L.
DO  - 10.1097/j.pain.0000000000002195
UR  - https://search.ebscohost.com/login.aspx?direct=true&db=psyh&AN=2022-31969-017&lang=de&site=ehost-live
UR  - eric.garland@socwk.utah.edu
DP  - EBSCOhost
DB  - APA PsycInfo
ER  - 

TY  - JOUR
ID  - 2014-14650-011
AN  - 2014-14650-011
AU  - Yount, Susan E.
AU  - Choi, Seung W.
AU  - Victorson, David
AU  - Ruo, Bernice
AU  - Cella, David
AU  - Anton, Susan
AU  - Hamilton, Alan
T1  - Brief, valid measures of dyspnea and related functional limitations in chronic obstructive pulmonary disease (COPD)
JF  - Value in Health
JO  - Value in Health
JA  - Value Health
Y1  - 2011/03//
VL  - 14
IS  - 2
SP  - 307
EP  - 315
PB  - Elsevier Science
SN  - 1098-3015
SN  - 1524-4733
AD  - Yount, Susan E., Department of Medical Social Sciences, Northwestern University, Feinberg School of Medicine, 625 North Michigan Avenue, Suite 2700, Chicago, IL, US, 60611
N1  - Accession Number: 2014-14650-011. PMID: 21402298 Partial author list: First Author & Affiliation: Yount, Susan E.; Department of Medical Social Sciences, Northwestern University, Feinberg School of Medicine, Chicago, IL, US. Other Publishers: Blackwell Publishing; Wiley-Blackwell Publishing Ltd. Release Date: 20151116. Correction Date: 20221128. Publication Type: Journal (0100), Peer Reviewed Journal (0110). Format Covered: Electronic. Document Type: Journal Article. Language: EnglishMajor Descriptor: Dyspnea; Psychometrics; Test Reliability; Test Validity; Chronic Obstructive Pulmonary Disease. Classification: Clinical Psychological Testing (2224); Physical & Somatic Disorders (3290). Population: Human (10); Male (30); Female (40). Age Group: Adulthood (18 yrs & older) (300); Aged (65 yrs & older) (380). Tests & Measures: Baseline Dyspnea Index; Medical Research Council Dyspnea Scale; Chronic Respiratory Questionnaire–Self-Administered; Functional Assessment of Chronic Illness Therapy–Dyspnea Scale; Hospital Anxiety and Depression Scale DOI: 10.1037/t03589-000; 36-Item Short Form Health Survey DOI: 10.1037/t07023-000. Methodology: Empirical Study; Interview; Quantitative Study; Scientific Simulation. References Available: Y. Page Count: 9. Issue Publication Date: Mar, 2011. Copyright Statement: Published by Elsevier Inc. International Society for Pharmacoeconomics and Outcomes Research (ISPOR). 2011. 
AB  - Objective: Chronic obstructive pulmonary disease (COPD) is a progressive disease with functional decline leading to disability. Dyspnea, the prominent symptom, can be measured using existing measures, but a lack of consensus about standardization of dyspnea measurement remains. We examined the psychometric performance of two item-response theory– based (IRT) measures of dyspnea and related functional limitations (FLs) in patients with COPD and simulated computerized adaptive testing (CAT) of the banks to determine the number of questions required to achieve high precision. Methods: A total of 102 patients completed banks measuring dyspnea and FLs (33 items), from which the 10-item dyspnea and FL short forms were scored as well as other self-report measures of respiratory and physical function and emotional distress. A subset of patients completed the banks 7 to 10 days later. Pulmonary function test results were obtained from medical charts. Results: The 33-item banks and 10-item short forms had excellent internal consistency (alphas > 0.9) and test-retest reliability (intraclass correlation coefficients > 0.89). Patients sorted by severity level on the Medical Research Council scale were differentiated by item banks (P < 0.001) and the short forms (P < 0.01). The banks and short forms were also associated with related measures of dyspnea (e.g., Baseline Dyspnea Index, r = 0.47–0.53), physical function (e.g., 36-Item Short Form Health Survey, r = −0.83 to −0.86) and forced expiratory volume in 1 second (r = −0.32 to −0.35). On average, CAT required 4 and 5 items for accurate measurement of dyspnea and FLs, respectively. Conclusion: The Functional Assessment of Chronic Illness Therapy–Dyspnea short forms and banks provide options for brief, psychometrically sound measures of dyspnea and/or FLs in COPD. (PsycInfo Database Record (c) 2022 APA, all rights reserved)
KW  - Computer adaptive testing
KW  - Chronic obstructive pulmonary disease
KW  - Dyspnea
KW  - Measurement
KW  - Aged
KW  - Comorbidity
KW  - Computer Simulation
KW  - Dyspnea
KW  - Female
KW  - Humans
KW  - Male
KW  - Psychometrics
KW  - Pulmonary Disease, Chronic Obstructive
KW  - Reproducibility of Results
KW  - Respiratory Function Tests
KW  - Severity of Illness Index
KW  - Surveys and Questionnaires
KW  - United States
KW  - Dyspnea
KW  - Psychometrics
KW  - Test Reliability
KW  - Test Validity
KW  - Chronic Obstructive Pulmonary Disease
U1  - Sponsor: Boehringer-Ingelheim Pharmaceuticals. Recipients: No recipient indicated
DO  - 10.1016/j.jval.2010.11.009
UR  - https://search.ebscohost.com/login.aspx?direct=true&db=psyh&AN=2014-14650-011&lang=de&site=ehost-live
UR  - s-yount@northwestern.edu
DP  - EBSCOhost
DB  - APA PsycInfo
ER  - 

TY  - JOUR
ID  - 2015-10124-019
AN  - 2015-10124-019
AU  - Wagner, Lynne I.
AU  - Schink, Julian
AU  - Bass, Michael
AU  - Patel, Shalini
AU  - Diaz, Maria Varela
AU  - Rothrock, Nan
AU  - Pearman, Timothy
AU  - Gershon, Richard
AU  - Penedo, Frank J.
AU  - Rosen, Steven
AU  - Cella, David
T1  - Bringing PROMIS to practice: Brief and precise symptom screening in ambulatory cancer care
JF  - Cancer
JO  - Cancer
JA  - Cancer
Y1  - 2015/03/15/
VL  - 121
IS  - 6
SP  - 927
EP  - 934
PB  - John Wiley & Sons
SN  - 0008-543X
SN  - 1097-0142
AD  - Wagner, Lynne I., Department of Medical Social Sciences, Northwestern University Feinberg School of Medicine, 633 North St. Clair, 19th Floor, Chicago, IL, US, 60611
N1  - Accession Number: 2015-10124-019. PMID: 25376427 Partial author list: First Author & Affiliation: Wagner, Lynne I.; Department of Medical Social Sciences, Northwestern University Feinberg School of Medicine, Chicago, IL, US. Release Date: 20150413. Publication Type: Journal (0100), Peer Reviewed Journal (0110). Format Covered: Electronic. Document Type: Journal Article. Language: EnglishMajor Descriptor: Neoplasms; Screening. Minor Descriptor: Risk Factors; Symptoms; Oncology. Classification: Cancer (3293); Outpatient Services (3371). Population: Human (10); Female (40); Outpatient (60). Age Group: Adulthood (18 yrs & older) (300); Young Adulthood (18-29 yrs) (320); Thirties (30-39 yrs) (340); Middle Age (40-64 yrs) (360); Aged (65 yrs & older) (380); Very Old (85 yrs & older) (390). Tests & Measures: National Comprehensive Cancer Network Distress Thermometer and Problem Checklist; Patient-Generated Subjective Global Assessment. Methodology: Empirical Study; Quantitative Study. References Available: Y. Page Count: 8. Issue Publication Date: Mar 15, 2015. Publication History: First Posted Date: Nov 6, 2014; Accepted Date: Aug 11, 2014; Revised Date: Aug 7, 2014; First Submitted Date: Jun 25, 2014. Copyright Statement: American Cancer Society. 2014. 
AB  - Background: Supportive oncology practice can be enhanced by the integration of a brief and validated electronic patient‐reported outcome assessment into the electronic health record (EHR) and clinical workflow. Methods: Six hundred thirty‐six women receiving gynecologic oncology outpatient care received instructions to complete clinical assessments through Epic MyChart, an EHR patient communication portal. Patient Reported Outcomes Measurement Information System (PROMIS) computer adaptive tests (CATs) were administered to assess fatigue, pain interference, physical function, depression, and anxiety. Checklists identified psychosocial concerns, informational and nutritional needs, and risk factors for inadequate nutrition. Assessment results, including PROMIS T scores with documented severity thresholds, were immediately populated in the EHR. Clinicians were notified of clinically elevated symptoms through EHR messages. EHR integration was designed to provide automated triage to social work providers for psychosocial concerns, to health educators for information, and to dietitians for nutrition‐related concerns. Results: Four thousand forty‐two MyChart messages sent, and 3203 (79%) were reviewed by patients. The assessment was started by 1493 patients (37%), and once they started, 93% (1386 patients) completed the assessment. According to first assessments only, 49.8% of the patients who reviewed the MyChart message completed the assessment. Mean PROMIS CAT T scores indicated a lower level of physical function and elevated anxiety in comparison with the general population. Fatigue, pain, and depression scores were comparable to those of the general population. Impaired physical functioning was the most common basis for clinical alerts and occurred in 4% of the patients. Conclusions: PROMIS CATs were used to measure common cancer symptoms in routine oncology outpatient care. Immediate EHR integration facilitated the use of symptom reporting as the basis for referral to psychosocial and supportive care. (PsycINFO Database Record (c) 2016 APA, all rights reserved)
KW  - eHealth
KW  - outcomes measurement
KW  - patient‐reported outcomes
KW  - psychosocial care
KW  - symptom management
KW  - Adult
KW  - Aged
KW  - Aged, 80 and over
KW  - Ambulatory Care
KW  - Electronic Health Records
KW  - Female
KW  - Genital Neoplasms, Female
KW  - Humans
KW  - Middle Aged
KW  - Palliative Care
KW  - Self Report
KW  - Surveys and Questionnaires
KW  - Treatment Outcome
KW  - Young Adult
KW  - Neoplasms
KW  - Screening
KW  - Risk Factors
KW  - Symptoms
KW  - Oncology
U1  - Sponsor: Coleman Foundation. Recipients: No recipient indicated
U1  - Sponsor: William W. Wirtz Cancer Innovation Fund. Recipients: No recipient indicated
U1  - Sponsor: National Institutes of Health, US. Grant: R01 CA60068; U5 AR057951. Recipients: No recipient indicated
DO  - 10.1002/cncr.29104
UR  - https://search.ebscohost.com/login.aspx?direct=true&db=psyh&AN=2015-10124-019&lang=de&site=ehost-live
UR  - lwagner@northwestern.edu
DP  - EBSCOhost
DB  - APA PsycInfo
ER  - 

TY  - JOUR
ID  - 2019-52191-006
AN  - 2019-52191-006
AU  - Kallinger, Selina
AU  - Scharm, Henry
AU  - Boecker, Maren
AU  - Forkmann, Thomas
AU  - Baumeister, Harald
T1  - Calibration of an item bank in 474 orthopedic patients using Rasch analysis for computer-adaptive assessment of anxiety
JF  - Clinical Rehabilitation
JO  - Clinical Rehabilitation
JA  - Clin Rehabil
Y1  - 2019/09//
VL  - 33
IS  - 9
SP  - 1468
EP  - 1478
PB  - Sage Publications
SN  - 0269-2155
SN  - 1477-0873
AD  - Kallinger, Selina, Department of Clinical Psychology and Psychotherapy, Institute of Psychology and Education, University of Ulm, Albert-Einstein-Allee 47, D-89081, Ulm, Germany
N1  - Accession Number: 2019-52191-006. PMID: 31018681 Partial author list: First Author & Affiliation: Kallinger, Selina; Department of Clinical Psychology and Psychotherapy, Institute of Psychology and Education, University of Ulm, Ulm, Germany. Other Publishers: Hodder Arnold. Release Date: 20200210. Correction Date: 20221128. Publication Type: Journal (0100), Peer Reviewed Journal (0110). Format Covered: Electronic. Document Type: Journal Article. Language: EnglishMajor Descriptor: Anxiety; Symptoms; Test Items. Minor Descriptor: Intervention; Item Analysis (Test); Item Response Theory; Musculoskeletal Disorders; Psychometrics; Rehabilitation. Classification: Physical & Somatic Disorders (3290). Population: Human (10); Male (30); Female (40). Age Group: Adulthood (18 yrs & older) (300); Middle Age (40-64 yrs) (360). Methodology: Empirical Study; Quantitative Study. References Available: Y. Page Count: 11. Issue Publication Date: Sep, 2019. Publication History: Accepted Date: Mar 28, 2019; First Submitted Date: Jul 5, 2018. Copyright Statement: The Author(s). 2019. 
AB  - Objective: To calibrate an item bank of anxiety-related questions for use in orthopedic patients within a computer-adaptive test. Design: This is a psychometric study. Setting: The sample of orthopedic patients was recruited in two orthopedic rehabilitation clinics in Germany. Subjects: A total of 474 orthopedic rehabilitation patients were recruited for this study. Interventions: Not applicable. Main measures: The main measure is an adapted version of an existing anxiety item pool for cardiovascular rehabilitation patients. Results: The results of the confirmatory factor analysis and Mokken analysis confirmed a one-factor structure and double monotonicity. An anxiety item bank (48 items) could be developed and calibrated using Rasch analysis. It fitted to the Rasch model with a non-significant item–trait interaction (χ²(203) = 172.59; P = .94) and was free of differential item functioning. Unidimensionality could be verified and the person separation reliability was .96. The category threshold parameters varied between 4.72 and 3.16 (7.88 logits). Conclusion: The unidimensional anxiety item bank provides the basis for a computer-adaptive test to assess a wide range of anxiety in rehabilitation patients with orthopedic diseases with very good psychometric characteristics. (PsycInfo Database Record (c) 2022 APA, all rights reserved)
KW  - Anxiety
KW  - orthopedic disease
KW  - computer-adaptive test
KW  - item bank
KW  - item response theory
KW  - Rasch model
KW  - Anxiety
KW  - Diagnosis, Computer-Assisted
KW  - Factor Analysis, Statistical
KW  - Female
KW  - Humans
KW  - Male
KW  - Middle Aged
KW  - Musculoskeletal Diseases
KW  - Psychometrics
KW  - Surveys and Questionnaires
KW  - Anxiety
KW  - Symptoms
KW  - Test Items
KW  - Intervention
KW  - Item Analysis (Test)
KW  - Item Response Theory
KW  - Musculoskeletal Disorders
KW  - Psychometrics
KW  - Rehabilitation
U1  - Sponsor: German Insurance Scheme, Germany. Grant: 8011-106-31/31.117. Recipients: No recipient indicated
DO  - 10.1177/0269215519846225
UR  - https://search.ebscohost.com/login.aspx?direct=true&db=psyh&AN=2019-52191-006&lang=de&site=ehost-live
UR  - selina.kallinger@uni-ulm.de
DP  - EBSCOhost
DB  - APA PsycInfo
ER  - 

TY  - JOUR
ID  - 2023-16423-001
AN  - 2023-16423-001
AU  - Azpilicueta, Ana E.
AU  - Cupani, Marcos
AU  - Ghío, Fernanda B.
AU  - Morán, Valeria E.
AU  - Garrido, Sebastián J.
AU  - Bruzzone, Manuel
T1  - Career decision self-efficacy item bank: A simulation study
JF  - Current Psychology: A Journal for Diverse Perspectives on Diverse Psychological Issues
JO  - Current Psychology: A Journal for Diverse Perspectives on Diverse Psychological Issues
JA  - Curr Psychol
Y1  - 2022/11/02/
PB  - Springer
SN  - 1046-1310
SN  - 1936-4733
AD  - Azpilicueta, Ana E.
N1  - Accession Number: 2023-16423-001. Other Journal Title: Current Psychological Research & Reviews. Partial author list: First Author & Affiliation: Azpilicueta, Ana E.; Facultad de Psicología, Universidad Nacional de Córdoba (UNC), Córdoba, Argentina. Other Publishers: Transaction Publishers. Release Date: 20221107. Publication Type: Journal (0100), Peer Reviewed Journal (0110). Format Covered: Electronic. Document Type: Journal. Language: EnglishMajor Descriptor: No terms assigned. Classification: General Psychology (2100). References Available: Y. Publication History: Accepted Date: Sep 6, 2022; First Submitted Date: Sep 10, 2021. Copyright Statement: The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature . Springer Nature or its licensor holds exclusive rights to this article under a publishing agreement with the author(s) or other rightsholder(s); author self-archiving of the accepted manuscript version of this article is solely governed by the terms of such publishing agreement and applicable law. 2022. 
AB  - The transformation of the current world of work, determined by technological advances, affects and challenges the career decision-making process. To face these changes, the career self-management model, derived from social cognitive career theory, offers an optimal framework that allows studying the career adaptive behaviors that favor the preparation for career decision-making. In order to obtain a measure for the core construct, career decision-making self-efficacy, we elaborated an item bank from the selection of reagents from two scales widely used in the field for its estimation. Item analysis was performed from the item response theory. The results evidenced the unidimensionality of the item pool, with difficulty indexes and ability levels covering most of the measured continuum. As regard reliability indexes (persons and items), we observed that the allocation of persons and items might be reproduced in a predictable way. Global fit of items was suitable in most of them. We also performed two analyses of the evidence of the validity of the scores obtained through the Career Decision Making Self-Efficacy-Item Bank. We carried out a simulation study to determine the real level of ability of the subjects, providing information on the quality of the item bank developed to accurately assess career decision self-efficacy. Thus, we conducted a concurrent validity evidence study to establish test-criterion relationships between career decision self-efficacy and the more significant outcome variables of the career decision-making process: career decision, career indecision, and decisional anxiety. In summary, we developed, calibrated, and validated the first item bank to estimate the central construct of the career decision-making process, which is the first step to develop a Computerized Adaptive Test. Based on this technology, personalized evaluations of specific skills or traits such as career decision self-efficacy can be obtained and incorporated into computer-assisted career guidance systems. (PsycInfo Database Record (c) 2022 APA, all rights reserved)
KW  - Career decision-making self-efficacy
KW  - Item bank
KW  - Item response theory
KW  - Simulation study
KW  - No terms assigned
DO  - 10.1007/s12144-022-03749-w
UR  - https://search.ebscohost.com/login.aspx?direct=true&db=psyh&AN=2023-16423-001&lang=de&site=ehost-live
UR  - ORCID: 0000-0002-7782-7802
UR  - ORCID: 0000-0001-6603-7285
UR  - ORCID: 0000-0003-3628-1636
UR  - ORCID: 0000-0002-4223-2470
UR  - ORCID: 0000-0003-2132-5552
UR  - ORCID: 0000-0002-6221-3162
UR  - estefaniaazpilicueta@gmail.com
DP  - EBSCOhost
DB  - APA PsycInfo
ER  - 

TY  - JOUR
ID  - 2020-11573-001
AN  - 2020-11573-001
AU  - Peute, Linda
AU  - Scheeve, Thom
AU  - Jaspers, Monique
T1  - Classification and regression tree and computer adaptive testing in cardiac rehabilitation: Instrument validation study
JF  - Journal of Medical Internet Research
JO  - Journal of Medical Internet Research
JA  - J Med Internet Res
Y1  - 2020/01/30/
VL  - 22
IS  - 1
PB  - JMIR Publications
SN  - 1439-4456
SN  - 1438-8871
AD  - Jaspers, Monique, Center of Human Factors Engineering of Health Information Technology, Department of Medical Informatics, Amsterdam Institute of Public Health, Amsterdam University Medical Centers, J1b-116, Meibergdreef 9, 1105 AZ, Amsterdam, Netherlands
N1  - Accession Number: 2020-11573-001. PMID: 32012065 Partial author list: First Author & Affiliation: Peute, Linda; Center of Human Factors Engineering of Health Information Technology, Department of Medical Informatics, Amsterdam Institute of Public Health, Amsterdam University Medical Centers, Amsterdam, Netherlands. Other Publishers: Gunther Eysenbach. Release Date: 20200727. Correction Date: 20201012. Publication Type: Journal (0100), Peer Reviewed Journal (0110). Format Covered: Electronic. Document Type: Journal Article. Language: EnglishMajor Descriptor: Adaptive Testing; Rehabilitation; Electronic Health Services. Minor Descriptor: Cardiovascular Disorders; Machine Learning; Needs Assessment; Questionnaires; Test Validity. Classification: Health & Mental Health Services (3370). Population: Human (10); Male (30); Female (40). Age Group: Adulthood (18 yrs & older) (300); Aged (65 yrs & older) (380). Tests & Measures: Multidimensional Perceived Social Support Scale; Hospital Anxiety and Depression Scale DOI: 10.1037/t03589-000; Generalized Anxiety Disorder 7 DOI: 10.1037/t02591-000; Patient Health Questionnaire-9 DOI: 10.1037/t06165-000. Methodology: Empirical Study; Quantitative Study. ArtID: e12509. Issue Publication Date: Jan 30, 2020. Publication History: First Posted Date: Jan 30, 2020; Accepted Date: Jul 19, 2019; Revised Date: Jun 11, 2019; First Submitted Date: Oct 17, 2018. Copyright Statement: Originally published in the Journal of Medical Internet Research (http://www.jmir.org), 30.01.2020. This is an open-access article distributed under the terms of the Creative Commons Attribution License (https://creativecommons.org/licenses/by/4.0/), which permits unrestricted use, distribution, and reproduction in any medium, provided the original work, first published in the Journal of Medical Internet Research, is properly cited. The complete bibliographic information, a link to the original publication on http://www.jmir.org/, as well as this copyright and license information must be included. Linda Peute, Thom Scheeve, Monique Jaspers
AB  - Background: There is a need for shorter-length assessments that capture patient questionnaire data while attaining high data quality without an undue response burden on patients. Computerized adaptive testing (CAT) and classification and regression tree (CART) methods have the potential to meet these needs and can offer attractive options to shorten questionnaire lengths. Objective: The objective of this study was to test whether CAT or CART was best suited to reduce the number of questionnaire items in multiple domains (eg, anxiety, depression, quality of life, and social support) used for a needs assessment procedure (NAP) within the field of cardiac rehabilitation (CR) without the loss of data quality. Methods: NAP data of 2837 CR patients from a multicenter Cardiac Rehabilitation Decision Support System (CARDSS) Web-based program was used. Patients used a Web-based portal, MyCARDSS, to provide their data. CAT and CART were assessed based on their performances in shortening the NAP procedure and in terms of sensitivity and specificity. Results: With CAT and CART, an overall reduction of 36% and 72% of NAP questionnaire length, respectively, was achieved, with a mean sensitivity and specificity of 0.765 and 0.817 for CAT, 0.777 and 0.877 for classification trees, and 0.743 and 0.40 for regression trees, respectively. Conclusions: Both CAT and CART can be used to shorten the questionnaires of the NAP used within the field of CR. CART, however, showed the best performance, with a twice as large overall decrease in the number of questionnaire items of the NAP compared to CAT and the highest sensitivity and specificity. To our knowledge, our study is the first to assess the differences in performance between CAT and CART for shortening questionnaire lengths. Future research should consider administering varied assessments of patients over time to monitor their progress in multiple domains. For CR professionals, CART integrated with MyCARDSS would provide a feedback loop that informs the rehabilitation progress of their patients by providing real-time patient measurements. (PsycInfo Database Record (c) 2020 APA, all rights reserved)
KW  - psychometrics
KW  - computing methodologies
KW  - mHealth
KW  - internet
KW  - cardiac rehabilitation
KW  - needs assessment
KW  - Aged
KW  - Cardiac Rehabilitation
KW  - Computers
KW  - Female
KW  - Humans
KW  - Male
KW  - Psychometrics
KW  - Quality of Life
KW  - Surveys and Questionnaires
KW  - Telemedicine
KW  - Adaptive Testing
KW  - Rehabilitation
KW  - Electronic Health Services
KW  - Cardiovascular Disorders
KW  - Machine Learning
KW  - Needs Assessment
KW  - Questionnaires
KW  - Test Validity
DO  - 10.2196/12509
UR  - https://search.ebscohost.com/login.aspx?direct=true&db=psyh&AN=2020-11573-001&lang=de&site=ehost-live
UR  - ORCID: 0000-0002-8288-0368
UR  - ORCID: 0000-0002-9859-0912
UR  - m.w.jaspers@amsterdamumc.nl
DP  - EBSCOhost
DB  - APA PsycInfo
ER  - 

TY  - JOUR
ID  - 2014-45906-010
AN  - 2014-45906-010
AU  - Dillon, Harvey
AU  - Cameron, Sharon
AU  - Tomlin, Dani
AU  - Glyde, Helen
T1  - Comments on 'factors influencing tests of auditory processing: A perspective on current issues and relevant concerns' by Tony Cacace and Dennis McFarland
JF  - Journal of the American Academy of Audiology
JO  - Journal of the American Academy of Audiology
JA  - J Am Acad Audiol
Y1  - 2014/07//Jul-Aug, 2014
VL  - 25
IS  - 7
SP  - 699
EP  - 703
PB  - American Academy of Audiology
SN  - 1050-0545
SN  - 2157-3107
N1  - Accession Number: 2014-45906-010. PMID: 25365374 Partial author list: First Author & Affiliation: Dillon, Harvey; National Acoustic Laboratories, Sydney, NSW, Australia. Other Publishers: Thieme Medical Publishers, Inc. Release Date: 20151109. Correction Date: 20220207. Publication Type: Journal (0100), Peer Reviewed Journal (0110). Format Covered: Print. Document Type: Comment/Reply. Language: EnglishMajor Descriptor: Auditory Perception; Hearing Disorders; Psychometrics; Test Reliability; Test Validity. Minor Descriptor: Diagnosis; Psychophysical Measurement; Responses. Classification: Health Psychology Testing (2226); Vision & Hearing & Sensory Disorders (3299). Population: Human (10). References Available: Y. Page Count: 5. Issue Publication Date: Jul-Aug, 2014. 
AB  - Comments on an article by Tony Cacace & Dennis McFarland (see record [rid]2013-34119-003[/rid]). Cacace and McFarland have written an interesting and provoking article on auditory processing. Cacace and McFarland's conclusion, that testing for auditory processing disorders (APD), included testing in other modalities is expressed strongly. The commentator completely agrees with Cacace & McFarland that it is necessary to disentangle, to the degree possible, the effects of cognitive abilities on auditory processing test scores. The use of adaptive test methods, with computer administration, and predefined stopping criteria dependent on test precision, as recommended by Cacace & McFarland, are key tools in constructing reliable tests. The differential test format also deals with the concern raised by Cacace & McFarland about verbal responses affecting test scores. Regarding Cacace & McFarland's suggestion that total computer administration be combined with a forced-choice design, the commentators would caution that there are potential pitfalls in this strategy, when testing is aimed at young children. Cacace and McFarland have stated that spoken responses are the result of complex motor skills. In summary, the commentators are in complete agreement with Cacace & McFarland that APD testing must be done in a manner that minimizes the effects of cognition on test scores. (PsycInfo Database Record (c) 2022 APA, all rights reserved)
KW  - tests of auditory processing disorders
KW  - test validity
KW  - psychometrics
KW  - diagnosis
KW  - test reliability
KW  - response selection
KW  - psychophysical methods
KW  - Auditory Perception
KW  - Auditory Perceptual Disorders
KW  - Evoked Potentials, Auditory, Brain Stem
KW  - Humans
KW  - Neuropsychological Tests
KW  - Psychoacoustics
KW  - Auditory Perception
KW  - Hearing Disorders
KW  - Psychometrics
KW  - Test Reliability
KW  - Test Validity
KW  - Diagnosis
KW  - Psychophysical Measurement
KW  - Responses
UR  - https://search.ebscohost.com/login.aspx?direct=true&db=psyh&AN=2014-45906-010&lang=de&site=ehost-live
DP  - EBSCOhost
DB  - APA PsycInfo
ER  - 

TY  - JOUR
ID  - 2014-09895-001
AN  - 2014-09895-001
AU  - Amtmann, Dagmar
AU  - Kim, Jiseon
AU  - Chung, Hyewon
AU  - Bamer, Alyssa M.
AU  - Askew, Robert L.
AU  - Wu, Salene
AU  - Cook, Karon F.
AU  - Johnson, Kurt L.
T1  - Comparing CESD-10, PHQ-9, and PROMIS depression instruments in individuals with multiple sclerosis
JF  - Rehabilitation Psychology
JO  - Rehabilitation Psychology
JA  - Rehabil Psychol
Y1  - 2014/05//
VL  - 59
IS  - 2
SP  - 220
EP  - 229
PB  - American Psychological Association
SN  - 0090-5550
SN  - 1939-1544
AD  - Amtmann, Dagmar, Department of Rehabilitation Medicine, University of Washington, Box 354237, Seattle, WA, US, 98195
N1  - Accession Number: 2014-09895-001. PMID: 24661030 Other Journal Title: Psychological Aspects of Disability. Partial author list: First Author & Affiliation: Amtmann, Dagmar; Department of Rehabilitation Medicine, University of Washington, Seattle, WA, US. Other Publishers: Division 22 of the American Psychological Association; Educational Publishing Foundation; Springer Publishing. Release Date: 20140324. Correction Date: 20221128. Publication Type: Journal (0100), Peer Reviewed Journal (0110). Format Covered: Electronic. Document Type: Journal Article. Language: EnglishConference Information: Annual meeting of the Consortium of Multiple Sclerosis Centers, May-Jun, 2012, San Diego, CA, US. Grant Information: Amtmann, Dagmar. Conference Note: Portions of this paper were presented at the aforementioned meeting. Major Descriptor: Major Depression; Measurement; Multiple Sclerosis; Psychometrics; Questionnaires. Minor Descriptor: Patient Reported Outcome Measures. Classification: Clinical Psychological Testing (2224); Physical & Somatic Disorders (3290). Population: Human (10); Male (30); Female (40). Location: US. Age Group: Adulthood (18 yrs & older) (300). Tests & Measures: Center for Epidemiological Studies Depression Scale-10; Patient Reported Outcome Measurement Information System-Depression Short Form; PROMIS Sleep Disturbance Short Form; Modified Fatigue Impact Scale; PROMIS Pain Interference Short Form; Center for Epidemiological Studies Depression Scale-20; Patient Reported Outcome Measurement Information System-Depression-8; Beck Depression Inventory DOI: 10.1037/t00741-000; Hospital Anxiety and Depression Scale DOI: 10.1037/t03589-000; Patient Health Questionnaire-9 DOI: 10.1037/t06165-000. Methodology: Empirical Study; Quantitative Study. References Available: Y. Page Count: 10. Issue Publication Date: May, 2014. Publication History: First Posted Date: Mar 24, 2014; Accepted Date: Dec 30, 2013; Revised Date: Oct 7, 2013; First Submitted Date: May 7, 2013. Copyright Statement: American Psychological Association. 2014. 
AB  - Purpose: This study evaluated psychometric properties of the Patient Health Questionnaire-9 (PHQ-9), the Center for Epidemiological Studies Depression Scale-10 (CESD-10), and the 8-item PROMIS Depression Short Form (PROMIS-D-8; 8b short form) in a sample of individuals living with multiple sclerosis (MS). Research Method: Data were collected by a self-reported mailed survey of a community sample of people living with MS (n = 455). Factor structure, interitem reliability, convergent/discriminant validity and assignment to categories of depression severity were examined. Results: A 1-factor, confirmatory factor analytic model had adequate fit for all instruments. Scores on the depression scales were more highly correlated with one another than with scores on measures of pain, sleep disturbance, and fatigue. The CESD-10 categorized about 37% of participants as having significant depressive symptoms. At least moderate depression was indicated for 24% of participants by PHQ-9. PROMIS-D-8 identified 19% of participants as having at least moderate depressive symptoms and about 7% having at least moderately severe depression. None of the examined scales had ceiling effects, but the PROMIS-D-8 had a floor effect. Conclusions: Overall, scores on all 3 scales demonstrated essential unidimensionality and had acceptable interitem reliability and convergent/discriminant validity. Researchers and clinicians can choose any of these scales to measure depressive symptoms in individuals living with MS. The PHQ-9 offers validated cutoff scores for diagnosing clinical depression. The PROMIS-D-8 measure minimizes the impact of somatic features on the assessment of depression and allows for flexible administration, including Computerize Adaptive Testing (CAT). The CESD-10 measures 2 aspects of depression, depressed mood and lack of positive affect, while still providing an interpretable total score. (PsycInfo Database Record (c) 2022 APA, all rights reserved)
KW  - CESD-10
KW  - PHQ-9
KW  - PROMIS
KW  - depression
KW  - multiple sclerosis
KW  - Center for Epidemiological Studies Depression Scale-10
KW  - Patient Health Questionnaire-9
KW  - Patient Reported Outcome Measurement Information System Depression Short Form
KW  - psychometrics
KW  - Depressive Disorder
KW  - Factor Analysis, Statistical
KW  - Female
KW  - Humans
KW  - Male
KW  - Middle Aged
KW  - Multiple Sclerosis
KW  - Psychiatric Status Rating Scales
KW  - Psychometrics
KW  - Reproducibility of Results
KW  - Surveys and Questionnaires
KW  - Major Depression
KW  - Measurement
KW  - Multiple Sclerosis
KW  - Psychometrics
KW  - Questionnaires
KW  - Patient Reported Outcome Measures
U1  - Sponsor: Department of Education. Recipients: No recipient indicated
U1  - Sponsor: National Institute on Disability and Rehabilitation Research. Grant: H133B080025; H133P120002. Recipients: No recipient indicated
U1  - Sponsor: National Institutes of Health, National Institute of Arthritis and Musculoskeletal and Skin Diseases. Grant: U01AR052171. Recipients: Amtmann, Dagmar (Prin Inv)
DO  - 10.1037/a0035919
UR  - https://search.ebscohost.com/login.aspx?direct=true&db=psyh&AN=2014-09895-001&lang=de&site=ehost-live
UR  - ORCID: 0000-0002-9579-2279
UR  - ORCID: 0000-0002-9267-0110
UR  - dagmara@u.washington.edu
DP  - EBSCOhost
DB  - APA PsycInfo
ER  - 

TY  - JOUR
ID  - 2014-35169-012
AN  - 2014-35169-012
AU  - Baum, George
AU  - Basen-Engquist, Karen
AU  - Swartz, Maria C.
AU  - Parker, Patricia A.
AU  - Carmack, Cindy L.
T1  - Comparing PROMIS computer-adaptive tests to the Brief Symptom Inventory in patients with prostate cancer
JF  - Quality of Life Research: An International Journal of Quality of Life Aspects of Treatment, Care & Rehabilitation
JO  - Quality of Life Research: An International Journal of Quality of Life Aspects of Treatment, Care & Rehabilitation
JA  - Qual Life Res
Y1  - 2014/09//
VL  - 23
IS  - 7
SP  - 2031
EP  - 2035
PB  - Springer
SN  - 0962-9343
SN  - 1573-2649
AD  - Carmack, Cindy L., Department of Behavioral Science, University of Texas, MD Anderson Cancer Center, P.O. Box 301439, Unit 1330, Houston, TX, US, 77230-1439
N1  - Accession Number: 2014-35169-012. Partial author list: First Author & Affiliation: Baum, George; Department of Behavioral Science, University of Texas, MD Anderson Cancer Center, Houston, TX, US. Release Date: 20141110. Publication Type: Journal (0100), Peer Reviewed Journal (0110). Format Covered: Electronic. Document Type: Journal Article. Language: EnglishGrant Information: Carmack, Cindy L. Major Descriptor: Neoplasms; Psychometrics; Test Reliability; Test Validity. Minor Descriptor: Caregiver Burden. Classification: Clinical Psychological Testing (2224); Cancer (3293). Population: Human (10); Male (30); Female (40). Location: US. Age Group: Adulthood (18 yrs & older) (300). Tests & Measures: Global Severity Index; Brief Symptom Inventory DOI: 10.1037/t00789-000; Patient-Reported Outcomes Measurement Information System DOI: 10.1037/t06780-000. Methodology: Empirical Study; Quantitative Study. Page Count: 5. Issue Publication Date: Sep, 2014. Publication History: First Posted Date: Feb 16, 2014; Accepted Date: Feb 7, 2014. Copyright Statement: Springer International Publishing Switzerland. 2014. 
AB  - Purpose: This study assessed whether the Patient-Reported Outcomes Measurement Information System (PROMIS) computer-adaptive tests (CATs) provided results similar to those of the Brief Symptom Inventory (BSI) with a low patient burden. Methods: Secondary data analysis of 136 prostate cancer patients who completed the 53-item BSI and the PROMIS CATs assessing depression, anxiety, and hostility. Results: The PROMIS CATs and BSI correlated significantly in measures of depression (.85), anxiety (.76), and anger/hostility (.66; p < .001 for all). Using our BSI cutoff points for depression, anxiety, and anger/hostility, ROC analysis yielded areas under the curve of .966 [standard error (SE) = .014, p < .001], .975 (SE = .012, p < .001), and .952 (SE = .027, p < .001), respectively. Conclusions: PROMIS CATs were highly correlated with the BSI subscales, indicating that the CATs performed well compared with the BSI, a widely used psychosocial measure. (PsycINFO Database Record (c) 2016 APA, all rights reserved)
KW  - computer-adaptive tests
KW  - BSI
KW  - Brief Symptom Inventory
KW  - prostate cancer
KW  - quality of life
KW  - PROMIS
KW  - health surveys
KW  - Neoplasms
KW  - Psychometrics
KW  - Test Reliability
KW  - Test Validity
KW  - Caregiver Burden
U1  - Sponsor: Patient- Reported Outcomes, Survey, and Population Research (PROSPR) Shared Resource. Recipients: No recipient indicated
U1  - Sponsor: National Institutes of Health, US. Other Details: Cancer Center Support Grant CA16672, given to The University of Texas MD Anderson Cancer Center. Recipients: No recipient indicated
U1  - Sponsor: National Cancer Institute, US. Grant: 5R21CA126854-02. Recipients: Carmack, Cindy L.
DO  - 10.1007/s11136-014-0647-2
UR  - https://search.ebscohost.com/login.aspx?direct=true&db=psyh&AN=2014-35169-012&lang=de&site=ehost-live
UR  - ccarmack@mdanderson.org
DP  - EBSCOhost
DB  - APA PsycInfo
ER  - 

TY  - JOUR
ID  - 2020-16502-007
AN  - 2020-16502-007
AU  - Sunderland, Matthew
AU  - Afzali, Mohammad H.
AU  - Batterham, Philip J.
AU  - Calear, Alison L.
AU  - Carragher, Natacha
AU  - Hobbs, Megan
AU  - Mahoney, Alison
AU  - Peters, Lorna
AU  - Slade, Tim
T1  - Comparing scores from full length, short form, and adaptive tests of the Social Interaction Anxiety and Social Phobia Scales
JF  - Assessment
JO  - Assessment
JA  - Assessment
Y1  - 2020/04//
VL  - 27
IS  - 3
SP  - 518
EP  - 532
PB  - Sage Publications
SN  - 1073-1911
SN  - 1552-3489
AD  - Sunderland, Matthew, Matilda Centre for Research in Mental Health and Substance Use, University of Sydney, Lvl 6, Jane Foss Russell Building, Sydney, NSW, Australia
N1  - Accession Number: 2020-16502-007. PMID: 30873852 Partial author list: First Author & Affiliation: Sunderland, Matthew; UNSW Sydney, Sydney, NSW, Australia. Release Date: 20200406. Correction Date: 20221128. Publication Type: Journal (0100), Peer Reviewed Journal (0110). Format Covered: Electronic. Document Type: Journal Article. Language: EnglishMajor Descriptor: Anxiety; Measurement; Social Interaction; Social Phobia; Test Construction. Minor Descriptor: Adaptive Testing; Clinical Practice; Severity (Disorders); Social Anxiety; Test Forms; Test Performance; Computerized Assessment. Classification: Clinical Psychological Testing (2224); Anxiety Disorders (3215). Population: Human (10); Male (30); Female (40). Location: Australia. Age Group: Adulthood (18 yrs & older) (300); Young Adulthood (18-29 yrs) (320); Thirties (30-39 yrs) (340). Tests & Measures: Social Interaction Anxiety Scale-Computerized Adaptive Version; Social Phobia Scale-Computerized Adaptive Version; Social Interaction Anxiety Scale-6; Social Phobia Scale-6. Methodology: Empirical Study; Quantitative Study. References Available: Y. Page Count: 15. Issue Publication Date: Apr, 2020. Copyright Statement: The Author(s). 2019. 
AB  - The current study developed and examined the performance of a computerized adaptive version of the Social Interaction Anxiety and Social Phobia Scales (SIAS/SPS) and compared results with a previously developed static short form (SIAS-6/SPS-6) in terms of measurement precision, concordance with the full forms, and sensitivity to treatment. Among an online sample of Australian adults, there were relatively minor differences in the performance of the adaptive tests and static short forms when compared with the full scales. Moreover, both adaptive and static short forms generated similar effect sizes across treatment in a clinical sample. This provides further evidence for the use of static or adaptive short forms of the SIAS/SPS rather than the lengthier 20-item versions. However, at the individual level, the adaptive tests were able to maintain an acceptable level of precision, using few items as possible, across the severity continua in contrast to the static short forms. (PsycInfo Database Record (c) 2022 APA, all rights reserved)
KW  - social anxiety disorder
KW  - computerized adaptive testing
KW  - short forms
KW  - scale development
KW  - SIAS
KW  - SPS
KW  - Anxiety
KW  - Measurement
KW  - Social Interaction
KW  - Social Phobia
KW  - Test Construction
KW  - Adaptive Testing
KW  - Clinical Practice
KW  - Severity (Disorders)
KW  - Social Anxiety
KW  - Test Forms
KW  - Test Performance
KW  - Computerized Assessment
DO  - 10.1177/1073191119832657
UR  - https://search.ebscohost.com/login.aspx?direct=true&db=psyh&AN=2020-16502-007&lang=de&site=ehost-live
UR  - ORCID: 0000-0003-2704-0531
UR  - ORCID: 0000-0003-0131-0089
UR  - ORCID: 0000-0002-7028-725X
UR  - matthew.sunderland@sydney.edu.au
DP  - EBSCOhost
DB  - APA PsycInfo
ER  - 

TY  - THES
ID  - 2015-99030-132
AN  - 2015-99030-132
AU  - Botello, Jennifer A.
T1  - Comparing the effect of two types of computer screen background lighting on students' reading engagement and achievement
JF  - Dissertation Abstracts International Section A: Humanities and Social Sciences
JO  - Dissertation Abstracts International Section A: Humanities and Social Sciences
Y1  - 2015///
VL  - 75
IS  - 8-A(E)
PB  - ProQuest Information & Learning
SN  - 0419-4209
SN  - 978-1-303-87102-3
N1  - Accession Number: 2015-99030-132. Other Journal Title: Dissertation Abstracts International. Partial author list: First Author & Affiliation: Botello, Jennifer A.; Lindenwood U., US. Release Date: 20150209. Correction Date: 20221128. Publication Type: Dissertation Abstract (0400). Format Covered: Electronic. Document Type: Dissertation. Dissertation Number: AAI3618650. ISBN: 978-1-303-87102-3. Language: EnglishMajor Descriptor: Academic Achievement; Computers. Minor Descriptor: Elementary School Students; Literacy; Reading; Standardized Tests; Student Engagement. Classification: Educational & School Psychology (3500). Population: Human (10). Age Group: Childhood (birth-12 yrs) (100); School Age (6-12 yrs) (180). Methodology: Empirical Study; Quantitative Study. 
AB  - With increased dependence on computer-based standardized tests to assess academic achievement, technological literacy has become an essential skill. Yet, because students have unequal access to technology, they may not have equal opportunities to perform well on these computer-based tests. The researcher had observed students taking the STAR Reading test (Renaissance Learning, 2009) and noticed a variance in scores in relation to classroom performance. The researcher intended, therefore, to explore variables that may affect the performance of students on a computer-based reading assessment. The researcher tested two different technology-related variables as students took a summative exam, the STAR Reading test. The purpose of this study was to explore how changes in visual stimuli affected the process of reading and student reading behavior. This quantitative study sought to ascertain whether changing the computer read-out to a black screen with white lettering made a difference in student engagement and comprehension among students in grades two through six during a computer-based adaptive test. The research site was one K-6 elementary school in a large suburban school district. The participants of the study were 316 children in grades two through six. One hundred and sixteen students were randomly sampled for student engagement data analysis. The researcher conducted a stratified random process to further select data for analysis. Students were exposed to both color display background variables throughout the study process. Teacher observers collected tallies on student engagement behaviors during the test-taking process. The researcher calculated the mean level of student engagement on each of five observed behaviors. The researcher also collected reading comprehension data for five subsequent benchmark sessions throughout the year. The engagement results of this study failed to support the hypothesis, which stated that elementary student behaviors during testing would verify a measureable difference in engagement when either a black or white display screen was presented. The results of the reading comprehension test also failed to support the hypothesis, which stated that there would be a measureable difference in elementary students' scores while taking computer-based tests when the computer screen was set to either black or white background. (PsycInfo Database Record (c) 2022 APA, all rights reserved)
KW  - computer-based standardized tests
KW  - observed behaviors
KW  - Academic Achievement
KW  - Computers
KW  - Elementary School Students
KW  - Literacy
KW  - Reading
KW  - Standardized Tests
KW  - Student Engagement
UR  - https://search.ebscohost.com/login.aspx?direct=true&db=psyh&AN=2015-99030-132&lang=de&site=ehost-live
DP  - EBSCOhost
DB  - APA PsycInfo
ER  - 

TY  - THES
ID  - 2011-99130-433
AN  - 2011-99130-433
AU  - Zhang, Zhonghua
T1  - Comparison of different equating methods and an application to link testlet-based tests
JF  - Dissertation Abstracts International Section A: Humanities and Social Sciences
JO  - Dissertation Abstracts International Section A: Humanities and Social Sciences
Y1  - 2011///
VL  - 72
IS  - 1-A
SP  - 167
EP  - 167
PB  - ProQuest Information & Learning
SN  - 0419-4209
SN  - 978-1-124-36038-6
N1  - Accession Number: 2011-99130-433. Other Journal Title: Dissertation Abstracts International. Partial author list: First Author & Affiliation: Zhang, Zhonghua; The Chinese U Hong Kong, Hong Kong. Release Date: 20111010. Publication Type: Dissertation Abstract (0400). Format Covered: Electronic. Document Type: Dissertation. Dissertation Number: AAI3436611. ISBN: 978-1-124-36038-6. Language: EnglishMajor Descriptor: Models; Psychometrics; Score Equating. Minor Descriptor: Treatment Effectiveness Evaluation. Classification: Psychometrics & Statistics & Methodology (2200). Population: Human (10). Methodology: Empirical Study; Quantitative Study. Page Count: 1. 
AB  - Test equating allows direct comparison of test scores from alternative forms measuring the same construct by employing equating procedures to put the test scores on the same metric. Three equating procedures are commonly used in the literature including the concurrent calibration method, the linking separate calibration methods (e.g. the moment methods and the characteristic curve methods), and FPC (Fixed Parameter Calibration) method. The first two types of methods for traditional IRT model have been well developed. FPC is being emphasized recently because of its utility for constructing item bank and computerized adaptive testing (CAT). However, there are few studies that examine the equating accuracy of the FPC method compared to that of the linking separate calibration method and the concurrent calibration method. The equating methods for the traditional IRT model are not appropriate for linking testlet-based tests because the local independence assumption of IRT model cannot be held for this type of tests. Some measurement models, such as testlet response model, bi-factor model, and Rasch testlet model, were advanced to calibrate the models for the testlet-based tests. Few equating methods, however, that take into consideration the additional local dependence among the examinees' responses to items within testlets have been developed for linking testelet-based tests. To address the need to better understand the FPC method and to develop new equating methods for linking testlet-based tests, the studies were to compare the effectiveness of the three types of equating methods under different linking situations and to develop equating methods for linking testlet-based tests. Besides the equating methods concerned, other factors, including sample size, ability distribution, and characteristics of common items and testlets that might affect equating results were also considered. Three simulation studies were carried out to accomplish the research purposes. The first study compared the equating accuracies of the FPC, the linking separate calibration, and the concurrent calibration method based on the IRT model to equate item parameters under different conditions. The results indicated that the FPC method using BILOG-MG performed as well as the linking separate calibration method and the concurrent calibration method for linking the equivalent groups. However, the FPC method produced larger equating errors than the other two methods did when the ability distributions of the base and target groups were substantially nonequivalent. Differences in difficulties between the common items set and the total test did not substantially affect the equating results with the three methods, with other conditions being held equal. As expected, both small sample size and less number of common items led to slight greater equating errors. The second study developed an item characteristic curve method and a testlet characteristic curve method for the testlet response model to transform the scale of item parameters. It then compared the effectiveness of the characteristic curve methods and the concurrent calibration methods under different conditions in linking item parameters from alternate test forms which were composed of dichotomously scored testlet-based items. The newly developed item characteristic curve method and the testlet characteristic curve method were shown to perform similarly as or even better than the Stocking-Lord test characteristic curve method and the concurrent calibration method did. Ignoring the local dependence in model calibration substantially increased equating errors. And larger testlet variances for the common testlets led to greater equating errors. The last study used the concurrent calibration method under the multidimensional Rasch testlet model to link the testlet-based tests in which the testlets were composed of dichotomous, polytomous, and mixed-format items. (PsycINFO Database Record (c) 2016 APA, all rights reserved)
KW  - different equating methods
KW  - application
KW  - link testlet-based tests
KW  - Models
KW  - Psychometrics
KW  - Score Equating
KW  - Treatment Effectiveness Evaluation
UR  - https://search.ebscohost.com/login.aspx?direct=true&db=psyh&AN=2011-99130-433&lang=de&site=ehost-live
DP  - EBSCOhost
DB  - APA PsycInfo
ER  - 

TY  - JOUR
ID  - 2018-48353-003
AN  - 2018-48353-003
AU  - Şenel, Selma
AU  - Kutlu, Ömer
T1  - Comparison of two test methods for VIS: Paper-pencil test and CAT
JF  - European Journal of Special Needs Education
JO  - European Journal of Special Needs Education
JA  - Eur J Spec Needs Educ
Y1  - 2018/12//
VL  - 33
IS  - 5
SP  - 631
EP  - 645
PB  - Taylor & Francis
SN  - 0885-6257
SN  - 1469-591X
AD  - Şenel, Selma
N1  - Accession Number: 2018-48353-003. Partial author list: First Author & Affiliation: Şenel, Selma; Balikesir University, Computer Research and Application Center, Balikesir, Turkey. Release Date: 20190214. Correction Date: 20221128. Publication Type: Journal (0100), Peer Reviewed Journal (0110). Format Covered: Electronic. Document Type: Journal Article. Language: EnglishMajor Descriptor: Adaptive Testing; Listening Comprehension; Test Administration; Testing Methods; Vision Disorders. Minor Descriptor: Special Education Students; Computerized Assessment. Classification: Educational Measurement (2227); Special & Compensatory Education (3570). Population: Human (10); Male (30); Female (40). Location: Turkey. Age Group: Childhood (birth-12 yrs) (100); School Age (6-12 yrs) (180); Adolescence (13-17 yrs) (200). Tests & Measures: Listening Comprehension Skills of Visually Impaired Students Test. Methodology: Empirical Study; Interview; Qualitative Study; Quantitative Study. References Available: Y. Page Count: 15. Issue Publication Date: Dec, 2018. Publication History: Accepted Date: Sep 27, 2017; First Submitted Date: Jul 5, 2017. Copyright Statement: Informa UK Limited, trading as Taylor & Francis Group. 2017. 
AB  - This paper examines listening comprehension skills of visually impaired students (VIS) using computerised adaptive testing (CAT) and reader-assisted paper-pencil testing (raPPT) and student views about them. Explanatory mixed method design was used in this study. Sample is comprised of 51 VIS, in 7th and 8th grades. 9 of these students were interviewed for determining student views about tests. Results indicated that scores obtained from CAT are significantly lower than scores obtained from raPPT. Additionally, a positive and high correlation was found between scores of CAT and raPPT. This result suggests that similar ability estimations were made by CAT and raPPT. Another finding is CAT made more reliable predictions, and was completed in shorter duration using fewer items. In qualitative part, student views were gathered through interviews and content analysis revealed three themes as technical features, test features, and psychological effects. In general, students reported positive views about CAT. VIS prefer CAT due to its listening/control options, shorter test durations, clarity of reading, and fairness of test, elimination of dependency to reader. Study provides implications for test developers and test-users to consider CAT as a multi-accommodation for VIS through its advantages. (PsycInfo Database Record (c) 2022 APA, all rights reserved)
KW  - Computerised adaptive test
KW  - visually impaired students
KW  - test accommodation
KW  - listening comprehension skill
KW  - reader-assisted test
KW  - Adaptive Testing
KW  - Listening Comprehension
KW  - Test Administration
KW  - Testing Methods
KW  - Vision Disorders
KW  - Special Education Students
KW  - Computerized Assessment
UR  - https://search.ebscohost.com/login.aspx?direct=true&db=psyh&AN=2018-48353-003&lang=de&site=ehost-live
UR  - selmasenel@balikesir.edu.tr
DP  - EBSCOhost
DB  - APA PsycInfo
ER  - 

TY  - JOUR
ID  - 2014-06810-002
AN  - 2014-06810-002
AU  - Kraemer, Helena Chmura
AU  - Freedman, Robert
T1  - Computer aids for the diagnosis of anxiety and depression
JF  - The American Journal of Psychiatry
JO  - The American Journal of Psychiatry
JA  - Am J Psychiatry
Y1  - 2014/02/01/
VL  - 171
IS  - 2
SP  - 134
EP  - 136
PB  - American Psychiatric Assn
SN  - 0002-953X
SN  - 1535-7228
N1  - Accession Number: 2014-06810-002. PMID: 24500454 Other Journal Title: American Journal of Insanity. Partial author list: First Author & Affiliation: Kraemer, Helena Chmura; Department of Psychiatry and Behavioral Sciences, Stanford University, Stanford, CA, US. Release Date: 20140317. Correction Date: 20221128. Publication Type: Journal (0100), Peer Reviewed Journal (0110). Format Covered: Electronic. Document Type: Comment/Reply. Language: EnglishMajor Descriptor: Adaptive Testing; Generalized Anxiety Disorder; Major Depression; Psychometrics; Computerized Assessment. Minor Descriptor: Anxiety. Classification: Clinical Psychological Testing (2224); Anxiety Disorders (3215). Population: Human (10). Tests & Measures: Computerized Adaptive Testing-Anxiety Inventory; Computerized Adaptive Testing-Depression Inventory. References Available: Y. Page Count: 3. Issue Publication Date: Feb 1, 2014. 
AB  - Comments on an article by Robert D. Gibbons et al. (see record [rid]2014-06810-010[/rid]). To assess anxiety, Gibbons et al. developed an inventory of 431 questions from initial tests in 798 patients. They developed algorithms to select questions that would produce good agreement between answers to a limited set of questions and the answers to all the questions. The Gibbons et al. approach is a truly outstanding contribution to measurement in medicine it is novel and exciting, and it promises to improve the accuracy and cost-effectiveness of diagnosis both in clinical practice and in research. In short, computerized adaptive testing measures might well be adopted into all current research studies addressing depression and anxiety, first because they might add to the detection of crucial signals in such studies, and second because their use would document where and how best to use these measures in clinical practice. (PsycInfo Database Record (c) 2022 APA, all rights reserved)
KW  - psychometrics
KW  - Computerized Adaptive Testing–Anxiety Inventory
KW  - generalized anxiety disorder
KW  - Anxiety Disorders
KW  - Diagnosis, Computer-Assisted
KW  - Female
KW  - Humans
KW  - Male
KW  - Psychiatric Status Rating Scales
KW  - Adaptive Testing
KW  - Generalized Anxiety Disorder
KW  - Major Depression
KW  - Psychometrics
KW  - Computerized Assessment
KW  - Anxiety
DO  - 10.1176/appi.ajp.2013.13111458
UR  - https://search.ebscohost.com/login.aspx?direct=true&db=psyh&AN=2014-06810-002&lang=de&site=ehost-live
DP  - EBSCOhost
DB  - APA PsycInfo
ER  - 

TY  - JOUR
ID  - 2017-17470-001
AN  - 2017-17470-001
AU  - Martin, Andrew J.
AU  - Lazendic, Goran
T1  - Computer-adaptive testing: Implications for students’ achievement, motivation, engagement, and subjective test experience
JF  - Journal of Educational Psychology
JO  - Journal of Educational Psychology
JA  - J Educ Psychol
Y1  - 2018/01//
VL  - 110
IS  - 1
SP  - 27
EP  - 45
PB  - American Psychological Association
SN  - 0022-0663
SN  - 1939-2176
N1  - Accession Number: 2017-17470-001. Partial author list: First Author & Affiliation: Martin, Andrew J.; School of Education, University of New South Wales, NSW, Australia. Other Publishers: Warwick & York. Release Date: 20170420. Correction Date: 20210719. Publication Type: Journal (0100), Peer Reviewed Journal (0110). Format Covered: Electronic. Document Type: Journal Article. Language: EnglishMajor Descriptor: Academic Achievement; Academic Achievement Motivation; Adaptive Testing; Student Engagement. Minor Descriptor: Elementary School Students; Experiences (Events). Classification: Academic Learning & Achievement (3550); Educational Measurement (2227). Population: Human (10); Male (30); Female (40). Location: Australia. Age Group: Childhood (birth-12 yrs) (100); School Age (6-12 yrs) (180). Tests & Measures: Motivation and Engagement Scale--Short; National Assessment Program—Literacy and Numeracy--Adaptive Form. Methodology: Empirical Study; Quantitative Study. Supplemental Data: Text Internet. References Available: Y. Page Count: 19. Issue Publication Date: Jan, 2018. Publication History: First Posted Date: Apr 20, 2017; Accepted Date: Mar 13, 2017; Revised Date: Feb 27, 2017; First Submitted Date: Apr 17, 2016. Copyright Statement: American Psychological Association. 2017. 
AB  - The present study investigated the implications of computer-adaptive testing (operationalized by way of multistage adaptive testing; MAT) and 'conventional' fixed order computer testing for various test-relevant outcomes in numeracy, including achievement, test-relevant motivation and engagement, and subjective test experience. It did so among N = 12,736 Australian elementary (years 3 and 5) and secondary (years 7 and 9) school students. Multilevel modeling assessed the extent to which Level 1 (student) test condition (fixed order vs. adaptive), gender, and year group factors and Level 2 (school) socioeducational advantage, location, structure, and size factors predicted students’ test-relevant outcomes. In terms of statistically significant main effects, students in the computer-adaptive testing condition generated lower achievement error rates (i.e., higher measurement precision). Other statistically significant computer-adaptive test effects emerged as a function of year-level and gender, with positive effects of computer-adaptive testing being relatively greater for females and older students: these students achieved more highly (year 9 students), reported higher test-relevant motivation and engagement (year 9 students), and reported more positive subjective test experience (females and year 9 students). These findings (a) confirm that computer-adaptive testing yields greater achievement measurement precision, (b) suggest some positive test-relevant motivation and engagement effects from computer-adaptive testing, (c) counter claims that computer-adaptive testing reduces students’ test-relevant motivation, engagement, and subjective experience, and (d) suggest positive computer-adaptive testing effects for older students at a developmental stage when they are typically less motivated and engaged. (PsycInfo Database Record (c) 2021 APA, all rights reserved)
AB  - Educational Impact and Implications Statement—With the growth in computer-based educational assessment, there are new opportunities to tailor testing to the characteristics and needs of students. Computer-adaptive testing is one such opportunity. Computer-adaptive testing presents items or sets of items (e.g., testlets) in a way to match different students’ abilities. Based on a large-scale numeracy assessment exercise in Australia, our study showed that computer-adaptive testing assessed achievement accurately, had positive effects for key parts of students’ test motivation and engagement, and benefited adolescent students who are at an age when they are typically less motivated and engaged. Computer-adaptive testing may therefore be a means to promote more positive experiences for students as they participate in computer-based and online educational testing. (PsycInfo Database Record (c) 2021 APA, all rights reserved)
KW  - achievement
KW  - engagement
KW  - motivation
KW  - numeracy
KW  - computer-adaptive testing
KW  - Academic Achievement
KW  - Academic Achievement Motivation
KW  - Adaptive Testing
KW  - Student Engagement
KW  - Elementary School Students
KW  - Experiences (Events)
U1  - Sponsor: Australian Curriculum, Assessment, and Reporting Authority, Australia. Recipients: No recipient indicated
DO  - 10.1037/edu0000205
L3  - 10.1037/edu0000205.supp (Supplemental)
UR  - https://search.ebscohost.com/login.aspx?direct=true&db=psyh&AN=2017-17470-001&lang=de&site=ehost-live
UR  - andrew.martin@unsw.edu.au
DP  - EBSCOhost
DB  - APA PsycInfo
ER  - 

TY  - JOUR
ID  - 2016-41330-004
AN  - 2016-41330-004
AU  - Gibbons, Robert D.
AU  - Weiss, David J.
AU  - Frank, Ellen
AU  - Kupfer, David
T1  - Computerized adaptive diagnosis and testing of mental health disorders
JF  - Annual Review of Clinical Psychology
JO  - Annual Review of Clinical Psychology
JA  - Annu Rev Clin Psychol
Y1  - 2016///
VL  - 12
SP  - 83
EP  - 104
PB  - Annual Reviews
SN  - 1548-5943
SN  - 1548-5951
AD  - Gibbons, Robert D., Center for Health Statistics, University of Chicago, Chicago, IL, US, 60612
N1  - Accession Number: 2016-41330-004. PMID: 26651865 Partial author list: First Author & Affiliation: Gibbons, Robert D.; Center for Health Statistics, University of Chicago, Chicago, IL, US. Release Date: 20170601. Correction Date: 20190211. Publication Type: Journal (0100), Peer Reviewed Journal (0110). Format Covered: Electronic. Document Type: Journal Article. Language: EnglishMajor Descriptor: Computer Assisted Diagnosis; Mental Disorders; Screening; Differential Item Functioning. Minor Descriptor: Anxiety; Tomography. Classification: Psychological Disorders (3210). Population: Human (10); Male (30); Female (40); Outpatient (60). Location: US. Age Group: Adulthood (18 yrs & older) (300); Young Adulthood (18-29 yrs) (320); Thirties (30-39 yrs) (340); Middle Age (40-64 yrs) (360); Aged (65 yrs & older) (380). Tests & Measures: CAT-Depression Inventory; Hamilton Rating Scale for Depression DOI: 10.1037/t04100-000; Center for Epidemiologic Studies Depression Scale. Methodology: Empirical Study; Quantitative Study. References Available: Y. Page Count: 22. Issue Publication Date: 2016. Publication History: First Posted Date: Nov 20, 2015. Copyright Statement: All rights reserved. Annual Reviews. 2016. 
AB  - In this review we explore recent developments in computerized adaptive diagnostic screening and computerized adaptive testing for the presence and severity of mental health disorders such as depression, anxiety, and mania. The statistical methodology is unique in that it is based on multidimensional item response theory (severity) and random forests (diagnosis) instead of traditional mental health measurement based on classical test theory (a simple total score) or unidimensional item response theory. We show that the information contained in large item banks consisting of hundreds of symptom items can be efficiently calibrated using multidimensional item response theory, and the information contained in these large item banks can be precisely extracted using adaptive administration of a small set of items for each individual. In terms of diagnosis, computerized adaptive diagnostic screening can accurately track an hour-long face-to-face clinician diagnostic interview for major depressive disorder (as an example) in less than a minute using an average of four questions with unprecedented high sensitivity and specificity. Directions for future research and applications are discussed. (PsycINFO Database Record (c) 2019 APA, all rights reserved)
KW  - computerized adaptive testing
KW  - item response theory
KW  - depression
KW  - anxiety
KW  - mania
KW  - differential item functioning
KW  - mental health measurement
KW  - mental health diagnosis
KW  - IRT
KW  - CAT
KW  - Diagnosis, Computer-Assisted
KW  - Humans
KW  - Mental Disorders
KW  - Models, Statistical
KW  - Computer Assisted Diagnosis
KW  - Mental Disorders
KW  - Screening
KW  - Differential Item Functioning
KW  - Anxiety
KW  - Tomography
U1  - Sponsor: National Institute of Mental Health, US. Grant: R01-MH66302. Recipients: No recipient indicated
DO  - 10.1146/annurev-clinpsy-021815-093634
UR  - https://search.ebscohost.com/login.aspx?direct=true&db=psyh&AN=2016-41330-004&lang=de&site=ehost-live
UR  - rdg@uchicago.edu
DP  - EBSCOhost
DB  - APA PsycInfo
ER  - 

TY  - THES
ID  - 2007-99007-004
AN  - 2007-99007-004
AU  - Chang, Shu-Ren
T1  - Computerized adaptive test item response times for correct and incorrect pretest and operational items: Testing fairness and test-taking strategies
JF  - Dissertation Abstracts International Section A: Humanities and Social Sciences
JO  - Dissertation Abstracts International Section A: Humanities and Social Sciences
Y1  - 2007///
VL  - 67
IS  - 10-A
SP  - 3792
EP  - 3792
PB  - ProQuest Information & Learning
SN  - 0419-4209
N1  - Accession Number: 2007-99007-004. Other Journal Title: Dissertation Abstracts International. Partial author list: First Author & Affiliation: Chang, Shu-Ren; U Nebraska - Lincoln, US. Release Date: 20070716. Publication Type: Dissertation Abstract (0400). Format Covered: Electronic. Document Type: Dissertation. Dissertation Number: AAI3239362. Language: EnglishMajor Descriptor: Ability Level; Adaptive Testing; Test Taking; Testing; Fairness. Minor Descriptor: Motivation; Pretesting. Classification: Psychometrics & Statistics & Methodology (2200). Population: Human (10). Methodology: Empirical Study; Quantitative Study. Page Count: 1. 
AB  - This study examined the amount of time spent on pretest and operational items that are answered correctly and incorrectly by different ability examinees when taking a computerized adaptive test (CAT). Consistent with other research, the results indicate that higher ability examinees spend more time than lower ability examinees on all items, regardless of whether the items are tailored to their ability level (operational items) or not (pretest items). It was expected, based on the literature, that examinees would spend more time on test questions they answered incorrectly than correctly. This was the case for most of the items, but not the ones near the end of the CAT, suggesting that examinees may not have sufficient time or motivation to devote to these items. Seven testing fairness issues related to a CAT were addressed along with recommendations. Five test-taking strategies derived from the results were provided to help ensure testing fairness. The presence of pretesting was found either to benefit or hinder examinees depending on their ability levels due to disproportionate amounts of time and effort used by these examinees with different ability levels on these items. Generally, examinees with lower ability levels have more time remaining after they finish the test than do examinees with higher ability levels. A design feature for a CAT that uses a time management wizard, time reminder, and pacing guidance to help examines to move through a test was proposed for future development. Finally, several recommendations to ensure testing fairness were provided for the future examiners' manual. (PsycINFO Database Record (c) 2016 APA, all rights reserved)
KW  - item response
KW  - test taking strategies
KW  - operational items
KW  - testing fairness
KW  - computerized adaptive test
KW  - motivation
KW  - pretest items
KW  - Ability Level
KW  - Adaptive Testing
KW  - Test Taking
KW  - Testing
KW  - Fairness
KW  - Motivation
KW  - Pretesting
UR  - https://search.ebscohost.com/login.aspx?direct=true&db=psyh&AN=2007-99007-004&lang=de&site=ehost-live
DP  - EBSCOhost
DB  - APA PsycInfo
ER  - 

TY  - THES
ID  - 2007-99001-021
AN  - 2007-99001-021
AU  - Marszalek, Jacob
T1  - Computerized adaptive testing and the experience of flow in examinees
JF  - Dissertation Abstracts International Section A: Humanities and Social Sciences
JO  - Dissertation Abstracts International Section A: Humanities and Social Sciences
Y1  - 2007///
VL  - 67
IS  - 7-A
SP  - 2465
EP  - 2465
PB  - ProQuest Information & Learning
SN  - 0419-4209
N1  - Accession Number: 2007-99001-021. Other Journal Title: Dissertation Abstracts International. Partial author list: First Author & Affiliation: Marszalek, Jacob; U Illinois At Urbana-Champaign, US. Release Date: 20070507. Correction Date: 20221128. Publication Type: Dissertation Abstract (0400). Format Covered: Electronic. Document Type: Dissertation. Language: EnglishMajor Descriptor: Academic Achievement; Motivation; Student Attitudes; Computerized Assessment. Classification: Educational & School Psychology (3500). Population: Human (10); Male (30); Female (40). Age Group: Childhood (birth-12 yrs) (100); School Age (6-12 yrs) (180); Adolescence (13-17 yrs) (200). Methodology: Empirical Study; Quantitative Study. Page Count: 1. 
AB  - In recent years, comparisons between computerized adaptive tests (CAT) and paper-and-pencil tests (P&Ps) have focused less on equivalency and more on the construct validity of shifting between the modalities. Examinee motivation is one area of concern, including test and computer anxiety. One type of motivation, flow, has a characteristic pattern of development similar to the pattern of item selection in CAT. Flow theory posits that positive reinforcement is gained through the sensation of achieving optimal performance. This study attempts to determine whether CAT facilitates flow in examinees, enabling them to perform better than on P&Ps. Covariates like test and computer anxiety, academic achievement, and certain personality traits are taken into account. A quasi-experimental design was used to compare 94 middle-school CAT examinees to 65 middle-school P&P examinees on their responses to the Flow State Scale 2 (FSS-2), and regression modeling revealed a significant interaction between test modality and test anxiety on state flow, and significant effects for trait flow, typical study time, gender, and ethnicity when controlling for other covariates. Significant effects on specific flow dimensions were also found. The effect of flow on test performance was examined, but may have been limited by sample size. Reliability and dimensionality was confirmed for several instruments, some for the first time with an adolescent population: FSS-2, the Dispositional Flow Scale 2, the State-Trait Anxiety Inventory for Children, the Test Anxiety Inventory-Short Form, and the Computer Attitude Scale. Interviews with students were also conducted to confirm the validity of measuring flow in testing situations. Implications of the results are discussed. (PsycInfo Database Record (c) 2022 APA, all rights reserved)
KW  - computerized adaptive tests
KW  - examinees
KW  - paper & pencil tests
KW  - motivation
KW  - academic performance
KW  - Academic Achievement
KW  - Motivation
KW  - Student Attitudes
KW  - Computerized Assessment
UR  - https://search.ebscohost.com/login.aspx?direct=true&db=psyh&AN=2007-99001-021&lang=de&site=ehost-live
DP  - EBSCOhost
DB  - APA PsycInfo
ER  - 

TY  - JOUR
ID  - 2000-05894-007
AN  - 2000-05894-007
AU  - Eggen, T. J. H. M.
AU  - Straetmans, G. J. J. M.
T1  - Computerized adaptive testing for classifying examinees into three categories
JF  - Educational and Psychological Measurement
JO  - Educational and Psychological Measurement
JA  - Educ Psychol Meas
Y1  - 2000/10//
VL  - 60
IS  - 5
SP  - 713
EP  - 734
PB  - Sage Publications
SN  - 0013-1644
SN  - 1552-3888
N1  - Accession Number: 2000-05894-007. Partial author list: First Author & Affiliation: Eggen, T. J. H. M.; Cito, Netherlands. Release Date: 20001101. Correction Date: 20190211. Publication Type: Journal (0100), Peer Reviewed Journal (0110). Format Covered: Print. Document Type: Journal Article. Language: EnglishMajor Descriptor: Taxonomies; Computerized Assessment. Classification: Tests & Testing (2220); Artificial Intelligence & Expert Systems (4120). Page Count: 22. Issue Publication Date: Oct, 2000. 
AB  - Explored the possibilities for using computerized adaptive testing in situations in which examinees are to be classified into 1 of 3 categories. Testing algorithms with 2 different statistical computation procedures are described and evaluated. The 1st computation procedure is based on statistical testing and the other on statistical estimation. Item selection methods based on maximum information (MI) considering content and exposure control are considered. The measurement quality of the proposed testing algorithms is reported. The results of the study are that a reduction of at least 22% in the mean number of items can be expected in a computerized adaptive test compared to an existing paper-and-pencil placement test. Furthermore, the results suggest that statistical testing is a promising alternative to statistical estimation. Finally, it is concluded that imposing constraints on the MI selection strategy does not negatively affect the quality of the testing algorithms. (PsycINFO Database Record (c) 2019 APA, all rights reserved)
KW  - classifying examinees into 3 categories using computerized adaptive testing
KW  - Taxonomies
KW  - Computerized Assessment
DO  - 10.1177/00131640021970862
UR  - https://search.ebscohost.com/login.aspx?direct=true&db=psyh&AN=2000-05894-007&lang=de&site=ehost-live
DP  - EBSCOhost
DB  - APA PsycInfo
ER  - 

TY  - JOUR
ID  - 2007-13340-003
AN  - 2007-13340-003
AU  - Hol, A. Michiel
AU  - Vorst, Harrie C. M.
AU  - Mellenbergh, Gideon J.
T1  - Computerized adaptive testing for polytomous motivation items: Administration mode effects and a comparison with short forms
JF  - Applied Psychological Measurement
JO  - Applied Psychological Measurement
JA  - Appl Psychol Meas
Y1  - 2007/09//
VL  - 31
IS  - 5
SP  - 412
EP  - 429
PB  - Sage Publications
SN  - 0146-6216
SN  - 1552-3497
AD  - Hol, A. Michiel, NOA, Test Research and Development, De Boelelaan 7, 1083 HJ, Amsterdam, Netherlands
N1  - Accession Number: 2007-13340-003. Partial author list: First Author & Affiliation: Hol, A. Michiel; University of Amsterdam, Amsterdam, Netherlands. Release Date: 20071015. Correction Date: 20200713. Publication Type: Journal (0100), Peer Reviewed Journal (0110). Format Covered: Electronic. Document Type: Journal Article. Language: EnglishMajor Descriptor: Adaptive Testing; Attitude Measures; Motivation; Statistical Validity; Computerized Assessment. Minor Descriptor: Test Administration; Test Forms; Test Items. Classification: Tests & Testing (2220). Population: Human (10); Male (30); Female (40). Location: Netherlands. Age Group: Adulthood (18 yrs & older) (300). Tests & Measures: Study Questionnaire (SQ)-Motivation subscale (MS). Methodology: Empirical Study; Quantitative Study. References Available: Y. Page Count: 18. Issue Publication Date: Sep, 2007. 
AB  - In a randomized experiment (n=515), a computerized and a computerized adaptive test (CAT) are compared. The item pool consists of 24 polytomous motivation items. Although items are carefully selected, calibration data show that Samejima's graded response model did not fit the data optimally. A simulation study is done to assess possible consequences of model misfit. CAT efficiency was studied by a systematic comparison of the CAT with two types of conventional fixed length short forms, which are created to be good CAT competitors. Results showed no essential administration mode effects. Efficiency analyses show that CAT outperformed the short forms in almost all aspects when results are aggregated along the latent trait scale. The real and the simulated data results are very similar, which indicate that the real data results are not affected by model misfit. (PsycInfo Database Record (c) 2020 APA, all rights reserved)
KW  - computer adaptive testing
KW  - attitude measurement
KW  - polytomous motivation items
KW  - test administration
KW  - statistical validity
KW  - Adaptive Testing
KW  - Attitude Measures
KW  - Motivation
KW  - Statistical Validity
KW  - Computerized Assessment
KW  - Test Administration
KW  - Test Forms
KW  - Test Items
U1  - Sponsor: Netherlands Organization for Scientific Research, Netherlands. Recipients: No recipient indicated
DO  - 10.1177/0146621606297314
UR  - https://search.ebscohost.com/login.aspx?direct=true&db=psyh&AN=2007-13340-003&lang=de&site=ehost-live
UR  - m.hol@noa-vu.nl
DP  - EBSCOhost
DB  - APA PsycInfo
ER  - 

TY  - JOUR
ID  - 2003-03945-004
AN  - 2003-03945-004
AU  - Cheng, Philip E.
AU  - Liou, Michelle
T1  - Computerized adaptive testing using the nearest-neighbors criterion
JF  - Applied Psychological Measurement
JO  - Applied Psychological Measurement
JA  - Appl Psychol Meas
Y1  - 2003/05//
VL  - 27
IS  - 3
SP  - 204
EP  - 216
PB  - Sage Publications
SN  - 0146-6216
SN  - 1552-3497
AD  - Liou, Michelle, Inst of Statistical Science, Academia Sinica, Taipei, Taiwan, 115
N1  - Accession Number: 2003-03945-004. Partial author list: First Author & Affiliation: Cheng, Philip E.; Academia Sinica, Inst of statistical Science, Taipei, Taiwan. Release Date: 20030526. Correction Date: 20190211. Publication Type: Journal (0100), Peer Reviewed Journal (0110). Format Covered: Print. Document Type: Journal Article. Language: EnglishMajor Descriptor: Adaptive Testing; Item Response Theory; Statistical Analysis; Statistical Tests; Computerized Assessment. Minor Descriptor: Item Analysis (Statistical); Statistical Estimation. Classification: Statistics & Mathematics (2240). Population: Human (10). References Available: Y. Page Count: 13. Issue Publication Date: May, 2003. 
AB  - Item selection procedures designed for computerized adaptive testing need to accurately estimate every taker's trait level (θ) and, at the same time, effectively use all items in a bank. Empirical studies showed that classical item selection procedures based on maximizing Fisher or other related information yielded highly varied item exposure rates; with these procedures, some items were frequently used whereas others were rarely selected. In the literature, methods have been proposed for controlling exposure rates; they tend to affect the accuracy in θ estimates, however. A modified version of the maximum Fisher information (MFI) criterion, coined the nearest neighbors (NN) criterion, is proposed in this study. The NN procedure improves to a moderate extent the undesirable item exposure rates associated with the MFI criterion and keeps sufficient precision in estimates. The NN criterion will be compared with a few other existing methods in an empirical study using the mean squared errors in θ estimates and plots of item exposure rates associated with different distributions. (PsycINFO Database Record (c) 2019 APA, all rights reserved)
KW  - computerized adaptive testing
KW  - nearest-neighbors criterion
KW  - item selection procedures
KW  - maximum Fisher information criterion
KW  - estimates
KW  - Adaptive Testing
KW  - Item Response Theory
KW  - Statistical Analysis
KW  - Statistical Tests
KW  - Computerized Assessment
KW  - Item Analysis (Statistical)
KW  - Statistical Estimation
DO  - 10.1177/0146621603027003002
UR  - https://search.ebscohost.com/login.aspx?direct=true&db=psyh&AN=2003-03945-004&lang=de&site=ehost-live
UR  - mliou@stat.sinica.edu.tw
DP  - EBSCOhost
DB  - APA PsycInfo
ER  - 

TY  - JOUR
ID  - 1992-44811-001
AN  - 1992-44811-001
AU  - Legg, Sue M.
AU  - Buhr, Dianne C.
T1  - Computerized adaptive testing with different groups
JF  - Educational Measurement: Issues and Practice
JO  - Educational Measurement: Issues and Practice
Y1  - 1992///Sum 1992
VL  - 11
IS  - 2
SP  - 23
EP  - 27
PB  - Blackwell Publishing
SN  - 0731-1745
SN  - 1745-3992
N1  - Accession Number: 1992-44811-001. Partial author list: First Author & Affiliation: Legg, Sue M.; U Florida Office of Instructional Resources, Gainesville, US. Other Publishers: Wiley-Blackwell Publishing Ltd. Release Date: 19921201. Correction Date: 20190211. Publication Type: Journal (0100), Peer Reviewed Journal (0110). Format Covered: Print. Document Type: Journal Article. Language: EnglishMajor Descriptor: Age Differences; Cognitive Ability; Human Sex Differences; Racial and Ethnic Differences; Computerized Assessment. Minor Descriptor: College Students; Experience Level; Digital Literacy. Classification: Curriculum & Programs & Teaching Methods (3530). Population: Human (10). Age Group: Adolescence (13-17 yrs) (200); Adulthood (18 yrs & older) (300). Methodology: Empirical Study. Page Count: 5. Issue Publication Date: Sum 1992. 
AB  - Determined how computerized adaptive testing (CMAT) affected 628 college and university examinees (aged 17–55 yrs) and whether examinees of different ethnic, gender, ability, age, and computer-experienced groups were differentially affected. Ss were 57.8% female, 42.2% male, 63.5% White, 9.7% Black, 20.7% Hispanic, and 3.5% Oriental. 30.5% of the Ss reported frequent use of a computer, 45.6% indicated occasional use, 20.2% reported using a computer once or twice, and 3.7% had never used a computer. Although some differences existed between the various groups, these differences did not affect Ss' performance on the test, with 2 exceptions: reading problems on the reading test and computer anxiety. A feeling of anxiety in the CMAT was related to (1) Ss' prior experience with computers, (2) ability, and (3) ethnic and gender group. (PsycINFO Database Record (c) 2019 APA, all rights reserved)
KW  - ethnicity & sex & ability & age & computer experience
KW  - computerized adaptive testing
KW  - 17–55 yr old college examinees
KW  - Age Differences
KW  - Cognitive Ability
KW  - Human Sex Differences
KW  - Racial and Ethnic Differences
KW  - Computerized Assessment
KW  - College Students
KW  - Experience Level
KW  - Digital Literacy
DO  - 10.1111/j.1745-3992.1992.tb00237.x
UR  - https://search.ebscohost.com/login.aspx?direct=true&db=psyh&AN=1992-44811-001&lang=de&site=ehost-live
DP  - EBSCOhost
DB  - APA PsycInfo
ER  - 

TY  - JOUR
ID  - 2010-18436-009
AN  - 2010-18436-009
AU  - Fritts, Barbara E.
AU  - Marszalek, Jacob M.
T1  - Computerized adaptive testing, anxiety levels, and gender differences
JF  - Social Psychology of Education: An International Journal
JO  - Social Psychology of Education: An International Journal
JA  - Soc Psychol Educ
Y1  - 2010/09//
VL  - 13
IS  - 3
SP  - 441
EP  - 458
PB  - Springer
SN  - 1381-2890
SN  - 1573-1928
AD  - Fritts, Barbara E., Division of Counseling and Educational Psychology, University of Missouri-Kansas City, 215 Education Building, 5100 Rockhill Road, Kansas City, MO, US, 64110
N1  - Accession Number: 2010-18436-009. Partial author list: First Author & Affiliation: Fritts, Barbara E.; Division of Counseling and Educational Psychology, University of Missouri-Kansas City, Kansas City, MO, US. Release Date: 20101115. Correction Date: 20221128. Publication Type: Journal (0100), Peer Reviewed Journal (0110). Format Covered: Electronic. Document Type: Journal Article. Language: EnglishMajor Descriptor: Academic Achievement; Computerized Assessment. Minor Descriptor: Human Sex Differences; Middle School Students; Test Anxiety. Classification: Educational & School Psychology (3500). Population: Human (10); Male (30); Female (40). Location: US. Age Group: Adolescence (13-17 yrs) (200). Tests & Measures: Test Anxiety Inventory—Short Form; State-Trait Anxiety Inventory for Children DOI: 10.1037/t06497-000; Computer Attitude Scale. Methodology: Empirical Study; Quantitative Study. References Available: Y. Page Count: 18. Issue Publication Date: Sep, 2010. Publication History: First Posted Date: Feb 21, 2010; Accepted Date: Jan 26, 2010; First Submitted Date: May 19, 2009. Copyright Statement: Springer Science+Business Media B.V. 2010. 
AB  - This study compares the amount of test anxiety experienced on a computerized adaptive test (CAT) to a paper-and-pencil test (P&P), as well as the state test anxiety experienced between males and females. Ninety-four middle school CAT examinees were compared to 65 middle school P&P examinees on their responses to the State-Trait Anxiety Inventory for Children (STAIC) after taking a standardized achievement test. Results of a multiple regression showed that P&P examinees had a higher mean STAIC score than CAT examinees after controlling for trait test anxiety and computer anxiety. Evidence of neither a main nor a moderator effect of gender was found. However, a subsequent path analysis gave evidence of an indirect effect of gender on STAIC score mediated by trait test anxiety. Results are discussed in the context of stereotype threat and the implications for the use of CAT in schools, given the digital divide between race and socioeconomic status. Recommendations for future research and practice are offered. (PsycInfo Database Record (c) 2022 APA, all rights reserved)
KW  - computerized adaptive testing
KW  - anxiety level
KW  - gender differences
KW  - middle school students
KW  - academic achievement
KW  - Academic Achievement
KW  - Computerized Assessment
KW  - Human Sex Differences
KW  - Middle School Students
KW  - Test Anxiety
DO  - 10.1007/s11218-010-9113-3
UR  - https://search.ebscohost.com/login.aspx?direct=true&db=psyh&AN=2010-18436-009&lang=de&site=ehost-live
UR  - ORCID: 0000-0002-4319-2319
UR  - Bef8t7@umkc.edu
DP  - EBSCOhost
DB  - APA PsycInfo
ER  - 

TY  - JOUR
ID  - 1996-03359-003
AN  - 1996-03359-003
AU  - Lunz, Mary E.
AU  - Bergstrom, Betty
T1  - Computerized adaptive testing: Tracking candidate response patterns
JF  - Journal of Educational Computing Research
JO  - Journal of Educational Computing Research
Y1  - 1995///
VL  - 13
IS  - 2
SP  - 151
EP  - 162
PB  - Baywood Publishing
SN  - 0735-6331
SN  - 1541-4140
N1  - Accession Number: 1996-03359-003. Partial author list: First Author & Affiliation: Lunz, Mary E.; American Society of Clinical Pathologists, Board of Registry, Chicago, IL, US. Other Publishers: Sage Publications. Release Date: 19960101. Correction Date: 20190211. Publication Type: Journal (0100), Peer Reviewed Journal (0110). Format Covered: Print. Document Type: Journal Article. Language: EnglishMajor Descriptor: Science Achievement; Test Taking; Computerized Assessment. Minor Descriptor: Students. Classification: Academic Learning & Achievement (3550); Educational Measurement (2227). Population: Human (10). Age Group: Adulthood (18 yrs & older) (300). Methodology: Empirical Study. Page Count: 12. Issue Publication Date: 1995. 
AB  - Tracked the effect of candidate response patterns on a computerized adaptive test. Data were from a certification examination in laboratory science administered in 1992 to 155 candidates, using a computerized adaptive algorithm. The 90-item certification examination was divided into 9 units of 10 items each to track the pattern of initial responses and response alterations on ability estimates and test precision across the 9 test units. The precision of the test was affected most by response alterations during early segments of the test. While candidates generally benefited from altering responses, individual candidates showed different patterns of response alterations across test segments. Test precision was minimally affected, suggesting that the tailoring of computerized adaptive testing is minimally affected by response alterations. (PsycINFO Database Record (c) 2019 APA, all rights reserved)
KW  - initial responses & response alterations
KW  - precision of computerized adaptive science test
KW  - students
KW  - Science Achievement
KW  - Test Taking
KW  - Computerized Assessment
KW  - Students
DO  - 10.2190/GNB7-5HQ0-GD8M-P0B2
UR  - https://search.ebscohost.com/login.aspx?direct=true&db=psyh&AN=1996-03359-003&lang=de&site=ehost-live
DP  - EBSCOhost
DB  - APA PsycInfo
ER  - 

TY  - JOUR
ID  - 2020-75129-001
AN  - 2020-75129-001
AU  - Gibbons, Robert D.
AU  - Kupfer, David J.
AU  - Frank, Ellen
AU  - Lahey, Benjamin B.
AU  - George-Milford, Brandie A.
AU  - Biernesser, Candice L.
AU  - Porta, Giovanna
AU  - Moore, Tara L.
AU  - Kim, Jong Bae
AU  - Brent, David A.
T1  - Computerized adaptive tests for rapid and accurate assessment of psychopathology dimensions in youth
JF  - Journal of the American Academy of Child & Adolescent Psychiatry
JO  - Journal of the American Academy of Child & Adolescent Psychiatry
JA  - J Am Acad Child Adolesc Psychiatry
Y1  - 2020/11//
VL  - 59
IS  - 11
SP  - 1264
EP  - 1273
PB  - Elsevier Science
SN  - 0890-8567
SN  - 1527-5418
AD  - Gibbons, Robert D., University of Chicago, 5841 S. Maryland Avenue, Room W260, MC2000, Chicago, IL, US, 60637
N1  - Accession Number: 2020-75129-001. PMID: 31465832 Other Journal Title: Journal of the American Academy of Child Psychiatry. Partial author list: First Author & Affiliation: Gibbons, Robert D.; University of Chicago, Chicago, IL, US. Other Publishers: Lippincott Williams & Wilkins. Release Date: 20201008. Correction Date: 20201221. Publication Type: Journal (0100), Peer Reviewed Journal (0110). Format Covered: Electronic. Document Type: Journal Article. Language: EnglishMajor Descriptor: Adaptive Testing; Child Psychopathology; Mental Disorders; Computerized Assessment; Test-Retest Reliability. Minor Descriptor: Adolescent Psychopathology; Conduct Disorder; Convergent Validity; Discriminant Validity; Major Depression; Oppositional Defiant Disorder; Predictive Validity; Test Construction; Test Validity; Suicidality. Classification: Clinical Psychological Testing (2224); Psychological Disorders (3210). Population: Human (10); Male (30); Female (40). Location: US. Age Group: Childhood (birth-12 yrs) (100); School Age (6-12 yrs) (180); Adolescence (13-17 yrs) (200). Tests & Measures: Kiddie Schedule for Affective Disorders and Schizophrenia Present and Lifetime Version; Mood and Feelings Questionnaire; Brief Mania Rating Scale; Brief Pediatric Symptom Checklist; Kiddie Computerized Adaptive Test-Suicide Scale; Child and Adolescent Psychopathology Scale DOI: 10.1037/t20276-000; Columbia-Suicide Severity Rating Scale DOI: 10.1037/t52667-000; Children's Global Assessment Scale; Screen for Child Anxiety Related Emotional Disorders DOI: 10.1037/t03542-000. Methodology: Empirical Study; Interview; Quantitative Study. Supplemental Data: Tables and Figures Internet. Page Count: 10. Issue Publication Date: Nov, 2020. Publication History: Accepted Date: Aug 19, 2019. Copyright Statement: American Academy of Child and Adolescent Psychiatry. 2019. 
AB  - Objective: At least half of youths with mental disorders are unrecognized and untreated. Rapid, accurate assessment of child mental disorders could facilitate identification and referral and potentially reduce the occurrence of functional disability that stems from early-onset mental disorders. Method: Computerized adaptive tests (CATs) based on multidimensional item response theory were developed for depression, anxiety, mania/hypomania, attention-deficit/hyperactivity disorder, conduct disorder, oppositional defiant disorder, and suicidality, based on parent and child ratings of 1,060 items each. In phase 1, CATs were developed from 801 participants. In phase 2, predictive, discriminant, and convergent validity were tested against semi-structured research interviews for diagnoses and suicidality in 497 patients and 104 healthy controls. Overall strength of association was determined by area under the receiver operating characteristic curve (AUC). Results: The child and parent independently completed the Kiddie−Computerized Adaptive Tests (K-CATs) in a median time of 7.56 and 5.03 minutes, respectively, with an average of 7 items per domain. The K-CATs accurately captured the presence of diagnoses (AUCs from 0.83 for generalized anxiety disorder to 0.92 for major depressive disorder) and suicidal ideation (AUC = 0.996). Strong correlations with extant measures were found (r ≥ 0.60). Test−retest reliability averaged r = 0.80. Conclusion: These K-CATs provide a new approach to child psychopathology screening and measurement. Testing can be completed by child and parent in less than 8 minutes and yields results that are highly convergent with much more time-consuming structured clinical interviews and dimensional severity assessment and measurement. Testing of the implementation of the K-CAT is now indicated. (PsycInfo Database Record (c) 2020 APA, all rights reserved)
KW  - adaptive testing
KW  - multidimensional item response theory
KW  - child and adolescent psychopathology
KW  - measurement
KW  - diagnosis
KW  - Adaptive Testing
KW  - Child Psychopathology
KW  - Mental Disorders
KW  - Computerized Assessment
KW  - Test-Retest Reliability
KW  - Adolescent Psychopathology
KW  - Conduct Disorder
KW  - Convergent Validity
KW  - Discriminant Validity
KW  - Major Depression
KW  - Oppositional Defiant Disorder
KW  - Predictive Validity
KW  - Test Construction
KW  - Test Validity
KW  - Suicidality
U1  - Sponsor: National Institutes of Health, US. Grant: MH100155. Recipients: No recipient indicated
U1  - Sponsor: National Center for Advancing Translational Sciences (NCATS), US. Grant: TR001857. Recipients: No recipient indicated
DO  - 10.1016/j.jaac.2019.08.009
UR  - https://search.ebscohost.com/login.aspx?direct=true&db=psyh&AN=2020-75129-001&lang=de&site=ehost-live
UR  - ORCID: 0000-0002-6463-2280
UR  - rdg@uchicago.edu
DP  - EBSCOhost
DB  - APA PsycInfo
ER  - 

TY  - JOUR
ID  - 2019-44765-001
AN  - 2019-44765-001
AU  - Geerards, Daan
AU  - Pusic, Andrea
AU  - Hoogbergen, Maarten
AU  - van der Hulst, René
AU  - Sidey-Gibbons, Chris
T1  - Computerized quality of life assessment: A randomized experiment to determine the impact of individualized feedback on assessment experience
JF  - Journal of Medical Internet Research
JO  - Journal of Medical Internet Research
JA  - J Med Internet Res
Y1  - 2019/07/11/
VL  - 21
IS  - 7
PB  - JMIR Publications
SN  - 1439-4456
SN  - 1438-8871
AD  - Sidey-Gibbons, Chris, Patient-Reported Outcomes, Value & Experience Center, Brigham and Women's Hospital, 75 Francis St, Boston, MA, US, 02115
N1  - Accession Number: 2019-44765-001. PMID: 31298217 Partial author list: First Author & Affiliation: Geerards, Daan; Patient-Reported Outcomes, Value & Experience Center, Brigham and Women's Hospital, Boston, MA, US. Other Publishers: Gunther Eysenbach. Release Date: 20200528. Correction Date: 20201012. Publication Type: Journal (0100), Peer Reviewed Journal (0110). Format Covered: Electronic. Document Type: Journal Article. Language: EnglishMajor Descriptor: Psychometrics; Public Health; Quality of Life; Test Validity. Minor Descriptor: Feedback; Client Records; Patient Reported Outcome Measures. Classification: Health Psychology Testing (2226); Health Psychology & Medicine (3360). Population: Human (10); Male (30); Female (40). Location: United Kingdom. Age Group: Adulthood (18 yrs & older) (300); Young Adulthood (18-29 yrs) (320); Thirties (30-39 yrs) (340); Middle Age (40-64 yrs) (360); Aged (65 yrs & older) (380). Tests & Measures: World Health Organization Quality of Life Scale. Methodology: Clinical Trial; Empirical Study; Quantitative Study. ArtID: e12212. Issue Publication Date: Jul 11, 2019. Publication History: First Posted Date: Jul 11, 2019; Accepted Date: May 2, 2019; Revised Date: Mar 28, 2019; First Submitted Date: Sep 14, 2018. Copyright Statement: Originally published in the Journal of Medical Internet Research (http://www.jmir.org), 11.07.2019. This is an open-access article distributed under the terms of the Creative Commons Attribution License (https://creativecommons.org/licenses/by/4.0/), which permits unrestricted use, distribution, and reproduction in any medium, provided the original work, first published in the Journal of Medical Internet Research, is properly cited. The complete bibliographic information, a link to the original publication on http://www.jmir.org/, as well as this copyright and license information must be included. Daan Geerards, Andrea Pusic, Maarten Hoogbergen, René van der Hulst, Chris Sidey-Gibbons
AB  - Background: Quality of life (QoL) assessments, or patient-reported outcome measures (PROMs), are becoming increasingly important in health care and have been associated with improved decision making, higher satisfaction, and better outcomes of care. Some physicians and patients may find questionnaires too burdensome; however, this issue could be addressed by making use of computerized adaptive testing (CAT). In addition, making the questionnaire more interesting, for example by providing graphical and contextualized feedback, may further improve the experience of the users. However, little is known about how shorter assessments and feedback impact user experience. Objective: We conducted a controlled experiment to assess the impact of tailored multimodal feedback and CAT on user experience in QoL assessment using validated PROMs. Methods: We recruited a representative sample from the general population in the United Kingdom using the Oxford Prolific academic Web panel. Participants completed either a CAT version of the World Health Organization Quality of Life assessment (WHOQOL-CAT) or the fixed-length WHOQOL-BREF, an abbreviated version of the WHOQOL-100. We randomly assigned participants to conditions in which they would receive no feedback, graphical feedback only, or graphical and adaptive text-based feedback. Participants rated the assessment in terms of perceived acceptability, engagement, clarity, and accuracy. Results: We included 1386 participants in our analysis. Assessment experience was improved when graphical and tailored text-based feedback was provided along with PROMs (Δ = 0.22, P < .001). Providing graphical feedback alone was weakly associated with improvement in overall experience (Δ = 0.10, P = .006). Graphical and text-based feedback made the questionnaire more interesting, and users were more likely to report they would share the results with a physician or family member (Δ = 0.17, P < .001, and Δ = 0.17, P < .001, respectively). No difference was found in perceived accuracy of the graphical feedback scores of the WHOQOL-CAT and WHOQOL-BREF (Δ = 0.06, P = .05). CAT (stopping rule [SE < 0.45]) resulted in the administration of 25% fewer items than the fixed-length assessment, but it did not result in an improved user experience (P = .21). Conclusions: Using tailored text-based feedback to contextualize numeric scores maximized the acceptability of electronic QoL assessment. Improving user experience may increase response rates and reduce attrition in research and clinical use of PROMs. In this study, CAT administration was associated with a modest decrease in assessment length but did not improve user experience. Patient-perceived accuracy of feedback was equivalent when comparing CAT with fixed-length assessment. Fixed-length forms are already generally acceptable to respondents; however, CAT might have an advantage over longer questionnaires that would be considered burdensome. Further research is warranted to explore the relationship between assessment length, feedback, and response burden in diverse populations. (PsycInfo Database Record (c) 2020 APA, all rights reserved)
KW  - quality of life
KW  - outcome assessment
KW  - patient-reported outcome measures
KW  - computer-adaptive testing
KW  - WHOQOL
KW  - psychometrics
KW  - feedback
KW  - Adolescent
KW  - Adult
KW  - Aged
KW  - Computers
KW  - Feedback
KW  - Female
KW  - Humans
KW  - Male
KW  - Middle Aged
KW  - Patient Reported Outcome Measures
KW  - Psychometrics
KW  - Quality of Life
KW  - Surveys and Questionnaires
KW  - Young Adult
KW  - Psychometrics
KW  - Public Health
KW  - Quality of Life
KW  - Test Validity
KW  - Feedback
KW  - Client Records
KW  - Patient Reported Outcome Measures
U1  - Sponsor: National Institute for Health Research. Grant: CDF 2017-10-19. Recipients: No recipient indicated
DO  - 10.2196/12212
UR  - https://search.ebscohost.com/login.aspx?direct=true&db=psyh&AN=2019-44765-001&lang=de&site=ehost-live
UR  - ORCID: 0000-0002-0116-7115
UR  - drcgibbons@gmail.com
DP  - EBSCOhost
DB  - APA PsycInfo
ER  - 

TY  - JOUR
ID  - 1999-01326-003
AN  - 1999-01326-003
AU  - Vispoel, Walter P.
AU  - Coffman, Don D.
T1  - Computerized-adaptive and self-adapted music-listening tests: Psychometric features and motivational benefits
T3  - Self-adapted testing
JF  - Applied Measurement in Education
JO  - Applied Measurement in Education
Y1  - 1994///
VL  - 7
IS  - 1
SP  - 25
EP  - 51
PB  - Lawrence Erlbaum
SN  - 0895-7347
SN  - 1532-4818
N1  - Accession Number: 1999-01326-003. Partial author list: First Author & Affiliation: Vispoel, Walter P.; U Iowa, Coll of Education, Iowa City, IA, US. Other Publishers: Taylor & Francis. Release Date: 19991201. Correction Date: 20190211. Publication Type: Journal (0100), Peer Reviewed Journal (0110). Format Covered: Print. Document Type: Journal Article. Language: EnglishMajor Descriptor: Adaptive Testing; Motivation; Music Education; Psychometrics; Computerized Assessment. Minor Descriptor: Aptitude Measures; Junior High School Students; Music Perception; Test Anxiety; Test Reliability; Test Taking; Test Validity. Classification: Educational Measurement (2227); Academic Learning & Achievement (3550). Population: Human (10); Male (30); Female (40). Location: US. Age Group: Adolescence (13-17 yrs) (200). Methodology: Empirical Study. References Available: Y. Page Count: 27. Issue Publication Date: 1994. 
AB  - Compared the efficiency, reliability, validity, and motivational benefits of computerized adaptive tests and self-adapted music listening tests (CAT and SAT, respectively). 53 junior high school music students (mean age 13.04 yrs) completed a tonal memory CAT, a tonal memory SAT, standardized music aptitude and achievement tests; and questionnaires assessing test anxiety, demographics, and attitudes about the CAT and SAT. Standardized music test scores and music course grades served as criterion measures in the concurrent validity analysis. Results showed that the SAT elicited more favorable attitudes from examinees and yielded ability estimates that were higher and less correlated with test anxiety than did the CAT. The CAT, however, required fewer items and less administration time to match the reliability and concurrent validity of the SAT and yielded higher levels of reliability and concurrent validity than the SAT when test length was held constant. Implications for future research are discussed. (PsycINFO Database Record (c) 2019 APA, all rights reserved)
KW  - self-adapted vs computer-adaptive music listening tests
KW  - test anxiety & attitudes & efficiency & reliability & validity & motivational benefits
KW  - junior high school students
KW  - Adaptive Testing
KW  - Motivation
KW  - Music Education
KW  - Psychometrics
KW  - Computerized Assessment
KW  - Aptitude Measures
KW  - Junior High School Students
KW  - Music Perception
KW  - Test Anxiety
KW  - Test Reliability
KW  - Test Taking
KW  - Test Validity
DO  - 10.1207/s15324818ame0701_4
UR  - https://search.ebscohost.com/login.aspx?direct=true&db=psyh&AN=1999-01326-003&lang=de&site=ehost-live
DP  - EBSCOhost
DB  - APA PsycInfo
ER  - 

TY  - THES
ID  - 2001-95021-059
AN  - 2001-95021-059
AU  - Bringsjord, Elizabeth Louise
T1  - Computerized-adaptive versus paper-and-pencil testing environments:  An experimental analysis of examinee experience
JF  - Dissertation Abstracts International Section A: Humanities and Social Sciences
JO  - Dissertation Abstracts International Section A: Humanities and Social Sciences
Y1  - 2001/11//
VL  - 62
IS  - 5-A
SP  - 1717
EP  - 1717
PB  - ProQuest Information & Learning
SN  - 0419-4209
N1  - Accession Number: 2001-95021-059. Other Journal Title: Dissertation Abstracts International. Partial author list: First Author & Affiliation: Bringsjord, Elizabeth Louise; State U New York At Albany, US. Release Date: 20020417. Correction Date: 20221128. Publication Type: Dissertation Abstract (0400). Format Covered: Electronic. Document Type: Dissertation. Language: EnglishMajor Descriptor: Adaptive Testing; Cognitive Processes; Emotional Responses; Test Anxiety; Computerized Assessment. Minor Descriptor: Human Channel Capacity; Testing Methods. Classification: Educational & School Psychology (3500). Population: Human (10). Age Group: Adulthood (18 yrs & older) (300). Methodology: Empirical Study. Page Count: 1. 
AB  - Computerized-adaptive testing (CAT) is quickly becoming the test mode of choice among many high-stakes testing agencies. Although some researchers suggest that CAT is comparable, if not better, than traditional paper-and-pencil testing, others have suggested that individual differences among examinees have not been fully explored. In particular, the additional cognitive demands imposed by CAT are not well understood. For example, in the CAT mode there is the added demand of moving back and forth from computer screen to scrap paper when one has to solve more complex problems. In traditional paper and-pencil (P&P) testing environments, there is but one mode with which to contend: paper-and-pencil. In addition, CAT may generate greater anxiety-which further increases cognitive load-among some test-takers than would be found with more traditional testing modes. For example, with CAT you cannot go back to review previous items or change previous answers. And CAT requires something that is anxiety provoking in and of itself for some individuals: use of a computer. The purpose of this study was to explore the effects, particularly the cognitive effects, of different testing environments on individual test-taker experience. Forty undergraduate college students were randomly assigned to one of two testing conditions: CAT mode or P&P testing mode. All subjects took the analytical section of the Graduate Record Examination. Methodology for the study included both qualitative and quantitative methods. Data were collected using paper-and-pencil instruments, videotaped recording of subject behavior, observation, and interview. Verbal protocol analysis of subsamples of subjects' work-toward-solution behavior on two analytical reasoning problems was also included. Results of multivariate and univariate analyses suggest significant differences across test mode, with respect to cognitive demands. Compared to subjects in the paper-and-pencil test mode, CAT subjects had significantly longer response times on analytical reasoning (AR) problems, relied more on scrap paper to model AR problems diagrammatically, and employed different problem-solving strategies. Differences in affective responses-including anxiety and frustration-further suggest test mode effects were present. CAT subjects expressed significantly more frustration with the test mode than did their P&P counterparts. Furthermore, CAT subjects with low computer confidence experienced significantly higher levels of test anxiety than did their more computer-confident peers. Implications for test developers and recommendations for further research are offered. (PsycInfo Database Record (c) 2022 APA, all rights reserved)
KW  - computerized adaptive testing
KW  - paper & pencil testing
KW  - cognitive demand
KW  - emotional responses
KW  - test anxiety
KW  - Adaptive Testing
KW  - Cognitive Processes
KW  - Emotional Responses
KW  - Test Anxiety
KW  - Computerized Assessment
KW  - Human Channel Capacity
KW  - Testing Methods
UR  - https://search.ebscohost.com/login.aspx?direct=true&db=psyh&AN=2001-95021-059&lang=de&site=ehost-live
DP  - EBSCOhost
DB  - APA PsycInfo
ER  - 

TY  - JOUR
ID  - 2008-03385-003
AN  - 2008-03385-003
AU  - Donovan, Neila J.
AU  - Kendall, Diane L.
AU  - Heaton, Shelley C.
AU  - Kwon, Sooyeon
AU  - Velozo, Craig A.
AU  - Duncan, Pamela W.
T1  - Conceptualizing functional cognition in stroke
JF  - Neurorehabilitation and Neural Repair
JO  - Neurorehabilitation and Neural Repair
JA  - Neurorehabil Neural Repair
Y1  - 2008/03//Mar-Apr, 2008
VL  - 22
IS  - 2
SP  - 122
EP  - 135
PB  - Sage Publications
SN  - 1545-9683
SN  - 1552-6844
AD  - Donovan, Neila J., VA HS&RD/RR&D Rehabilitation Outcomes Research Center, 1601 SW Archer Road 151B, Gainesville, FL, US, 32608
N1  - Accession Number: 2008-03385-003. PMID: 17761809 Other Journal Title: Journal of Neurologic Rehabilitation. Partial author list: First Author & Affiliation: Donovan, Neila J.; VA HS&RD/RR&D Rehabilitation Outcomes Research Center, Gainesville, FL, US. Release Date: 20080616. Correction Date: 20121015. Publication Type: Journal (0100), Peer Reviewed Journal (0110). Format Covered: Electronic. Document Type: Journal Article. Language: EnglishGrant Information: Donovan, Neila J. Major Descriptor: Activities of Daily Living; Adaptive Testing; Cerebrovascular Accidents; Cognitive Processes; Neuropsychological Assessment. Classification: Cardiovascular Disorders (3295). Population: Human (10). References Available: Y. Page Count: 14. Issue Publication Date: Mar-Apr, 2008. 
AB  - Background: Up to 65% of individuals demonstrate poststroke cognitive impairments, which may increase hospital stay and caregiver burden. Randomized stroke clinical trials have emphasized physical recovery over cognition. Neuropsychological assessments have had limited utility in randomized clinical trials. These issues accentuate the need for a measure of functional cognition (the ability to accomplish everyday activities that rely on cognitive abilities, such as locating keys, conveying information, or planning activities). Objective: The aim of the study was to present the process used to establish domains of functional cognition for development of computer adaptive measure of functional cognition for stroke. Methods: Functional cognitive domains involved in identifying relevant neuropsychological constructs from the literature were conceptualized and finalized after advisory panel feedback from experts in neurology, neuropsychology, aphasiology, clinical trials, and epidemiology. Results: The following 17 domains were proposed: receptive aphasia, expressive aphasia, agraphia, alexia, calculation, visuospatial, visuoperceptual, visuoconstruction, attention, language usage, executive functions, orientation, processing speed, memory, working memory, mood, awareness and abstract reasoning. The advisory panel recommended retaining the first 12 domains. Recommended changes included: to address only encoding and retrieval of recent information in the memory domain; to add domains for limb apraxia and poststroke depression; and to keep orientation as a separate domain or reclassify it under memory or attention. The final 10 domains included: language, reading and writing, numeric/calculation, limb praxis, visuospatial function, social use of language, emotional function, attention, executive function, and memory. Conclusion: Conceptualizing domains of functional cognition is the first step in developing a computer adaptive measure of functional cognition for stroke. Additional steps include developing, refining, and field-testing items, psychometric analysis, and computer adaptive test programming. (PsycINFO Database Record (c) 2016 APA, all rights reserved)
KW  - functional cognition
KW  - stroke
KW  - computer adaptive measure
KW  - Brain
KW  - Cognition
KW  - Cognition Disorders
KW  - Computer Simulation
KW  - Disability Evaluation
KW  - Humans
KW  - Neuropsychological Tests
KW  - Outcome Assessment (Health Care)
KW  - Psychometrics
KW  - Stroke
KW  - Activities of Daily Living
KW  - Adaptive Testing
KW  - Cerebrovascular Accidents
KW  - Cognitive Processes
KW  - Neuropsychological Assessment
U1  - Sponsor: GlaxoSmithKline, Inc (GSK). Recipients: No recipient indicated
U1  - Sponsor: US Department of Veterans Affairs, Office of Research and Development Rehabilitation R&D Service, US. Grant: 04296H. Other Details: Rehabilitation Research Associate Investigator Award. Recipients: Donovan, Neila J.
DO  - 10.1177/1545968307306239
UR  - https://search.ebscohost.com/login.aspx?direct=true&db=psyh&AN=2008-03385-003&lang=de&site=ehost-live
UR  - neilad@phhp.ufl.edu
DP  - EBSCOhost
DB  - APA PsycInfo
ER  - 

TY  - JOUR
ID  - 2011-04338-004
AN  - 2011-04338-004
AU  - Donovan, Neila J.
AU  - Heaton, Shelley C.
AU  - Kimberg, Cara I.
AU  - Wen, Pey-Shan
AU  - Waid-Ebbs, J. Kay
AU  - Coster, Wendy
AU  - Singletary, Floris
AU  - Velozo, Craig A.
T1  - Conceptualizing functional cognition in traumatic brain injury rehabilitation
JF  - Brain Injury
JO  - Brain Injury
JA  - Brain Inj
Y1  - 2011/04//
VL  - 25
IS  - 4
SP  - 348
EP  - 364
PB  - Informa Healthcare
SN  - 0269-9052
SN  - 1362-301X
AD  - Donovan, Neila J., Louisiana State University, Department of Communication Sciences and Disorders, 72 Hatcher Hall, Baton Rouge, LA, US, 70803
N1  - Accession Number: 2011-04338-004. PMID: 21323413 Partial author list: First Author & Affiliation: Donovan, Neila J.; North Florida/South Georgia Veterans Health System Rehabilitation Outcomes Research Center, University of Florida, Gainsville, FL, US. Other Publishers: Taylor & Francis. Release Date: 20110321. Correction Date: 20221117. Publication Type: Journal (0100), Peer Reviewed Journal (0110). Format Covered: Electronic. Document Type: Journal Article. Language: EnglishGrant Information: Donovan, Neila J. Major Descriptor: Cognitive Ability; Recovery (Disorders); Rehabilitation; Traumatic Brain Injury. Minor Descriptor: Computerized Assessment. Classification: Neuropsychological Assessment (2225); Neurological Disorders & Brain Damage (3297). Population: Human (10). Tests & Measures: Glasgow Outcome Scale; Activity Measure for Post-Acute Care; Global Functional Assessment; Patient-Reported Outcome Questionnaire; Verbal Fluency Test; California Verbal Learning Test-2nd Edition; Letter Digit Coding Test; Wechsler Adult Intelligence Scale-III; Generalized Self-Efficacy Scale; Causal Dimension Scale-Revised; Frontal Systems Behavioral Scale; Katz Adjustment Scale-Revised; Functional Assessment Measure; Hayling and Brixton Test; Patient Competency Rating Scale; Speed and Capacity of Language Processing Test; Test of Everyday Attention; Applied Cognition Scale; Emotion and Personality Inventory; Mayo-Portland Adaptability Inventory-3; Assessment of Client Functioning Inventory; Coping Response Inventory; Attention Questionnaire; Quality of Community Integration Questionnaire; Memory Failures in Everyday Memory Questionnaire; Classical Test; Brown-Peterson Test of Short Term Memory; Digit Span Test; Spatial Span Test; Neuropsychological Test Battery; Stimulus Response Compatibility Test; Object Memory Test; Memory Span Test; Alternating Fluency Test; Cartoon Test; Eckman Faces Battery; Florida Affect Battery; Neuropsychology Behavior and Affect Profile–Self and Observer Versions; Strategy Application Test-Revised; Online Error Monitoring Task; Prospective Memory Task; Measure of Empathic Tendency; Doors and People Test; Glasgow Outcome Scale-Extended Version; Biber Cognitive Estimation Test; Judgement of Line Orientation; Buschke-Fuld Selective Reminding Test; Cancellation Test of Digits; Gulhane Aphasia Test; Categorization Test; Self-Ordered Pointing Test; Design Fluency Test; Structured Interview for Assessing Perceptual Abnormalities; Corsi Block Test; Complutense Verbal Learning Test; Barthel Index-Modified; General Symptoms Index; Deductive Reasoning Test; Toglia Category Assessment; Observed Tasks of Daily Living-Revised; Cognitive Failures Questionnaire DOI: 10.1037/t00791-000; Stroop Test DOI: 10.1037/t05449-000; Delis-Kaplan Executive Function System DOI: 10.1037/t15082-000; Functional Independence Measure DOI: 10.1037/t02229-000; Paced Auditory Serial Addition Task DOI: 10.1037/t07640-000; Functional Activities Questionnaire DOI: 10.1037/t04022-000; Sustained Attention to Response Task DOI: 10.1037/t28308-000; Halstead-Reitan Neuropsychological Test Battery DOI: 10.1037/t00275-000; Disability Rating Scale DOI: 10.1037/t29015-000; Neurobehavioral Rating Scale DOI: 10.1037/t29145-000; Controlled Oral Word Association Test DOI: 10.1037/t10132-000; Faux Pas Test--Child DOI: 10.1037/t49744-000; Mini Mental State Examination; Dysexecutive Questionnaire DOI: 10.1037/t48942-000; Brixton Spatial Anticipation Test DOI: 10.1037/t69253-000; Community Integration Questionnaire DOI: 10.1037/t01841-000; Porteus Maze Test; Recognition Memory Test; Rey Auditory Verbal Learning Test DOI: 10.1037/t27193-000; Behavior Rating Inventory of Executive Function DOI: 10.1037/t73087-000; Symptom Checklist-90–Revised DOI: 10.1037/t01210-000; Wechsler Memory Scale III; Wechsler Memory Scale--Revised; Word Memory Test (The); Benton Visual Retention Test; Boston Naming Test DOI: 10.1037/t27208-000; California Verbal Learning Test DOI: 10.1037/t48844-000; Hospital Anxiety and Depression Scale DOI: 10.1037/t03589-000; Mini-Mental State Examination DOI: 10.1037/t07757-000; Visual Verbal Learning Test; WAIS-R (Wechsler Adult Intelligence Scale-Revised); Wisconsin Card Sorting Test DOI: 10.1037/t31298-000; Impact of Event Scale DOI: 10.1037/t00303-000; National Adult Reading Test; Rey-Osterrieth Complex Figure Test; Satisfaction With Life Scale DOI: 10.1037/t01069-000; Symbol Digit Modalities Test DOI: 10.1037/t27513-000; Trail Making Test DOI: 10.1037/t00757-000; d2 Test of Attention DOI: 10.1037/t03299-000. Methodology: Literature Review. Supplemental Data: Web Sites Internet. References Available: Y. Page Count: 17. Issue Publication Date: Apr, 2011. Publication History: Accepted Date: Jan 17, 2011; Revised Date: Nov 24, 2010; First Submitted Date: Apr 22, 2010. Copyright Statement: Informa UK Ltd. 2011. 
AB  - Primary objective: To conceptualize functional cognitive constructs across the continuum of traumatic brain injury (TBI) recovery, to form the foundation for the Computer Adaptive Measure of Functional Cognition for TBI (CAMFC-TBI). Background: TBI often has a profound impact on a survivor’s ability to return to previous level of functioning and significantly reduces the overall quality of life for survivors and caregivers. Few assessments are designed to evaluate TBI’s impact on cognitive functioning in everyday life. Neuropsychological tests are time consuming and may have questionable ecological validity for predicting functional outcomes. Global functional assessments contain few cognitive items and may lack psychometric rigour. Presently there is a lack of efficient, precise, ecologically valid functional cognitive measures. Main outcome and results: Studies that used neuropsychological and global functional assessments were reviewed to direct conceptualization of functional cognitive constructs across TBI recovery stages. An advisory panel reviewed study methodology and functional cognitive constructs development. They validated the need for the CAMFC-TBI and the six functional cognitive constructs: attention, memory, processing speed, executive functioning, social communication and emotional management. Conclusion: Conceptualizing functional cognitive constructs is the first step in CAMFC-TBI development. Future project stages include item pool development, qualitative testing, field-testing, psychometric analysis and computerized adaptive test programming. (PsycInfo Database Record (c) 2022 APA, all rights reserved)
KW  - cognitive functioning
KW  - traumatic brain injury
KW  - rehabilitation
KW  - Computer Adaptive Measure of Functional Cognition for TBI
KW  - Brain Injuries
KW  - Cognition
KW  - Concept Formation
KW  - Female
KW  - Humans
KW  - Male
KW  - Neuropsychological Tests
KW  - Psychometrics
KW  - Quality of Life
KW  - Recovery of Function
KW  - Cognitive Ability
KW  - Recovery (Disorders)
KW  - Rehabilitation
KW  - Traumatic Brain Injury
KW  - Computerized Assessment
U1  - Sponsor: National Institutes of Health, National Center for Medical Rehabilitation Research, US. Grant: 5R21HD045869-03. Recipients: No recipient indicated
U1  - Sponsor: US Department of Veterans Affairs, Office of Rehabilitation Research and Development, North Florida/South Georgia Veterans Medical Center; Rehabilitation Outcomes Research Center, US. Grant: 04296H. Other Details: Associate Investigator Award. Recipients: Donovan, Neila J.
DO  - 10.3109/02699052.2011.556105
UR  - https://search.ebscohost.com/login.aspx?direct=true&db=psyh&AN=2011-04338-004&lang=de&site=ehost-live
UR  - ORCID: 0000-0002-8921-8801
UR  - ORCID: 0000-0001-5860-522X
UR  - ndonovan@lsu.edu
DP  - EBSCOhost
DB  - APA PsycInfo
ER  - 

TY  - JOUR
ID  - 2011-13238-004
AN  - 2011-13238-004
AU  - Ortner, Tuulia M.
AU  - Caspers, Juliane
T1  - Consequences of test anxiety on adaptive versus fixed item testing
JF  - European Journal of Psychological Assessment
JO  - European Journal of Psychological Assessment
JA  - Eur J Psychol Assess
Y1  - 2011///
VL  - 27
IS  - 3
SP  - 157
EP  - 163
PB  - Hogrefe Publishing
SN  - 1015-5759
SN  - 2151-2426
AD  - Ortner, Tuulia M., Department of Psychology, Division for Psychological Assessment, Free University Berlin, Habelschwerdter Allee 45, D-14195, Berlin, Germany
N1  - Accession Number: 2011-13238-004. Other Journal Title: Evaluación Psicológica. Partial author list: First Author & Affiliation: Ortner, Tuulia M.; Free University, Berlin, Germany. Other Publishers: Hogrefe & Huber Publishers. Release Date: 20110627. Correction Date: 20190211. Publication Type: Journal (0100), Peer Reviewed Journal (0110). Format Covered: Electronic. Document Type: Journal Article. Language: EnglishMajor Descriptor: Adaptive Testing; Test Anxiety; Test Construction; Test Items; Computerized Assessment. Minor Descriptor: Fairness; Consequence. Classification: Tests & Testing (2220). Population: Human (10); Male (30); Female (40). Location: Germany. Age Group: Adolescence (13-17 yrs) (200); Adulthood (18 yrs & older) (300); Young Adulthood (18-29 yrs) (320). Tests & Measures: Adaptive Matrices Test; Test Anxiety Inventory-German version, short form. Methodology: Empirical Study; Quantitative Study. References Available: Y. Page Count: 7. Issue Publication Date: 2011. Copyright Statement: Hogrefe Publishing. 2011. 
AB  - We investigated the effects of test anxiety on test performance using computerized adaptive testing (CAT) versus conventional fixed item testing (FIT). We hypothesized that tests containing mainly items with medium probabilities of being solved would have negative effects on test performance for testtakers high in test anxiety. A total of 110 students (aged 16 to 20) from a German secondary modern school filled out a short form of the Test Anxiety Inventory (TAI-G; Wacker, Jaunzeme, & Jaksztat, 2008) and then were presented with items from the Adaptive Matrices Test (AMT; Hornke, Etzel, & Rettig, 1999) on the computer, either in CAT form or in a fixed item test form with a selection of items arranged in order of increasing item difficulty. Additionally, half of the students were given a short summary of information about the mode of item selection in adaptive testing before working on the CAT. In a moderated regression approach, a significant interaction of test anxiety and test mode was revealed. The effect of test mode on the AMT score was stronger for students with higher scores on test anxiety than for students with lower test anxiety. Furthermore, getting information about CAT led to significantly better results than receiving standard test instructions. Results are discussed with reference to test fairness. (PsycINFO Database Record (c) 2019 APA, all rights reserved)
KW  - CAT
KW  - adaptive testing
KW  - fairness
KW  - matrices
KW  - test anxiety
KW  - computerized adaptive testing
KW  - fixed item testing
KW  - test items
KW  - item selection
KW  - test instructions
KW  - Adaptive Testing
KW  - Test Anxiety
KW  - Test Construction
KW  - Test Items
KW  - Computerized Assessment
KW  - Fairness
KW  - Consequence
DO  - 10.1027/1015-5759/a000062
UR  - https://search.ebscohost.com/login.aspx?direct=true&db=psyh&AN=2011-13238-004&lang=de&site=ehost-live
UR  - ORCID: 0000-0002-5703-6915
UR  - tuulia.ortner@fu-berlin.de
DP  - EBSCOhost
DB  - APA PsycInfo
ER  - 

TY  - JOUR
ID  - 2020-41396-001
AN  - 2020-41396-001
AU  - Tarescavage, Anthony M.
AU  - Menton, William H.
T1  - Construct validity of the personality inventory for ICD-11 (PiCD): Evidence from the MMPI-2-RF and CAT-PD-SF
JF  - Psychological Assessment
JO  - Psychological Assessment
JA  - Psychol Assess
Y1  - 2020/09//
VL  - 32
IS  - 9
SP  - 889
EP  - 895
PB  - American Psychological Association
SN  - 1040-3590
SN  - 1939-134X
AD  - Tarescavage, Anthony M., Department of Psychology, John Carroll University, 1 John Carroll Boulevard, University Heights, OH, US, 44118
N1  - Accession Number: 2020-41396-001. PMID: 32525344 Other Journal Title: Psychological Assessment: A Journal of Consulting and Clinical Psychology. Partial author list: First Author & Affiliation: Tarescavage, Anthony M.; Department of Psychology, John Carroll University, University Heights, OH, US. Release Date: 20200611. Correction Date: 20220414. Publication Type: Journal (0100), Peer Reviewed Journal (0110). Format Covered: Electronic. Document Type: Journal Article. Language: EnglishGrant Information: Tarescavage, Anthony M. Major Descriptor: Construct Validity; Inventories; Minnesota Multiphasic Personality Inventory; Personality Disorders; Personality Measures. Minor Descriptor: Adaptive Testing; College Students; Measurement; Test Construction; Test Reliability; Test Validity. Classification: Tests & Testing (2220); Personality Disorders (3217). Population: Human (10); Male (30); Female (40). Location: US. Age Group: Adolescence (13-17 yrs) (200); Adulthood (18 yrs & older) (300); Young Adulthood (18-29 yrs) (320). Tests & Measures: Computerized Adaptive Test of Personality Disorders Static Form; Minnesota Multiphasic Personality Inventory-2-Restructured Form DOI: 10.1037/t15121-000. Methodology: Empirical Study; Quantitative Study. Supplemental Data: Tables and Figures Internet. References Available: Y. Page Count: 7. Issue Publication Date: Sep, 2020. Publication History: First Posted Date: Jun 11, 2020; Accepted Date: May 13, 2020; Revised Date: Mar 9, 2020; First Submitted Date: Sep 30, 2019. Copyright Statement: American Psychological Association. 2020. 
AB  - The Personality Inventory for ICD-11 (PiCD) was recently developed to assess the ICD-11 model of personality disorders. The purpose of this study was to examine the construct validity of the PiCD using the Minnesota Multiphasic Personality Inventory (MMPI)-2-Restructured Form (MMPI-2-RF) and the Computerized Adaptive Test of Personality Disorders Static Form (CAT-PD-SF). We administered these tests to 328 college students (150 males, 178 females). We found that the PiCD had adequate internal consistency reliability. Correlations between scores from the PiCD scales and the criterion measures generally indicated adequate discriminant validity. Along the same lines, convergent validity was adequate for the PiCD Negative Affective, Disinhibition, and Dissocial scales. However, the evidence was more mixed for the PiCD Detachment and Anankastic domains, which may be due to limitations with the content domains for these scales. Consistent with other research and theoretical expectations, a conjoint exploratory factor analysis utilizing the PiCD and MMPI-2-RF PSY-5 scales also indicated that anankastic and disinhibition may be more appropriately conceptualized as measuring opposite poles of one construct. Implications of these findings for the PiCD and the ICD-11 model are discussed. (PsycInfo Database Record (c) 2022 APA, all rights reserved)
AB  - Public Significance Statement—This article evaluates the quality of a new measure of chronic personality disorders, the PiCD. (PsycInfo Database Record (c) 2022 APA, all rights reserved)
KW  - PiCD
KW  - MMPI-2-RF
KW  - CAT-PD-SF
KW  - validity
KW  - reliability
KW  - Construct Validity
KW  - Inventories
KW  - Minnesota Multiphasic Personality Inventory
KW  - Personality Disorders
KW  - Personality Measures
KW  - Adaptive Testing
KW  - College Students
KW  - Measurement
KW  - Test Construction
KW  - Test Reliability
KW  - Test Validity
U1  - Sponsor: Sponsor name not included. Other Details: Receives research funding from the MMPI-2-RF test publisher. Recipients: Tarescavage, Anthony M.
DO  - 10.1037/pas0000914
L3  - 10.1037/pas0000914.supp (Supplemental)
UR  - https://search.ebscohost.com/login.aspx?direct=true&db=psyh&AN=2020-41396-001&lang=de&site=ehost-live
UR  - ORCID: 0000-0002-0151-712X
UR  - ORCID: 0000-0001-9608-2968
UR  - atarescavage@jcu.edu
DP  - EBSCOhost
DB  - APA PsycInfo
ER  - 

TY  - JOUR
ID  - 2021-80646-001
AN  - 2021-80646-001
AU  - Flens, Gerard
AU  - Terwee, Caroline B.
AU  - Smits, Niels
AU  - Williams, Guido
AU  - Spinhoven, Philip
AU  - Roorda, Leo D.
AU  - de Beurs, Edwin
T1  - Construct validity, responsiveness, and utility of change indicators of the Dutch-Flemish PROMIS item banks for depression and anxiety administered as computerized adaptive test (CAT): A comparison with the Brief Symptom Inventory (BSI)
JF  - Psychological Assessment
JO  - Psychological Assessment
JA  - Psychol Assess
Y1  - 2022/01//
VL  - 34
IS  - 1
SP  - 58
EP  - 69
PB  - American Psychological Association
SN  - 1040-3590
SN  - 1939-134X
AD  - Flens, Gerard, Alliance for Quality in Mental Health Care, Museumlaan 7, 3581 HK, Utrecht, Netherlands
N1  - Accession Number: 2021-80646-001. PMID: 34472957 Other Journal Title: Psychological Assessment: A Journal of Consulting and Clinical Psychology. Partial author list: First Author & Affiliation: Flens, Gerard; Alliance for Quality in Mental Health Care, Utrecht, Netherlands. Release Date: 20210902. Correction Date: 20220127. Publication Type: Journal (0100), Peer Reviewed Journal (0110). Format Covered: Electronic. Document Type: Journal Article. Language: EnglishMajor Descriptor: Adaptive Testing; Anxiety; Construct Validity; Major Depression; Psychometrics. Minor Descriptor: Inventories; Item Response Theory; Statistical Correlation. Classification: Clinical Psychological Testing (2224); Affective Disorders (3211). Population: Human (10); Male (30); Female (40); Outpatient (60). Location: Netherlands. Age Group: Adulthood (18 yrs & older) (300); Young Adulthood (18-29 yrs) (320); Thirties (30-39 yrs) (340); Middle Age (40-64 yrs) (360); Aged (65 yrs & older) (380). Tests & Measures: Dutch-Flemish PROMIS Adult v1.0 Item Banks for Depression and Anxiety; Outcome Questionnaire-45; Symptom Questionnaire-48; Beck Depression Inventory–II DOI: 10.1037/t00742-000; Padua Inventory DOI: 10.1037/t11846-000; Panic Disorder Severity Scale DOI: 10.1037/t60094-000; Brief Symptom Inventory DOI: 10.1037/t00789-000; Montgomery-Asberg Depression Rating Scale DOI: 10.1037/t04111-000; Yale-Brown Obsessive Compulsive Scale DOI: 10.1037/t57982-000. Methodology: Empirical Study; Quantitative Study. References Available: Y. Page Count: 12. Issue Publication Date: Jan, 2022. Publication History: First Posted Date: Sep 2, 2021; Accepted Date: Jul 22, 2021; Revised Date: Jul 20, 2021; First Submitted Date: Mar 24, 2021. Copyright Statement: American Psychological Association. 2021. 
AB  - We evaluated construct validity, responsiveness, and utility of change indicators of the Dutch-Flemish PROMIS adult v1.0 item banks for Depression and Anxiety administered as computerized adaptive test (CAT). Specifically, the CATs were compared to the Brief Symptom Inventory (BSI) using pre- and re-test data of adult patients treated for common mental disorders (N = 400; median pre-to-re-test interval = 215 days). Construct validity was evaluated with Pearson’s correlations and Cohen’s ds; responsiveness with Pearson’s correlations and pre-post effect sizes (ES); utility of change indicators with kappa coefficients and percentages of (dis)agreement. The results showed that the PROMIS CATs measure similar constructs as matching BSI scales. Under the assumption of measuring similar constructs, the CAT and BSI Depression scales were similarly responsive. For the Anxiety scales, we found a higher responsiveness for CAT (ES = 0.64) compared to the BSI (ES = 0.50). Finally, both CATs categorized the change scores of more patients as changed compared to matching BSI scales, indicating that the PROMIS CATs may be more able to detect actual change than the BSI. Based on these findings, the PROMIS CATs may be considered a modest improvement over matching BSI scales as tools for reviewing treatment progress with patients. We discuss several additional differences between the PROMIS CATs and the BSI to help test users choose instruments. These differences include the adopted measurement theory (Item Response Theory vs. Classical Test Theory), the mode of administration (CAT vs. fixed items), and the area of application (universal vs. predominantly clinical). (PsycInfo Database Record (c) 2022 APA, all rights reserved)
AB  - This study suggests that PROMIS CATs for Depression and Anxiety measure the same constructs as matching BSI subscales in Dutch adult patients treated for common mental disorders, are at least as able to detect change over time, and categorize the change scores of more patients as actually changed. Based on these findings, the PROMIS CATs may be considered a modest improvement as tools for reviewing treatment progress with patients. (PsycInfo Database Record (c) 2022 APA, all rights reserved)
KW  - depression
KW  - anxiety
KW  - clinical assessment
KW  - PROMIS CAT
KW  - psychometric properties
KW  - Adaptive Testing
KW  - Anxiety
KW  - Construct Validity
KW  - Major Depression
KW  - Psychometrics
KW  - Inventories
KW  - Item Response Theory
KW  - Statistical Correlation
DO  - 10.1037/pas0001068
UR  - https://search.ebscohost.com/login.aspx?direct=true&db=psyh&AN=2021-80646-001&lang=de&site=ehost-live
UR  - ORCID: 0000-0003-3832-8477
UR  - ORCID: 0000-0001-9348-5390
UR  - ORCID: 0000-0001-8008-5750
UR  - ORCID: 0000-0003-3669-9266
UR  - ORCID: 0000-0003-4570-2826
UR  - ORCID: 0000-0002-6683-4628
UR  - g.flens@akwaggz.nl
DP  - EBSCOhost
DB  - APA PsycInfo
ER  - 

TY  - JOUR
ID  - 2004-21596-004
AN  - 2004-21596-004
AU  - Ariel, Adelaide
AU  - Veldkamp, Bernard P.
AU  - van der Linden, Wim J.
T1  - Constructing Rotating Item Pools for Constrained Adaptive Testing
JF  - Journal of Educational Measurement
JO  - Journal of Educational Measurement
JA  - J Educ Meas
Y1  - 2004///Win 2004
VL  - 41
IS  - 4
SP  - 345
EP  - 359
PB  - Blackwell Publishing
SN  - 0022-0655
SN  - 1745-3984
AD  - Ariel, Adelaide, Department of Research Methodology, Measurement, and Data Analysis, University of Twente, P.O. Box 217, 7500 AE, Enschede, Netherlands
N1  - Accession Number: 2004-21596-004. Partial author list: First Author & Affiliation: Ariel, Adelaide; Department of Research Methodology, Measurement, and Data Analysis, University of Twente, Enschede, Netherlands. Other Publishers: Wiley-Blackwell Publishing Ltd. Release Date: 20050103. Correction Date: 20190211. Publication Type: Journal (0100), Peer Reviewed Journal (0110). Format Covered: Print. Document Type: Journal Article. Language: EnglishMajor Descriptor: Adaptive Testing; Educational Measurement; Test Items; Computerized Assessment. Classification: Statistics & Mathematics (2240). Population: Human (10). Tests & Measures: Law School Admission Test. Methodology: Empirical Study; Quantitative Study. References Available: Y. Page Count: 15. Issue Publication Date: Win 2004. 
AB  - Preventing items in adaptive testing from being over- or underexposed is one of the main problems in computerized adaptive testing. Though the problem of overexposed items can be solved using a probabilistic item-exposure control method, such methods are unable to deal with the problem of underexposed items. Using a system of rotating item pools, on the other hand, is a method that potentially solves both problems. In this method, a master pool is divided into (possibly overlapping) smaller item pools, which are required to have similar distributions of content and statistical attributes. These pools are rotated among the testing sites to realize desirable exposure rates for the items. A test assembly model, motivated by Gulliksen's matched random subtests method, was explored to help solve the problem of dividing a master pool into a set of smaller pools. Different methods to solve the model are proposed. An item pool from the Law School Admission Test was used to evaluate the performances of computerized adaptive tests from systems of rotating item pools constructed using these methods. (PsycINFO Database Record (c) 2019 APA, all rights reserved)
KW  - rotating item pools
KW  - constrained adaptive testing
KW  - item exposure
KW  - computerized adaptive tests
KW  - Adaptive Testing
KW  - Educational Measurement
KW  - Test Items
KW  - Computerized Assessment
DO  - 10.1111/j.1745-3984.2004.tb01170.x
UR  - https://search.ebscohost.com/login.aspx?direct=true&db=psyh&AN=2004-21596-004&lang=de&site=ehost-live
UR  - ORCID: 0000-0003-3543-2164
UR  - w.j.vanderlinden@utwente.nl
UR  - b.p.veldkanp@utwente.nl
UR  - a.ariel@utwente.nl
DP  - EBSCOhost
DB  - APA PsycInfo
ER  - 

TY  - THES
ID  - 2011-99150-525
AN  - 2011-99150-525
AU  - Weber, Peggy Ann (Raethke)
T1  - Content clustering of a computerized-adaptive test
JF  - Dissertation Abstracts International Section A: Humanities and Social Sciences
JO  - Dissertation Abstracts International Section A: Humanities and Social Sciences
Y1  - 2011///
VL  - 72
IS  - 2-A
SP  - 576
EP  - 576
PB  - ProQuest Information & Learning
SN  - 0419-4209
SN  - 978-1-124-37648-6
N1  - Accession Number: 2011-99150-525. Other Journal Title: Dissertation Abstracts International. Partial author list: First Author & Affiliation: Weber, Peggy Ann (Raethke); Bethel U., US. Release Date: 20111107. Correction Date: 20221128. Publication Type: Dissertation Abstract (0400). Format Covered: Electronic. Document Type: Dissertation. Dissertation Number: AAI3437448. ISBN: 978-1-124-37648-6. Language: EnglishMajor Descriptor: Academic Achievement; Teaching; Computerized Assessment. Minor Descriptor: Middle School Students. Classification: Educational & School Psychology (3500). Population: Human (10). Location: US. Age Group: Childhood (birth-12 yrs) (100); School Age (6-12 yrs) (180). Methodology: Empirical Study; Quantitative Study. Page Count: 1. 
AB  - School districts across the country are meeting the accountability challenges set forth by No Child Left Behind (NCLB) federal legislation, as well as challenges put forth at their state and local levels. Districts' accountability efforts have often brought them to contract with outside organizations offering standardized assessments for progress monitoring of students in intervention programs, and for on-going local assessments for program evaluation and planning. One such organization, Northwest Evaluation Association (NWEA), has developed several computer-based testing products to assist districts in their accountability and teaching/learning efforts. Their flagship product, Measures of Academic Progress (MAP), is a computerized-adaptive test (CAT), and is typically delivered on-line to districts. Using Item Response Theory (IRT) selection algorithms, test questions are selected based on students' answers throughout the test experience. Students are presented with their first question (item) based on a predetermined value of their ability from previous testing and what is known about the item (Baker, 2001). Once they respond to the item, the software calibrates a temporary achievement level for that student, and selects their next question based on their estimated competency. Use of such powerful test assembly methods can enable a test to derive precise performance information about each student. Tests are assembled on demand, and results are immediately available to instructors and district personnel. While precise information at a broader range of abilities is possible using such test assembly methods, item selection processes typically present items in a randomized order to maximize item bank usage, test security, and measurement properties. Little research has been done on item order with young test takers in the elementary grades, and the literature on item order with regard to computerized adaptive testing is silent. Of particular interest was whether a content-clustered version of a computerized adaptive test would result in stronger performance with young test takers, and whether the efficient testing processes already available via on-line testing could be improved. Data was reviewed in light of demographic sub-groups, including gender, ethnicity, and grade level. The current study delivered both the standard version of a computerized adaptive test, MAP, along with a modified version, to a test population of 6589 students in a large southern U.S. school district of second, third, fourth and fifth graders. Using a double-blind selection process, half of the students received the standard MAP, and half received the modified version, which clustered items by goal strand across the five sub-tests of the assessment. The study indicated that overall scores on MAP are not increased by content clustering, nor are there significant time gains on the overall test, for all students as a whole. However, results do indicate areas of significance with regard to ethnic sub-group differences. Overall test performance was stronger for African Americans on the modified version, while overall test duration was reduced for the three major ethnic sub-groups, particularly Hispanics. Boys improved their scores overall on the modified version, and gender differences on goal strands were present, which may indicate that certain goal strands scores increase under clustered conditions. (PsycInfo Database Record (c) 2022 APA, all rights reserved)
KW  - computerized adaptive test
KW  - middle school students
KW  - teaching
KW  - academic achievement
KW  - Academic Achievement
KW  - Teaching
KW  - Computerized Assessment
KW  - Middle School Students
UR  - https://search.ebscohost.com/login.aspx?direct=true&db=psyh&AN=2011-99150-525&lang=de&site=ehost-live
DP  - EBSCOhost
DB  - APA PsycInfo
ER  - 

TY  - THES
ID  - 1995-95020-119
AN  - 1995-95020-119
AU  - Newman, Larry S.
T1  - Content validity of a computerized adaptive licensing and certification examination: A comparison of content-balancing methods
JF  - Dissertation Abstracts International Section A: Humanities and Social Sciences
JO  - Dissertation Abstracts International Section A: Humanities and Social Sciences
Y1  - 1995/10//
VL  - 56
IS  - 4-A
SP  - 1328
EP  - 1328
PB  - ProQuest Information & Learning
SN  - 0419-4209
N1  - Accession Number: 1995-95020-119. Other Journal Title: Dissertation Abstracts International. Partial author list: First Author & Affiliation: Newman, Larry S.; Temple U, US. Release Date: 19950101. Correction Date: 20190211. Publication Type: Dissertation Abstract (0400). Format Covered: Electronic. Document Type: Dissertation. Dissertation Number: AAM9527525. Language: EnglishMajor Descriptor: Adaptive Testing; Content Validity; Professional Examinations; Test Validity. Classification: General Psychology (2100); Psychometrics & Statistics & Methodology (2200). Population: Human (10). Age Group: Adulthood (18 yrs & older) (300). Methodology: Empirical Study. Page Count: 1. 
AB  - The benefits of adaptive testing, reduced test length and increased measurement precision, are in part achieved through the application of a maximum-information item selection algorithm. While such an algorithm is an effective means for tailoring the difficulty of adaptive tests to examinee ability, its use with multidimensional content domains might result in examinations that lack content validity. Licensure and certification examinations typically assess knowledge of multidimensional content domains. As such, balancing test content is an important consideration for these types of tests. This study investigated the effects of content balancing on an adaptive licensure and certification examination. A computer simulation was used with data distributions attained from previous administrations of a conventional test. The performance of the adaptive test was examined under four content-balancing methods and fifteen stopping rules. The results showed that employing a rigorous content-balancing method had no significant adverse effect on the efficiency or accuracy of the adaptive test under study. Findings, also indicated that ability estimation bias contributed to slight inaccuracies in the determination of the passing status for examinees whose abilities were near the minimum passing ability. The findings of this study support the use of adaptive testing for licensure and certification examinations under the following conditions: rigorous content balancing is employed, the item bank contains a sufficient number of items to optimally measure a range of abilities, and if estimation bias is found to affect the pass/fail decision, it is corrected. Implications of these findings are discussed for school certification testing. (PsycINFO Database Record (c) 2019 APA, all rights reserved)
KW  - content validity of computerized adaptive licensing & certification examination
KW  - licensure & certification examination takers
KW  - Adaptive Testing
KW  - Content Validity
KW  - Professional Examinations
KW  - Test Validity
UR  - https://search.ebscohost.com/login.aspx?direct=true&db=psyh&AN=1995-95020-119&lang=de&site=ehost-live
DP  - EBSCOhost
DB  - APA PsycInfo
ER  - 

TY  - CHAP
ID  - 1999-02356-007
AN  - 1999-02356-007
AU  - Vispoel, Walter P.
ED  - Drasgow, Fritz
ED  - Olson-Buchanan, Julie B.
T1  - Creating computerized adaptive tests of music aptitude: Problems, solutions, and future directions
T2  - Innovations in computerized assessment.
Y1  - 1999///
SP  - 151
EP  - 176
CY  - Mahwah, NJ
PB  - Lawrence Erlbaum Associates Publishers
SN  - 0-8058-2876-1
SN  - 0-8058-2877-X
N1  - Accession Number: 1999-02356-007. Partial author list: First Author & Affiliation: Vispoel, Walter P.; U Iowa, Iowa City, IA, US. Release Date: 19990601. Correction Date: 20200907. Publication Type: Book (0200), Edited Book (0280). Format Covered: Print. Document Type: Chapter. ISBN: 0-8058-2876-1, ISBN Hardcover; 0-8058-2877-X, ISBN Paperback. Language: EnglishMajor Descriptor: Adaptive Testing; Aptitude Measures; Musical Ability; Computerized Assessment. Minor Descriptor: Auditory Stimulation; Computer Software. Classification: Personality Scales & Inventories (2223); Personality Traits & Processes (3120). Population: Human (10). Intended Audience: Psychology: Professional & Research (PS). Page Count: 26. 
AB  - In this chapter the author explains how he used the audio presentation capabilities of computers to develop the 1st computerized adaptive test for assessing music aptitude. He describes how the stimuli were developed and how the items were calibrated. Given the length of time over which the assessment was validated, the author gives special attention to how the changes in available hardware and software affected the evolution of the assessment. (PsycInfo Database Record (c) 2020 APA, all rights reserved)
KW  - development of computerized adaptive test of musical aptitude
KW  - Adaptive Testing
KW  - Aptitude Measures
KW  - Musical Ability
KW  - Computerized Assessment
KW  - Auditory Stimulation
KW  - Computer Software
UR  - https://search.ebscohost.com/login.aspx?direct=true&db=psyh&AN=1999-02356-007&lang=de&site=ehost-live
DP  - EBSCOhost
DB  - APA PsycInfo
ER  - 

TY  - JOUR
ID  - 2022-03183-024
AN  - 2022-03183-024
AU  - You, Dokyoung S.
AU  - Cook, Karon F.
AU  - Domingue, Benjamin W.
AU  - Ziadni, Maisa S.
AU  - Hah, Jennifer M.
AU  - Darnall, Beth D.
AU  - Mackey, Sean C.
T1  - Customizing CAT administration of the PROMIS misuse of prescription pain medication item bank for patients with chronic pain
JF  - Pain Medicine
JO  - Pain Medicine
JA  - Pain Med
Y1  - 2021/07//
VL  - 22
IS  - 7
SP  - 1669
EP  - 1675
PB  - Oxford University Press
SN  - 1526-2375
SN  - 1526-4637
AD  - Mackey, Sean C., Department Anesthesiology, Perioperative and Pain Medicine, Stanford University School of Medicine, 1070 Arastradero Road, Suite 200, MC 5596, Palo Alto, CA, US, 94304
N1  - Accession Number: 2022-03183-024. Partial author list: First Author & Affiliation: You, Dokyoung S.; Department Anesthesiology, Perioperative and Pain Medicine, Stanford University School of Medicine, Palo Alto, CA, US. Other Publishers: Blackwell Publishing; Wiley-Blackwell Publishing Ltd. Release Date: 20220711. Publication Type: Journal (0100), Peer Reviewed Journal (0110). Format Covered: Electronic. Document Type: Journal Article. Language: EnglishGrant Information: You, Dokyoung S. Major Descriptor: Drug Administration Methods; Drug Therapy; Pain; Prescription Drugs; Prescription Drug Misuse. Minor Descriptor: Adaptive Testing; Chronic Pain; Opiates. Classification: Clinical Psychopharmacology (3340). Population: Human (10); Male (30); Female (40). Location: US. Age Group: Adulthood (18 yrs & older) (300). Methodology: Empirical Study; Quantitative Study. References Available: Y. Page Count: 7. Issue Publication Date: Jul, 2021. Publication History: First Posted Date: May 4, 2021. Copyright Statement: Published by Oxford University Press on behalf of the American Academy of Pain Medicine. All rights reserved. The Author(s). 2021. 
AB  - Objective: The 22-item PROMIS®-Rx Pain Medication Misuse item bank (Bank-22) imposes a high response burden. This study aimed to characterize the performance of the Bank-22 in a computer adaptive testing (CAT) setting based on varied stopping rules. Methods: The 22 items were administered to 288 patients. We performed a CAT simulation using default stopping rules (CATPROMIS). In 5 other simulations, a 'best health' response rule was added to decrease response burden. This rule stopped CAT administration when a participant selected 'never' to a specified number of initial Bank-22 items (2–6 in this study, designated CATAlt2-Alt6). The Bank-22 and 7-item short form (SF-7) scores were compared to scores based on CATPROMIS, and the 5 CAT variations. Results: Bank-22 scores correlated highly with the SF-7 and CATPROMIS, Alt5, Alt6 scores (r=0.87–0.95) and moderately with CATAlt2-Alt4 scores (r=0.63–0.74). In all CAT conditions, the greatest differences with Bank-22 scores were at the lower end of misuse T-scores. The smallest differences with Bank-22 and CATPROMIS scores were observed with CATAlt5 and CATAlt6. Compared to the SF-7, CATAlt5 and CATAlt6 reduced overall response burden by about 42%. Finally, the correlations between PROMIS-Rx Misuse and Anxiety T-scores remained relatively unchanged across the conditions (r=0.31–0.43, Ps < .001). Conclusions: Applying a stopping rule based on number of initial 'best health' responses reduced response burden for respondents with lower levels of misuse. The tradeoff was less measurement precision for those individuals, which could be an acceptable tradeoff when the chief concern is in discriminating higher levels of misuse. (PsycInfo Database Record (c) 2022 APA, all rights reserved)
KW  - Prescription Opioids
KW  - Opioid Misuse
KW  - NIH PROMIS
KW  - Computer Adaptive Testing
KW  - PROMIS-Rx Pain Medication Misuse
KW  - Drug Administration Methods
KW  - Drug Therapy
KW  - Pain
KW  - Prescription Drugs
KW  - Prescription Drug Misuse
KW  - Adaptive Testing
KW  - Chronic Pain
KW  - Opiates
U1  - Sponsor: Sponsor name not included. Grant: K23 DA048972. Recipients: You, Dokyoung S.
U1  - Sponsor: Sponsor name not included. Grant: K23DA047473. Recipients: Ziadni, Maisa S.
U1  - Sponsor: Sponsor name not included. Grant: K23DA035302. Recipients: Hah, Jennifer M.
U1  - Sponsor: Sponsor name not included. Grant: R01AT008561. Recipients: Darnall, Beth D.
U1  - Sponsor: Sponsor name not included. Grant: K24DA029262. Recipients: Mackey, Sean C.
U1  - Sponsor: Redlich Endowment. Recipients: Mackey, Sean C.
DO  - 10.1093/pm/pnab159
UR  - https://search.ebscohost.com/login.aspx?direct=true&db=psyh&AN=2022-03183-024&lang=de&site=ehost-live
UR  - ORCID: 0000-0001-7744-5348
UR  - ORCID: 0000-0003-3590-511X
UR  - ORCID: 0000-0002-9739-3968
UR  - ORCID: 0000-0001-8371-0646
UR  - smackey@stanford.edu
DP  - EBSCOhost
DB  - APA PsycInfo
ER  - 

TY  - JOUR
ID  - 2017-57787-003
AN  - 2017-57787-003
AU  - Flens, G.
AU  - de Beurs, E.
T1  - De toekomst van ROM: Computer-gestuurd adaptief testen = The future of ROM: Computerised adaptive testing
JF  - Tijdschrift voor Psychiatrie
JO  - Tijdschrift voor Psychiatrie
JA  - Tijdschr Psychiatr
Y1  - 2017///
VL  - 59
IS  - 12
SP  - 767
EP  - 774
PB  - Uitgeverij Boom
SN  - 0303-7339
SN  - 1875-7456
AD  - Flens, G.
N1  - Accession Number: 2017-57787-003. Translated Title: The future of ROM: Computerised adaptive testing. Partial author list: First Author & Affiliation: Flens, G.; Onderzoeker Stichting Benchmark GGZ, Netherlands. Release Date: 20180201. Publication Type: Journal (0100), Peer Reviewed Journal (0110). Format Covered: Electronic. Document Type: Journal Article. Language: Dutch; FlemishMajor Descriptor: Adaptive Testing; Item Response Theory; Measurement; Mental Health Services; Psychiatric Symptoms. Classification: Clinical Psychological Testing (2224); Health & Mental Health Treatment & Prevention (3300). Population: Human (10). Methodology: Empirical Study; Quantitative Study. References Available: Y. Page Count: 8. Issue Publication Date: 2017. Publication History: Accepted Date: Jun 6, 2017. 
AB  - Background: Measurement instruments that are used for monitoring patients in mental health care are developed according to the principles of classical test theory. Because the assumptions underlying this theory are outdated, this is a good time to work towards a new method of measurement known as computerised adaptive testing (CAT), the method being based on item response theory. Aim: To introduce the CAT-methodology into Dutch mental health care, and provide an overview of the current and desirable developments. Method: We explain what CAT is and why mental health care should warmly welcome this new development. We also outline the limitations of CAT, summarise the developments that have already been made nationally and internationally and consider some developments we think are desirable. Results: PROMIS item banks for anxiety and depression for adults show that CAT is more efficient than instruments currently in use and is able to produce very precise outcomes. Conclusion: The first CATS for anxiety and depression will be available late 2017 or early 2018 for adults receiving mental health care in the Netherlands. Recent results are very impressive and CAT-technology will increase the efficiency of symptom measurements, bringing these measurements to a higher level. (PsycINFO Database Record (c) 2018 APA, all rights reserved)
AB  - Achtergrond: Meetinstrumenten die worden gebruikt voor het monitoren van de behandeluitkomst bij patiënten in de ggz zijn ontwikkeld op basis van de principes uit de klassieke testtheorie. Omdat de aannames achter deze theorie achterhaald zijn, is het tijd om toe te werken naar een nieuwe meetmethode die is gebaseerd op de itemresponstheorie: computergestuurd adaptief testen (CAT). Doel: De CAT-methodiek introduceren bij de Nederlandse ggz, en een overzicht geven van de huidige en toekomstige ontwikkelingen. Methode: We lichten toe wat CAT is, wat de voordelen zijn voor de ggz, wat de beperkingen zijn, welke nationale en internationale ontwikkelingen er al zijn geweest, en welke ontwikkelingen nog wenselijk zijn. Resultaten: De patient-reported outcomes measurement information system(PROMIS)-itembanken voor angst en depressie voor volwassenen illustreren dat het mogelijk is om met CAT efficiënter te meten dan met het klassieke meetinstrumentarium, met zeer precieze uitkomsten. Conclusie: Eind 2017, begin 2018 komen de eerste gevalideerde CAT’s voor angst en depressie beschikbaar voor volwassenen in de ggz. De huidige resultaten zijn veelbelovend en de CAT-technologie brengt meten in de ggz op een hoger plan. (PsycINFO Database Record (c) 2018 APA, all rights reserved)
KW  - anxiety
KW  - computerized adaptive testing
KW  - depression
KW  - item response theory
KW  - routine outcome monitoring
KW  - Adaptive Testing
KW  - Item Response Theory
KW  - Measurement
KW  - Mental Health Services
KW  - Psychiatric Symptoms
UR  - https://search.ebscohost.com/login.aspx?direct=true&db=psyh&AN=2017-57787-003&lang=de&site=ehost-live
UR  - gerard.flens@sbggz.nl
DP  - EBSCOhost
DB  - APA PsycInfo
ER  - 

TY  - JOUR
ID  - 2021-45292-001
AN  - 2021-45292-001
AU  - Wenzel, Elizabeth S.
AU  - Gibbons, Robert D.
AU  - O'Hara, Michael W.
AU  - Duffecy, Jennifer
AU  - Maki, Pauline M.
T1  - Depression and anxiety symptoms across pregnancy and the postpartum in low-income black and latina women 
JF  - Archives of Women's Mental Health
JO  - Archives of Women's Mental Health
JA  - Arch Womens Ment Health
Y1  - 2021/05/10/
PB  - Springer
SN  - 1434-1816
SN  - 1435-1102
AD  - Maki, Pauline M.
N1  - Accession Number: 2021-45292-001. PMID: 33970310 Partial author list: First Author & Affiliation: Wenzel, Elizabeth S.; Department of Psychology, University of Illinois at Chicago, Chicago, IL, US. Release Date: 20210513. Publication Type: Journal (0100), Peer Reviewed Journal (0110). Format Covered: Electronic. Document Type: Journal. Language: EnglishMajor Descriptor: No terms assigned. Classification: Health Psychology & Medicine (3360). Publication History: Accepted Date: Apr 25, 2021; First Submitted Date: Oct 29, 2020. Copyright Statement: The Author(s), under exclusive licence to Springer-Verlag GmbH Austria, part of Springer Nature. 2021. 
AB  - Underserved women of color experience high rates of perinatal affective disorders, but most research to date on the natural history of these disorders has been conducted on White women. The present study investigated longitudinal changes in anxiety and depression in a sample of perinatal non-Hispanic Black and Latina women. Categorical (yes/no) measures of positive anxiety and depression screens, as well as total symptom scores, were measured longitudinally across the perinatal period in 178 women (115 non-Hispanic Black, 63 Latina) using the CAT-MH™, a computerized adaptive test. Time (up to 4 visits) and race/ethnicity effects were assessed in linear mixed effects models. Rates of positive anxiety screenings were 13.6%, 3.2%, 8.5%, and 0% in Latina women and 2.6%, 4.2%, 6.1%, and 5.8% in non-Hispanic Black women in the 1st, 2nd, and 3rd trimesters, and postpartum, respectively. Rates of positive anxiety screenings overall were highest in the first trimester (OR = 0.20; 95% CI 0.04–0.98), and there was a significant time-by-race/ethnicity interaction for positive anxiety screens (OR = 8.88; 95% CI 1.42–55.51), as positive screens were most frequent in the first trimester and sharply declined for Latina women, while rates were relatively consistent across the perinatal period in non-Hispanic Black women. Rates of positive depression screens did not change over time, but there was a trend (OR = 1.93; 95% CI 0.93–4.03) for a time-by-race/ethnicity interaction in a direction similar to that seen for anxiety. The odds of positive anxiety screens vary by race/ethnicity and trimester, suggesting that anxiety screening and anxiety interventions may be most resourcefully used in the first trimester for Latina women in particular. (PsycInfo Database Record (c) 2021 APA, all rights reserved)
KW  - Perinatal
KW  - Depression
KW  - Anxiety
KW  - Adaptive tests
KW  - Longitudinal
KW  - Race
KW  - Ethnicity
KW  - No terms assigned
DO  - 10.1007/s00737-021-01139-y
UR  - https://search.ebscohost.com/login.aspx?direct=true&db=psyh&AN=2021-45292-001&lang=de&site=ehost-live
UR  - pmaki1@uic.edu
DP  - EBSCOhost
DB  - APA PsycInfo
ER  - 

TY  - THES
ID  - 2015-99130-318
AN  - 2015-99130-318
AU  - Mao, Liyang
T1  - Designing p-optimal item pools for multidimensional computerized adaptive testing
JF  - Dissertation Abstracts International Section A: Humanities and Social Sciences
JO  - Dissertation Abstracts International Section A: Humanities and Social Sciences
Y1  - 2015///
VL  - 76
IS  - 1-A(E)
PB  - ProQuest Information & Learning
SN  - 0419-4209
SN  - 978-1-321-16507-4
N1  - Accession Number: 2015-99130-318. Other Journal Title: Dissertation Abstracts International. Partial author list: First Author & Affiliation: Mao, Liyang; Michigan State U., US. Release Date: 20150629. Correction Date: 20221128. Publication Type: Dissertation Abstract (0400). Format Covered: Electronic. Document Type: Dissertation. Dissertation Number: AAI3635440. ISBN: 978-1-321-16507-4. Language: EnglishMajor Descriptor: Adaptive Testing; Statistical Analysis; Test Construction; Computerized Assessment. Classification: Educational & School Psychology (3500). Population: Human (10). Methodology: Empirical Study; Quantitative Study. 
AB  - The interest in multidimensional computerized adaptive testing (MCAT) has grown considerably over the last few years. While a significant amount of research has been conducted on item selection and ability estimation methods for MCAT, few studies specifically addressed the item pool design for MCAT. To ensure a proper functioning of MCAT, a well-designed item pool is imperative. A well-designed item pool should consist of a number of well-balanced items that achieve appropriate test precision, item usage, as well as lower the cost of item creation. One method to develop such an item pool is the  p-optimality method, which is proposed by Reckase (2003 & 2007) for unidimensional CAT. This paper aims to develop p-optimal item pools for MCAT by extending the Reckase's method to a multidimensional context. The extension includes the generation of a multidimensional optimal item based on the D-Optimality item selection creation, the definition of the MDIFF-bin to describe multidimensional item succinctly for item pool design, and the interpretation for the p-optimal item pool in a multidimensional context. In this paper, a total of 24 p-optimal item pools were designed and then developed for different test specification, with different correlation among dimensions, based on different bin size, and under the condition with or without item exposure control. The characteristics for the 24  p-optimal item pools are summarized. A simulation study was conducted to evaluate the performance of the p-optimal item pools against baseline pools existing in research literature. Results show that p-optimal item pools achieve similar levels of measurement accuracy as baseline pools, but they consist of fewer items and perform better in terms of item pool usage and test security. The characteristics and the performance of the p-optimal item pools are affected by factors such as test specification, correlation among dimensions, bin size, and item exposure control. The results in this study can provide a general guideline for the item pool development for MCAT. More importantly, because the p-optimal item pool is specifically tailored to the MCAT programs, the p-optimal item pool design procedure described in this study can be adapted to other MCAT programs with different features and purposes. The end product of the p-optimal item pool design can be used as an instructive guide for item creation, item pool development, and item pool management. (PsycInfo Database Record (c) 2022 APA, all rights reserved)
KW  - multidimensional context
KW  - test specification
KW  - baseline pools
KW  - exposure control
KW  - computerized adaptive testing
KW  - Adaptive Testing
KW  - Statistical Analysis
KW  - Test Construction
KW  - Computerized Assessment
UR  - https://search.ebscohost.com/login.aspx?direct=true&db=psyh&AN=2015-99130-318&lang=de&site=ehost-live
DP  - EBSCOhost
DB  - APA PsycInfo
ER  - 

TY  - THES
ID  - 2020-28122-033
AN  - 2020-28122-033
AU  - Gaudet, Charles Edward
T1  - Detection of cognitive change: Examination of approaches for improving the accuracy on impact
JF  - Dissertation Abstracts International: Section B: The Sciences and Engineering
JO  - Dissertation Abstracts International: Section B: The Sciences and Engineering
Y1  - 2020///
VL  - 81
IS  - 4-B
PB  - ProQuest Information & Learning
SN  - 0419-4217
N1  - Accession Number: 2020-28122-033. Other Journal Title: Dissertation Abstracts International. Partial author list: First Author & Affiliation: Gaudet, Charles Edward; University of Rhode Island, Psychology, US. Release Date: 20200504. Publication Type: Dissertation Abstract (0400). Format Covered: Electronic. Document Type: Dissertation. Dissertation Number: AAI22588679. Language: EnglishMajor Descriptor: Brain Concussion; Cognitive Assessment; Injuries; Measurement; Test-Retest Reliability. Minor Descriptor: Cognitive Processes; Neuropsychological Assessment; Public Health. Classification: Health & Mental Health Treatment & Prevention (3300); Physiological Psychology & Neuroscience (2500). Population: Human (10). Methodology: Empirical Study; Quantitative Study. 
AB  - Concussion is an increasingly recognized public health concern. Proper assessment and management of concussion are critical factors in mitigating adverse effects associated with this injury. Neuropsychological assessment has demonstrated utility in identifying cognitive symptoms related to concussion and monitoring their resolution. Early methods involved administering paper and pencil tests to appraise cognitive domains thought to be most affected by concussion. As interest in concussion and methods of assessment evolved, baseline testing became an integral component of assessment in sport-related concussion (SRC). Baseline testing consists of administering a healthy, or non-injured, individual a battery of cognitive tests that subsequently serve as a reference point to evaluate the individual's performance on the same tests following a suspected injury or change in cognitive status. Recent advances, spurred by an interest in increasing access to baseline testing, contributed to the adoption of computerized neurocognitive tests (CNTs). CNTs allow for baseline testing of groups of individuals, in one setting, and in a short amount of time. Immediate Post Concussion and Cognitive Testing (ImPACT) has emerged as the most commonly used CNT in the assessment and management of SRC. This body of research aimed to explore ImPACT's reliability and validity to appraise its efficacy in accurately detecting cognitive change associated with concussion and explore potential improvements. The first chapter is devoted to examining ImPACT's test-retest reliability, which refers to the expected consistency in results over time in healthy individuals. This study examines ImPACT score reports for 107 healthy individuals that included testing at two time points. Results reveal less than adequate test-retest reliability attributable to, at least in part, a restricted range of possible scores, or the presence of a ceiling effect, on numerous subscales. Additional discussion includes corrective measures, such as proactively identifying individuals producing maximum scores on baseline testing, extending the length of subscales, and incorporating adaptive testing. The second chapter evaluates ImPACT's validity, and specifically, its classification accuracy in differentiating between individuals with and without concussion. This study incorporates a novel approach through its use of standardized regression based (SRB) reliable change index (RCI) scores to measure post-injury testing deviations from baseline scores. The SRB methodology, coupled with discriminant function analyses (DFAs), is compared to current interpretive procedures. The study includes 129 individuals without concussion whose SRB RCI scores are compared to 81 individuals with concussion. Results of analyses suggest that the current interpretive procedure performs at a chance level in accurately identifying individuals with concussion; conversely, the SRB method and DFA approach yield positive predictive values exceeding 80%, however sensitivities below 50%. Additionally, the Post Concussion Symptom Scale (PCSS), a self-report measure of symptoms, is largely equivalent to ImPACT's cognitive measures in classification accuracy. Collectively, these results raise considerable concern regarding ImPACT's efficacy as a measure to aid in the assessment and management of concussion. (PsycInfo Database Record (c) 2020 APA, all rights reserved)
KW  - cognitive change
KW  - management
KW  - concussion
KW  - Brain Concussion
KW  - Cognitive Assessment
KW  - Injuries
KW  - Measurement
KW  - Test-Retest Reliability
KW  - Cognitive Processes
KW  - Neuropsychological Assessment
KW  - Public Health
UR  - https://search.ebscohost.com/login.aspx?direct=true&db=psyh&AN=2020-28122-033&lang=de&site=ehost-live
DP  - EBSCOhost
DB  - APA PsycInfo
ER  - 

TY  - JOUR
ID  - 2021-06654-003
AN  - 2021-06654-003
AU  - Dutra, Letícia Rocha
AU  - Coster, Wendy J.
AU  - Neves, Jorge A. B.
AU  - de Brito Brandão, Marina
AU  - Sampaio, Rosana Ferreira
AU  - Mancini, Marisa Cotta
T1  - Determinants of time to care for children and adolescents with disabilities
JF  - OTJR: Occupation, Participation and Health
JO  - OTJR: Occupation, Participation and Health
JA  - OTJR (Thorofare N J)
Y1  - 2021/01//
VL  - 41
IS  - 1
SP  - 15
EP  - 23
PB  - Sage Publications
SN  - 1539-4492
SN  - 1938-2383
AD  - Mancini, Marisa Cotta, Graduate Program in Rehabilitation Sciences, Universidade Federal de Minas Gerais, Av. Antonio Carlos 6627, Campus Pampulha, MG, Belo Horizonte, Brazil, 31270-901
N1  - Accession Number: 2021-06654-003. PMID: 32741244 Other Journal Title: Occupational Therapy Journal of Research. Partial author list: First Author & Affiliation: Dutra, Letícia Rocha; Universidade Federal de Minas Gerais, Belo Horizonte, Brazil. Other Publishers: SLACK. Release Date: 20220228. Publication Type: Journal (0100), Peer Reviewed Journal (0110). Format Covered: Electronic. Document Type: Journal Article. Language: EnglishMajor Descriptor: Autism Spectrum Disorders; Cerebral Palsy; Caregiving. Minor Descriptor: Pediatrics. Classification: Home Care & Hospice (3375). Population: Human (10); Male (30); Female (40). Location: Brazil. Age Group: Childhood (birth-12 yrs) (100); Preschool Age (2-5 yrs) (160); School Age (6-12 yrs) (180); Adolescence (13-17 yrs) (200); Adulthood (18 yrs & older) (300); Young Adulthood (18-29 yrs) (320). Tests & Measures: Pediatric Evaluation of Disability Inventory–-Computer Adaptive Test; Brazilian Economic Classification Criterion; Childhood Autism Rating Scale DOI: 10.1037/t49458-000. Methodology: Empirical Study; Quantitative Study. Page Count: 9. Issue Publication Date: Jan, 2021. Copyright Statement: The Author(s). 2020. 
AB  - Time use studies uncover the organization of daily routine of families of children with disabilities. The objective of this study is to identify determinants of time spent caring for children/adolescents with cerebral palsy (CP), autism spectrum disorder (ASD), and typical development (TD). Participants were caregivers of children/adolescents with/without disability. Structural equation modeling tested a proposed model of time spent in child care. The variables in the model were as follows: questionnaire (families’ socioeconomic status [SES]), children’s functioning (The Pediatric Evaluation of Disability Inventory–Computer Adaptive Test [PEDI-CAT]); hours of care (daily diaries), number of adaptations used, and help with child care (parents’ report). Distinct variable combinations explained 78% of the variation in the time to care (TD model), followed by 42% (ASD) and 29% (CP). Adaptations indirectly affected time to care through its effect on functioning (CP); family’s SES affected functioning through its effect on adaptation use (ASD). In conclusion, knowledge of factors affecting caregivers’ time spent on children’s care help occupational therapists implement family-centered strategies. (PsycInfo Database Record (c) 2022 APA, all rights reserved)
KW  - time spent on caregiving
KW  - children with disabilities
KW  - cerebral palsy
KW  - autism spectrum disorder
KW  - Autism Spectrum Disorders
KW  - Cerebral Palsy
KW  - Caregiving
KW  - Pediatrics
U1  - Sponsor: National Council for Scientific and Technological Development. Grant: 306948/2014-1. Recipients: No recipient indicated
U1  - Sponsor: State of Minas Gerais, Fundação de Amparo a Pesquisa do Estado de Minas Gerais, Brazil. Grant: CDS—PPM-00611-17. Other Details: Research Support Foundation. Recipients: No recipient indicated
U1  - Sponsor: Coordination for the Improvement of Higher Education Personnel. Grant: 001. Recipients: No recipient indicated
UR  - https://search.ebscohost.com/login.aspx?direct=true&db=psyh&AN=2021-06654-003&lang=de&site=ehost-live
UR  - marisacmancini@gmail.com
DP  - EBSCOhost
DB  - APA PsycInfo
ER  - 

TY  - JOUR
ID  - 2020-13397-001
AN  - 2020-13397-001
AU  - Tian, Xueyin
AU  - Dai, Buyun
T1  - Developing a computerized adaptive test to assess stress in Chinese college students
JF  - Frontiers in Psychology
JO  - Frontiers in Psychology
JA  - Front Psychol
Y1  - 2020/02/07/
VL  - 11
PB  - Frontiers Media S.A.
SN  - 1664-1078
AD  - Dai, Buyun
N1  - Accession Number: 2020-13397-001. PMID: 32116885 Partial author list: First Author & Affiliation: Tian, Xueyin; School of Psychology, Jiangxi Normal University, Nanchang, China. Other Publishers: Frontiers Research Foundation. Release Date: 20201105. Publication Type: Journal (0100), Peer Reviewed Journal (0110). Format Covered: Electronic. Document Type: Journal Article. Language: EnglishMajor Descriptor: Adaptive Testing; Item Response Theory; Stress; Test Reliability; Test Validity. Minor Descriptor: College Students; Measurement; Psychological Stress; Test Construction; Test Items. Classification: Clinical Psychological Testing (2224); Psychological Disorders (3210). Population: Human (10); Male (30); Female (40). Location: China. Age Group: Adolescence (13-17 yrs) (200); Adulthood (18 yrs & older) (300); Young Adulthood (18-29 yrs) (320). Tests & Measures: Computerized Adaptive Test-Stress; Psychological Stress Measurement Scale--Chinese Version; College Student Stress Scale--Chinese Version; Psychological Stress Feeling Scale for College Students--Chinese Version; Psychological Stress Test-Chinese Version; Stress Scale for College Students--Chinese Version; Depression Anxiety Stress Scale--Chinese Version; Stress Overload Scale--Chinese Version; Perceived Stress Scale--Chinese Version. Methodology: Empirical Study; Quantitative Study. ArtID: 7. Issue Publication Date: Feb 7, 2020. Publication History: First Posted Date: Feb 7, 2020; Accepted Date: Jan 3, 2020; First Submitted Date: Aug 29, 2019. Copyright Statement: This is an open-access article distributed under the terms of the Creative Commons Attribution License (CC BY). The use, distribution or reproduction in other forums is permitted, provided the original author(s) and the copyright owner(s) are credited and that the original publication in this journalis cited, in accordance with accepted academic practice. No use, distribution or reproduction is permitted which does not comply with these terms. Tian and Dai. 2020. 
AB  - Stress is among the most prevalent problems in life; thus, measurement of stress is of great importance for disease prevention and evaluation. This work aims to develop a computerized adaptive test (CAT) application to measure stress (CAT-S) based on item response theory (IRT). Two types of analyses were performed. The first analysis was to meet the psychometric requirements of the CAT-S. A Paper and Pencil (P&P) test involving 226 items was developed based on eight stress-related scales, and 972 Chinese college students completed the test. The first seven scales were used to build the item bank, and the last scale (i.e., the Perceived Stress Scale, PSS) was used to determine the convergent validity of the CAT-S. With some statistical considerations, such as item fit, discrimination, differential item functioning (DIF), and the assumption of unidimensionality, the final item bank comprised 93 items. The second analysis was to simulate the CAT adaptively using the existing item response. A Bayesian method called Expected a Posterior method (EAP) was used to estimate θ. For the item selection strategy, the greatest item information was considered at each step. The stopping rule was determined by the fixed length (10, 11, 12, …, 20, and 93) or the prespecified level of measurement precision (standard errors of 0.3, 0.4, 0.5, 0.6, 0.7, and 0.8). Finally, the criterion validity was tested by using PSS as a criterion and analyzing the effect of CAT-S diagnosis with a receiver operating curve (ROC). The results showed that (1) the final stress item bank had good quality based on the psychometric evaluation, (2) the CAT-scores were highly correlated with the scores of the final item bank, (3) the scores of the P&P form of PSS were correlated with those of the CAT-S (r > 0.5), (4) the value of the area under the ROC curve (AUC) was greater than 0.7 under each stopping rule, and (5) the CAT-S needed only a small number of items to obtain a highly precise measure of stress. Therefore, the CAT-S presented the theoretically expected advantages, which enabled a rapid, accurate, and efficient dynamic and intelligent measurement of stress. (PsycInfo Database Record (c) 2020 APA, all rights reserved)
KW  - stress
KW  - computerized adaptive test
KW  - item response theory
KW  - item bank
KW  - measurement precision
KW  - college students
KW  - China
KW  - reliability
KW  - validity
KW  - Adaptive Testing
KW  - Item Response Theory
KW  - Stress
KW  - Test Reliability
KW  - Test Validity
KW  - College Students
KW  - Measurement
KW  - Psychological Stress
KW  - Test Construction
KW  - Test Items
U1  - Sponsor: Jiangxi Normal University, Young Talent Cultivation Program, China. Date: from 2017. Recipients: No recipient indicated
U1  - Sponsor: Jiangxi Education Department, China. Grant: GJJ170230. Other Details: Science and Technology Project. Recipients: No recipient indicated
DO  - 10.3389/fpsyg.2020.00007
UR  - https://search.ebscohost.com/login.aspx?direct=true&db=psyh&AN=2020-13397-001&lang=de&site=ehost-live
UR  - biweijianpsy@qq.com
DP  - EBSCOhost
DB  - APA PsycInfo
ER  - 

TY  - JOUR
ID  - 2019-48655-006
AN  - 2019-48655-006
AU  - Sunderland, Matthew
AU  - Batterham, Philip
AU  - Carragher, Natacha
AU  - Calear, Alison
AU  - Slade, Tim
T1  - Developing and validating a computerized adaptive test to measure broad and specific factors of internalizing in a community sample
JF  - Assessment
JO  - Assessment
JA  - Assessment
Y1  - 2019/09//
VL  - 26
IS  - 6
SP  - 1030
EP  - 1045
PB  - Sage Publications
SN  - 1073-1911
SN  - 1552-3489
AD  - Sunderland, Matthew, National Drug and Alcohol Research Centre, UNSW Australia, Building R1, Randwick Campus, Sydney, NSW, Australia, 2052
N1  - Accession Number: 2019-48655-006. PMID: 28467115 Partial author list: First Author & Affiliation: Sunderland, Matthew; University of New South Wales (UNSW) Australia, Sydney, NSW, Australia. Release Date: 20190905. Publication Type: Journal (0100), Peer Reviewed Journal (0110). Format Covered: Electronic. Document Type: Journal Article. Language: EnglishGrant Information: Sunderland, Matthew. Major Descriptor: Adaptive Testing; Affective Disorders; Anxiety Disorders; Internalization; Psychometrics. Minor Descriptor: Comorbidity; Item Response Theory; Psychopathology; Screening Tests; Test Construction; Test Validity. Classification: Clinical Psychological Testing (2224); Psychological Disorders (3210). Population: Human (10); Male (30); Female (40); Outpatient (60). Location: Australia. Age Group: Adulthood (18 yrs & older) (300). Tests & Measures: Multidimensional Computerized Adaptive Test; Legacy Screening Scale; Short OCD Screener; Social Phobia Screener; DSM-5 Symptom Checklist; Panic Disorder Screener DOI: 10.1037/t47047-000; Mini International Neuropsychiatric Interview DOI: 10.1037/t18597-000; Generalized Anxiety Disorder 7 DOI: 10.1037/t02591-000; Physical Health Questionnaire DOI: 10.1037/t03593-000; Social Phobia Inventory; Patient-Reported Outcomes Measurement Information System DOI: 10.1037/t06780-000. Methodology: Empirical Study; Quantitative Study. Supplemental Data: Other Internet. References Available: Y. Page Count: 16. Issue Publication Date: Sep, 2019. Copyright Statement: The Author(s). 2017. 
AB  - Highly efficient assessments that better account for comorbidity between mood and anxiety disorders (internalizing) are required to identify individuals who are most at risk of psychopathology in the community. The current study examined the efficiency and validity associated with a multidimensional computerized adaptive test (CAT) to measure broad and specific levels of internalizing psychopathology. The sample comprised 3,175 respondents to an online survey. Items from five banks (generalized anxiety, depression, obsessive–compulsive disorder, panic disorder, social anxiety disorder) were jointly calibrated using a bifactor item response theory model. Simulations indicated that an adaptive algorithm could accurately (rs ≥ 0.90) estimate general internalizing and specific disorder scores using on average 44 items in comparison with the full 133-item bank (67% reduction in items). Scores on the CAT demonstrate convergent and divergent validity with previously validated short severity scales and could significantly differentiate cases of DSM-5 disorder. As such, the CAT validly measures both broad and specific constructs of internalizing disorders in a manner similar to the full item bank and a static brief form but with greater gains in efficiency and, therefore, a reduced degree of respondent burden. (PsycINFO Database Record (c) 2019 APA, all rights reserved)
KW  - comorbidity
KW  - mood disorders
KW  - anxiety disorders
KW  - internalizing
KW  - screening scales
KW  - computerized adaptive testing
KW  - item response theory
KW  - Adaptive Testing
KW  - Affective Disorders
KW  - Anxiety Disorders
KW  - Internalization
KW  - Psychometrics
KW  - Comorbidity
KW  - Item Response Theory
KW  - Psychopathology
KW  - Screening Tests
KW  - Test Construction
KW  - Test Validity
U1  - Sponsor: National Health and Medical Research Council, Australia. Grant: 1043952. Other Details: Project Grant. Recipients: No recipient indicated
U1  - Sponsor: National Health and Medical Research Council. Grant: 1052327; 1083311; 1013199. Other Details: Fellowships. Recipients: Sunderland, Matthew; Batterham, Philip; Calear, Alison
DO  - 10.1177/1073191117707817
UR  - https://search.ebscohost.com/login.aspx?direct=true&db=psyh&AN=2019-48655-006&lang=de&site=ehost-live
UR  - ORCID: 0000-0002-7028-725X
UR  - matthews@unsw.edu.au
DP  - EBSCOhost
DB  - APA PsycInfo
ER  - 

TY  - JOUR
ID  - 2017-04201-002
AN  - 2017-04201-002
AU  - Chen, Ping
AU  - Wang, Chun
AU  - Xin, Tao
AU  - Chang, Hua‐Hua
T1  - Developing new online calibration methods for multidimensional computerized adaptive testing
JF  - British Journal of Mathematical and Statistical Psychology
JO  - British Journal of Mathematical and Statistical Psychology
JA  - Br J Math Stat Psychol
Y1  - 2017/02//
VL  - 70
IS  - 1
SP  - 81
EP  - 117
PB  - Wiley-Blackwell Publishing Ltd.
SN  - 0007-1102
SN  - 2044-8317
AD  - Chen, Ping, Collaborative Innovation Center of Assessment toward Basic Education Quality, Beijing Normal University, No. 19, Xin Jie Kou Wai Street, Hai Dian District, Beijing, China, 100875
N1  - Accession Number: 2017-04201-002. PMID: 28130937 Partial author list: First Author & Affiliation: Chen, Ping; Beijing Normal University, Beijing, China. Other Publishers: British Psychological Society. Release Date: 20170209. Correction Date: 20190211. Publication Type: Journal (0100), Peer Reviewed Journal (0110). Format Covered: Electronic. Document Type: Journal Article. Language: EnglishConference Information: Annual Meeting of the Psychometric Society, 2013, Arnhem, Netherlands. Conference Note: Part of the paper was originally presented in the aforementioned conference. Major Descriptor: Algorithms; Item Response Theory; Computerized Assessment. Classification: Statistics & Mathematics (2240). Tests & Measures: Armed Services Vocational Aptitude Battery DOI: 10.1037/t11801-000. Methodology: Empirical Study; Mathematical Model; Quantitative Study; Scientific Simulation. References Available: Y. Page Count: 37. Issue Publication Date: Feb, 2017. Publication History: Revised Date: Sep 14, 2016; First Submitted Date: Apr 19, 2016. Copyright Statement: The British Psychological Society. 2017. 
AB  - Multidimensional computerized adaptive testing (MCAT) has received increasing attention over the past few years in educational measurement. Like all other formats of CAT, item replenishment is an essential part of MCAT for its item bank maintenance and management, which governs retiring overexposed or obsolete items over time and replacing them with new ones. Moreover, calibration precision of the new items will directly affect the estimation accuracy of examinees’ ability vectors. In unidimensional CAT (UCAT) and cognitive diagnostic CAT, online calibration techniques have been developed to effectively calibrate new items. However, there has been very little discussion of online calibration in MCAT in the literature. Thus, this paper proposes new online calibration methods for MCAT based upon some popular methods used in UCAT. Three representative methods, Method A, the ‘one EM cycle’ method and the ‘multiple EM cycles’ method, are generalized to MCAT. Three simulation studies were conducted to compare the three new methods by manipulating three factors (test length, item bank design, and level of correlation between coordinate dimensions). The results showed that all the new methods were able to recover the item parameters accurately, and the adaptive online calibration designs showed some improvements compared to the random design under most conditions. (PsycINFO Database Record (c) 2019 APA, all rights reserved)
KW  - multidimensional computerized adaptive testing
KW  - item bank construction
KW  - item replenishment
KW  - online calibration
KW  - multidimensional item response theory model
KW  - algorithm
KW  - Algorithms
KW  - Calibration
KW  - Computer Simulation
KW  - Educational Measurement
KW  - Models, Statistical
KW  - Online Systems
KW  - Psychometrics
KW  - Reproducibility of Results
KW  - Sensitivity and Specificity
KW  - Algorithms
KW  - Item Response Theory
KW  - Computerized Assessment
U1  - Sponsor: National Natural Science Foundation of China, China. Grant: 31300862; 31660278. Recipients: No recipient indicated
U1  - Sponsor: Specialized Research Fund for the Doctoral Program of Higher Education. Grant: 20130003120002. Recipients: No recipient indicated
U1  - Sponsor: KLAS. Grant: 130028614. Recipients: No recipient indicated
DO  - 10.1111/bmsp.12083
UR  - https://search.ebscohost.com/login.aspx?direct=true&db=psyh&AN=2017-04201-002&lang=de&site=ehost-live
UR  - ORCID: 0000-0002-2920-4205
UR  - pchen@bnu.edu.cn
DP  - EBSCOhost
DB  - APA PsycInfo
ER  - 

TY  - JOUR
ID  - 2006-21491-003
AN  - 2006-21491-003
AU  - Turk, Dennis C.
AU  - Dworkin, Robert H.
AU  - Burke, Laurie B.
AU  - Gershon, Richard
AU  - Rothman, Margaret
AU  - Scott, Jane
AU  - Allen, Robert R.
AU  - Atkinson, J. Hampton
AU  - Chandler, Julie
AU  - Cleeland, Charles
AU  - Cowan, Penny
AU  - Dimitrova, Rozalina
AU  - Dionne, Raymond
AU  - Farrar, John T.
AU  - Haythornthwaite, Jennifer A.
AU  - Hertz, Sharon
AU  - Jadad, Alejandro R.
AU  - Jensen, Mark P.
AU  - Kellstein, David
AU  - Kerns, Robert D.
AU  - Manning, Donald C.
AU  - Martin, Susan
AU  - Max, Mitchell B.
AU  - McDermott, Michael P.
AU  - McGrath, Patrick
AU  - Moulin, Dwight E.
AU  - Nurmikko, Turo
AU  - Quessy, Steve
AU  - Raja, Srinivasa
AU  - Rappaport, Bob A.
AU  - Rauschkolb, Christine
AU  - Robinson, James P.
AU  - Royal, Mike A.
AU  - Simon, Lee
AU  - Stauffer, Joseph W.
AU  - Stucki, Gerold
AU  - Tollett, Jane
AU  - von Stein, Thorsten
AU  - Wallace, Mark S.
AU  - Wernicke, Joachim
AU  - White, Richard E.
AU  - Williams, Amanda C.
AU  - Witter, James
AU  - Wyrwich, Kathleen W.
T1  - Developing patient-reported outcome measures for pain clinical trials: IMMPACT recommendations
JF  - Pain
JO  - Pain
JA  - Pain
Y1  - 2006/11//
VL  - 125
IS  - 3
SP  - 208
EP  - 215
PB  - Elsevier Science
SN  - 0304-3959
SN  - 1872-6623
AD  - Turk, Dennis C., University of Washington, Seattle, WA, US, 98195
N1  - Accession Number: 2006-21491-003. PMID: 17069973 Partial author list: First Author & Affiliation: Turk, Dennis C.; University of Washington, Seattle, WA, US. Other Publishers: Lippincott Williams & Wilkins. Release Date: 20070212. Correction Date: 20221128. Publication Type: Journal (0100), Peer Reviewed Journal (0110). Format Covered: Electronic. Document Type: Journal Article. Language: EnglishMajor Descriptor: Chronic Pain; Classical Test Theory; Clinical Trials; Item Response Theory; Psychometrics. Minor Descriptor: Patient Reported Outcome Measures. Classification: Clinical Psychological Testing (2224); Physical & Somatic Disorders (3290). Population: Human (10). References Available: Y. Page Count: 8. Issue Publication Date: Nov, 2006. 
AB  - There is a lack of standardized and comprehensive outcome measures for pain trials that have adequate comparative information for relevant samples, that can be used across a variety of research applications, and that allow investigators to combine or compare groups with different demographic or disease characteristics. Measures based on classical test theory (CTT) have a number of limitations, including respondent burden, inability to compare measures putatively assessing the same construct, and assumptions about linearity that may be unwarranted. Item response theory (IRT) methods offer potential advantages, and there is a growing awareness of the potential of IRT to complement and even replace some traditional psychometric approaches. Computer Adaptive Testing (CAT) will likely be used increasingly as the technology improves and familiarity with this approach grows. The Initiative on Methods, Measurement, and Pain Assessment in Clinical Trials (IMMPACT) recommended that six core outcome domains should be considered when designing chronic pain clinical trials: (1) pain; (2) physical functioning; (3) emotional functioning; (4) participant ratings of improvement and satisfaction with treatment; (5) symptoms and adverse events; and (6) participant disposition. (PsycInfo Database Record (c) 2022 APA, all rights reserved)
KW  - chronic pain
KW  - clinical trials
KW  - patient-reported outcomes
KW  - psychometrics
KW  - Computer Adaptive Testing
KW  - classical test theory
KW  - item response theory
KW  - Clinical Trials as Topic
KW  - Humans
KW  - Outcome Assessment (Health Care)
KW  - Pain
KW  - Pain Measurement
KW  - Practice Guidelines as Topic
KW  - Practice Patterns, Physicians'
KW  - Surveys and Questionnaires
KW  - United States
KW  - Chronic Pain
KW  - Classical Test Theory
KW  - Clinical Trials
KW  - Item Response Theory
KW  - Psychometrics
KW  - Patient Reported Outcome Measures
U1  - Sponsor: Abbott Laboratories. Other Details: University of Rochester Office of Professional Education. Recipients: No recipient indicated
U1  - Sponsor: Allergan. Other Details: University of Rochester Office of Professional Education. Recipients: No recipient indicated
U1  - Sponsor: Alpharma. Other Details: University of Rochester Office of Professional Education. Recipients: No recipient indicated
U1  - Sponsor: AstraZeneca. Other Details: University of Rochester Office of Professional Education. Recipients: No recipient indicated
U1  - Sponsor: Celgene. Other Details: University of Rochester Office of Professional Education. Recipients: No recipient indicated
U1  - Sponsor: Eli Lilly and Co.. Other Details: University of Rochester Office of Professional Education. Recipients: No recipient indicated
U1  - Sponsor: NeurogesX. Other Details: University of Rochester Office of Professional Education. Recipients: No recipient indicated
U1  - Sponsor: Novartis Pharmaceuticals. Other Details: University of Rochester Office of Professional Education. Recipients: No recipient indicated
U1  - Sponsor: Pfizer. Other Details: University of Rochester Office of Professional Education. Recipients: No recipient indicated
U1  - Sponsor: Schwarz Biosciences. Other Details: University of Rochester Office of Professional Education. Recipients: No recipient indicated
DO  - 10.1016/j.pain.2006.09.028
UR  - https://search.ebscohost.com/login.aspx?direct=true&db=psyh&AN=2006-21491-003&lang=de&site=ehost-live
UR  - ORCID: 0000-0003-3761-8704
UR  - ORCID: 0000-0002-9568-2571
UR  - Turkdc@u.washington.edu
DP  - EBSCOhost
DB  - APA PsycInfo
ER  - 

TY  - JOUR
ID  - 2008-06905-012
AN  - 2008-06905-012
AU  - Walter, Otto B.
AU  - Becker, Janine
AU  - Bjorner, Jakob B.
AU  - Fliege, Herbert
AU  - Klapp, Burghard F.
AU  - Rose, Matthias
T1  - Development and evaluation of a computer adaptive test for 'anxiety' (Anxiety-CAT)
JF  - Quality of Life Research: An International Journal of Quality of Life Aspects of Treatment, Care & Rehabilitation
JO  - Quality of Life Research: An International Journal of Quality of Life Aspects of Treatment, Care & Rehabilitation
JA  - Qual Life Res
Y1  - 2007/08//
VL  - 16
IS  - Suppl1
SP  - 143
EP  - 155
PB  - Springer
SN  - 0962-9343
SN  - 1573-2649
AD  - Walter, Otto B., Psychological Institute IV, Statistics and Quantitative Methods, University of Munster, Fliednerstr. 21, 48149, Munster, Germany
N1  - Accession Number: 2008-06905-012. Partial author list: First Author & Affiliation: Walter, Otto B.; Psychological Institute IV, Statistics and Quantitative Methods, University of Munster, Munster, Germany. Release Date: 20080609. Correction Date: 20190211. Publication Type: Journal (0100), Peer Reviewed Journal (0110). Format Covered: Electronic. Document Type: Journal Article. Language: EnglishMajor Descriptor: Item Response Theory; Mental Disorders; Somatoform Disorders; Test Reliability; Test Validity. Minor Descriptor: Adaptive Testing; Anxiety; Computer Applications; Computer Software; Test Construction; Computerized Assessment. Classification: Statistics & Mathematics (2240); Psychological & Physical Disorders (3200). Population: Human (10); Male (30); Female (40); Inpatient (50); Outpatient (60). Location: Germany. Age Group: Adulthood (18 yrs & older) (300). Tests & Measures: Berlin Mood Questionnaire; Complaint Questionnaire; Narcissism-Inventory; Psychological General Well-Being Index; Self Efficacy and Optimism Inventory; Daily Life Questionnaire DOI: 10.1037/t23250-000; SF-36 Health Survey; State Trait Anxiety Inventory; Beck Depression Inventory DOI: 10.1037/t00741-000; Center for Epidemiologic Studies Depression Scale; Perceived Stress Scale DOI: 10.1037/t02889-000. Methodology: Empirical Study; Quantitative Study. References Available: Y. Page Count: 13. Issue Publication Date: Aug, 2007. 
AB  - Within the framework of item response theory (IRT), we developed a German version of an item bank, as well as a software application that can be employed to measure anxiety by means of a computer adaptive test (CAT). A sample of n = 2348 psychiatric and psychosomatic patients answered a set of up to 13 standardized questionnaires. 81 items drawn from these questionnaires were considered pertinent to the anxiety construct. Various tests were conducted to ensure the suitability of these items for an IRT-based assessment. After these tests, 50 items remained in the item bank and were calibrated using the Generalized Partial Credit Model. Simulation studies conducted on an independent sample of n = 1528 respondents indicate that 6-8 items suffice to measure the latent trait with high precision (standard error ≤0.32). CAT scores correlated highly with scores estimated from all available items (r = .97) and scale scores of the State Trait Anxiety Inventory (STAI, state scale, r = .93). Within a routine clinical setting, 102 in-patients answered the Anxiety-CAT along with a number of established anxiety questionnaires. The correlation between the Anxiety-CAT and the STAI state scale was still high (r = .60), but lower than the correlations found in the simulation studies. The Anxiety-CAT was able to differentiate between mental health disorders in a similar manner as established questionnaires. These results suggest that the Anxiety-CAT does indeed exhibit the advantages expected from theory, but the results of further studies are needed in order to judge its full potential for research and clinical practice. (PsycINFO Database Record (c) 2019 APA, all rights reserved)
KW  - test development
KW  - test evaluation
KW  - test validity
KW  - test reliability
KW  - computer adaptive test
KW  - anxiety
KW  - item response theory
KW  - computer software application
KW  - psychiatric patients
KW  - psychosomatic patients
KW  - Item Response Theory
KW  - Mental Disorders
KW  - Somatoform Disorders
KW  - Test Reliability
KW  - Test Validity
KW  - Adaptive Testing
KW  - Anxiety
KW  - Computer Applications
KW  - Computer Software
KW  - Test Construction
KW  - Computerized Assessment
U1  - Sponsor: Deutsche Forschungsgemeinschaft, Germany. Grant: RO 2258/2-1. Recipients: No recipient indicated
DO  - 10.1007/s11136-007-9191-7
UR  - https://search.ebscohost.com/login.aspx?direct=true&db=psyh&AN=2008-06905-012&lang=de&site=ehost-live
UR  - walterob@psy.uni-muenster.de
DP  - EBSCOhost
DB  - APA PsycInfo
ER  - 

TY  - JOUR
ID  - 2020-60634-001
AN  - 2020-60634-001
AU  - Abberger, Birgit
AU  - Haschke, Anne
AU  - Wirtz, Markus
AU  - Kroehne, Ulf
AU  - Bengel, Juergen
AU  - Baumeister, Harald
T1  - Development and evaluation of a computer adaptive test to assess anxiety in cardiovascular rehabilitation patients
JF  - Archives of Physical Medicine and Rehabilitation
JO  - Archives of Physical Medicine and Rehabilitation
JA  - Arch Phys Med Rehabil
Y1  - 2013/12//
VL  - 94
IS  - 12
SP  - 2433
EP  - 2439
PB  - Elsevier Science
SN  - 0003-9993
SN  - 1532-821X
AD  - Abberger, Birgit, Dept of Rehabilitation and Psychotherapy, Institute of Psychology, University of Freiburg, Engelbergerstrasse 41, D-79085, Freiburg, Germany
N1  - Accession Number: 2020-60634-001. PMID: 23880319 Other Journal Title: Archives of Physical Medicine. Partial author list: First Author & Affiliation: Abberger, Birgit; Department of Rehabilitation and Psychotherapy, Institute of Psychology, University of Freiburg, Freiburg, Germany. Other Publishers: American Congress of Physical Medicine; W B Saunders. Release Date: 20200824. Correction Date: 20230112. Publication Type: Journal (0100), Peer Reviewed Journal (0110). Format Covered: Electronic. Document Type: Journal Article. Language: EnglishConference Information: Rehabilitation Science Colloquium, Mar, 2012, Hamburg, Germany. Conference Note: Portions of this research were presented at the aforementioned conference and at the German Psychological Society, May 10, 2013, Trier, Germany. Major Descriptor: Adaptive Testing; Anxiety; Cardiovascular Disorders; Test Construction; Computerized Assessment. Minor Descriptor: Convergent Validity; Discriminant Validity; Patients; Rehabilitation; Simulation; Test Validity. Classification: Clinical Psychological Testing (2224); Cardiovascular Disorders (3295). Population: Human (10); Male (30); Female (40). Location: Germany. Age Group: Adulthood (18 yrs & older) (300). Tests & Measures: Computer Adaptive Test for the Assessment of Anxiety in Cardiovascular Rehabilitation Patients; Hospital Anxiety and Depression Scale DOI: 10.1037/t03589-000; 12-Item Short Form Health Survey DOI: 10.1037/t07021-000. Methodology: Empirical Study; Longitudinal Study; Quantitative Study. References Available: Y. Page Count: 7. Issue Publication Date: Dec, 2013. Copyright Statement: American Congress of Rehabilitation Medicine. 2017. 
AB  - Objective: To develop and evaluate a computer adaptive test for the assessment of anxiety in cardiovascular rehabilitation patients (ACAT-cardio) that tailors an optimal test for each patient and enables precise and time-effective measurement. Design: Simulation study, validation study (against the anxiety subscale of the Hospital Anxiety and Depression Scale and the physical component summary scale of the 12-Item Short-Form Health Survey), and longitudinal study (beginning and end of rehabilitation). Setting: Cardiac rehabilitation centers. Participants: Cardiovascular rehabilitation patients: simulation study sample (nZ106; mean age, 57.8y; 25.5% women) and validation and longitudinal study sample (nZ138; mean age, 58.6 and 57.9y, respectively; 16.7% and 12.1% women, respectively). Interventions: Not applicable. Main Outcome Measures: Hospital Anxiety and Depression Scale, 12-Item Short-Form Health Survey, and ACAT-cardio. Results: The mean number of items was 9.2 with an average processing time of 1:13 minutes when an SE .50 was used as a stopping rule; with an SE .32, there were 28 items and a processing time of 3:47 minutes. Validity could be confirmed via correlations between .68 and .81 concerning convergent validity (ACAT-cardio vs Hospital Anxiety and Depression Scale anxiety subscale) and correlations between .47 and .30 concerning discriminant validity (ACAT-cardio vs 12-Item Short-Form Health Survey physical component summary scale). Sensitivity to change was moderate to high with standardized response means between .45 and .82. Conclusions: The ACAT-cardio shows good psychometric properties and provides the opportunity for an innovative and time-effective assessment of anxiety in cardiovascular rehabilitation. A more flexible stopping rule might further improve the ACAT-cardio. Additionally, testing in other cardiovascular populations would increase generalizability. (PsycInfo Database Record (c) 2023 APA, all rights reserved)
KW  - computer adaptive test
KW  - anxiety
KW  - cardiovascular rehabilitation
KW  - patients
KW  - CAT
KW  - simulation
KW  - test development
KW  - test validity
KW  - ACAT-cardio
KW  - Anxiety
KW  - Cardiac Rehabilitation
KW  - Cardiovascular Diseases
KW  - Female
KW  - Humans
KW  - Longitudinal Studies
KW  - Male
KW  - Middle Aged
KW  - Psychiatric Status Rating Scales
KW  - Psychometrics
KW  - Surveys and Questionnaires
KW  - Adaptive Testing
KW  - Anxiety
KW  - Cardiovascular Disorders
KW  - Test Construction
KW  - Computerized Assessment
KW  - Convergent Validity
KW  - Discriminant Validity
KW  - Patients
KW  - Rehabilitation
KW  - Simulation
KW  - Test Validity
U1  - Sponsor: Illa and Werner Zarnekow Foundation. Grant: T225-18.152. Recipients: No recipient indicated
DO  - 10.1016/j.apmr.2013.07.009
UR  - https://search.ebscohost.com/login.aspx?direct=true&db=psyh&AN=2020-60634-001&lang=de&site=ehost-live
UR  - abberger@psychologie.uni-freiburg.de
DP  - EBSCOhost
DB  - APA PsycInfo
ER  - 

TY  - JOUR
ID  - 2016-58563-010
AN  - 2016-58563-010
AU  - Eisen, Susan V.
AU  - Schultz, Mark R.
AU  - Ni, Pengsheng
AU  - Haley, Stephen M.
AU  - Smith, Eric G.
AU  - Spiro, Avron
AU  - Osei-Bonsu, Princess E.
AU  - Nordberg, Sam
AU  - Jette, Alan M.
T1  - Development and validation of a computerized-adaptive test for PTSD (P-CAT)
JF  - Psychiatric Services
JO  - Psychiatric Services
JA  - Psychiatr Serv
Y1  - 2016/10/01/
VL  - 67
IS  - 10
SP  - 1116
EP  - 1123
PB  - American Psychiatric Assn
SN  - 1075-2730
SN  - 1557-9700
AD  - Eisen, Susan V.
N1  - Accession Number: 2016-58563-010. PMID: 27247175 Other Journal Title: Hospital & Community Psychiatry. Partial author list: First Author & Affiliation: Eisen, Susan V.; Center for Healthcare Organization and Implementation Research, Edith Nourse Rogers Memorial Veterans Hospital, Bedford, MA, US. Release Date: 20170309. Correction Date: 20221128. Publication Type: Journal (0100), Peer Reviewed Journal (0110). Format Covered: Electronic. Document Type: Journal Article. Language: EnglishConference Information: Annual Meeting of the International Society for Traumatic Stress Studies, Nov, 2013. Grant Information: Spiro, Avron. Conference Note: Some of the results were presented at the aforementioned conference. Major Descriptor: Adaptive Testing; Posttraumatic Stress Disorder; Psychometrics; Test Construction; Test Validity. Classification: Clinical Psychological Testing (2224); Anxiety Disorders (3215). Population: Human (10); Male (30); Female (40). Location: US. Age Group: Adulthood (18 yrs & older) (300); Young Adulthood (18-29 yrs) (320); Thirties (30-39 yrs) (340); Middle Age (40-64 yrs) (360). Tests & Measures: Structured Clinical Interview for DSM-IV-PTSD Module; Mississippi Scale for Combat-Related PTSD; PTSD-Computerized-Adaptive Test [Appended] DOI: 10.1037/t60371-000; Primary Care PTSD Screen DOI: 10.1037/t04709-000; PTSD Checklist—Civilian Version DOI: 10.1037/t02622-000. Methodology: Empirical Study; Field Study; Quantitative Study. References Available: Y. Page Count: 8. Issue Publication Date: Oct 1, 2016. Publication History: First Posted Date: Jun 1, 2016; Accepted Date: Feb 26, 2016; Revised Date: Jan 28, 2016; First Submitted Date: Sep 4, 2015. 
AB  - Objective: The primary purpose was to develop, field test, and validate a computerized-adaptive test (CAT) for posttraumatic stress disorder (PTSD) to enhance PTSD assessment and decrease the burden of symptom monitoring. Methods: Data sources included self-report and interviewe-radministered diagnostic interviews. The sample included 1,288 veterans. In phase 1, 89 items from a previously developed PTSD item pool were administered to a national sample of 1,085 veterans. A multidimensional graded-response item response theory model was used to calibrate items for incorporation into a CAT for PTSD (P-CAT). In phase 2, in a separate sample of 203 veterans, the P-CAT was validated against three other self-report measures (PTSD Checklist, Civilian Version; Mississippi Scale for Combat-Related PTSD; and Primary Care PTSD Screen) and the PTSD module of the Structured Clinical Interview for DSM-IV. Results: A bifactor model with one general PTSD factor and four subfactors consistent with DSM-5 (reexperiencing, avoidance, negative mood-cognitions, and arousal), yielded good fit. The P-CAT discriminated veterans with PTSD from those with other mental health conditions and those with no mental health conditions (Cohen’s d effect sizes >.90). The P-CAT also discriminated those with and without a PTSD diagnosis and those who screened positive versus negative for PTSD. Concurrent validity was supported by high correlations (r = .85–.89) with the validation measures. Conclusions: The P-CAT appears to be a promising tool for efficient and accurate assessment of PTSD symptomatology. Further testing is needed to evaluate its responsiveness to change. With increasing availability of computers and other technologies, CAT may be a viable and efficient assessment method. (PsycInfo Database Record (c) 2022 APA, all rights reserved)
KW  - PTSD-Computerized-Adaptive Test
KW  - psychometrics
KW  - test validity
KW  - test development
KW  - Adaptive Testing
KW  - Posttraumatic Stress Disorder
KW  - Psychometrics
KW  - Test Construction
KW  - Test Validity
U1  - Sponsor: US Department of Veterans Affairs, Health Services Research Development Service (HSR&D), US. Grant: IIR 09-342. Recipients: No recipient indicated
U1  - Sponsor: US Department of Veterans Affairs, Clinical Sciences Research and Development Service, US. Other Details: Senior Research Career Scientist award. Recipients: Spiro, Avron
U1  - Sponsor: HSR&D. Other Details: Career Development Award. Recipients: Smith, Eric G.
DO  - 10.1176/appi.ps.201500382
UR  - https://search.ebscohost.com/login.aspx?direct=true&db=psyh&AN=2016-58563-010&lang=de&site=ehost-live
UR  - seisen@bu.edu
DP  - EBSCOhost
DB  - APA PsycInfo
ER  - 

TY  - JOUR
ID  - 2021-20839-001
AN  - 2021-20839-001
AU  - Fan, Honglan
AU  - Cai, Yan
AU  - Wang, Siyu
AU  - Tu, Dongbo
T1  - Development and validation of static short form and adaptive test for the taijin kyofusho scale to measure the severity of culture-bound social anxiety
JF  - Current Psychology: A Journal for Diverse Perspectives on Diverse Psychological Issues
JO  - Current Psychology: A Journal for Diverse Perspectives on Diverse Psychological Issues
JA  - Curr Psychol
Y1  - 2021/02/20/
PB  - Springer
SN  - 1046-1310
SN  - 1936-4733
AD  - Tu, Dongbo
N1  - Accession Number: 2021-20839-001. Other Journal Title: Current Psychological Research & Reviews. Partial author list: First Author & Affiliation: Fan, Honglan; School of Psychology, Jiangxi normal university, Nanchang, China. Other Publishers: Transaction Publishers. Release Date: 20210225. Publication Type: Journal (0100), Peer Reviewed Journal (0110). Format Covered: Electronic. Document Type: Journal. Language: EnglishMajor Descriptor: No terms assigned. Classification: General Psychology (2100). Publication History: Accepted Date: Feb 11, 2021. Copyright Statement: The Author(s), under exclusive licence to Springer Science+Business Media, LLC part of Springer Nature. 2021. 
AB  - Simpler and more precise tools are needed to measure Taijin Kyofusho which is a culture-bound anxiety disorder in East Asian countries. This study aimed to develop and validate a short form and a computerized adaptive test (CAT) of Taijin Kyofusho Scale (TKS), as well as compare the measurement precision of the short form, the CAT version and its original version. Item Response Theory (IRT) method was used to develop static short form and to simulate CAT. The short form consisted of 12 items (a 61% reduction) and the CAT version consisted of average 11.72 items (a 62% reduction) from the original TKS, respectively. Both short form and CAT version have similar levels of accuracy and precision in comparison to the original scale at the group level. However, at the individual level, the CAT version can maintain a more consistent level of precision across the continuum of severity than the short form. The short form of the TKS is sufficient for an initial assessment or screening in the community population. And the CAT version of the TKS is more suitable for tailored treatments in the clinical practice, which could detect detailed changes in the severity of Taijin Kyofusho. (PsycInfo Database Record (c) 2021 APA, all rights reserved)
KW  - Social anxiety disorder
KW  - Scale development
KW  - Short form
KW  - CAT
KW  - Taijin Kyofusho scale
KW  - No terms assigned
DO  - 10.1007/s12144-021-01497-x
UR  - https://search.ebscohost.com/login.aspx?direct=true&db=psyh&AN=2021-20839-001&lang=de&site=ehost-live
UR  - tudongbo@aliyun.com
DP  - EBSCOhost
DB  - APA PsycInfo
ER  - 

TY  - JOUR
ID  - 2023-35702-001
AN  - 2023-35702-001
AU  - MacGregor, Chloe
AU  - Ruth, Nicolas
AU  - Müllensiefen, Daniel
T1  - Development and validation of the first adaptive test of emotion perception in music
JF  - Cognition and Emotion
JO  - Cognition and Emotion
JA  - Cogn Emot
Y1  - 2023/01/03/
PB  - Taylor & Francis
SN  - 0269-9931
SN  - 1464-0600
AD  - MacGregor, Chloe
N1  - Accession Number: 2023-35702-001. PMID: 36592153 Partial author list: First Author & Affiliation: MacGregor, Chloe. Release Date: 20230105. Publication Type: Journal (0100), Peer Reviewed Journal (0110). Format Covered: Electronic. Document Type: Journal. Language: EnglishMajor Descriptor: No terms assigned. Classification: Human Experimental Psychology (2300). References Available: Y. Publication History: Accepted Date: Dec 9, 2022; Revised Date: Dec 6, 2022; First Submitted Date: May 9, 2022. Copyright Statement: The Author(s). 2023. 
AB  - ABSTRACT The Musical Emotion Discrimination Task (MEDT) is a short, non-adaptive test of the ability to discriminate emotions in music. Test-takers hear two performances of the same melody, both played by the same performer but each trying to communicate a different basic emotion, and are asked to determine which one is 'happier', for example. The goal of the current study was to construct a new version of the MEDT using a larger set of shorter, more diverse music clips and an adaptive framework to expand the ability range for which the test can deliver measurements. The first study analysed responses from a large sample of participants (N = 624) to determine how musical features contributed to item difficulty, which resulted in a quantitative model of musical emotion discrimination ability rooted in Item Response Theory (IRT). This model informed the construction of the adaptive MEDT. A second study contributed preliminary evidence for the validity and reliability of the adaptive MEDT, and demonstrated that the new version of the test is suitable for a wider range of abilities. This paper therefore presents the first adaptive musical emotion discrimination test, a new resource for investigating emotion processing which is freely available for research use. (PsycInfo Database Record (c) 2023 APA, all rights reserved)
KW  - Emotion perception
KW  - music perception
KW  - emotional abilities
KW  - item response model
KW  - emotion processing
KW  - No terms assigned
DO  - 10.1080/02699931.2022.2162003
UR  - https://search.ebscohost.com/login.aspx?direct=true&db=psyh&AN=2023-35702-001&lang=de&site=ehost-live
UR  - ORCID: 0000-0001-7297-1760
UR  - ORCID: 0000-0001-8169-5454
UR  - cstac001@gold.ac.uk
DP  - EBSCOhost
DB  - APA PsycInfo
ER  - 

TY  - JOUR
ID  - 2020-03364-001
AN  - 2020-03364-001
AU  - Gibbons, Robert D.
AU  - Alegria, Margarita
AU  - Markle, Sheri
AU  - Fuentes, Larimar
AU  - Zhang, Liting
AU  - Carmona, Rodrigo
AU  - Collazos, Francisco
AU  - Wang, Ye
AU  - Baca‐García, Enrique
T1  - Development of a computerized adaptive substance use disorder scale for screening and measurement: The CAT-SUD
JF  - Addiction
JO  - Addiction
JA  - Addiction
Y1  - 2020/07//
VL  - 115
IS  - 7
SP  - 1382
EP  - 1394
PB  - Wiley-Blackwell Publishing Ltd.
SN  - 0965-2140
SN  - 1360-0443
AD  - Gibbons, Robert D., Department of Public Health Sciences, University of Chicago, 5841 South Maryland Ave., MC2000, Chicago, IL, US, 60637-1447
N1  - Accession Number: 2020-03364-001. PMID: 31943486 Other Journal Title: British Journal of Addiction. Partial author list: First Author & Affiliation: Gibbons, Robert D.; Department of Medicine, University of Chicago Biological Sciences, Chicago, IL, US. Other Publishers: Blackwell Publishing. Release Date: 20200116. Correction Date: 20200629. Publication Type: Journal (0100), Peer Reviewed Journal (0110). Format Covered: Electronic. Document Type: Journal Article. Language: EnglishMajor Descriptor: Item Response Theory; Screening Tests; Test Construction; Substance Use Disorder. Minor Descriptor: Adaptive Testing; Diagnosis; Rating Scales; Test Items; Trauma. Classification: Clinical Psychological Testing (2224); Substance Abuse & Addiction (3233). Population: Human (10); Male (30); Female (40). Location: Spain; US. Age Group: Adulthood (18 yrs & older) (300); Young Adulthood (18-29 yrs) (320); Thirties (30-39 yrs) (340); Middle Age (40-64 yrs) (360); Aged (65 yrs & older) (380). Tests & Measures: Computerized Adaptive Substance Use Disorder Scale. Methodology: Empirical Study; Interview; Quantitative Study. Supplemental Data: Appendixes Internet; Experimental Materials Internet. Page Count: 13. Issue Publication Date: Jul, 2020. Publication History: Accepted Date: Dec 6, 2019; Revised Date: May 24, 2019; First Submitted Date: Nov 27, 2018. Copyright Statement: Society for the Study of Addiction. 2020. 
AB  - Background and aims: The focus of this paper is on the improvement of substance use disorder (SUD) screening and measurement. Using a multi‐dimensional item response theory model, the bifactor model, we provide a psychometric harmonization between SUD, depression, anxiety, trauma, social isolation, functional impairment and risk‐taking behavior symptom domains, providing a more balanced view of SUD. The aims are to (1) develop the item‐bank, (2) calibrate the item‐bank using a bifactor model that includes a primary dimension and symptom‐specific subdomains, (3) administer using computerized adaptive testing (CAT) and (4) validate the CAT‐SUD in Spanish and English in the United States and Spain. Design: Item bank construction, item calibration phase, CAT‐SUD validation phase. Setting: Primary care, community clinics, emergency departments and patient‐to‐patient referrals in Spain (Barcelona and Madrid) and the United States (Boston and Los Angeles). Participants/cases: Calibration phase: the CAT‐SUD was developed via simulation from complete item responses in 513 participants. Validation phase: 297 participants received the Composite International Diagnostic Interview (CIDI) and the CAT‐SUD. Measurements: A total of 252 items from five subdomains: (1) SUD, (2) psychological disorders, (3) risky behavior, (4) functional impairment and (5) social support. CAT‐SUD scale scores and CIDI SUD diagnosis. Findings: Calibration: the bifactor model provided excellent fit to the multi‐dimensional item bank; 168 items had high loadings (> 0.4 with the majority > 0.6) on the primary SUD dimension. Using an average of 11 items (four to 26), which represents a 94% reduction in respondent burden (average administration time of approximately 2 minutes), we found a correlation of 0.91 with the 168‐item scale (precision of 5 points on a 100‐point scale). Validation: strong agreement was found between the primary CAT‐SUD dimension estimate and the results of a structured clinical interview. There was a 20‐fold increase in the likelihood of a CIDI SUD diagnosis across the range of the CAT‐SUD (AUC = 0.85). Conclusions: We have developed a new approach for the screening and measurement of SUD and related severity based on multi‐dimensional item response theory. The bifactor model harmonized information from mental health, trauma, social support and traditional SUD items to provide a more complete characterization of SUD. The CAT‐SUD is highly predictive of a current SUD diagnosis based on a structured clinical interview, and may be predictive of the development of SUD in the future. (PsycInfo Database Record (c) 2020 APA, all rights reserved)
KW  - bifactor model
KW  - Computerized adaptive testing
KW  - item response theory
KW  - Latino
KW  - mental health
KW  - substance use disorder
KW  - CAT-SUD
KW  - test development
KW  - Item Response Theory
KW  - Screening Tests
KW  - Test Construction
KW  - Substance Use Disorder
KW  - Adaptive Testing
KW  - Diagnosis
KW  - Rating Scales
KW  - Test Items
KW  - Trauma
U1  - Sponsor: National Institute of Mental Health, US. Grant: R01-MH100155. Other Details: Through an administrative supplement. Recipients: No recipient indicated
DO  - 10.1111/add.14938
UR  - https://search.ebscohost.com/login.aspx?direct=true&db=psyh&AN=2020-03364-001&lang=de&site=ehost-live
UR  - ORCID: 0000-0003-0194-9499
UR  - ORCID: 0000-0002-6463-2280
UR  - rdg@uchicago.edu
DP  - EBSCOhost
DB  - APA PsycInfo
ER  - 

TY  - JOUR
ID  - 2019-54864-015
AN  - 2019-54864-015
AU  - Flens, Gerard
AU  - Smits, Niels
AU  - Terwee, Caroline B.
AU  - Dekker, Joost
AU  - Huijbrechts, Irma
AU  - Spinhoven, Philip
AU  - de Beurs, Edwin
T1  - Development of a computerized adaptive test for anxiety based on the Dutch–Flemish version of the PROMIS item bank
JF  - Assessment
JO  - Assessment
JA  - Assessment
Y1  - 2019/10//
VL  - 26
IS  - 7
SP  - 1362
EP  - 1374
PB  - Sage Publications
SN  - 1073-1911
SN  - 1552-3489
AD  - Flens, Gerard, Stichting Benchmark GGZ (SBG), Rembrandtlaan 46, Bilthoven, 3723 BK, Utrecht, Netherlands
N1  - Accession Number: 2019-54864-015. PMID: 29231048 Partial author list: First Author & Affiliation: Flens, Gerard; Foundation for Benchmarking Mental Health Care, Bilthoven, Netherlands. Release Date: 20191010. Correction Date: 20221128. Publication Type: Journal (0100), Peer Reviewed Journal (0110). Format Covered: Electronic. Document Type: Journal Article. Language: EnglishMajor Descriptor: Adaptive Testing; Anxiety; Test Construction; Test Reliability; Computerized Assessment. Classification: Clinical Psychological Testing (2224); Anxiety Disorders (3215). Population: Human (10); Male (30); Female (40). Location: Netherlands. Age Group: Adulthood (18 yrs & older) (300); Young Adulthood (18-29 yrs) (320); Thirties (30-39 yrs) (340); Middle Age (40-64 yrs) (360); Aged (65 yrs & older) (380). Tests & Measures: Patient-Reported Outcomes Measurement Information System Item Bank-Dutch–Flemish Version; Mini International Neuropsychiatric Interview-Dutch Translation; Computerized Adaptive Test for Anxiety DOI: 10.1037/t70307-000. Methodology: Empirical Study; Quantitative Study. References Available: Y. Page Count: 13. Issue Publication Date: Oct, 2019. Copyright Statement: The Author(s). 2017. 
AB  - We used the Dutch–Flemish version of the USA PROMIS adult V1.0 item bank for Anxiety as input for developing a computerized adaptive test (CAT) to measure the entire latent anxiety continuum. First, psychometric analysis of a combined clinical and general population sample (N = 2,010) showed that the 29-item bank has psychometric properties that are required for a CAT administration. Second, a post hoc CAT simulation showed efficient and highly precise measurement, with an average number of 8.64 items for the clinical sample, and 9.48 items for the general population sample. Furthermore, the accuracy of our CAT version was highly similar to that of the full item bank administration, both in final score estimates and in distinguishing clinical subjects from persons without a mental health disorder. We discuss the future directions and limitations of CAT development with the Dutch–Flemish version of the PROMIS Anxiety item bank. (PsycInfo Database Record (c) 2022 APA, all rights reserved)
KW  - assessment
KW  - anxiety
KW  - clinical subjects
KW  - general population
KW  - item response theory
KW  - computerized adaptive test
KW  - PROMIS
KW  - Adaptive Testing
KW  - Anxiety
KW  - Test Construction
KW  - Test Reliability
KW  - Computerized Assessment
DO  - 10.1177/1073191117746742
UR  - https://search.ebscohost.com/login.aspx?direct=true&db=psyh&AN=2019-54864-015&lang=de&site=ehost-live
UR  - ORCID: 0000-0002-6683-4628
UR  - gerard.flens@sbggz.nl
DP  - EBSCOhost
DB  - APA PsycInfo
ER  - 

TY  - JOUR
ID  - 2020-48994-001
AN  - 2020-48994-001
AU  - Hu, Yiyuan
AU  - Cai, Yan
AU  - Tu, Dongbo
AU  - Guo, Yingying
AU  - Liu, Siyang
T1  - Development of a computerized adaptive test for separation anxiety disorder among adolescents
JF  - Frontiers in Psychology
JO  - Frontiers in Psychology
JA  - Front Psychol
Y1  - 2020/06/18/
VL  - 11
PB  - Frontiers Media S.A.
SN  - 1664-1078
AD  - Cai, Yan
N1  - Accession Number: 2020-48994-001. PMID: 32625130 Partial author list: First Author & Affiliation: Hu, Yiyuan; School of Psychology, Jiangxi Normal University, Nanchang, China. Other Publishers: Frontiers Research Foundation. Release Date: 20220307. Correction Date: 20221128. Publication Type: Journal (0100), Peer Reviewed Journal (0110). Format Covered: Electronic. Document Type: Journal Article. Language: EnglishMajor Descriptor: Adaptive Testing; Test Construction; Test Reliability; Test Validity; Separation Anxiety Disorder. Minor Descriptor: Item Response Theory. Classification: Clinical Psychological Testing (2224); Anxiety Disorders (3215). Population: Human (10); Male (30); Female (40). Location: China. Age Group: Childhood (birth-12 yrs) (100); School Age (6-12 yrs) (180); Adolescence (13-17 yrs) (200); Adulthood (18 yrs & older) (300); Young Adulthood (18-29 yrs) (320). Tests & Measures: Separation Anxiety Assessment Scale—Child and Adolescent Version; Screen for Child Anxiety Related Disorders; Spence Children’s Anxiety Scale—Child and Adolescent Version; Separation Anxiety Avoidance Inventory—Child Version; Computerized Adaptive Test-Separation Anxiety Disorder; Separation Anxiety Symptom Inventory DOI: 10.1037/t39354-000; Multidimensional Anxiety Scale for Children DOI: 10.1037/t05050-000. Methodology: Empirical Study; Quantitative Study. References Available: Y. ArtID: 1077. Issue Publication Date: Jun 18, 2020. Publication History: First Posted Date: Jun 18, 2020; Accepted Date: Apr 28, 2020; First Submitted Date: Oct 13, 2019. Copyright Statement: This is an open-access article distributed under the terms of the Creative Commons Attribution License (CC BY). The use, distribution or reproduction in other forums is permitted, provided the original author(s) and the copyright owner(s) are credited and that the original publication in this journal is cited, in accordance with accepted academic practice. No use, distribution or reproduction is permitted which does not comply with these terms. Hu, Cai, Tu, Guo and Liu. 2020. 
AB  - Background: Separation anxiety disorder (SAD) is one of the most common mental disorders among children and adolescents, and it may seriously affect their growth, daily life, and learning. Self-report scales have been used for diagnosis, which require lengthy testing and personnel. Methods: A total of 1,241 adolescents were recruited from 16 junior- and senior-high schools in China. The initial item bank was selected from classical SAD scales according to the DSM-5. First, the optimal model was selected using item response theory (IRT) according to data fit. Then, per the IRT analysis, items that did not meet the psychometric requirements were deleted (e.g., discriminating values < 0.2). Consequently, a computerized adaptive test (CAT) for SAD was formed (CAT-SAD). Results: An average of 17 items per participant was required to achieve and maintain a 0.3 standard error of measurement in the SAD severity estimate. The estimated correlation of the CAT-SAD with the total 68-item test score was 0.955. CAT-SAD scores were strongly related to the probability of a SAD diagnosis with the Separation Anxiety Assessment Scale—Child and Adolescent Version. Therefore, SAD could be accurately predicted by the CAT-SAD. Conclusions: Exploratory factor analyses revealed that SAD was unidimensional. The CAT-SAD, which has good reliability and validity and high sensitivity and specificity, provides an efficient test for adolescents with SAD as compared to standard paper-and-pencil tests. It can be used to diagnose varying degrees of SAD quickly and reliably and ease the burden on adolescents. Potential applications for inexpensive, efficient, and accurate screening of SAD are discussed. (PsycInfo Database Record (c) 2022 APA, all rights reserved)
KW  - separation anxiety disorder
KW  - adolescent
KW  - computerized adaptive testing
KW  - item response theory
KW  - DSM-5
KW  - Adaptive Testing
KW  - Test Construction
KW  - Test Reliability
KW  - Test Validity
KW  - Separation Anxiety Disorder
KW  - Item Response Theory
U1  - Sponsor: National Natural Science Foundation of China, China. Grant: 31760288; 31660278; 31960186. Recipients: No recipient indicated
DO  - 10.3389/fpsyg.2020.01077
UR  - https://search.ebscohost.com/login.aspx?direct=true&db=psyh&AN=2020-48994-001&lang=de&site=ehost-live
UR  - ORCID: 0000-0002-2603-065X
UR  - cy1979123@aliyun.com
DP  - EBSCOhost
DB  - APA PsycInfo
ER  - 

TY  - JOUR
ID  - 2020-32831-001
AN  - 2020-32831-001
AU  - Postigo, Álvaro
AU  - Cuesta, Marcelino
AU  - Pedrosa, Ignacio
AU  - Muñiz, José
AU  - García-Cueto, Eduardo
T1  - Development of a computerized adaptive test to assess entrepreneurial personality
JF  - Psicologia: Reflexão e Crítica
JO  - Psicologia: Reflexão e Crítica
Y1  - 2020/05/11/
VL  - 33
PB  - Springer
SN  - 0102-7972
SN  - 1678-7153
AD  - Postigo, Álvaro, Department of Psychology, University of Oviedo, Plaza Feijoo s/n, 33003, Oviedo, Spain
N1  - Accession Number: 2020-32831-001. PMID: 32394294 Partial author list: First Author & Affiliation: Postigo, Álvaro; Department of Psychology, University of Oviedo, Oviedo, Spain. Other Publishers: Universidade Federal do Rio Grande do Sul. Release Date: 20230102. Publication Type: Journal (0100), Peer Reviewed Journal (0110). Format Covered: Electronic. Document Type: Journal Article. Language: EnglishMajor Descriptor: Adaptive Behavior; Adaptive Testing; Entrepreneurship; Personality Traits; Test Construction. Minor Descriptor: Economic Development; Economics; Personality Development; Personality Measures; Well Being. Classification: Personality Scales & Inventories (2223); Occupational Interests & Guidance (3610). Population: Human (10); Male (30); Female (40). Age Group: Adulthood (18 yrs & older) (300). Tests & Measures: NEO Five-Factor Inventory; Control of Attention Scale; Measure of Entrepreneurial Tendencies and Abilities DOI: 10.1037/t31144-000; Battery for the Assessment of the Enterprising Personality--Computer Adaptive Test Version DOI: 10.1037/t84032-000. Methodology: Empirical Study; Quantitative Study. References Available: Y. ArtID: 6. Issue Publication Date: May 11, 2020. Publication History: First Posted Date: May 11, 2020; Accepted Date: Apr 29, 2020; First Submitted Date: Feb 10, 2020. Copyright Statement: This article is licensed under a Creative Commons Attribution 4.0 International License, which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons licence, and indicate if changes were made. The images or other third party material in this article are included in the article’s Creative Commons licence, unless indicated otherwise in a credit line to the material. If material is not included in the article’s Creative Commons licence and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this licence, visit http://creativecommons.org/licenses/by/4.0/. The Creative Commons Public Domain Dedication waiver (http://creativecommons.org/publicdomain/zero/1.0/) applies to the data made available in this article, unless otherwise stated in a credit line to the data. The Author(s). 2020. 
AB  - Background/objective: Entrepreneurial behavior is of great importance nowadays owing to its significance in the generation of economic, social, personal, and cultural wellbeing. This behavior is influenced by cognitive and personality characteristics, as well as by socioeconomic and contextual factors. Entrepreneurial personality is made up of a set of psychological traits including self-efficacy, autonomy, innovation, internal locus of control, achievement motivation, optimism, stress tolerance, and risk-taking. The aim of this research is the development of a computerized adaptive test (CAT) to evaluate entrepreneurial personality. Method: A bank of 120 items was created evaluating various aspects of the entrepreneurial personality. The items were calibrated with the Samejima Graded Response Model using a sample of 1170 participants (Mage = 42.34; SDage = 12.96). Results: The bank of items had an essentially unidimensional fit to the model. The CAT exhibited high accuracy for evaluating a wide range of θ scores, using a mean of 16 items with a very low standard error (M = 0.157). Relative validity evidence for the CAT was obtained with two additional tests of entrepreneurial personality (the Battery for the Assessment of the Enterprising Personality and the Measure of Enterpreneurial Tendencies and Abilities), with correlations of .908 and .657, respectively. Conclusions: The CAT developed has appropriate psychometric properties for the evaluation of entrepreneurial people. (PsycInfo Database Record (c) 2023 APA, all rights reserved)
KW  - Entrepreneurial personality
KW  - Evaluation
KW  - Computerized adaptive test
KW  - Adults
KW  - Adaptive Behavior
KW  - Adaptive Testing
KW  - Entrepreneurship
KW  - Personality Traits
KW  - Test Construction
KW  - Economic Development
KW  - Economics
KW  - Personality Development
KW  - Personality Measures
KW  - Well Being
U1  - Sponsor: Principality of Asturias. Grant: BP17-78. Other Details: Predoctoral grant. Recipients: No recipient indicated
U1  - Sponsor: Spanish Ministry of Economy and Competitiveness, Spain. Grant: PSI2017-85724-P. Recipients: No recipient indicated
DO  - 10.1186/s41155-020-00144-x
UR  - https://search.ebscohost.com/login.aspx?direct=true&db=psyh&AN=2020-32831-001&lang=de&site=ehost-live
UR  - ORCID: 0000-0002-1059-117X
UR  - ORCID: 0000-0002-1060-9536
UR  - ORCID: 0000-0003-4228-8965
UR  - postigoalvaro@uniovi.es
DP  - EBSCOhost
DB  - APA PsycInfo
ER  - 

TY  - JOUR
ID  - 2013-43230-021
AN  - 2013-43230-021
AU  - Kopec, Jacek A.
AU  - Sayre, Eric C.
AU  - Davis, Aileen M.
AU  - Badley, Elizabeth M.
AU  - Abrahamowicz, Michal
AU  - Pouchot, Jacques
AU  - Sherlock, Lesley
AU  - Esdaile, John M.
T1  - Development of a paper-and-pencil semi-adaptive questionnaire for 5 domains of health-related quality of life (PAT-5D-QOL)
JF  - Quality of Life Research: An International Journal of Quality of Life Aspects of Treatment, Care & Rehabilitation
JO  - Quality of Life Research: An International Journal of Quality of Life Aspects of Treatment, Care & Rehabilitation
JA  - Qual Life Res
Y1  - 2013/12//
VL  - 22
IS  - 10
SP  - 2829
EP  - 2842
PB  - Springer
SN  - 0962-9343
SN  - 1573-2649
AD  - Kopec, Jacek A., School of Population and Public Health, University of British Columbia, Vancouver, BC, Canada
N1  - Accession Number: 2013-43230-021. PMID: 23653158 Partial author list: First Author & Affiliation: Kopec, Jacek A.; School of Population and Public Health, University of British Columbia, Vancouver, BC, Canada. Release Date: 20140113. Correction Date: 20190211. Publication Type: Journal (0100), Peer Reviewed Journal (0110). Format Covered: Electronic. Document Type: Journal Article. Language: EnglishMajor Descriptor: Psychometrics; Quality of Life; Test Construction; Test Reliability; Test Validity. Minor Descriptor: Health Related Quality of Life. Classification: Health Psychology Testing (2226); Promotion & Maintenance of Health & Wellness (3365). Population: Human (10); Male (30); Female (40). Location: Canada. Age Group: Adulthood (18 yrs & older) (300); Aged (65 yrs & older) (380). Tests & Measures: Paper and Pencil Semi-Adaptive Test-5D-Quality of Life-Version 2; Computerized Adaptive Test-5D-Quality of Life; Paper and Pencil Semi-Adaptive Test-5D-Quality of Life [Appended] DOI: 10.1037/t31131-000; 36-Item Short Form Health Survey DOI: 10.1037/t07023-000. Methodology: Empirical Study; Interview; Focus Group; Qualitative Study; Scientific Simulation. References Available: Y. Page Count: 14. Issue Publication Date: Dec, 2013. Publication History: First Posted Date: May 8, 2013; Accepted Date: Apr 16, 2013. Copyright Statement: Springer Science+Business Media Dordrecht. 2013. 
AB  - Objectives: To develop a paper-and-pencil semi-adaptive test for 5 domains of health-related quality of life (PAT- 5D-QOL) based on item response theory (IRT). Methods: The questionnaire uses items from previously developed item banks for 5 domains: (1) walking, (2) handling objects, (3) daily activities, (4) pain or discomfort, and (5) feelings. For each domain, respondents are initially classified into 4 functional levels. Depending on the level, they are instructed to respond to a different set of 5 additional questions. IRT scores for each domain and overall health utility scores are obtained using a simple spreadsheet. The questions were selected using psychometric and conceptual criteria. The format of the questionnaire was developed through focus groups and cognitive interviews. Feasibility was tested in two population surveys. A simulation study was conducted to compare PAT-5D-QOL with a computerized adaptive test (CAT-5D-QOL) and a fixed questionnaire, developed from the same item banks, in terms of accuracy, bias, precision, and ceiling and floor effects. Results: Close to 90 % of the participants in feasibility studies followed the skip instructions properly. In a simulation study, scores on PAT-5D-QOL for all domains tended to be more accurate, more precise, less biased, and less affected by a ceiling effect than scores on a fixed IRT-based questionnaire of the same length. PAT-5D-QOL was slightly inferior to a fully adaptive instrument. Conclusions: PAT-5D-QOL is a novel, semi-adaptive, IRT-based measure of health-related quality of life with a broad range of potential applications. (PsycINFO Database Record (c) 2019 APA, all rights reserved)
KW  - Paper and Pencil Semi Adaptive Test-5D-Quality of Life
KW  - psychometrics
KW  - test reliability
KW  - test validity
KW  - test development
KW  - Activities of Daily Living
KW  - Affect
KW  - Feasibility Studies
KW  - Female
KW  - Focus Groups
KW  - Health Status Indicators
KW  - Humans
KW  - Male
KW  - Pain Measurement
KW  - Predictive Value of Tests
KW  - Psychometrics
KW  - Quality of Life
KW  - Reproducibility of Results
KW  - Surveys and Questionnaires
KW  - Psychometrics
KW  - Quality of Life
KW  - Test Construction
KW  - Test Reliability
KW  - Test Validity
KW  - Health Related Quality of Life
DO  - 10.1007/s11136-013-0419-4
UR  - https://search.ebscohost.com/login.aspx?direct=true&db=psyh&AN=2013-43230-021&lang=de&site=ehost-live
UR  - ORCID: 0000-0002-3172-3952
UR  - ORCID: 0000-0002-9903-9399
UR  - jkopec@arthritisresearch.ca
DP  - EBSCOhost
DB  - APA PsycInfo
ER  - 

TY  - JOUR
ID  - 2020-09229-001
AN  - 2020-09229-001
AU  - Amtmann, Dagmar
AU  - Bamer, Alyssa M.
AU  - Alschuler, Kevin N.
AU  - Bocell, Fraser D.
AU  - Ehde, Dawn M.
AU  - Jensen, Mark P.
AU  - Johnson, Kurt
AU  - Nery-Hurwit, Mara B.
AU  - Salem, Rana
AU  - Silverman, Arielle
AU  - Smith, Amanda E.
AU  - Terrill, Alexandra L.
AU  - Molton, Ivan
T1  - Development of a resilience item bank and short forms
JF  - Rehabilitation Psychology
JO  - Rehabilitation Psychology
JA  - Rehabil Psychol
Y1  - 2020/05//
VL  - 65
IS  - 2
SP  - 145
EP  - 157
PB  - American Psychological Association
SN  - 0090-5550
SN  - 1939-1544
AD  - Amtmann, Dagmar, Department of Rehabilitation Medicine, University of Washington, Box 354237, 4907 25th Avenue North East, Seattle, WA, US, 98105
N1  - Accession Number: 2020-09229-001. PMID: 32039618 Other Journal Title: Psychological Aspects of Disability. Partial author list: First Author & Affiliation: Amtmann, Dagmar; Department of Rehabilitation Medicine, University of Washington, Seattle, WA, US. Other Publishers: Division 22 of the American Psychological Association; Educational Publishing Foundation; Springer Publishing. Release Date: 20200210. Correction Date: 20210211. Publication Type: Journal (0100), Peer Reviewed Journal (0110). Format Covered: Electronic. Document Type: Journal Article. Language: EnglishConference Information: International Society for Quality of Life Research annual meeting, 2016, Copenhagen, Denmark. Conference Note: Preliminary results of this study were presented at the aforementioned conference and at the annual meeting of the same conference in 2017 (Philadelphia, Pennsylvania). Major Descriptor: Disabilities; Item Response Theory; Measurement; Resilience (Psychological); Test Construction. Minor Descriptor: Chronic Illness; Test Forms; Test Reliability; Test Validity. Classification: Personality Scales & Inventories (2223); Personality Traits & Processes (3120). Population: Human (10); Male (30); Female (40). Location: US. Age Group: Adulthood (18 yrs & older) (300); Young Adulthood (18-29 yrs) (320); Thirties (30-39 yrs) (340); Middle Age (40-64 yrs) (360); Aged (65 yrs & older) (380). Tests & Measures: University of Washington Resilience Scale DOI: 10.1037/t76191-000. Methodology: Empirical Study; Interview; Focus Group; Quantitative Study. Page Count: 13. Issue Publication Date: May, 2020. Publication History: First Posted Date: Feb 10, 2020; Accepted Date: Dec 19, 2019; Revised Date: Sep 30, 2019; First Submitted Date: Jan 10, 2019. Copyright Statement: American Psychological Association. 2020. 
AB  - Purpose: The purpose of this study was to develop a publicly available, psychometrically sound item bank and short forms for measuring resilience in any population, but especially resilience in individuals with chronic medical conditions or long-term disability. Research Methods: A panel of 9 experts including disability researchers, clinical psychologists, and health outcomes researchers developed a definition of resilience that guided item development. The rigorous methodology used focus groups, cognitive interviews, and modern psychometric theory quantitative methods, including item response theory (IRT). Items were administered to a sample of people with chronic medical conditions commonly associated with disability (N = 1,457) and to a general population sample (N = 300) representative of the Unites States general population with respect to age, gender, race, and ethnicity. Results: The final item bank includes 28 items calibrated to IRT with the scores on a T-metric. A mean of 50 represents the mean resilience in the general population sample. Four and eight item short forms are available, and their scores are highly correlated with the item bank score (r ≥ .94). Reliability is excellent across most of the resilience continuum. Initial analyses provide strong support for validity of the score. Conclusions: The findings support reliability and validity of the University of Washington Resilience Scale (UWRS) for assessing resilience in any population, including individuals with chronic health conditions or disabilities. It can be administered using computerized adaptive testing or by short forms. (PsycInfo Database Record (c) 2021 APA, all rights reserved)
AB  - Impact and Implications—This study describes the development and psychometric properties of the first resilience measure using stakeholder engagement to define the construct as well as modern psychometric methods, including item response theory (IRT). The UWRS has significant potential to advance both practice and research through its range of uses. It offers flexibility to use computerized adaptive testing or short forms, reducing respondent burden. It is suitable for assessing resilience in the general population as well as people with chronic health conditions or disability. The UWRS is also freely available, which facilitates its use in both research and clinical practice. Use of this psychometrically sound, person-centered, brief, flexible, and free resilience instrument will facilitate observational and intervention research, as well as research comparing resilience across populations and studies. (PsycInfo Database Record (c) 2021 APA, all rights reserved)
KW  - resilience
KW  - measurement
KW  - item response theory
KW  - scale development
KW  - Disabilities
KW  - Item Response Theory
KW  - Measurement
KW  - Resilience (Psychological)
KW  - Test Construction
KW  - Chronic Illness
KW  - Test Forms
KW  - Test Reliability
KW  - Test Validity
U1  - Sponsor: National Institute on Disability, Independent Living, and Rehabilitation Research, Administration for Community Living, US. Grant: H133B080024; 90RT5023-01-00. Recipients: No recipient indicated
DO  - 10.1037/rep0000312
UR  - https://search.ebscohost.com/login.aspx?direct=true&db=psyh&AN=2020-09229-001&lang=de&site=ehost-live
UR  - ORCID: 0000-0003-2621-3461
UR  - ORCID: 0000-0003-1142-9729
UR  - dagmara@u.washington.edu
DP  - EBSCOhost
DB  - APA PsycInfo
ER  - 

TY  - JOUR
ID  - 2020-97473-001
AN  - 2020-97473-001
AU  - Xu, Lingling
AU  - Jin, Ruyi
AU  - Huang, Feifei
AU  - Zhou, Yanhui
AU  - Li, Zonglong
AU  - Zhang, Minqiang
T1  - Development of computerized adaptive testing for emotion regulation
JF  - Frontiers in Psychology
JO  - Frontiers in Psychology
JA  - Front Psychol
Y1  - 2020/12/01/
VL  - 11
PB  - Frontiers Media S.A.
SN  - 1664-1078
AD  - Zhang, Minqiang
N1  - Accession Number: 2020-97473-001. PMID: 33335495 Partial author list: First Author & Affiliation: Xu, Lingling; School of Psychology, South China Normal University, Guangzhou, China. Other Publishers: Frontiers Research Foundation. Release Date: 20220120. Publication Type: Journal (0100), Peer Reviewed Journal (0110). Format Covered: Electronic. Document Type: Journal Article. Language: EnglishMajor Descriptor: Adaptive Testing; Emotional Regulation; Item Response Theory; Psychometrics; Test Construction. Minor Descriptor: Measurement; Test Items; Test Validity. Classification: Personality Scales & Inventories (2223); Personality Traits & Processes (3120). Population: Human (10); Male (30); Female (40). Location: China. Age Group: Adolescence (13-17 yrs) (200); Adulthood (18 yrs & older) (300); Young Adulthood (18-29 yrs) (320); Thirties (30-39 yrs) (340); Middle Age (40-64 yrs) (360). Tests & Measures: Computerized Adaptive Testing for Emotion Regulation. Methodology: Empirical Study; Quantitative Study. References Available: Y. ArtID: 561358. Issue Publication Date: Dec 1, 2020. Publication History: First Posted Date: Dec 1, 2020; Accepted Date: Nov 5, 2020; First Submitted Date: May 12, 2020. Copyright Statement: This is an open-access article distributed under the terms of the Creative Commons Attribution License (CC BY). The use, distribution or reproduction in other forums is permitted, provided the original author(s) and the copyright owner(s) are credited and that the original publication in this journal is cited, in accordance with accepted academic practice. No use, distribution or reproduction is permitted which does not comply with these terms. Xu, Jin, Huang, Zhou, Li and Zhang. 2020. 
AB  - Emotion regulation (ER) plays a vital role in individuals’ well-being and successful functioning. In this study, we attempted to develop a computerized adaptive testing (CAT) to efficiently evaluate ER, namely the CAT-ER. The initial CAT-ER item bank comprised 154 items from six commonly used ER scales, which were completed by 887 participants recruited in China. We conducted unidimensionality testing, item response theory (IRT) model comparison and selection, and IRT item analysis including local independence, item fit, differential item functioning, and item discrimination. Sixty-three items with good psychometric properties were retained in the final CAT-ER. Then, two CAT simulation studies were implemented to assess the CAT-ER, which revealed that the CAT-ER developed in this study performed reasonably well, considering that it greatly lessened the test items and time without losing measurement accuracy. (PsycInfo Database Record (c) 2022 APA, all rights reserved)
KW  - emotion regulation
KW  - computerized adaptive testing
KW  - item response theory
KW  - item bank
KW  - measurement
KW  - Adaptive Testing
KW  - Emotional Regulation
KW  - Item Response Theory
KW  - Psychometrics
KW  - Test Construction
KW  - Measurement
KW  - Test Items
KW  - Test Validity
U1  - Sponsor: Graduate School of South China Normal University, China. Other Details: Innovation Project. Recipients: No recipient indicated
DO  - 10.3389/fpsyg.2020.561358
UR  - https://search.ebscohost.com/login.aspx?direct=true&db=psyh&AN=2020-97473-001&lang=de&site=ehost-live
UR  - zhangmq1117@qq.com
DP  - EBSCOhost
DB  - APA PsycInfo
ER  - 

TY  - JOUR
ID  - 2022-48971-001
AN  - 2022-48971-001
AU  - Dong, Fanghong
AU  - Moore, Tyler M.
AU  - Westfall, Megan
AU  - Kohler, Christian
AU  - Calkins, Monica E.
T1  - Development of empirically derived brief program evaluation measures in pennsylvania first‐episode psychosis coordinated specialty care programs
JF  - Early Intervention in Psychiatry
JO  - Early Intervention in Psychiatry
JA  - Early Interv Psychiatry
Y1  - 2022/03/27/
PB  - Wiley-Blackwell Publishing Ltd.
SN  - 1751-7885
SN  - 1751-7893
N1  - Accession Number: 2022-48971-001. Partial author list: First Author & Affiliation: Dong, Fanghong; School of Nursing, University of Pennsylvania, Philadelphia, PA, US. Release Date: 20220331. Publication Type: Journal (0100), Peer Reviewed Journal (0110). Format Covered: Electronic. Document Type: Journal. Language: EnglishMajor Descriptor: No terms assigned. Classification: Health & Mental Health Treatment & Prevention (3300). Publication History: Accepted Date: Mar 13, 2022; First Submitted Date: Mar 10, 2021. Copyright Statement: John Wiley & Sons Australia, Ltd. 2022. 
AB  - Aim The Pennsylvania first episode psychosis program evaluation (PA‐FEP‐PE) core assessment battery was developed as a standard and comprehensive clinical assessment and data collection tool in Pennsylvania coordinated specialty care programs (CSC). To reduce administrative time and maximize clinical utility by maintaining acceptable levels of precision, we aimed to generate a short form using item response theory (IRT)‐based computer‐adaptive test (CAT) simulation and analyse the implementation and acceptability of the short form among providers from PA‐CSC. Methods FEP participants (n = 759; age 14–36) from nine coordinated specialty care programs completed 156 items drawn from the PA‐FEP‐PE battery. Multidimensional IRT‐based CAT simulations were used to select the best PA‐FEP‐PE items for abbreviated forms. Results A 67‐item PA‐FEP‐PE short form was developed to capture six factors: (1) positive affect and surgency (with negative loadings on Anxious‐Misery items); (2) psychiatric services satisfaction; (3) antipsychotic side effect severity; (4) family turmoil and associated traumas; (5) trauma load; and (6) psychosis. The total number of items was reduced more than 50% in the PA‐FEP‐PE shortened forms. The short form demonstrated good psychometric properties, and it was well accepted by our providers in the implementation. Conclusions The empirical derivation and implementation of abbreviated measures of key domains and constructs in FEP care have streamlined and facilitated PA‐FEP program evaluation. Our work supports potential application of IRT based methods to empirically reduce core assessment battery measures in large‐scale data collection efforts such as in the Early Psychosis Intervention Network. (PsycInfo Database Record (c) 2022 APA, all rights reserved)
KW  - CAT simulation
KW  - coordinated specialty care
KW  - core assessment battery
KW  - first‐episode psychosis
KW  - multidimensional item response theory
KW  - No terms assigned
DO  - 10.1111/eip.13298
UR  - https://search.ebscohost.com/login.aspx?direct=true&db=psyh&AN=2022-48971-001&lang=de&site=ehost-live
UR  - ORCID: 0000-0002-0546-0263
UR  - mcalkins@pennmedicine.upenn.edu
DP  - EBSCOhost
DB  - APA PsycInfo
ER  - 

TY  - JOUR
ID  - 2014-06810-010
AN  - 2014-06810-010
AU  - Gibbons, Robert D.
AU  - Weiss, David J.
AU  - Pilkonis, Paul A.
AU  - Frank, Ellen
AU  - Moore, Tara
AU  - Kim, Jong Bae
AU  - Kupfer, David J.
T1  - Development of the CAT-ANX: A computerized adaptive test for anxiety
JF  - The American Journal of Psychiatry
JO  - The American Journal of Psychiatry
JA  - Am J Psychiatry
Y1  - 2014/02/01/
VL  - 171
IS  - 2
SP  - 187
EP  - 194
PB  - American Psychiatric Assn
SN  - 0002-953X
SN  - 1535-7228
AD  - Gibbons, Robert D.
N1  - Accession Number: 2014-06810-010. PMID: 23929270 Other Journal Title: American Journal of Insanity. Partial author list: First Author & Affiliation: Gibbons, Robert D. Release Date: 20140317. Correction Date: 20221128. Publication Type: Journal (0100), Peer Reviewed Journal (0110). Format Covered: Electronic. Document Type: Journal Article. Language: EnglishMajor Descriptor: Adaptive Testing; Generalized Anxiety Disorder; Major Depression; Psychometrics; Computerized Assessment. Minor Descriptor: Anxiety. Classification: Clinical Psychological Testing (2224); Anxiety Disorders (3215). Population: Human (10); Male (30); Female (40); Outpatient (60). Location: US. Age Group: Adulthood (18 yrs & older) (300); Young Adulthood (18-29 yrs) (320); Thirties (30-39 yrs) (340); Middle Age (40-64 yrs) (360); Aged (65 yrs & older) (380). Tests & Measures: Computerized Adaptive Testing–Anxiety Inventory; Hamilton Rating Scale for Depression DOI: 10.1037/t04100-000; Center for Epidemiologic Studies Depression Scale; Patient Health Questionnaire DOI: 10.1037/t02598-000; Structured Clinical Interview for DSM-IV. Methodology: Empirical Study; Interview; Quantitative Study. References Available: Y. Page Count: 8. Issue Publication Date: Feb 1, 2014. Publication History: Accepted Date: May 6, 2013; Revised Date: Apr 26, 2013; Apr 3, 2013; First Submitted Date: Feb 8, 2013. 
AB  - Objective: The authors developed a computerized adaptive test for anxiety that decreases patient and clinician burden and increases measurement precision. Method: A total of 1,614 individuals with and without generalized anxiety disorder from a psychiatric clinic and community mental health center were recruited. The focus of the present study was the development of the Computerized Adaptive Testing–Anxiety Inventory (CAT-ANX). The Structured Clinical Interview for DSM-IV was used to obtain diagnostic classifications of generalized anxiety disorder and major depressive disorder. Results: An average of 12 items per subject was required to achieve a 0.3 standard error in the anxiety severity estimate and maintain a correlation of 0.94 with the total 431-item test score. CAT-ANX scores were strongly related to the probability of a generalized anxiety disorder diagnosis. Using both the Computerized Adaptive Testing–- Depression Inventory and the CAT-ANX, comorbid major depressive disorder and generalized anxiety disorder can be accurately predicted. Conclusions: Traditional measurement fixes the number of items but allows measurement uncertainty to vary. Computerized adaptive testing fixes measurement uncertainty and allows the number and content of items to vary, leading to a dramatic decrease in the number of items required for a fixed level of measurement uncertainty. Potential applications for inexpensive, efficient, and accurate screening of anxiety in primary care settings, clinical trials, psychiatric epidemiology, molecular genetics, children, and other cultures are discussed. (PsycInfo Database Record (c) 2022 APA, all rights reserved)
KW  - psychometrics
KW  - Computerized Adaptive Testing–Anxiety Inventory
KW  - generalized anxiety disorder
KW  - Adolescent
KW  - Adult
KW  - Anxiety Disorders
KW  - Diagnosis, Computer-Assisted
KW  - Female
KW  - Humans
KW  - Male
KW  - Middle Aged
KW  - Models, Psychological
KW  - Psychiatric Status Rating Scales
KW  - Adaptive Testing
KW  - Generalized Anxiety Disorder
KW  - Major Depression
KW  - Psychometrics
KW  - Computerized Assessment
KW  - Anxiety
U1  - Sponsor: National Institute of Mental Health, US. Grant: R01-MH66302. Recipients: No recipient indicated
DO  - 10.1176/appi.ajp.2013.13020178
UR  - https://search.ebscohost.com/login.aspx?direct=true&db=psyh&AN=2014-06810-010&lang=de&site=ehost-live
UR  - rdg@uchicago.edu
DP  - EBSCOhost
DB  - APA PsycInfo
ER  - 

TY  - JOUR
ID  - 2020-78695-001
AN  - 2020-78695-001
AU  - Arnett, Anne B.
AU  - Beighley, Jennifer S.
AU  - Kurtz-Nelson, Evangeline C.
AU  - Hoekzema, Kendra
AU  - Wang, Tianyun
AU  - Bernier, Raphe A.
AU  - Eichler, Evan E.
T1  - Developmental predictors of cognitive and adaptive outcomes in genetic subtypes of autism spectrum disorder
JF  - Autism Research
JO  - Autism Research
JA  - Autism Res
Y1  - 2020/10//
VL  - 13
IS  - 10
SP  - 1659
EP  - 1669
PB  - John Wiley & Sons
SN  - 1939-3792
SN  - 1939-3806
AD  - Arnett, Anne B., Department of Psychiatry and Behavioral Sciences, University of Washington, Box 357920, Seattle, WA, US, 98195
N1  - Accession Number: 2020-78695-001. PMID: 32918531 Partial author list: First Author & Affiliation: Arnett, Anne B.; Department of Psychiatry and Behavioral Sciences, University of Washington, Seattle, WA, US. Other Publishers: Wiley-Blackwell Publishing Ltd. Release Date: 20201116. Correction Date: 20221128. Publication Type: Journal (0100), Peer Reviewed Journal (0110). Format Covered: Electronic. Document Type: Journal Article. Language: EnglishGrant Information: Arnett, Anne B. Major Descriptor: Adaptive Behavior; Autism Spectrum Disorders; Cognitive Development; Genetics; Subtypes (Disorders). Minor Descriptor: Childhood Development; Developmental Stages; Genes; Genotypes; Phenotypes. Classification: Neurodevelopmental & Autism Spectrum Disorders (3250). Population: Human (10); Male (30); Female (40). Location: US. Age Group: Childhood (birth-12 yrs) (100); Preschool Age (2-5 yrs) (160); School Age (6-12 yrs) (180); Adolescence (13-17 yrs) (200); Adulthood (18 yrs & older) (300); Young Adulthood (18-29 yrs) (320); Thirties (30-39 yrs) (340); Middle Age (40-64 yrs) (360). Methodology: Empirical Study; Quantitative Study. Supplemental Data: Tables and Figures Internet; Other Internet. References Available: Y. Page Count: 11. Issue Publication Date: Oct, 2020. Publication History: First Posted Date: Sep 12, 2020; Accepted Date: Aug 16, 2020; First Submitted Date: Feb 14, 2020. Copyright Statement: International Society for Autism Research and Wiley Periodicals LLC. 2020. 
AB  - Approximately one‐fourth of autism spectrum disorder (ASD) cases are associated with a disruptive genetic variant. Many of these ASD genotypes have been described previously, and are characterized by unique constellations of medical, psychiatric, developmental, and behavioral features. Development of precision medicine care for affected individuals has been challenging due to the phenotypic heterogeneity that exists even within each genetic subtype. In the present study, we identify developmental milestones that predict cognitive and adaptive outcomes for five of the most common ASD genotypes. Sixty‐five youth with a known pathogenic variant involving ADNP, CHD8, DYRK1A, GRIN2B, or SCN2A genes participated in cognitive and adaptive testing. Exploratory linear regressions were used to identify developmental milestones that predicted cognitive and adaptive outcomes within each gene group. We hypothesized that the earliest and most predictive milestones would vary across gene groups, but would be consistent across outcomes within each genetic subtype. Within the ADNP group, age of walking predicted cognitive outcomes, while age of first words predicted adaptive behaviors. Age of phrases predicted adaptive functioning in the CHD8 group, but cognitive outcomes were not clearly associated with early developmental milestones. Verbal milestones were the strongest predictors of cognitive and adaptive outcomes for individuals with mutations to DYRK1A, GRIN2B, or SCN2A. These trends inform decisions about treatment planning and long‐term expectations for affected individuals, and they add to the growing body of research linking molecular genetic function to brain development and phenotypic outcomes. (PsycInfo Database Record (c) 2022 APA, all rights reserved)
KW  - subtypes of ASD
KW  - genetics
KW  - genomic syndromes
KW  - intellectual disability
KW  - developmental psychology
KW  - developmental milestones
KW  - cognitive outcome
KW  - adaptive outcomes
KW  - Adaptive Behavior
KW  - Autism Spectrum Disorders
KW  - Cognitive Development
KW  - Genetics
KW  - Subtypes (Disorders)
KW  - Childhood Development
KW  - Developmental Stages
KW  - Genes
KW  - Genotypes
KW  - Phenotypes
U1  - Sponsor: National Institute of Mental Health, US. Grant: 5K99MH116064. Recipients: Arnett, Anne B.
U1  - Sponsor: National Institute of Mental Health, US. Grant: R01MH100047. Recipients: Bernier, Raphe A.
U1  - Sponsor: National Institute of Mental Health, US. Grant: MH101221. Recipients: Eichler, Evan E.
DO  - 10.1002/aur.2385
UR  - https://search.ebscohost.com/login.aspx?direct=true&db=psyh&AN=2020-78695-001&lang=de&site=ehost-live
UR  - ORCID: 0000-0003-1342-0488
UR  - arnettab@uw.edu
DP  - EBSCOhost
DB  - APA PsycInfo
ER  - 

TY  - THES
ID  - 2020-31099-253
AN  - 2020-31099-253
AU  - Ju, Unhee
T1  - Diagnostic tools for improving the amount of adaptation in adaptive tests using overall and conditional indices of adaptation
JF  - Dissertation Abstracts International Section A: Humanities and Social Sciences
JO  - Dissertation Abstracts International Section A: Humanities and Social Sciences
Y1  - 2020///
VL  - 81
IS  - 3-A
PB  - ProQuest Information & Learning
SN  - 0419-4209
SN  - 978-1085696203
N1  - Accession Number: 2020-31099-253. Other Journal Title: Dissertation Abstracts International. Partial author list: First Author & Affiliation: Ju, Unhee; Michigan State University, Measurement and Quantitative Methods - Doctor of Philosophy, US. Release Date: 20200528. Correction Date: 20221128. Publication Type: Dissertation Abstract (0400). Format Covered: Electronic. Document Type: Dissertation. Dissertation Number: AAI22620004. ISBN: 978-1085696203. Language: EnglishMajor Descriptor: Adaptive Testing; Information; Simulation; Statistical Analysis. Minor Descriptor: Computers; Item Content (Test); Measurement. Classification: Educational & School Psychology (3500). Population: Human (10). Methodology: Empirical Study; Quantitative Study; Scientific Simulation. 
AB  - In recent years, computerized adaptive testing (CAT) has been widely used in educational and clinical settings. The basic idea of CAT is relatively straightforward. A computer is used to administer items tailored for individuals to maximize the measurement precision of their proficiency estimates. However, the administration of CAT is not so simple. Those who administer CATs must, while trying to optimize an item selection criterion, consider a variety of practical issues such as test security, content balancing, the purpose of testing, and other test specifications. Such extraneous factors make it possible that a CAT might have so many constraints that in practice it is barely adaptive at all. This concern is at the forefront of the current study, which poses two key questions: How adaptive is a highly adaptive test really? How can the level of adaptation be improved? This study aims to develop three new statistical indicators to measure the amount of adaptation conditional on the examinees' proficiency levels in CAT. It also aims to evaluate the feasibility and utility of these adaptation measures in helping to diagnose and improve adaptivity that occurs during the CAT administration. Extending work done by Reckase, Ju, and Kim (2018), the proposed measures are based on three components—the differences in the locations between the selected items and the examinee's current proficiency estimates, the variations in the item locations administered to each examinee, and the magnitude of information that the test presents to each examinee. Hence, they can be used to assess adaptivity during the CAT process, as well as to identify differences in the level of adaptation for individuals or subgroups of examinees. To demonstrate the performance of the proposed adaptation indices, this study conducted analyses of real operational testing data from a healthcare licensure examination, as well as comprehensive simulation studies under various conditions that affect adaptivity in a CAT. The key findings of the study suggest that the proposed adaptation indices are likely to function as intended to sensitively detect the magnitude of adaptivity for a CAT over the proficiency continuum. These new measures shed light on how much adaptation of a given test occurs across individual proficiency levels or subpopulations. With some guidelines for the interpretation of these measures recommended in this study, the adaptation indices can also readily serve as diagnostic tools in practice for helping test practitioners design item pools and adaptive tests that support high adaptivity. (PsycInfo Database Record (c) 2022 APA, all rights reserved)
KW  - computerized adaptive testing
KW  - statistical indicators
KW  - simulation
KW  - information
KW  - Adaptive Testing
KW  - Information
KW  - Simulation
KW  - Statistical Analysis
KW  - Computers
KW  - Item Content (Test)
KW  - Measurement
UR  - https://search.ebscohost.com/login.aspx?direct=true&db=psyh&AN=2020-31099-253&lang=de&site=ehost-live
DP  - EBSCOhost
DB  - APA PsycInfo
ER  - 

TY  - JOUR
ID  - 2002-08134-005
AN  - 2002-08134-005
AU  - Pitkin, Angela K.
AU  - Vispoel, Walter P.
T1  - Differences between self-adapted and computerized adaptive tests: A meta-analysis
JF  - Journal of Educational Measurement
JO  - Journal of Educational Measurement
JA  - J Educ Meas
Y1  - 2001///Fal 2001
VL  - 38
IS  - 3
SP  - 235
EP  - 247
PB  - Blackwell Publishing
SN  - 0022-0655
SN  - 1745-3984
N1  - Accession Number: 2002-08134-005. Partial author list: First Author & Affiliation: Pitkin, Angela K.; U Iowa, Evaluation & Examination Services, Iowa City, IA, US. Other Publishers: Wiley-Blackwell Publishing Ltd. Release Date: 20021120. Correction Date: 20190211. Publication Type: Journal (0100), Peer Reviewed Journal (0110). Format Covered: Print. Document Type: Journal Article. Language: EnglishMajor Descriptor: Adaptive Testing; Test Anxiety; Test Scores; Computerized Assessment. Minor Descriptor: Test Performance. Classification: Educational Measurement (2227). Methodology: Meta Analysis. References Available: Y. Page Count: 13. Issue Publication Date: Fal 2001. 
AB  - Self-adapted testing has been described as a variation of computerized adaptive testing that reduces test anxiety and thereby enhances test performance. The purpose of this study was to gain a better understanding of these proposed effects of self-adapted tests (SATs); meta-analysis procedures were used to estimate differences between SATs and computerized adaptive tests (CATs) in proficiency estimates and post-test anxiety levels across studies in which these two types of tests have been compared. After controlling for measurement error the results showed that SATs yielded proficiency estimates that were 0.12 standard deviation units higher and post-test anxiety levels that were 0.19 standard deviation units lower than those yielded by CATs. The authors speculate about possible reasons for these differences and discuss advantages and disadvantages of using SATs in operational settings. (PsycINFO Database Record (c) 2019 APA, all rights reserved)
KW  - self-adapted & computerized adaptive tests
KW  - test performance
KW  - test anxiety
KW  - Adaptive Testing
KW  - Test Anxiety
KW  - Test Scores
KW  - Computerized Assessment
KW  - Test Performance
DO  - 10.1111/j.1745-3984.2001.tb01125.x
UR  - https://search.ebscohost.com/login.aspx?direct=true&db=psyh&AN=2002-08134-005&lang=de&site=ehost-live
UR  - walter-vispoel@uiowa.edu
DP  - EBSCOhost
DB  - APA PsycInfo
ER  - 

TY  - JOUR
ID  - 2020-31793-001
AN  - 2020-31793-001
AU  - Kratz, Anna L.
AU  - Boileau, Nicholas R.
AU  - Sander, Angelle M.
AU  - Nakase-Richardson, Risa
AU  - Hanks, Robin A.
AU  - Massengale, Jill P.
AU  - Miner, Jennifer A.
AU  - Carlozzi, Noelle E.
T1  - Do emotional distress and functional problems in persons with traumatic brain injury contribute to perceived sleep-related impairment in caregivers?
T3  - Caregivers of Service Members/Veterans and Civilians With Traumatic Brain Injury
JF  - Rehabilitation Psychology
JO  - Rehabilitation Psychology
JA  - Rehabil Psychol
Y1  - 2020/11//
VL  - 65
IS  - 4
SP  - 432
EP  - 442
PB  - American Psychological Association
SN  - 0090-5550
SN  - 1939-1544
SN  - 978-1-4338-9417-6
AD  - Kratz, Anna L., Department of Physical Medicine and Rehabilitation, University of Michigan, NCRC B16-G031N, 2800 Plymouth Road, Ann Arbor, MI, US, 48101
N1  - Accession Number: 2020-31793-001. PMID: 32406737 Other Journal Title: Psychological Aspects of Disability. Partial author list: First Author & Affiliation: Kratz, Anna L.; Department of Physical Medicine and Rehabilitation, University of Michigan, Ann Arbor, MI, US. Other Publishers: Division 22 of the American Psychological Association; Educational Publishing Foundation; Springer Publishing. Release Date: 20200514. Correction Date: 20201123. Publication Type: Journal (0100), Peer Reviewed Journal (0110). Format Covered: Electronic. Document Type: Journal Article. ISBN: 978-1-4338-9417-6. Language: EnglishGrant Information: Carlozzi, Noelle E. Major Descriptor: Anger; Attitudes; Caregivers; Distress; Traumatic Brain Injury. Minor Descriptor: Anxiety; Caregiver Burden; Dyads; Sleep Deprivation. Classification: Neurological Disorders & Brain Damage (3297); Home Care & Hospice (3375). Population: Human (10); Male (30); Female (40). Location: US. Age Group: Adulthood (18 yrs & older) (300); Young Adulthood (18-29 yrs) (320); Thirties (30-39 yrs) (340); Middle Age (40-64 yrs) (360); Aged (65 yrs & older) (380). Tests & Measures: Patient-Reported Outcomes Measurement Information System-Sleep-Related Impairment Computer Adaptive Test; Mayo-Portland Adaptability Inventory-Fourth Edition; Quality of Life in Neurological Disorders Scale. Methodology: Empirical Study; Quantitative Study. Page Count: 11. Issue Publication Date: Nov, 2020. Publication History: First Posted Date: May 14, 2020; Accepted Date: Apr 10, 2020; Revised Date: Apr 9, 2020; First Submitted Date: May 13, 2019. Copyright Statement: American Psychological Association. 2020. 
AB  - Objective: The goal of this study was to examine the association between characteristics of persons with traumatic brain injury (PwTBI) and perceived sleep-related impairment of the caregivers. Method: Fifty-two dyads (n = 23 civilians, n = 29 service members/veterans [SMVs]) were enrolled. Caregivers completed the Patient-Reported Outcomes Measurement Information System Sleep-Related Impairment computer adaptive test, and PwTBI completed Quality of Life in Neurological Disorders measures of depression, anxiety, anger, cognitive functioning, and upper and lower extremity functioning. Hierarchical linear regression models, stratified by civilian/SMV group, were employed to assess prediction of caregiver-perceived sleep-related impairment from emotional distress of the PwTBI (anxiety, depressed mood, and anger) and perceived functional status of the PwTBI (cognitive, upper extremity, lower extremity functioning). Results: Compared with caregivers of civilians, caregivers of SMVs reported higher perceived sleep-related impairment. Regression results showed that characteristics of the PwTBI accounted for moderate amounts of variance in the sleep-related impairment of caregivers of both civilians and SMVs. Within-group analyses showed that the strongest predictor of sleep-related impairment of caregivers of civilians was self-reported cognitive function of the PwTBI (β = −0.82, p = .08); the strongest predictor of sleep-related impairment of caregivers of SMVs was self-reported anger of the PwTBI (β = 0.54, p = .07). Conclusions: In both caregivers of civilians and SMVs with TBI, characteristics of the PwTBI were related to perceived caregiver sleep-related impairment. These preliminary data can inform future research with larger samples that examine the impact of multiple characteristics of the caregiver and care recipient on caregiver sleep. Findings highlight the potential importance of considering the dynamics of the dyad in rehabilitation programming not only for the PwTBI but for caregivers as well. (PsycInfo Database Record (c) 2020 APA, all rights reserved)
AB  - Impact and Implications: Characteristics of care recipients with traumatic brain injury (TBI) may have a substantial impact on caregiver sleep. Successful interventions to improve sleep in caregivers may need to address factors that are specific to the caregiver (e.g., emotional distress, health behaviors) as well as specific to the care recipient with TBI (e.g., functional limitations, emotional distress). (PsycInfo Database Record (c) 2020 APA, all rights reserved)
KW  - caregivers
KW  - military
KW  - sleep
KW  - traumatic brain injury
KW  - Anger
KW  - Attitudes
KW  - Caregivers
KW  - Distress
KW  - Traumatic Brain Injury
KW  - Anxiety
KW  - Caregiver Burden
KW  - Dyads
KW  - Sleep Deprivation
U1  - Sponsor: National Institutes of Health, National Institute of Nursing Research, US. Grant: R01NR013658. Recipients: Carlozzi, Noelle E. (Prin Inv)
U1  - Sponsor: National Center for Advancing Translational Sciences, US. Grant: UL1TR000433. Recipients: No recipient indicated
U1  - Sponsor: National Institute of Arthritis and Musculoskeletal and Skin Diseases, US. Grant: K01AR064275. Recipients: Kratz, Anna L.
DO  - 10.1037/rep0000327
UR  - https://search.ebscohost.com/login.aspx?direct=true&db=psyh&AN=2020-31793-001&lang=de&site=ehost-live
UR  - ORCID: 0000-0002-4056-4404
UR  - ORCID: 0000-0002-3664-3898
UR  - alkratz@med.umich.edu
DP  - EBSCOhost
DB  - APA PsycInfo
ER  - 

TY  - JOUR
ID  - 2022-80524-005
AN  - 2022-80524-005
AU  - Xi, Chongqin
AU  - Tu, Dongbo
AU  - Cai, Yan
T1  - Dual-objective item selection methods in computerized adaptive test using the higher-order cognitive diagnostic models
JF  - Applied Psychological Measurement
JO  - Applied Psychological Measurement
JA  - Appl Psychol Meas
Y1  - 2022/07//
VL  - 46
IS  - 5
SP  - 422
EP  - 438
PB  - Sage Publications
SN  - 0146-6216
SN  - 1552-3497
AD  - Cai, Yan, School of Psychology, Jiangxi Normal University, 99 Ziyang Ave, Nanchang, China, 330022
N1  - Accession Number: 2022-80524-005. PMID: 35812813 Partial author list: First Author & Affiliation: Xi, Chongqin; School of Psychology, Jiangxi Normal University, Nanchang, China. Release Date: 20220728. Correction Date: 20220801. Publication Type: Journal (0100), Peer Reviewed Journal (0110). Format Covered: Electronic. Document Type: Journal Article. Language: EnglishMajor Descriptor: Adaptive Testing; Cognitive Ability; Responses. Minor Descriptor: Cognitive Assessment; Discrimination; Estimation; Models; Simulation. Classification: Tests & Testing (2220); Cognitive Processes (2340). Population: Human (10). Tests & Measures: Cognitive Diagnostic Computerized Adaptive Testing. Methodology: Empirical Study; Mathematical Model; Quantitative Study; Scientific Simulation. References Available: Y. Page Count: 17. Issue Publication Date: Jul, 2022. Copyright Statement: The Author(s). 2022. 
AB  - To efficiently obtain information about both the general abilities and detailed cognitive profiles of examinees from a single model that uses a single-calibration process, higher-order cognitive diagnostic computerized adaptive testing (CD-CAT) that employ higher-order cognitive diagnostic models have been developed. However, the current item selection methods used in higher-order CD-CAT adaptively select items according to only the attribute profiles, which might lead to low precision regarding general abilities; hence, an appropriate method was proposed for this CAT system in this study. Under the framework of the higher-order models, the responses were affected by attribute profiles, which were governed by general abilities. It is reasonable to hold that the item responses were affected by a combination of general abilities and attribute profiles. Based on the logic of Shannon entropy and the generalized deterministic, inputs, noisy 'and' gate (G-DINA) model discrimination index (GDI), two new item selection methods were proposed for higher-order CD-CAT by considering the above combination in this study. The simulation results demonstrated that the new methods achieved more accurate estimations of both general abilities and cognitive profiles than the existing methods and maintained distinct advantages in terms of item pool usage. (PsycInfo Database Record (c) 2022 APA, all rights reserved)
KW  - dual-objective
KW  - cognitive diagnostic computerized adaptive testing
KW  - higher-order cognitive diagnostic models
KW  - item selection method
KW  - Adaptive Testing
KW  - Cognitive Ability
KW  - Responses
KW  - Cognitive Assessment
KW  - Discrimination
KW  - Estimation
KW  - Models
KW  - Simulation
U1  - Sponsor: National Natural Science Foundation of China, China. Grant: 62167004; 31960186; 32160203. Recipients: No recipient indicated
DO  - 10.1177/01466216221089342
UR  - https://search.ebscohost.com/login.aspx?direct=true&db=psyh&AN=2022-80524-005&lang=de&site=ehost-live
UR  - cy1979123@aliyun.com
DP  - EBSCOhost
DB  - APA PsycInfo
ER  - 

TY  - JOUR
ID  - 2016-09419-002
AN  - 2016-09419-002
AU  - Haverman, Lotte
AU  - Grootenhuis, Martha A.
AU  - Raat, Hein
AU  - van Rossum, Marion A. J.
AU  - van Dulmen-den Broeder, Eline
AU  - Hoppenbrouwers, Karel
AU  - Correia, Helena
AU  - Cella, David
AU  - Roorda, Leo D.
AU  - Terwee, Caroline B.
T1  - Dutch–Flemish translation of nine pediatric item banks from the Patient-Reported Outcomes Measurement Information System (PROMIS)®
JF  - Quality of Life Research: An International Journal of Quality of Life Aspects of Treatment, Care & Rehabilitation
JO  - Quality of Life Research: An International Journal of Quality of Life Aspects of Treatment, Care & Rehabilitation
JA  - Qual Life Res
Y1  - 2016/03//
VL  - 25
IS  - 3
SP  - 761
EP  - 765
PB  - Springer
SN  - 0962-9343
SN  - 1573-2649
AD  - Haverman, Lotte, Psychosocial Department, Emma Children's Hospital, Academic Medical Center, Meibergdreef 9, 1105 AZ, Amsterdam, Netherlands
N1  - Accession Number: 2016-09419-002. PMID: 25820548 Partial author list: First Author & Affiliation: Haverman, Lotte; Psychosocial Department, Emma Children's Hospital, Academic Medical Center, Amsterdam, Netherlands. Release Date: 20160822. Correction Date: 20190114. Publication Type: Journal (0100), Peer Reviewed Journal (0110). Format Covered: Electronic. Document Type: Journal Article. Language: EnglishMajor Descriptor: Foreign Language Translation; Measurement; Psychometrics; Quality of Care; Test Validity. Minor Descriptor: Pediatrics; Test Reliability. Classification: Health Psychology Testing (2226); Health Psychology & Medicine (3360). Population: Human (10); Male (30); Female (40). Location: Belgium; Netherlands. Age Group: Childhood (birth-12 yrs) (100); School Age (6-12 yrs) (180); Adolescence (13-17 yrs) (200). Tests & Measures: Patient-Reported Outcomes Measurement Information System--Dutch–Flemish Version. Methodology: Empirical Study; Quantitative Study. References Available: Y. Page Count: 5. Issue Publication Date: Mar, 2016. Publication History: First Posted Date: Mar 28, 2015; Accepted Date: Mar 16, 2015. Copyright Statement: This article is published with open access at Springerlink.com. The Author(s). 2015. 
AB  - Purpose: The Patient-Reported Outcomes Measurement Information System (PROMIS®) is a new, state-of-the-art assessment system for measuring patient-reported health and well-being of adults and children. It has the potential to be more valid, reliable, and responsive than existing PROMs. The items banks are designed to be self-reported and completed by children aged 8–18 years. The PROMIS items can be administered in short forms or through computerized adaptive testing. This paper describes the translation and cultural adaption of nine PROMIS item banks (151 items) for children in Dutch–Flemish. Methods: The translation was performed by FACITtrans using standardized PROMIS methodology and approved by the PROMIS Statistical Center. The translation included four forward translations, two back-translations, three independent reviews (at least two Dutch, one Flemish), and pretesting in 24 children from the Netherlands and Flanders. Results: For some items, it was necessary to have separate translations for Dutch and Flemish: physical function—mobility (three items), anger (one item), pain interference (two items), and asthma impact (one item). Challenges faced in the translation process included scarcity or overabundance of possible translations, unclear item descriptions, constructs broader/smaller in the target language, difficulties in rank ordering items, differences in unit of measurement, irrelevant items, or differences in performance of activities. By addressing these challenges, acceptable translations were obtained for all items. Conclusion: The Dutch–Flemish PROMIS items are linguistically equivalent to the original USA version. Short forms are now available for use, and entire item banks are ready for cross-cultural validation in the Netherlands and Flanders. (PsycINFO Database Record (c) 2019 APA, all rights reserved)
KW  - Quality improvement
KW  - PROMIS
KW  - Pediatric item banks
KW  - HRQOL
KW  - Measurements
KW  - PROs
KW  - Foreign Language Translation
KW  - Measurement
KW  - Psychometrics
KW  - Quality of Care
KW  - Test Validity
KW  - Pediatrics
KW  - Test Reliability
U1  - Sponsor: Dutch Arthritis Association, Netherlands. Recipients: No recipient indicated
U1  - Sponsor: Pfizer BV Pharmaceuticals. Recipients: No recipient indicated
DO  - 10.1007/s11136-015-0966-y
UR  - https://search.ebscohost.com/login.aspx?direct=true&db=psyh&AN=2016-09419-002&lang=de&site=ehost-live
UR  - cb.terwee@vumc.nl
UR  - l.roorda@reade.nl
UR  - d-cella@northwestern.edu
UR  - helena-correia@northwestern.edu
UR  - Karel.Hoppenbrouwers@med.kuleuven.be
UR  - E.vanDulmen-denBroeder@vumc.nl
UR  - m.a.vanrossum@amc.uva.nl
UR  - h.raat@erasmusmc.nl
UR  - m.a.grootenhuis@amc.uva.nl
UR  - L.Haverman@amc.nl
DP  - EBSCOhost
DB  - APA PsycInfo
ER  - 

TY  - JOUR
ID  - 2014-10096-007
AN  - 2014-10096-007
AU  - Junghaenel, Doerte U.
AU  - Schneider, Stefan
AU  - Stone, Arthur A.
AU  - Christodoulou, Christopher
AU  - Broderick, Joan E.
T1  - Ecological validity and clinical utility of Patient-Reported Outcomes Measurement Information System (PROMIS®) instruments for detecting premenstrual symptoms of depression, anger, and fatigue
JF  - Journal of Psychosomatic Research
JO  - Journal of Psychosomatic Research
JA  - J Psychosom Res
Y1  - 2014/04//
VL  - 76
IS  - 4
SP  - 300
EP  - 306
PB  - Elsevier Science
SN  - 0022-3999
SN  - 1879-1360
AD  - Junghaenel, Doerte U., Department of Psychiatry and Behavioral Science, Stony Brook University, Putnam Hall, South Campus, Stony Brook, NY, US, 11794-8790
N1  - Accession Number: 2014-10096-007. PMID: 24630180 Partial author list: First Author & Affiliation: Junghaenel, Doerte U.; Department of Psychiatry and Behavioral Science, Stony Brook University, Stony Brook, NY, US. Release Date: 20140505. Correction Date: 20200806. Publication Type: Journal (0100), Peer Reviewed Journal (0110). Format Covered: Electronic. Document Type: Journal Article. Language: EnglishMajor Descriptor: Anger; Fatigue; Menstrual Disorders; Psychometrics. Minor Descriptor: Clinical Trials; Ecological Psychology; Information Systems; Major Depression; Patients. Classification: Health Psychology Testing (2226); Psychological & Physical Disorders (3200). Population: Human (10); Female (40). Location: US. Age Group: Adulthood (18 yrs & older) (300); Young Adulthood (18-29 yrs) (320); Thirties (30-39 yrs) (340); Middle Age (40-64 yrs) (360). Tests & Measures: Computerized Adaptive Testing. Methodology: Empirical Study; Quantitative Study. References Available: Y. Page Count: 7. Issue Publication Date: Apr, 2014. Publication History: Accepted Date: Jan 31, 2014; Revised Date: Jan 28, 2014; First Submitted Date: Oct 9, 2013. Copyright Statement: All rights reserved. Elsevier Inc. 2014. 
AB  - Objective: This study examined the ecological validity and clinical utility of NIH Patient Reported-Outcomes Measurement Information System (PROMIS®) instruments for anger, depression, and fatigue in women with premenstrual symptoms. Methods: One-hundred women completed daily diaries and weekly PROMIS assessments over 4weeks. Weekly assessments were administered through Computerized Adaptive Testing (CAT). Weekly CATs and corresponding daily scores were compared to evaluate ecological validity. To test clinical utility, we examined if CATs could detect changes in symptom levels, if these changes mirrored those obtained from daily scores, and if CATs could identify clinically meaningful premenstrual symptom change. Results: PROMIS CAT scores were higher in the pre-menstrual than the baseline (ps < .0001) and post-menstrual (ps < .0001) weeks. The correlations between CATs and aggregated daily scores ranged from .73 to .88 supporting ecological validity. Mean CAT scores showed systematic changes in accordance with the menstrual cycle and the magnitudes of the changes were similar to those obtained from the daily scores. Finally, Receiver Operating Characteristic (ROC) analyses demonstrated the ability of the CATs to discriminate between women with and without clinically meaningful premenstrual symptom change. Conclusions: PROMIS CAT instruments for anger, depression, and fatigue demonstrated validity and utility in premenstrual symptom assessment. The results provide encouraging initial evidence of the utility of PROMIS instruments for the measurement of affective premenstrual symptoms. (PsycInfo Database Record (c) 2020 APA, all rights reserved)
KW  - ecological validity
KW  - clinical utility
KW  - Patient-Reported Outcomes Measurement Information System
KW  - premenstrual symptoms
KW  - depression
KW  - anger
KW  - fatigue
KW  - Adolescent
KW  - Adult
KW  - Affective Symptoms
KW  - Anger
KW  - Depression
KW  - Fatigue
KW  - Female
KW  - Humans
KW  - Middle Aged
KW  - Patient Outcome Assessment
KW  - Premenstrual Syndrome
KW  - Reproducibility of Results
KW  - Time Factors
KW  - Anger
KW  - Fatigue
KW  - Menstrual Disorders
KW  - Psychometrics
KW  - Clinical Trials
KW  - Ecological Psychology
KW  - Information Systems
KW  - Major Depression
KW  - Patients
U1  - Sponsor: National Institutes of Health, US. Grant: 1 U01AR057948-01. Recipients: No recipient indicated
U1  - Sponsor: National Institutes of Health, US. Grant: cooperative agreements U54AR057951; U01AR052177; U54AR057943; U54AR057926; U01AR057948; U01AR052170; U01AR057954; U01AR052171; U01AR052181; U01AR057956; U01AR052158; U01AR057929; U01AR057936; U01AR052155; U01AR057971; U01AR057940; U01AR057967; U01AR052186. Other Details: PROMIS®; Common Fund Initiative. Recipients: No recipient indicated
DO  - 10.1016/j.jpsychores.2014.01.010
UR  - https://search.ebscohost.com/login.aspx?direct=true&db=psyh&AN=2014-10096-007&lang=de&site=ehost-live
UR  - ORCID: 0000-0003-1752-0910
UR  - Doerte.Junghaenel@stonybrook.edu
DP  - EBSCOhost
DB  - APA PsycInfo
ER  - 

TY  - THES
ID  - 2009-99120-026
AN  - 2009-99120-026
AU  - Guyer, Rick D.
T1  - Effect of early misfit in Computerized Adaptive Testing on the recovery of theta
JF  - Dissertation Abstracts International: Section B: The Sciences and Engineering
JO  - Dissertation Abstracts International: Section B: The Sciences and Engineering
Y1  - 2009///
VL  - 69
IS  - 12-B
SP  - 7849
EP  - 7849
PB  - ProQuest Information & Learning
SN  - 0419-4217
SN  - 978-0-549-94058-6
N1  - Accession Number: 2009-99120-026. Other Journal Title: Dissertation Abstracts International. Partial author list: First Author & Affiliation: Guyer, Rick D.; U Minnesota, US. Release Date: 20090831. Correction Date: 20190211. Publication Type: Dissertation Abstract (0400). Format Covered: Electronic. Document Type: Dissertation. Dissertation Number: AAI3338946. ISBN: 978-0-549-94058-6. Language: EnglishMajor Descriptor: Adaptive Testing; Independent Variables; Computerized Assessment. Classification: Psychometrics & Statistics & Methodology (2200). Population: Human (10). Methodology: Empirical Study; Quantitative Study. Page Count: 1. 
AB  - This study focused on how early misfit affected the recovery of θ for a computerized adaptive test (CAT). Number of misfitting items, generating θ, item selection method, and θ estimation method were independent variables in this study. It was found that CAT could recover from misfit-as-correct-responses for low ability simulees given a sufficient number of items. CAT could not recover from misfit-as-incorrect-responses for high ability simulees. Implications of the study and suggestions for future research were provided. (PsycINFO Database Record (c) 2019 APA, all rights reserved)
KW  - computerized adaptive test
KW  - early misfit
KW  - independent variables
KW  - Adaptive Testing
KW  - Independent Variables
KW  - Computerized Assessment
UR  - https://search.ebscohost.com/login.aspx?direct=true&db=psyh&AN=2009-99120-026&lang=de&site=ehost-live
DP  - EBSCOhost
DB  - APA PsycInfo
ER  - 

TY  - JOUR
ID  - 2013-18917-005
AN  - 2013-18917-005
AU  - Walter, Otto B.
AU  - Rose, Matthias
T1  - Effect of item order on item calibration and item bank construction for computer adaptive tests
JF  - Psychological Test and Assessment Modeling
JO  - Psychological Test and Assessment Modeling
JA  - Psychol Test Assess Model
Y1  - 2013///
VL  - 55
IS  - 1
SP  - 81
EP  - 91
PB  - Pabst Science Publishers
SN  - 2190-0493
SN  - 2190-0507
AD  - Walter, Otto B., Universitat Bielefeld, Fakultat fur Psychology und Sportwissenschaft, Postfach 100131, 33501, Bielefeld, Germany
N1  - Accession Number: 2013-18917-005. Other Journal Title: Psychologische Beitrage; Psychology Science. Partial author list: First Author & Affiliation: Walter, Otto B.; Universitat Bielefeld, Fakultat fur Psychology und Sportwissenschaft, Bielefeld, Germany. Release Date: 20130930. Correction Date: 20190211. Publication Type: Journal (0100), Peer Reviewed Journal (0110). Format Covered: Print. Document Type: Journal Article. Language: EnglishMajor Descriptor: Anxiety; Item Response Theory; Mathematical Modeling; Computerized Assessment. Classification: Statistics & Mathematics (2240). Population: Human (10). Methodology: Mathematical Model. References Available: Y. Page Count: 11. Issue Publication Date: 2013. 
AB  - Item banks are typically constructed from responses to items that are presented in one fixed order; therefore, order effects between subsequent items may violate the independence assumption. We investigated the effect of item order on item bank construction, item calibration, and ability estimation. 15 polytomous items similar to items used in a pilot version of a computer adaptive test for anxiety (Walter et al., 2005; Walter et al., 2007) were presented in one fixed order or in a order randomly generated for each respondent. A total of n = 520 out-patients participated in the study. Item calibration (Generalized Partial Credit Model) yielded only small differences of slope and location parameters. Simulated test runs using either the full item bank or an adaptive algorithm produced very similar ability estimates (expected a posteriori estimation). These results indicate that item order had little impact on item calibration and ability estimation for this item set. (PsycINFO Database Record (c) 2019 APA, all rights reserved)
KW  - computer adaptive tests
KW  - item response theory
KW  - mathematical modeling
KW  - anxiety
KW  - Anxiety
KW  - Item Response Theory
KW  - Mathematical Modeling
KW  - Computerized Assessment
UR  - https://search.ebscohost.com/login.aspx?direct=true&db=psyh&AN=2013-18917-005&lang=de&site=ehost-live
UR  - otto.walter@chante.de
DP  - EBSCOhost
DB  - APA PsycInfo
ER  - 

TY  - JOUR
ID  - 1981-26708-001
AN  - 1981-26708-001
AU  - Lurie, Ellen S.
AU  - Steffen, John J.
T1  - Effective components of covert reinforcement
JF  - The Journal of Psychology: Interdisciplinary and Applied
JO  - The Journal of Psychology: Interdisciplinary and Applied
JA  - J Psychol
Y1  - 1980/11//
VL  - 106
IS  - 2
SP  - 241
EP  - 248
PB  - Heldref Publications
SN  - 0022-3980
SN  - 1940-1019
N1  - Accession Number: 1981-26708-001. Partial author list: First Author & Affiliation: Lurie, Ellen S.; U Cincinnati. Other Publishers: Taylor & Francis. Release Date: 19810901. Correction Date: 20140303. Publication Type: Journal (0100), Peer Reviewed Journal (0110). Format Covered: Print. Document Type: Journal Article. Language: EnglishMajor Descriptor: College Students; Operant Conditioning; Reinforcement; Test Anxiety. Classification: Classroom Dynamics & Student Adjustment & Attitudes (3560). Population: Human (10). Age Group: Adulthood (18 yrs & older) (300). Page Count: 8. Issue Publication Date: Nov, 1980. 
AB  - 45 undergraduates who scored 18 or above on the Test Anxiety Scale took part in a program designed to reduce test-taking anxiety. Ss were randomly assigned to 1 of 5 conditions: standard covert reinforcement; covert rehearsal, which was similar to covert reinforcement except that the reinforcing scene was eliminated; information control, which involved discussion of adaptive test-taking strategies; attention placebo, which included discussion of early childhood evaluative experiences; and delayed treatment, in which Ss were offered treatment following posttreatment assessment. Prior to the inception of treatment, all Ss were given the Wonderlic Personnel Test and the State Anxiety Questionnaire; this procedure was repeated following treatment termination. Ss who received covert rehearsal reported less anxiety than Ss in other conditions. Results are inconclusive on the remaining dependent measures. An alternative conception to the operant model of the effective components of covert reinforcement is presented. (22 ref) (PsycINFO Database Record (c) 2016 APA, all rights reserved)
KW  - covert reinforcement vs rehearsal vs information control vs attention placebo vs delayed treatment
KW  - test-taking anxiety
KW  - college students
KW  - College Students
KW  - Operant Conditioning
KW  - Reinforcement
KW  - Test Anxiety
DO  - 10.1080/00223980.1980.9915188
UR  - https://search.ebscohost.com/login.aspx?direct=true&db=psyh&AN=1981-26708-001&lang=de&site=ehost-live
DP  - EBSCOhost
DB  - APA PsycInfo
ER  - 

TY  - JOUR
ID  - 1995-19717-001
AN  - 1995-19717-001
AU  - Rocklin, Thomas R.
AU  - O'Donnell, Angela M.
AU  - Holst, Patricia M.
T1  - Effects and underlying mechanisms of self-adapted testing
JF  - Journal of Educational Psychology
JO  - Journal of Educational Psychology
JA  - J Educ Psychol
Y1  - 1995/03//
VL  - 87
IS  - 1
SP  - 103
EP  - 116
PB  - American Psychological Association
SN  - 0022-0663
SN  - 1939-2176
N1  - Accession Number: 1995-19717-001. Partial author list: First Author & Affiliation: Rocklin, Thomas R.; U Iowa, Div of Psychological & Quantitative Foundations, Iowa City, US. Other Publishers: Warwick & York. Release Date: 19950601. Correction Date: 20190211. Publication Type: Journal (0100), Peer Reviewed Journal (0110). Format Covered: Print. Document Type: Journal Article. Language: EnglishMajor Descriptor: Adaptive Testing; Item Analysis (Statistical); Computerized Assessment. Classification: Tests & Testing (2220). Population: Human (10). Age Group: Adulthood (18 yrs & older) (300). Methodology: Empirical Study. References Available: Y. Page Count: 14. Issue Publication Date: Mar, 1995. Publication History: Accepted Date: Jul 21, 1994; Revised Date: Jul 18, 1994; First Submitted Date: Feb 23, 1994. Copyright Statement: American Psychological Association. 1995. 
AB  - Undergraduates participated in 3 experiments related to self-adapted testing. Experiment 1 demonstrated that, in comparison with computerized adaptive testing, self-adapted testing reduced the influence of anxiety on performance but took longer and was less efficient. Experiment 2 indicated that benefits of self-adapted testing cannot be attributed solely to item ordering. Instead, active choice of item difficulty seems to be necessary. Experiment 3 demonstrated that the provision of feedback increased the efficiency of the test but had no effect on estimates of ability derived. The potential of self-adapted testing to reduce the influence of extraneous sources of variation in test performance is discussed. (PsycINFO Database Record (c) 2019 APA, all rights reserved)
KW  - self vs computerized adapted testing & role of examinee control & item by item feedback
KW  - 18–41 yr olds
KW  - Adaptive Testing
KW  - Item Analysis (Statistical)
KW  - Computerized Assessment
DO  - 10.1037/0022-0663.87.1.103
UR  - https://search.ebscohost.com/login.aspx?direct=true&db=psyh&AN=1995-19717-001&lang=de&site=ehost-live
DP  - EBSCOhost
DB  - APA PsycInfo
ER  - 

TY  - THES
ID  - 1996-73242-001
AN  - 1996-73242-001
AU  - Arrowood, Vada Ellis
T1  - Effects of computerized adaptive test anxiety on nursing licensure examination
JF  - Dissertation Abstracts International Section A: Humanities and Social Sciences
JO  - Dissertation Abstracts International Section A: Humanities and Social Sciences
Y1  - 1994///
VL  - 54
IS  - 9-A
SP  - 3410
EP  - 3410
PB  - ProQuest Information & Learning
SN  - 0419-4209
N1  - Accession Number: 1996-73242-001. Other Journal Title: Dissertation Abstracts International. Partial author list: First Author & Affiliation: Arrowood, Vada Ellis; U Missouri, Columbia, US. Release Date: 19960801. Correction Date: 20190211. Publication Type: Dissertation Abstract (0400). Format Covered: Electronic. Document Type: Dissertation. Language: EnglishMajor Descriptor: Nursing Students; Professional Examinations; Test Anxiety; Computerized Assessment. Classification: Professional Education & Training (3410). Population: Human (10). Age Group: Adulthood (18 yrs & older) (300). Methodology: Empirical Study. Page Count: 1. 
KW  - computerized adaptive testing method for licensure examination
KW  - test anxiety
KW  - nursing students
KW  - Nursing Students
KW  - Professional Examinations
KW  - Test Anxiety
KW  - Computerized Assessment
UR  - https://search.ebscohost.com/login.aspx?direct=true&db=psyh&AN=1996-73242-001&lang=de&site=ehost-live
DP  - EBSCOhost
DB  - APA PsycInfo
ER  - 

TY  - JOUR
ID  - 1993-71080-001
AN  - 1993-71080-001
AU  - Tatsuoka, Kikumi
AU  - Birenbaum, Menucha
T1  - Effects of instructional backgrounds on test performances
JF  - Journal of Computer-Based Instruction
JO  - Journal of Computer-Based Instruction
Y1  - 1981/08//
VL  - 8
IS  - 1
SP  - 1
EP  - 8
PB  - Assn. for the Development of Computer-Based Instructional Systems
SN  - 0098-597X
N1  - Accession Number: 1993-71080-001. Partial author list: First Author & Affiliation: Tatsuoka, Kikumi; U Illinois, Computer-Based Education Research Lab, Urbana, US. Release Date: 19930301. Correction Date: 20151207. Publication Type: Journal (0100), Peer-Reviewed Status-Unknown (0130). Format Covered: Print. Document Type: Journal Article. Language: EnglishMajor Descriptor: Adaptive Testing; Computer Assisted Instruction; Educational Background. Minor Descriptor: Junior High School Students; Test Performance. Classification: Curriculum & Programs & Teaching Methods (3530). Population: Human (10). Age Group: Adolescence (13-17 yrs) (200). Methodology: Empirical Study. Page Count: 8. Issue Publication Date: Aug, 1981. 
AB  - An adaptive testing procedure was adopted in a routing system for sending an examinee to his or her appropriate instructional level in a series of lessons teaching signed-number operations, written on the computer-based education system PLATO. Two samples totaling 175 8th graders were tested. The procedure worked well for most Ss but not for those who were exposed to a different conceptual framework of instruction prior to the PLATO instructional lesson. Differences in prior and subsequent instructional methods affected the learning of more advanced materials and produced lower achievement scores on the posttest given at the end of the program. (PsycINFO Database Record (c) 2016 APA, all rights reserved)
KW  - instructional background & adaptive testing in PLATO computer based education program
KW  - test performance
KW  - 8th graders
KW  - Adaptive Testing
KW  - Computer Assisted Instruction
KW  - Educational Background
KW  - Junior High School Students
KW  - Test Performance
UR  - https://search.ebscohost.com/login.aspx?direct=true&db=psyh&AN=1993-71080-001&lang=de&site=ehost-live
DP  - EBSCOhost
DB  - APA PsycInfo
ER  - 

TY  - JOUR
ID  - 2009-20300-003
AN  - 2009-20300-003
AU  - Frey, Andreas
AU  - Hartig, Johannes
AU  - Moosbrugger, Helfried
T1  - Effekte des adaptiven testens auf die motivation zur testbearbeitung am beispiel des Frankfurter Adaptiven Konzentrationsleistungs-Tests = Effects of adaptive testing on test-taking motivation with the example of the Frankfurt Adaptive Concentration Test
JF  - Diagnostica
JO  - Diagnostica
Y1  - 2009///
VL  - 55
IS  - 1
SP  - 20
EP  - 28
PB  - Hogrefe Verlag GmbH & Co. KG
SN  - 0012-1924
SN  - 2190-622X
AD  - Frey, Andreas, Leibniz-Institut fur die Padagogik der Naturwissenschaften (IPN), Universitat Kiel, PISA 2006 Olshausenstrasse 62, 24098, Kiel, Germany
N1  - Accession Number: 2009-20300-003. Translated Title: Effects of adaptive testing on test-taking motivation with the example of the Frankfurt Adaptive Concentration Test. Partial author list: First Author & Affiliation: Frey, Andreas; Leibniz-Institut fur die Padagogik der Naturwissenschaften (IPN), Universitat Kiel, Kiel, Germany. Release Date: 20100208. Correction Date: 20190211. Publication Type: Journal (0100), Peer Reviewed Journal (0110). Format Covered: Electronic. Document Type: Journal Article. Language: GermanMajor Descriptor: Adaptive Testing; Motivation; Psychometrics; Test Taking; Computerized Assessment. Classification: Tests & Testing (2220); Engineering & Environmental Psychology (4000). Population: Human (10). Location: Germany. Tests & Measures: Frankfurt Adaptive Concentration Test. Methodology: Empirical Study; Quantitative Study. References Available: Y. Page Count: 9. Issue Publication Date: 2009. Copyright Statement: Hogrefe Verlag Göttingen. 2009. 
AB  - Since the early work on Computerized Adaptive Testing (CAT), a positive effect of CAT on participants’ test taking motivation has been widely accepted. However, in recent publications this assumption is frequently criticized and a negative effect is stated. To bring light to the controversy, the effect of adaptive testing with the Frankfurt Adaptive Concentration Test (FACT, Moosbrugger & Heyden, 1997) on test-taking motivation was examined experimentally (N = 79). Two independent samples received the FACT either in an adaptive or a non-adaptive version. Test-taking motivation was significantly lower in the adaptive condition than in the non-adaptive condition, p = .045, η² = .051. Further analyses showed that the differences in test-taking motivation between the adaptive und the non-adaptive condition are mainly due to the perceived probability of success and not to other motivational components like the perceived probability of failure or the perceived challenge. (PsycINFO Database Record (c) 2019 APA, all rights reserved)
KW  - test taking motivation
KW  - Frankfurt Adaptive Concentration Test
KW  - motivation
KW  - psychometrics
KW  - Computerized Adaptive Testing
KW  - Adaptive Testing
KW  - Motivation
KW  - Psychometrics
KW  - Test Taking
KW  - Computerized Assessment
DO  - 10.1026/0012-1924.55.1.20
UR  - https://search.ebscohost.com/login.aspx?direct=true&db=psyh&AN=2009-20300-003&lang=de&site=ehost-live
UR  - ORCID: 0000-0001-6361-4374
UR  - ORCID: 0000-0001-5334-9538
UR  - moosbrugger@psych.uni-frankfurt.de
UR  - hartig@dipf.de
UR  - frey@ipn.uni-kiel.de
DP  - EBSCOhost
DB  - APA PsycInfo
ER  - 

TY  - THES
ID  - 2016-58398-081
AN  - 2016-58398-081
AU  - Yang, Lihong
T1  - Enhancing item pool utilization when designing multistage computerized adaptive tests
JF  - Dissertation Abstracts International Section A: Humanities and Social Sciences
JO  - Dissertation Abstracts International Section A: Humanities and Social Sciences
Y1  - 2017///
VL  - 77
IS  - 12-A(E)
PB  - ProQuest Information & Learning
SN  - 0419-4209
SN  - 978-1369051353
N1  - Accession Number: 2016-58398-081. Other Journal Title: Dissertation Abstracts International. Partial author list: First Author & Affiliation: Yang, Lihong; Michigan State University, Measurement and Quantitative Methods, US. Release Date: 20170525. Correction Date: 20221128. Publication Type: Dissertation Abstract (0400). Format Covered: Electronic. Document Type: Dissertation. Dissertation Number: AAI10150094. ISBN: 978-1369051353. Language: EnglishMajor Descriptor: Algorithms; Educational Measurement; Item Response Theory. Classification: Educational & School Psychology (3500). Population: Human (10). Methodology: Empirical Study; Quantitative Study. 
AB  - In recent years, the multistage adaptive test (MST) has gained increasing popularity in the field of educational measurement and operational testing. MST refers to a test in which pre-constructed sets of items are administered adaptively and are scored as a unit (Hendrickson, 2007). As a special case of Computerized Adaptive Testing (CAT), a MST program needs the following components: an item response theory (IRT) model or non-IRT-based alternatives; an item pool design; module assembly; ability estimation; routing algorithm; and scoring (Yan et al., 2014). A significant amount of research has been conducted on components like module assembly, ability estimation, routing and scoring, but few studies have addressed the component of item pool design. An item pool is defined as consisting of a maximal number of combinations of items that meet all content specifications for a test and provide sufficient item information for estimation at a series of ability levels (van der Linden et al., 2006). An item pool design is very important because any successful MST assembly is inseparable from an optimal item pool that provides sufficient and high-quality items (Luecht & Nungester, 1998). Reckase (2003, 2010) developed the p-optimality method to design optimal item pools using the unidimensional Rasch model in CAT, and it has been proved to be efficient for different item types and IRT models. The present study extended this method to MST context in supporting and developing different MST panel designs under different test configurations. The study compared the performance of the MST assembled under the most popularly studied panel designs in the literature, such as 1-2, 1-3, 1-2-2, and 1-2-3. A combination of short, medium and long tests with different routing test proportions were used to build up different tests. Using one of the most popularly investigated IRT models, the Rasch model, simulated optimal item pools were generated with and without practical constraints of exposure control. A total number of 72 optimal items pools were generated and the measurement accuracy was evaluated by an overall sample and conditional sample using various statistical measures. The p-optimality method was also applied in an operational MST licensure test to see if it is feasible in supporting test assembly and achieving sufficient measurement accuracy in practice. Results showed that the different MST panel designs achieved sufficient measurement accuracy by using the items from the optimal item pools built with the p-optimality method. The same was true with the operational item pool. Measurement accuracy was related to test length, but not so much to the routing test proportions. Exposure control affected the item pool size, but the distributions of the item parameters and item pool characteristics for all the MST panel designs were similar under the two conditions. The item pool sizes under the exposure control conditions were several times larger than those under no exposure control, depending on the types of MST panel designs and routing test proportions. The results from this study provide information for how to enhance item pool utilization when designing multistage computerized adaptive tests, facilitating the MST assembly process, and improving the scoring accuracy. (PsycInfo Database Record (c) 2022 APA, all rights reserved)
KW  - item pool utilization
KW  - multistage computerized adaptive tests
KW  - algorithms
KW  - item response theory
KW  - Algorithms
KW  - Educational Measurement
KW  - Item Response Theory
UR  - https://search.ebscohost.com/login.aspx?direct=true&db=psyh&AN=2016-58398-081&lang=de&site=ehost-live
DP  - EBSCOhost
DB  - APA PsycInfo
ER  - 

TY  - JOUR
ID  - 2005-04038-003
AN  - 2005-04038-003
AU  - Walter, Otto B.
AU  - Becker, Janine
AU  - Fliege, Herbert
AU  - Bjorner, Jakob
AU  - Kosinski, Mark
AU  - Walter, Marc
AU  - Klapp, Burghard F.
AU  - Rose, Matthias
T1  - Entwicklungsschritte für einen computeradaptiven Test zur Erfassung von Angst (A-CAT₁) = Developmental steps for a computer-adapted test for anxiety
JF  - Diagnostica
JO  - Diagnostica
Y1  - 2005///
VL  - 51
IS  - 2
SP  - 88
EP  - 100
PB  - Hogrefe Verlag GmbH & Co. KG
SN  - 0012-1924
SN  - 2190-622X
AD  - Rose, Matthias, Medizinische Klinik mit Schwerpunkt Psychosomatik und Psychotherapie, Charite, Universitatsmedizin Berlin, Luisenstrasse 13a, 10117, Berlin, Germany
N1  - Accession Number: 2005-04038-003. Translated Title: Developmental steps for a computer-adapted test for anxiety. Partial author list: First Author & Affiliation: Walter, Otto B.; Medizinische Klinik mit Schwerpunkt Psychosomatik und Psychotherapie, Charité, Universitätsmedizin Berlin, Berlin, Germany. Release Date: 20050509. Correction Date: 20221128. Publication Type: Journal (0100), Peer Reviewed Journal (0110). Format Covered: Print. Document Type: Journal Article. Language: GermanMajor Descriptor: Adaptive Testing; Anxiety; Test Construction; Computerized Assessment. Minor Descriptor: Item Response Theory. Classification: Clinical Psychological Testing (2224); Anxiety Disorders (3215). Population: Human (10). Location: Germany. Tests & Measures: State Trait Anxiety Inventory. Methodology: Empirical Study; Quantitative Study. References Available: Y. Page Count: 13. Issue Publication Date: 2005. 
AB  - Psychological constructs are usually assessed by instruments based on classical test theory (CTT). Since the sixties, item response theory (IRT) offers a promising alternative to these traditional ways of test construction. On the basis of IRT computerized adaptive tests (CATs) can be developed that select items from an item bank according to previous responses of the patients. These individually tailored tests promise to maximize the measurement precision and require fewer items than conventional questionnaires. We constructed a CAT for anxiety to examine whether the theoretical advantages materialize in a practical application. Data of N=2,348 patients were collected at the Department of Internal Medicine and Psychosomatics, Charité University Medicine Berlin. As part of the diagnostic routines a broad set of established questionnaires was answered. Out of these questionnaires, 81 items with relevance to anxiety were selected in a Delphi process. The properties of these items were examined by confirmatory factor analysis with analysis of residual correlations (MplusTM, evaluation of item response curves (TestgrafTM), and computation of discrimination parameters (ParscaleTM). Fifty items remained in the item pool after the selection process. Item parameters were estimated according to the Generalized Partial Credit Model (GPCM). Simulation studies show that anxiety can be estimated with approx. 7 items (standard error of ≤.32, corresponding to a reliability level of p ≥.90). Furthermore, our simulation studies indicate that the CAT algorithm has higher discriminative power for patients at high and low levels of anxiety compared to a conventional CCT based sum score (STAI state scale). (PsycInfo Database Record (c) 2022 APA, all rights reserved)
KW  - computerized adaptive tests
KW  - anxiety
KW  - item response theory
KW  - test development
KW  - Adaptive Testing
KW  - Anxiety
KW  - Test Construction
KW  - Computerized Assessment
KW  - Item Response Theory
DO  - 10.1026/0012-1924.51.2.88
UR  - https://search.ebscohost.com/login.aspx?direct=true&db=psyh&AN=2005-04038-003&lang=de&site=ehost-live
UR  - rose@charite.de
DP  - EBSCOhost
DB  - APA PsycInfo
ER  - 

TY  - JOUR
ID  - 2014-54086-001
AN  - 2014-54086-001
AU  - Swanholm, Eric
AU  - McDonald, Wade
AU  - Makris, Una
AU  - Noe, Carl
AU  - Gatchel, Robert
T1  - Estimates of minimally important differences (MIDs) for two Patient‐Reported Outcomes Measurement Information System (PROMIS) computer‐adaptive tests in chronic pain patients
JF  - Journal of Applied Biobehavioral Research
JO  - Journal of Applied Biobehavioral Research
JA  - J Appl Biobehav Res
Y1  - 2014/12//
VL  - 19
IS  - 4
SP  - 217
EP  - 232
PB  - Wiley-Blackwell Publishing Ltd.
SN  - 1071-2089
SN  - 1751-9861
AD  - Swanholm, Eric, University of Texas Southwestern Medical Center, 425 University Blvd. Clinic, Round Rock, TX, US, 78665
N1  - Accession Number: 2014-54086-001. Partial author list: First Author & Affiliation: Swanholm, Eric; University of Texas Southwestern Medical Center, Round Rock, TX, US. Other Publishers: Bellwether Publishing; Blackwell Publishing. Release Date: 20141229. Correction Date: 20221128. Publication Type: Journal (0100), Peer Reviewed Journal (0110). Format Covered: Electronic. Document Type: Journal Article. Language: EnglishMajor Descriptor: Chronic Pain; Information Systems; Psychometrics. Minor Descriptor: Anxiety; Treatment Effectiveness Evaluation; Behavioral Medicine. Classification: Health Psychology Testing (2226); Physical & Somatic Disorders (3290). Population: Human (10); Male (30); Female (40). Location: US. Age Group: Adulthood (18 yrs & older) (300). Tests & Measures: Pain Disability Questionnaire; Pain Catastrophizing Scale DOI: 10.1037/t01304-000; Patient-Reported Outcomes Measurement Information System DOI: 10.1037/t06780-000; 36-Item Short Form Health Survey DOI: 10.1037/t07023-000; Brief Illness Perception Questionnaire DOI: 10.1037/t10379-000. Methodology: Empirical Study; Quantitative Study. References Available: Y. Page Count: 16. Issue Publication Date: Dec, 2014. Copyright Statement: Wiley Periodicals, Inc. 2014. 
AB  - Anchor‐ and distribution‐based methods were combined to evaluate and establish minimally important differences (MIDs) for two Patient‐Reported Outcomes Measurement Information System (PROMIS) measures in an outpatient chronic pain population. These included the computer‐adaptive test (CAT) versions of two PROMIS measures: Depressive Symptoms and Anxiety‐Related Symptoms (PROMIS; Cella, Gershon, Lai & Choi). Participants (n = 170) undergoing a behavioral medicine evaluation in an interdisciplinary pain management clinic completed two PROMIS CATs and multiple clinical anchor measures/questions. Modeled after similar analyses (Yost, Eton, Garcia, & Cella), three a priori criteria were used to select usable cross‐sectional anchor‐based MID estimates; these included a minimum Spearman correlation of .3 between the PROMIS measure and anchor item/categories, a minimum comparison group sample size of 10 within each anchor, and an effect size between .2 and .8 for each anchor‐based estimate. For each PROMIS measure, the mean standard error of measurement was calculated and incorporated into MID analyses. Using a large sample (n = 170), a number of the cross‐sectional T‐score anchor‐based MID estimates (57%) were not included due to failure to meet a priori criteria. Based on the analyses, the following T‐score MID ranges are recommended: Depression CAT (3.5–5.5) and Anxiety CAT (3.0–5.5). The average effect sizes for MID estimates ranged from .32 to .57. This study is among the first to address MIDs for PROMIS measures; it is the first study to establish usable MIDs for psychological symptoms on outpatients with chronic/persistent pain. The results may be used to gauge minimally important clinical difference and/or treatment response for individuals within this patient population. MIDs for PROs are particularly useful when treatment responses are significant to the patient but are difficult to evaluate during the clinical visit. (PsycInfo Database Record (c) 2022 APA, all rights reserved)
KW  - chronic pain patients
KW  - psychometrics
KW  - Patient-Reported Outcomes Measurement Information System
KW  - anxiety
KW  - treatment responses
KW  - Chronic Pain
KW  - Information Systems
KW  - Psychometrics
KW  - Anxiety
KW  - Treatment Effectiveness Evaluation
KW  - Behavioral Medicine
DO  - 10.1111/jabr.12026
UR  - https://search.ebscohost.com/login.aspx?direct=true&db=psyh&AN=2014-54086-001&lang=de&site=ehost-live
UR  - ericnswanholm@gmail.com
DP  - EBSCOhost
DB  - APA PsycInfo
ER  - 

TY  - THES
ID  - 2021-94590-250
AN  - 2021-94590-250
AU  - Jing, Shumin
T1  - Estimating psychometric properties of computerized multistage testing
JF  - Dissertation Abstracts International Section A: Humanities and Social Sciences
JO  - Dissertation Abstracts International Section A: Humanities and Social Sciences
Y1  - 2022///
VL  - 83
IS  - 3-A
PB  - ProQuest Information & Learning
SN  - 0419-4209
SN  - 979-8544292630
N1  - Accession Number: 2021-94590-250. Other Journal Title: Dissertation Abstracts International. Partial author list: First Author & Affiliation: Jing, Shumin; The University of Iowa, Psychological & Quantitative Foundations, US. Release Date: 20211216. Correction Date: 20221128. Publication Type: Dissertation Abstract (0400). Format Covered: Electronic. Document Type: Dissertation. Dissertation Number: AAI28651781. ISBN: 979-8544292630. Language: EnglishMajor Descriptor: Item Response Theory; Measurement; Psychometrics; Simulation; Test Items. Minor Descriptor: Error of Measurement; Estimation; Statistical Probability. Classification: Educational & School Psychology (3500). Population: Human (10). Methodology: Empirical Study; Quantitative Study. 
AB  - In recent decades, computerized multistage testing (MST) has been utilized as an alternative testing mode for large-scale testing programs. Although the usage of MST has been discussed extensively, there is little literature that addresses its psychometric properties. The motivation of this dissertation arises from the need to properly estimate psychometric properties under the MST framework. The primary purpose of this dissertation is to propose an integrated set of formulas for estimating psychometric properties for MST, including error variance, reliability, and classification indices. To achieve this goal, existing methods that are used for the traditional P\\&P tests are extended to the MST context, if possible; and new methods are proposed based on the characteristics of MST (i.e., routing probability) for the ML and EAP estimators. For the ML estimator, a conventional approach to computing MST test information analytically is also included for estimating the psychometric properties. The proposed approach and the conventional approach are then applied to a single set of MST data, with varied conditions. Further, a simulation approach is employed for the purpose of understanding and comparing the performance of the proposed approach and conventional approach. In the simulation approach, replications of test data are used to obtain results.The main findings of this dissertation were as follows: (1) for the ML estimator, the proposed analytic approach performed better than the conventional approach, indicated by comparing their results with those from the simulation approach; (2) for the EAP estimator, results from using the Bayesian approach approach led to smaller values of marginal error variances and higher values of reliability, compared with the quadrature approach; (3) longer test resulted in decreased error variances and improved reliability; (4) for the EAP Bayesian approach, increasing sample size had a limited impact on the estimation results; (5) for all the approaches, conditional classification consistency and accuracy gradually decreased when the ability parameter was moving further away from the cut scores. All of the estimation formulas provided in this dissertation can be applied directly to MST data in practice. (PsycInfo Database Record (c) 2022 APA, all rights reserved)
KW  - Computerized Multistage Testing
KW  - Item Response Theory
KW  - Psychometric Properties
KW  - Item Response Theory
KW  - Measurement
KW  - Psychometrics
KW  - Simulation
KW  - Test Items
KW  - Error of Measurement
KW  - Estimation
KW  - Statistical Probability
UR  - https://search.ebscohost.com/login.aspx?direct=true&db=psyh&AN=2021-94590-250&lang=de&site=ehost-live
DP  - EBSCOhost
DB  - APA PsycInfo
ER  - 

TY  - JOUR
ID  - 2023-11609-003
AN  - 2023-11609-003
AU  - Cabán-Ruiz, Michelle A.
AU  - Dávila-Marrero, Elixmahir
T1  - Evaluación psicológica comprensiva en un niño con lesión cerebral: Un estudio de caso = Comprehensive psychological evaluation of a child with brain injury: A case study
JF  - Revista Puertorriqueña de Psicología
JO  - Revista Puertorriqueña de Psicología
JA  - Rev Puertorriquena Psicol
Y1  - 2021///
VL  - 32
IS  - 1
SP  - 50
EP  - 63
PB  - Asociación de Psicólogos de Puerto Rico
SN  - 1946-2026
SN  - 2331-6950
AD  - Dávila-Marrero, Elixmahir
N1  - Accession Number: 2023-11609-003. Translated Title: Comprehensive psychological evaluation of a child with brain injury: A case study. Translated Serial Title: Puerto Rican Journal of Psychology. Partial author list: First Author & Affiliation: Cabán-Ruiz, Michelle A.; Universidad de Puerto Rico, Recinto Rio Piedras, San Juan, Puerto Rico. Release Date: 20221117. Publication Type: Journal (0100), Peer Reviewed Journal (0110). Format Covered: Electronic. Document Type: Journal Article. Language: Spanish; CastilianMajor Descriptor: Psychodiagnosis; Psychological Assessment; Special Education; Brain Injuries. Minor Descriptor: Achievement Measures; Differential Diagnosis; Magnetic Resonance Imaging; Measurement. Classification: Neurological Disorders & Brain Damage (3297). Population: Human (10). Age Group: Childhood (birth-12 yrs) (100); School Age (6-12 yrs) (180). Methodology: Clinical Case Study. References Available: Y. Page Count: 14. Issue Publication Date: 2021. Publication History: Accepted Date: Nov 2, 2021; First Submitted Date: May 29, 2021. 
AB  - A comprehensive psychological evaluation is distinguished by a process that takes in consideration the history and clinical observations in a meaningful and contextual way in order to identify the most appropriate instruments and thus carry out a comprehensive analysis of the scores, facilitating an accurate diagnostic impression. We present a case study comparing and analyzing data from two psychological evaluations carried out at the age of 7 years and 3 months and, at 9 years and 3 months, in a child with a subarachnoid cyst affecting the frontal, temporal, and right parietal lobes. As a result of the brain injury, the child exhibited changes in emotional modulation, executive functioning, and learning. As part of the comprehensive evaluation, the following assessment protocol was used: developmental history, previous psychological evaluations, results of magnetic resonance imaging of the brain, tests of verbal and non-verbal intelligence, achievement tests, neuropsychological and adaptive tests. The results obtained highlight positive changes in the child when comparing the two times under consideration. The comprehensive approach allowed analyzing the child's functioning in different areas and offering a differential diagnosis, considering the brain injury, and adjusting the therapeutic and academic recommendations. (PsycInfo Database Record (c) 2022 APA, all rights reserved)
AB  - Una evaluación psicológica comprensiva se distingue por ser un proceso que toma en consideración de manera significativa y contextual la historia, y las observaciones clínicas, para identificar los instrumentos de evaluación más apropiados y realizar un análisis integral de las puntuaciones, facilitando la impresión diagnóstica. Presentamos un estudio de caso en el que se comparan y analizan datos de dos evaluaciones psicológicas realizadas, a la edad de 7 años y 3 meses y, a los 9 años y 3 meses, a un niño con un quiste subaracnoideo impactando el lóbulo frontal, temporal, y parietal derecho. Como resultado de la lesión cerebral, el niño presentó cambios en la modulación emocional, el funcionamiento ejecutivo y el aprendizaje. Como parte de la evaluación compresiva se utilizó el siguiente protocolo de evaluación: historial de desarrollo, evaluaciones psicológicas previas, resultados de imagen de resonancia magnética del cerebro, pruebas de inteligencia verbales y no verbales, aprovechamiento académico, neuropsicológicas y adaptativas. Los resultados destacan cambios favorables en el niño al comparar los dos tiempos de evaluación bajo consideración. El acercamiento comprensivo permitió analizar el funcionamiento del niño en diferentes áreas y ofrecer un diagnóstico diferencial, considerando la lesión cerebral y ajustando las recomendaciones terapéuticas y académicas. (PsycInfo Database Record (c) 2022 APA, all rights reserved)
KW  - case study
KW  - comprehensive psychological assessment
KW  - special education
KW  - subarachnoid cyst
KW  - Psychodiagnosis
KW  - Psychological Assessment
KW  - Special Education
KW  - Brain Injuries
KW  - Achievement Measures
KW  - Differential Diagnosis
KW  - Magnetic Resonance Imaging
KW  - Measurement
UR  - https://search.ebscohost.com/login.aspx?direct=true&db=psyh&AN=2023-11609-003&lang=de&site=ehost-live
UR  - elixmahir.davila@upr.edu
DP  - EBSCOhost
DB  - APA PsycInfo
ER  - 

TY  - JOUR
ID  - 2012-24237-008
AN  - 2012-24237-008
AU  - Wu, Huey-Min
AU  - Kuo, Bor-Chen
AU  - Yang, Jinn-Min
T1  - Evaluating knowledge structure-based adaptive testing algorithms and system development
JF  - Journal of Educational Technology & Society
JO  - Journal of Educational Technology & Society
JA  - J Educ Techno Soc
Y1  - 2012/04//
VL  - 15
IS  - 2
SP  - 73
EP  - 88
PB  - International Forum of Educational Technology & Society
SN  - 1176-3647
SN  - 1436-4522
AD  - Wu, Huey-Min
N1  - Accession Number: 2012-24237-008. Partial author list: First Author & Affiliation: Wu, Huey-Min; Research Center for Testing and Assessment, National Academy for Educational Research, New Taipei City, Taiwan. Release Date: 20130610. Correction Date: 20190211. Publication Type: Journal (0100), Peer Reviewed Journal (0110). Format Covered: Electronic. Document Type: Journal Article. Language: EnglishGrant Information: Wu, Huey-Min. Major Descriptor: Adaptive Testing; Algorithms; Student Characteristics; Computerized Assessment. Classification: Curriculum & Programs & Teaching Methods (3530). Population: Human (10). Methodology: Empirical Study; Quantitative Study. References Available: Y. Page Count: 16. Issue Publication Date: Apr, 2012. Publication History: Accepted Date: Mar 31, 2011; Revised Date: Jan 26, 2011; First Submitted Date: Sep 11, 2009. Copyright Statement: International Forum of Educational Technology & Society (IFETS)
AB  - In recent years, many computerized test systems have been developed for diagnosing students’ learning profiles. Nevertheless, it remains a challenging issue to find an adaptive testing algorithm to both shorten testing time and precisely diagnose the knowledge status of students. In order to find a suitable algorithm, four adaptive testing algorithms, based on ordering theory, item relational structure theory, Diagnosys, and domain experts, were evaluated based on the training sample size, prediction accuracy, and the use of test items by the simulation study with paper-based test data. Based on the results of simulation study, ordering theory has the best performance. An ordering-theory-based knowledge-structure-adaptive testing system was developed and evaluated. The results of this system showed that the two different interfaces, paper-based and computer-based, did not affect the examinees’ performance. In addition, the effect of correct guessing was discussed, and two methods with adaptive testing algorithms were proposed to mitigate this effect. The experimental results showed that the proposed methods improve the effect of correct guessing. (PsycINFO Database Record (c) 2019 APA, all rights reserved)
KW  - adaptive testing algorithms
KW  - system development
KW  - student knowledge status
KW  - performance
KW  - Adaptive Testing
KW  - Algorithms
KW  - Student Characteristics
KW  - Computerized Assessment
U1  - Sponsor: National Science Council of Taiwan, China. Recipients: Wu, Huey-Min; Kuo, Bor-Chen; Yang, Jinn-Min
U1  - Sponsor: National Taichung University of Education. Grant: NSC-92-2521-S-142-003; NSC 97-2511-S-142-004; NSC 98-2410-H-142-005-MY2; NSC-100-2410-H-656-007; 98T202-3. Recipients: Wu, Huey-Min; Kuo, Bor-Chen; Yang, Jinn-Min
UR  - https://search.ebscohost.com/login.aspx?direct=true&db=psyh&AN=2012-24237-008&lang=de&site=ehost-live
UR  - ygm@ms3.ntcu.edu.tw
UR  - kbc@mail.ntcu.edu.tw
UR  - lhswu@seed.net.tw
DP  - EBSCOhost
DB  - APA PsycInfo
ER  - 

TY  - THES
ID  - 2004-99023-068
AN  - 2004-99023-068
AU  - Zenisky, April L.
T1  - Evaluating the effects of several multi-stage testing design variables on selected psychometric outcomes for certification and licensure assessment
JF  - Dissertation Abstracts International Section A: Humanities and Social Sciences
JO  - Dissertation Abstracts International Section A: Humanities and Social Sciences
Y1  - 2004///
VL  - 65
IS  - 6-A
SP  - 2174
EP  - 2174
PB  - ProQuest Information & Learning
SN  - 0419-4209
N1  - Accession Number: 2004-99023-068. Other Journal Title: Dissertation Abstracts International. Partial author list: First Author & Affiliation: Zenisky, April L.; U Massachusetts Amherst, US. Release Date: 20050418. Publication Type: Dissertation Abstract (0400). Format Covered: Electronic. Document Type: Dissertation. Dissertation Number: AAI3136800. Language: EnglishMajor Descriptor: Professional Certification; Professional Licensing; Psychometrics; Testing. Classification: Psychometrics & Statistics & Methodology (2200). Population: Human (10). Age Group: Adulthood (18 yrs & older) (300). Methodology: Empirical Study; Quantitative Study. Page Count: 1. 
AB  - Computer-based testing is becoming popular with credentialing agencies because new test designs are possible and the evidence is clear that these new designs can increase the reliability and validity of candidate scores and pass/fail decisions. Research on MST to date suggests that the measurement quality of MST results is comparable to full-fledged computer-adaptive tests and improved over computerized fixed-form tests. MST's promise dwells in this potential for improved measurement with greater control than other adaptive approaches for constructing test forms. Recommending use of the MST design and advising how best to set up the design, however, are two different things. The purpose of the current simulation study was to advance an established line of research on MST methodology by enhancing understanding of how several important design variables affect outcomes for high-stakes credentialing. Modeling of the item bank, the candidate population, and the statistical characteristics of test items reflect an operational credentialing exam's conditions. Studied variables were module arrangement (4 designs), amount of overall test information (4 levels), distribution of information over stages (2 variations), strategies for between-stage routing (4 levels), and pass rates (3 levels), for 384 conditions total. Results showed that high levels of decision accuracy (DA) and decision consistency (DC) were consistently observed, even when test information was reduced by as much as 25%. No differences due to the choice of module arrangement were found. With high overall test information, results were optimal when test information was divided equally among stages; with reduced test information gathering more test information at Stage 1 provided the best results. Generalizing simulation study findings is always problematic. In practice, psychometric models never completely explain candidate performance, and with MST, there is always the potential psychological impact on candidates if test difficulty shifts are noticed. At the same time, two findings seem to stand out in this research: (1) with limited amounts of overall test information, it may be best to capitalize on available information with accurate branching decisions early, and (2) there may be little statistical advantage in exceeding test information much above 10 as gains in reliability and validity appear minimal. (PsycINFO Database Record (c) 2016 APA, all rights reserved)
KW  - multi-stage testing
KW  - psychometrics
KW  - certification
KW  - licensure assessment
KW  - Professional Certification
KW  - Professional Licensing
KW  - Psychometrics
KW  - Testing
UR  - https://search.ebscohost.com/login.aspx?direct=true&db=psyh&AN=2004-99023-068&lang=de&site=ehost-live
DP  - EBSCOhost
DB  - APA PsycInfo
ER  - 

TY  - JOUR
ID  - 2009-03818-004
AN  - 2009-03818-004
AU  - Fliege, Herbert
AU  - Becker, Janine
AU  - Walter, Otto B.
AU  - Rose, Matthias
AU  - Bjorner, Jakob B.
AU  - Klapp, Burghard F.
T1  - Evaluation of a computer-adaptive test for the assessment of depression (D-CAT) in clinical application
JF  - International Journal of Methods in Psychiatric Research
JO  - International Journal of Methods in Psychiatric Research
JA  - Int J Methods Psychiatr Res
Y1  - 2009///
VL  - 18
IS  - 1
SP  - 23
EP  - 36
PB  - John Wiley & Sons
SN  - 1049-8931
SN  - 1557-0657
AD  - Fliege, Herbert, Department of Psychosomatic Medicine and Psychotherapy, Charite Universitatsmedizin Berlin, Luisenstrasse 13 A, D-10117, Berlin, Germany
N1  - Accession Number: 2009-03818-004. PMID: 19194856 Partial author list: First Author & Affiliation: Fliege, Herbert; Clinic for Internal Medicine, Department of Psychosomatic Medicine and Psychotherapy, Charite Universitatsmedizin Berlin, Berlin, Germany. Release Date: 20090803. Correction Date: 20100705. Publication Type: Journal (0100), Peer Reviewed Journal (0110). Format Covered: Print. Document Type: Journal Article. Language: EnglishConference Information: PROMIS Inaugural Conference, Sep, 2006, Gaithersburg, MD, US. Conference Note: Parts of the results have been orally presented at the aforementioned conference. Major Descriptor: Computers; Item Response Theory; Major Depression; Psychometrics; Test Validity. Classification: Health Psychology Testing (2226); Affective Disorders (3211). Population: Human (10); Male (30); Female (40). Location: Germany. Age Group: Adulthood (18 yrs & older) (300); Young Adulthood (18-29 yrs) (320); Thirties (30-39 yrs) (340); Middle Age (40-64 yrs) (360); Aged (65 yrs & older) (380). Tests & Measures: Computer-Adaptive Test for Depression. Methodology: Empirical Study; Quantitative Study. References Available: Y. Page Count: 14. Issue Publication Date: 2009. 
AB  - In the past, a German Computerized Adaptive Test, based on Item Response Theory (IRT), was developed for purposes of assessing the construct depression [Computer-adaptive test for depression (D-CAT)]. This study aims at testing the feasibility and validity of the real computer-adaptive application. The D-CAT, supplied by a bank of 64 items, was administered on personal digital assistants (PDAs) to 423 consecutive patients suffering from psychosomatic and other medical conditions (78 with depression). Items were adaptively administered until a predetermined reliability (r ≥ 0.90) was attained. For validation purposes, the Hospital Anxiety and Depression Scale (HADS), the Center for Epidemiological Studies Depression (CES-D) scale, and the Beck Depression Inventory (BDI) were administered. Another sample of 114 patients was evaluated using standardized diagnostic interviews [Composite International Diagnostic Interview (CIDI)]. The D-CAT was quickly completed (mean 74 seconds), well accepted by the patients and reliable after an average administration of only six items. In 95% of the cases, 10 items or less were needed for a reliable score estimate. Correlations between the D-CAT and the HADS, CES-D, and BDI ranged between r = 0.68 and r = 0.77. The D-CAT distinguished between diagnostic groups as well as established questionnaires do. The D-CAT proved an efficient, well accepted and reliable tool. Discriminative power was comparable to other depression measures, whereby the CAT is shorter and more precise. Item usage raises questions of balancing the item selection for content in the future. (PsycINFO Database Record (c) 2016 APA, all rights reserved)
KW  - Computerized Adaptive Test
KW  - Item Response Theory
KW  - depression
KW  - test validity
KW  - psychometrics
KW  - Adolescent
KW  - Adult
KW  - Aged
KW  - Computer Systems
KW  - Depression
KW  - Discriminant Analysis
KW  - Female
KW  - Humans
KW  - Male
KW  - Middle Aged
KW  - Patient Compliance
KW  - Psychometrics
KW  - Reproducibility of Results
KW  - Retrospective Studies
KW  - Statistics, Nonparametric
KW  - Surveys and Questionnaires
KW  - Young Adult
KW  - Computers
KW  - Item Response Theory
KW  - Major Depression
KW  - Psychometrics
KW  - Test Validity
U1  - Sponsor: Charité Universitätsmedizin Berlin, Germany. Grant: UFF-2006-088. Other Details: Research Fund. Recipients: No recipient indicated
DO  - 10.1002/mpr.274
UR  - https://search.ebscohost.com/login.aspx?direct=true&db=psyh&AN=2009-03818-004&lang=de&site=ehost-live
UR  - herbert.fliege@charite.de
DP  - EBSCOhost
DB  - APA PsycInfo
ER  - 

TY  - JOUR
ID  - 2014-54451-001
AN  - 2014-54451-001
AU  - Devine, Janine
AU  - Fliege, Herbert
AU  - Kocalevent, Rüya
AU  - Mierke, Annett
AU  - Klapp, Burghard F.
AU  - Rose, Matthias
T1  - Evaluation of Computerized Adaptive Tests (CATs) for longitudinal monitoring of depression, anxiety, and stress reactions
JF  - Journal of Affective Disorders
JO  - Journal of Affective Disorders
JA  - J Affect Disord
Y1  - 2016/01/15/
VL  - 190
SP  - 846
EP  - 853
PB  - Elsevier Science
SN  - 0165-0327
SN  - 1573-2517
AD  - Devine, Janine, Department of Psychosomatic Medicine, Center for Internal Medicine and Dermatology, Universitatsmedizin Berlin, Chariteplatz 1, 10117, Berlin, Germany
N1  - Accession Number: 2014-54451-001. PMID: 25481813 Partial author list: First Author & Affiliation: Devine, Janine; Department of Psychosomatic Medicine, Center for Internal Medicine and Dermatology, Universitatsmedizin Berlin, Berlin, Germany. Release Date: 20141208. Correction Date: 20190211. Publication Type: Journal (0100), Peer Reviewed Journal (0110). Format Covered: Electronic. Document Type: Journal Article. Language: EnglishMajor Descriptor: Psychometrics; Test Reliability; Test Validity; Computerized Assessment. Minor Descriptor: Anxiety; Item Response Theory; Major Depression; Stress Reactions. Classification: Clinical Psychological Testing (2224); Psychological Disorders (3210). Population: Human (10); Male (30); Female (40); Inpatient (50). Location: Germany. Age Group: Adulthood (18 yrs & older) (300); Young Adulthood (18-29 yrs) (320); Thirties (30-39 yrs) (340); Middle Age (40-64 yrs) (360); Aged (65 yrs & older) (380). Tests & Measures: Generalized Anxiety Disorder Questionnaire-7; Patient Health Questionnaire-9 DOI: 10.1037/t06165-000; Perceived Stress Questionnaire DOI: 10.1037/t10467-000. Methodology: Empirical Study; Longitudinal Study; Quantitative Study. References Available: Y. Page Count: 8. Issue Publication Date: Jan 15, 2016. Publication History: First Posted Date: Nov 10, 2014; Accepted Date: Oct 30, 2014; Revised Date: Oct 26, 2014; First Submitted Date: Jul 12, 2014. Copyright Statement: All rights reserved. Elsevier B.V. 2014. 
AB  - Background: Computerized adaptive testing (CAT) based on Item Response Theory, (IRT) offers an efficient way for accurate measurement of patient reported outcomes. The efficiency lies within a minimal response burden and a high measurement precision over a broad measurement range. The objective of the study was to evaluate and compare the responsiveness of CATs measuring anxiety, depression, and stress reaction to standard static self-assessment tools. Methods: Longitudinal data of n = 595 psychosomatic inpatients were analyzed for evaluating retest-reliability and sensitivity to change of the CATs compared to static measures (GAD-7, PHQ-9, and PSQ) using correlational and ANOVA statistics. The study hypothesized that CATs are at least as retest-reliable and as sensitive to change as static tools. Results: The three CATs show a low burden for patients, administering on average 5–7 (±2–6SD) items with similar retest-reliability compared to the static tools applied (A-CAT: r = .78 vs. GAD-7: r = .75, D-CAT: r = .71 vs. PHQ-9: r = .75, S-CAT: r = .80 vs. PSQ worries scale: r = .80). The CATs were overall as sensitive to change as the static tools (Cohen's d ranged between .19 and .69). Limitations: This is a monocenter, observational, longitudinal study without external clinical criteria; thus generalization to other settings may be limited. Conclusions: The tested CATs belong to the first generation of CATs being used in daily routine for more than a decade. They are as retest reliable and sensitive to change as static tools. Newer CATs may provide further practical advantages. (PsycINFO Database Record (c) 2019 APA, all rights reserved)
KW  - Computer Adaptive Test
KW  - Mental health
KW  - Questionnaire
KW  - Monitor
KW  - Item Response Theory
KW  - Sensitivity to change
KW  - Adolescent
KW  - Adult
KW  - Aged
KW  - Aged, 80 and over
KW  - Anxiety
KW  - Depression
KW  - Diagnosis, Computer-Assisted
KW  - Female
KW  - Humans
KW  - Longitudinal Studies
KW  - Male
KW  - Middle Aged
KW  - Monitoring, Ambulatory
KW  - Reproducibility of Results
KW  - Self-Assessment
KW  - Stress, Psychological
KW  - Young Adult
KW  - Psychometrics
KW  - Test Reliability
KW  - Test Validity
KW  - Computerized Assessment
KW  - Anxiety
KW  - Item Response Theory
KW  - Major Depression
KW  - Stress Reactions
U1  - Sponsor: Deutsche Forschungsgemeinschaft, Germany. Grant: RO2258/2. Other Details: CATs. Recipients: No recipient indicated
U1  - Sponsor: Charité University Hospital Berlin, Department of Psychosomatics and Psychotherapy, Germany. Recipients: No recipient indicated
DO  - 10.1016/j.jad.2014.10.063
UR  - https://search.ebscohost.com/login.aspx?direct=true&db=psyh&AN=2014-54451-001&lang=de&site=ehost-live
UR  - matthias.rose@charite.de
UR  - r.kocalevent@uke.de
UR  - herbert.fliege@auswaertiges-amt.de
UR  - janine.devine@web.de
DP  - EBSCOhost
DB  - APA PsycInfo
ER  - 

TY  - JOUR
ID  - 2020-75813-011
AN  - 2020-75813-011
AU  - Teresi, Jeanne A.
AU  - Ocepek-Welikson, Katja
AU  - Ramirez, Mildred
AU  - Kleinman, Marjorie
AU  - Ornstein, Katherine
AU  - Siu, Albert
AU  - Luchsinger, Jose
T1  - Evaluation of the measurement properties of the Perceived Stress Scale (PSS) in Hispanic caregivers to patients with Alzheimer’s disease and related disorders
JF  - International Psychogeriatrics
JO  - International Psychogeriatrics
JA  - Int Psychogeriatr
Y1  - 2020/09//
VL  - 32
IS  - 9
SP  - 1073
EP  - 1084
PB  - Cambridge University Press
SN  - 1041-6102
SN  - 1741-203X
AD  - Teresi, Jeanne A., Research Division, Hebrew Home at Riverdale, 5901 Palisade Avenue, New York, NY, US, 10471
N1  - Accession Number: 2020-75813-011. PMID: 32312342 Partial author list: First Author & Affiliation: Teresi, Jeanne A.; Research Division, Hebrew Home at Riverdale, New York, NY, US. Release Date: 20210225. Correction Date: 20210712. Publication Type: Journal (0100), Peer Reviewed Journal (0110). Format Covered: Electronic. Document Type: Journal Article. Language: EnglishConference Information: Annual Meeting of the Gerontological Society of America, Nov, 2018, Boston, MA, US. Grant Information: Siu, Albert. Conference Note: Some of these analyses were presented at the aforementioned conference. Major Descriptor: Alzheimer's Disease; Caregiver Burden; Caregivers; Psychometrics; Stress. Minor Descriptor: Education; Measurement; Urban Environments; Latinos/Latinas; Differential Item Functioning; Perceived Stress. Classification: Health Psychology Testing (2226); Home Care & Hospice (3375). Population: Human (10); Male (30); Female (40). Location: US. Age Group: Adulthood (18 yrs & older) (300); Young Adulthood (18-29 yrs) (320); Thirties (30-39 yrs) (340); Middle Age (40-64 yrs) (360). Tests & Measures: Perceived Stress Scale DOI: 10.1037/t02889-000. Methodology: Empirical Study; Quantitative Study. Supplemental Data: Other Internet. References Available: Y. Page Count: 12. Issue Publication Date: Sep, 2020. Publication History: First Posted Date: Apr 21, 2020; Accepted Date: Mar 18, 2020; Revised Date: Mar 18, 2020; Dec 9, 2019; First Submitted Date: Oct 7, 2019. Copyright Statement: International Psychogeriatric Association. 2020. 
AB  - Objectives: The Perceived Stress Scale (PSS) is the most widely used measure of perceived stress; however, minimal psychometric evaluation has been performed among Hispanic respondents, and even less among Hispanic caregivers to persons with Alzheimer’s disease and related disorders (ADRDs). Design: Secondary data analysis. Setting: New York City, NY, USA. Participants: A sample of 453 community dwelling Hispanic caregivers to patients with ADRD. Measurements: Latent variable models were used to evaluate the PSS. Exploratory and confirmatory factor analyses were used to examine unidimensionality. Differential item functioning (DIF) was examined for age, education, and language using the graded item response model. Results: The factor and bifactor analyses results supported essential unidimensionality of the item set; however, positively worded items were observed using response item theory to be less informative than the negatively worded items. Reliability estimates were high. Salient DIF was not observed for age, education, or language of interview using the primary DIF detection method. Sensitivity analyses using a second DIF detection method identified uniform language-DIF for the item, 'In the last month, how often have you felt that you were on top of things?' However, the non-compensatory DIF value was below the threshold considered salient. Conclusions: In summary, the 10-item PSS performed well in a sample of English- and Spanish-speaking Hispanic caregivers to patients with ADRD. Very little DIF, and none of high magnitude and impact, was observed. However, the negatively worded items, perhaps because they are more directly reflective of stress, were more informative. In the context of a short-form measure or computerized adaptive test, more informative items are those that would be selected for inclusion. (PsycInfo Database Record (c) 2021 APA, all rights reserved)
KW  - stress measurement
KW  - differential item functioning
KW  - Latinx caregivers
KW  - dementia
KW  - Adult
KW  - Aged
KW  - Aged, 80 and over
KW  - Alzheimer Disease
KW  - Caregivers
KW  - Factor Analysis, Statistical
KW  - Female
KW  - Hispanic Americans
KW  - Humans
KW  - Language
KW  - Male
KW  - Middle Aged
KW  - Psychometrics
KW  - Quality of Life
KW  - Reproducibility of Results
KW  - Stress, Psychological
KW  - Surveys and Questionnaires
KW  - Translations
KW  - Alzheimer's Disease
KW  - Caregiver Burden
KW  - Caregivers
KW  - Psychometrics
KW  - Stress
KW  - Education
KW  - Measurement
KW  - Urban Environments
KW  - Latinos/Latinas
KW  - Differential Item Functioning
KW  - Perceived Stress
U1  - Sponsor: National Institute on Aging, US. Grant: 1P30AG028741. Other Details: Mount Sinai Claude D. Pepper Older Americans Independence Center. Recipients: Siu, Albert
U1  - Sponsor: National Institute on Aging, US. Grant: 1P30AG059303. Other Details: Columbia University Alzheimer’s Disease Resource Center for Minority Aging Research; Manly. Recipients: Luchsinger, Jose
DO  - 10.1017/S1041610220000502
UR  - https://search.ebscohost.com/login.aspx?direct=true&db=psyh&AN=2020-75813-011&lang=de&site=ehost-live
UR  - teresimeas@aol.com
DP  - EBSCOhost
DB  - APA PsycInfo
ER  - 

TY  - JOUR
ID  - 2017-26708-009
AN  - 2017-26708-009
AU  - Franz, Annabel O.
AU  - Harrop, Tiffany M.
AU  - McCord, David M.
T1  - Examining the construct validity of the MMPI–2–RF interpersonal functioning scales using the Computerized Adaptive Test of Personality Disorder as a comparative framework
JF  - Journal of Personality Assessment
JO  - Journal of Personality Assessment
JA  - J Pers Assess
Y1  - 2017/07//
VL  - 99
IS  - 4
SP  - 416
EP  - 423
PB  - Taylor & Francis
SN  - 0022-3891
SN  - 1532-7752
AD  - Franz, Annabel O., Department of Psychiatry, Medical University of South Carolina, 67 President St., Charleston, SC, US, 29425
N1  - Accession Number: 2017-26708-009. PMID: 27661293 Other Journal Title: Journal of Projective Techniques & Personality Assessment. Partial author list: First Author & Affiliation: Franz, Annabel O.; Department of Psychology, Western Carolina University, Cullowhee, NC, US. Other Publishers: Lawrence Erlbaum. Release Date: 20170706. Correction Date: 20200917. Publication Type: Journal (0100), Peer Reviewed Journal (0110). Format Covered: Electronic. Document Type: Journal Article. Language: EnglishMajor Descriptor: Minnesota Multiphasic Personality Inventory; Social Skills; Test Forms; Test Validity; Interpersonal Relationships. Minor Descriptor: Construct Validity; Personality Disorders. Classification: Personality Scales & Inventories (2223); Group & Interpersonal Processes (3020). Population: Human (10); Male (30); Female (40). Location: US. Age Group: Adulthood (18 yrs & older) (300). Tests & Measures: Computerized Adaptive Test of Personality Disorder DOI: 10.1037/t75895-000. Methodology: Empirical Study; Quantitative Study. References Available: Y. Page Count: 8. Issue Publication Date: Jul, 2017. Publication History: Revised Date: Jul 3, 2016; First Submitted Date: Oct 27, 2015. Copyright Statement: Taylor & Francis. 2017. 
AB  - This study aimed to examine the construct validity of the Minnesota Multiphasic Personality Inventory–2 Restructured Form (MMPI–2–RF) interpersonal functioning scales (Ben-Porath & Tellegen, 2008/2011) using as a criterion measure the Computerized Adaptive Test of Personality Disorder–Static Form (CAT–PD–SF; Simms et al., 2011). Participants were college students (n = 98) recruited through the university subject pool. A series of a priori hypotheses were developed for each of the 6 interpersonal functioning scales of the MMPI–2–RF, expressed as predicted correlations with construct-relevant CAT–PD–SF scales. Of the 27 specific predictions, 21 were supported by substantial (≥ |.30|) correlations. The MMPI–2–RF Family Problems scale (FML) demonstrated the strongest correlations with CAT–PD–SF scales Anhedonia and Mistrust; Cynicism (RC3) was most highly correlated with Mistrust and Norm Violation; Interpersonal Passivity (IPP) was most highly correlated with Domineering and Rudeness; Social Avoidance (SAV) was most highly correlated with Social Withdrawal and Anhedonia; Shyness (SHY) was most highly correlated with Social Withdrawal and Anxioiusness; and Disaffiliativeness (DSF) was most highly correlated with Emotional Detachment and Mistrust. Results are largely consistent with hypotheses suggesting support for both models of constructs relevant to interpersonal functioning. Future research designed to more precisely differentiate Social Avoidance (SAV) and Shyness (SHY) is suggested. (PsycInfo Database Record (c) 2020 APA, all rights reserved)
KW  - construct validity
KW  - Minnesota Multiphasic Personality Inventory–2 Restructured Form
KW  - interpersonal functioning scales
KW  - college students
KW  - Adolescent
KW  - Adult
KW  - Female
KW  - Humans
KW  - Interpersonal Relations
KW  - MMPI
KW  - Male
KW  - Personality Disorders
KW  - Psychiatric Status Rating Scales
KW  - Reproducibility of Results
KW  - Young Adult
KW  - Minnesota Multiphasic Personality Inventory
KW  - Social Skills
KW  - Test Forms
KW  - Test Validity
KW  - Interpersonal Relationships
KW  - Construct Validity
KW  - Personality Disorders
DO  - 10.1080/00223891.2016.1222394
UR  - https://search.ebscohost.com/login.aspx?direct=true&db=psyh&AN=2017-26708-009&lang=de&site=ehost-live
UR  - franza@musc.edu
DP  - EBSCOhost
DB  - APA PsycInfo
ER  - 

TY  - JOUR
ID  - 2023-08376-001
AN  - 2023-08376-001
AU  - Addington, Elizabeth L.
AU  - Cummings, Peter
AU  - Jackson, Kathryn
AU  - Yang, DerShung
AU  - Moskowitz, Judith T.
T1  - Exploring retention, usage, and efficacy of web-based delivery of positive emotion regulation skills during the covid-19 pandemic
JF  - Affective Science
JO  - Affective Science
Y1  - 2022/10/04/
PB  - Springer
SN  - 2662-2041
SN  - 2662-205X
AD  - Addington, Elizabeth L.
N1  - Accession Number: 2023-08376-001. Partial author list: First Author & Affiliation: Addington, Elizabeth L.; Department of Medical Social Sciences, Northwestern University Feinberg School of Medicine, Chicago, IL, US. Release Date: 20221006. Publication Type: Journal (0100), Peer Reviewed Journal (0110). Format Covered: Electronic. Document Type: Journal. Language: EnglishMajor Descriptor: No terms assigned. Classification: Affective Disorders (3211). References Available: Y. Publication History: Accepted Date: Jun 16, 2022; First Submitted Date: Mar 22, 2022. Copyright Statement: The Society for Affective Science. 2022. 
AB  - COVID-19 prompted distress and increased reliance on digital mental health interventions, which previously demonstrated low rates of retention and adherence. This single-arm trial evaluated whether self-guided, web-based, positive affect regulation skills (PARK) were engaging and associated with changes in well-being during the pandemic. Over 6 weeks, PARK delivers brief lessons and practices in skills designed to increase positive emotions: noticing positive events, savoring, gratitude, mindfulness, positive reappraisal, personal strengths, and self-compassion. Patient-Reported Outcome Measurement Information System (PROMIS) computer adaptive tests of anxiety, depression, social isolation, positive affect, and meaning and purpose were administered at baseline, post-intervention, and 6 months after baseline. Retention and usage of PARK were measured by the web-based assessment and intervention platforms. The sample (n = 616) was predominantly female, non-Hispanic, white, and well-educated. Of those who completed baseline, only 42% completed a follow-up assessment; 30% never logged into PARK. Among those who did, 86% used at least one skill, but only 14% completed PARK. Across retention and usage metrics, older age predicted more engagement. In multivariable models, people of color and people with greater baseline anxiety were more likely to complete PARK. All well-being indicators improved over time, with greater improvements in anxiety and social isolation among participants who accessed at least one PARK skill compared to those who did not. Retention and usage rates mirrored pre-pandemic trends, but within this select sample, predictors of engagement differed from prior research. Findings underscore the need for additional efforts to ensure equitable access to digital mental health interventions and research. Trials registration: NCT04367922. (PsycInfo Database Record (c) 2022 APA, all rights reserved)
KW  - COVID-19
KW  - Positive emotion
KW  - Depression
KW  - Anxiety
KW  - Emotional well-being
KW  - eHealth
KW  - No terms assigned
DO  - 10.1007/s42761-022-00135-4
UR  - https://search.ebscohost.com/login.aspx?direct=true&db=psyh&AN=2023-08376-001&lang=de&site=ehost-live
UR  - ORCID: 0000-0002-1399-3318
UR  - ORCID: 0000-0002-5839-5485
UR  - elizabeth.addington@northwestern.edu
DP  - EBSCOhost
DB  - APA PsycInfo
ER  - 

TY  - JOUR
ID  - 2015-18390-012
AN  - 2015-18390-012
AU  - Guo, Lei
AU  - Zheng, Chanjin
AU  - Bian, Yufang
T1  - Exposure control methods and termination rules in variable-length cognitive diagnostic computerized adaptive testing
JF  - Acta Psychologica Sinica
JO  - Acta Psychologica Sinica
JA  - Xin Li Xue Bao
Y1  - 2015/01//
VL  - 47
IS  - 1
SP  - 129
EP  - 140
PB  - Science Press
SN  - 0439-755X
AD  - Bian, Yufang, National Key Laboratory of Cognitive Neuroscience and Learning, Beijing Normal University, Beijing, China, 100875
N1  - Accession Number: 2015-18390-012. Partial author list: First Author & Affiliation: Guo, Lei; Faculty of Psychology, Southwest University, Chongqing, China. Release Date: 20150706. Correction Date: 20190211. Publication Type: Journal (0100), Peer Reviewed Journal (0110). Format Covered: Print. Document Type: Journal Article. Language: ChineseMajor Descriptor: Adaptive Testing; Computerized Assessment. Minor Descriptor: Algorithms; Methodology; Test Administration. Classification: Psychometrics & Statistics & Methodology (2200). Population: Human (10). References Available: Y. Page Count: 12. Issue Publication Date: Jan, 2015. 
AB  - Comparing to the nonadaptive testing, the major advantage of computerized adaptive testing (CAT) is that the examinees achieve the same degree of measurement precision (i.e., fixed precision). But few studies are devoted to the termination rules in variable-length cognitive diagnostic computerized adaptive testing (CD-CAT). Inspired by the termination rule research in traditional CAT, this paper proposed four termination rules for variable-length CD-CAT. The new termination rules were standard error of attribute method (SEA), difference of the adjacent posterior probability method (DAPP), halving algorithm (HA) and hybrid method (HM), respectively. Then, the four new termination rules were compared with the HSU and KL method under two scenarios: with and without item exposure control. Three exposure control methods were considered, i.e., simple, modified restrictive progressive (MRP) and modified restrictive threshold (MRT) method. The MRP and MRT methods were extension of the Wang et al.'s (2011) work to the variable-length CD-CAT scenario. The results indicated that: (1) When the criterion of variable-length termination rule was conservative, the mean of the test length and the percentage of examinees reaching the maximum test length were large, and the classification accuracy rate for examinees who finished the CAT using fixed precision was high. (2) Without the item exposure control, the four new variable-length termination rules had a similar performance compared to the HSU method. With the increase of maximum posterior probability and the decrease of £, the classification accuracy rate and the mean test length presented a increasing trend. But the item pool usage was unsatisfactory. (3) With the item exposure control, item pool usage was greatly improved in the six variable-length termination rules while the classification accuracy rates were maintained. Different exposure control methods had a different effect on the different variable-length termination rules. The relative criterion termination rules such as DAPP and KL methods were easily affected by the item exposure control. (4) Taken all together, the SEA, HM, and HA methods were comparable to the HSU method, and followed by the KL and DAPP method. (PsycINFO Database Record (c) 2019 APA, all rights reserved)
KW  - cognitive diagnostic computerized adaptive testing
KW  - variable-length termination rule
KW  - exposure control
KW  - classification accuracy rate
KW  - DINA model
KW  - Adaptive Testing
KW  - Computerized Assessment
KW  - Algorithms
KW  - Methodology
KW  - Test Administration
DO  - 10.3724/SP.J.1041.2015.00129
UR  - https://search.ebscohost.com/login.aspx?direct=true&db=psyh&AN=2015-18390-012&lang=de&site=ehost-live
UR  - bianyufang66@126.com
DP  - EBSCOhost
DB  - APA PsycInfo
ER  - 

TY  - JOUR
ID  - 2019-63765-001
AN  - 2019-63765-001
AU  - Niazi, Shehzad K.
AU  - Spaulding, Aaron
AU  - Vargas, Emily
AU  - Chauhan, Mohit
AU  - Nordan, Lisa
AU  - Vizzini, Michael
AU  - Puspitasari, Ajeng J.
AU  - Uitti, Ryan J.
AU  - Rummans, Teresa
T1  - Feasibility study of three-phase implementation of international consortium for health outcomes measurement depression and anxiety standard set in an outpatient consultation-liaison psychiatry practice
JF  - Psychosomatics: Journal of Consultation and Liaison Psychiatry
JO  - Psychosomatics: Journal of Consultation and Liaison Psychiatry
JA  - Psychosomatics
Y1  - 2020/01//Jan-Feb, 2020
VL  - 61
IS  - 1
SP  - 8
EP  - 18
PB  - Elsevier Science
SN  - 0033-3182
SN  - 1545-7206
AD  - Niazi, Shehzad K., Department of Psychiatry and Psychology, Mayo Clinic Florida, 4500 San Pablo Road, Jacksonville, FL, US, 32224
N1  - Accession Number: 2019-63765-001. PMID: 31648776 Other Journal Title: Journal of the Academy of Consultation-Liaison Psychiatry. Partial author list: First Author & Affiliation: Niazi, Shehzad K.; Department of Psychiatry and Psychology, Mayo Clinic Florida, Jacksonville, FL, US. Other Publishers: American Psychiatric Assn. Release Date: 20191024. Correction Date: 20210902. Publication Type: Journal (0100), Peer Reviewed Journal (0110). Format Covered: Electronic. Document Type: Journal Article. Language: EnglishMajor Descriptor: Anxiety Disorders; Consultation Liaison Psychiatry; Major Depression; Outpatients; Health Outcomes. Minor Descriptor: Measurement; Psychotherapeutic Outcomes; Quality of Care; Treatment Effectiveness Evaluation; Quality of Services. Classification: Outpatient Services (3371). Population: Human (10); Male (30); Female (40). Location: US. Age Group: Adulthood (18 yrs & older) (300). Methodology: Empirical Study; Quantitative Study. References Available: Y. Page Count: 11. Issue Publication Date: Jan-Feb, 2020. Publication History: Accepted Date: Aug 21, 2019; Revised Date: Aug 21, 2019; First Submitted Date: Aug 1, 2019. Copyright Statement: Published by Elsevier Inc. All rights reserved. Academy of Consultation-Liaison Psychiatry. 2019. 
AB  - Objective: We describe a three-phase implementation of the International Consortium for Health Outcomes Measurement Depression and Anxiety Standard Set in a Consultation-Liaison Psychiatry practice. Methods: During the preintervention phase, we reviewed patient-reported outcome tools and engaged stakeholders and leadership. During phase 1, the standard set was converted into an electronic previsit intake assessment that was implemented in a physician champion's practice. Patients completed the intake on a tablet, and computer adaptive testing was used to reduce response burden. Physician-facing data display facilitated use during subsequent in-person visits. An electronic version of the follow-up standard set was used during follow-up visits. During phase 2, a second physician tested scalability and the intervention was disseminated department wide in phase 3. Results: During phase 1, 186 intakes and 67 follow-up electronic patient-reported outcome sets were completed. Average patient age was 54 years, and 44% were male. On average, patients ranked the tool 4.4 out of 5 and spent 22 minutes completing the intake. Time-driven activity-based costing found the new process to be cost-effective. During phase 2, 386 patients completed electronic patient-reported outcome sets, with 315 follow-up visits. Patients ranked the tool as 4.0 out of 5 and spent 26 minutes completing the questions. During phase 3, 2166 patients completed intake electronic patient-reported outcome sets and 1249 follow-up visits. Patients ranked the tool 4.3 out of 5 and spent 26 minutes on it. Scores and completion time did not differ greatly between phases. Conclusions: Integration of the International Consortium for Health Outcomes Measurement Depression and Anxiety Standard Set is feasible. Future research comparing International Consortium for Health Outcomes Measurement set with other approaches and in different settings is needed. (PsycInfo Database Record (c) 2021 APA, all rights reserved)
KW  - electronic patient-reported outcomes
KW  - patient-reported outcomes
KW  - psychiatry
KW  - quality improvement
KW  - Adult
KW  - Aged
KW  - Alcoholism
KW  - Ambulatory Care
KW  - Anxiety
KW  - Computers, Handheld
KW  - Data Collection
KW  - Depression
KW  - Electronic Health Records
KW  - Feasibility Studies
KW  - Female
KW  - Humans
KW  - Implementation Science
KW  - Male
KW  - Mass Screening
KW  - Middle Aged
KW  - Obsessive-Compulsive Disorder
KW  - Patient Health Questionnaire
KW  - Patient Reported Outcome Measures
KW  - Phobia, Social
KW  - Psychiatry
KW  - Psychosomatic Medicine
KW  - Quality Improvement
KW  - Stakeholder Participation
KW  - Anxiety Disorders
KW  - Consultation Liaison Psychiatry
KW  - Major Depression
KW  - Outpatients
KW  - Health Outcomes
KW  - Measurement
KW  - Psychotherapeutic Outcomes
KW  - Quality of Care
KW  - Treatment Effectiveness Evaluation
KW  - Quality of Services
U1  - Sponsor: Mayo Clinic, Robert D. and Patricia E. Kern Center for the Science of Healthcare Delivery, US. Recipients: No recipient indicated
DO  - 10.1016/j.psym.2019.08.006
UR  - https://search.ebscohost.com/login.aspx?direct=true&db=psyh&AN=2019-63765-001&lang=de&site=ehost-live
UR  - ORCID: 0000-0001-9727-6756
UR  - Niazi.shehzad@mayo.edu
DP  - EBSCOhost
DB  - APA PsycInfo
ER  - 

TY  - JOUR
ID  - 2011-29877-009
AN  - 2011-29877-009
AU  - Anseel, Frederik
AU  - Van Yperen, Nico W.
AU  - Janssen, Onne
AU  - Duyck, Wouter
T1  - Feedback type as a moderator of the relationship between achievement goals and feedback reactions
JF  - Journal of Occupational and Organizational Psychology
JO  - Journal of Occupational and Organizational Psychology
JA  - J Occup Organ Psychol
Y1  - 2011/12//
VL  - 84
IS  - 4
SP  - 703
EP  - 722
PB  - Wiley-Blackwell Publishing Ltd.
SN  - 0963-1798
SN  - 2044-8325
AD  - Anseel, Frederik, Department of Personnel Management, Ghent University, Henri Dunantlaan 2, 9000, Ghent, Belgium
N1  - Accession Number: 2011-29877-009. Other Journal Title: Journal of Occupational Psychology. Partial author list: First Author & Affiliation: Anseel, Frederik; Ghent University, Ghent, Belgium. Other Publishers: British Psychological Society. Release Date: 20120123. Publication Type: Journal (0100), Peer Reviewed Journal (0110). Format Covered: Electronic. Document Type: Journal Article. Language: EnglishMajor Descriptor: Achievement; Employee Attitudes; Feedback; Goals; Job Performance. Classification: Personnel Attitudes & Job Satisfaction (3650). Population: Human (10); Male (30); Female (40). Location: Belgium. Age Group: Adolescence (13-17 yrs) (200); Adulthood (18 yrs & older) (300); Young Adulthood (18-29 yrs) (320); Thirties (30-39 yrs) (340); Middle Age (40-64 yrs) (360). Methodology: Empirical Study; Field Study; Quantitative Study. References Available: Y. Page Count: 20. Issue Publication Date: Dec, 2011. Publication History: Revised Date: May 25, 2010; First Submitted Date: Nov 17, 2009. Copyright Statement: The British Psychological Society. 2010. 
AB  - The aim of the current study is to shed new light on the inconsistent relationship between performance-approach (PAp) goals and feedback reactions by examining feedback type as a moderator. Results of a field experiment (N = 939) using a web-based work simulation task showed that the effect of achievement-approach goals was moderated by feedback type. Relative to individuals pursuing mastery-approach goals, individuals pursuing PAp goals responded more negatively to comparative feedback but not to task-referenced feedback. In line with the hypothesized mediated moderation model, the interaction between achievement goals and feedback type also indirectly affected task performance through feedback reactions. Providing employees with feedback is a key psychological principle used in a wide range of human resource and performance management instruments (e.g., developmental assessment centres, multi-source/360° feedback, training, selection, performance appraisal, management education, computer-adaptive testing, and coaching). The current study suggests that organizations need to strike a balance between encouraging learning and encouraging performance, as too much emphasis on comparative performance (both in goal inducement and in feedback style) may be detrimental to employees’ reactions and rate of performance improvement. (PsycINFO Database Record (c) 2016 APA, all rights reserved)
KW  - achievement goals
KW  - feedback reactions
KW  - employee reactions
KW  - performance appraisal
KW  - Achievement
KW  - Employee Attitudes
KW  - Feedback
KW  - Goals
KW  - Job Performance
DO  - 10.1348/096317910X516372
UR  - https://search.ebscohost.com/login.aspx?direct=true&db=psyh&AN=2011-29877-009&lang=de&site=ehost-live
UR  - ORCID: 0000-0003-2114-6212
UR  - frederik.anseel@ugent.be
DP  - EBSCOhost
DB  - APA PsycInfo
ER  - 

TY  - JOUR
ID  - 2017-70124-003
AN  - 2017-70124-003
AU  - Stark, Stephen
AU  - Chernyshenko, Oleksandr S.
AU  - Drasgow, Fritz
AU  - Nye, Christopher D.
AU  - White, Leonard A.
AU  - Heffner, Tonia
AU  - Farmer, William L.
T1  - From ABLE to TAPAS: A new generation of personality tests to support military selection and classification decisions
T3  - Selected New Developments in Military Enlistment Testing
JF  - Military Psychology
JO  - Military Psychology
JA  - Mil Psychol
Y1  - 2014/05//
VL  - 26
IS  - 3
SP  - 153
EP  - 164
PB  - Educational Publishing Foundation
SN  - 0899-5605
SN  - 1532-7876
SN  - 1-4338-1903-1
AD  - Stark, Stephen, University of South Florida, Department of Psychology, PCD 4118G, Tampa, FL, US, 33620
N1  - Accession Number: 2017-70124-003. Partial author list: First Author & Affiliation: Stark, Stephen; University of South Florida, Department of Psychology, Tampa, FL, US. Other Publishers: Lawrence Erlbaum; Taylor & Francis. Release Date: 20180326. Correction Date: 20201210. Publication Type: Journal (0100), Peer Reviewed Journal (0110). Format Covered: Electronic. Document Type: Journal Article. ISBN: 1-4338-1903-1. Language: EnglishMajor Descriptor: Military Personnel; Personality Measures; Personnel Selection. Minor Descriptor: Adaptive Testing; Scoring (Testing); Test Construction; Computerized Assessment. Classification: Tests & Testing (2220); Military Psychology (3800). Population: Human (10). Location: US. Tests & Measures: Assessment of Individual Motivation; Navy Computerized Adaptive Personality Scales; Tailored Adaptive Personality Assessment System. References Available: Y. Page Count: 12. Issue Publication Date: May, 2014. Copyright Statement: American Psychological Association. 2014. 
AB  - This article discusses 3 modern forced-choice personality tests developed for the U.S Armed Services to provide resistance to faking and other forms of response distortion: the Assessment of Individual Motivation, the Navy Computerized Adaptive Personality Scales, and the Tailored Adaptive Personality Assessment System. These tests represent the transition from Likert to forced-choice formats and from static to computerized adaptive item selection to meet the challenges of large-scale, high-stakes testing environments. For each test, we briefly describe the personality constructs that are assessed, the response format and scoring methods, and selected ongoing research and development efforts. We also highlight the potential of these tests for personnel selection, classification, and diagnostic screening purposes. (PsycInfo Database Record (c) 2020 APA, all rights reserved)
KW  - personality
KW  - computerized adaptive testing
KW  - forced choice
KW  - ideal point
KW  - narrow factors
KW  - personality measures
KW  - Assessment of Individual Motivation
KW  - Navy Computerized Adaptive Personality Scales
KW  - Tailored Adaptive Personality Assessment System
KW  - test development
KW  - format
KW  - scoring
KW  - personnel selection
KW  - classification
KW  - Military Personnel
KW  - Personality Measures
KW  - Personnel Selection
KW  - Adaptive Testing
KW  - Scoring (Testing)
KW  - Test Construction
KW  - Computerized Assessment
DO  - 10.1037/mil0000044
UR  - https://search.ebscohost.com/login.aspx?direct=true&db=psyh&AN=2017-70124-003&lang=de&site=ehost-live
UR  - sestark@usf.edu
DP  - EBSCOhost
DB  - APA PsycInfo
ER  - 

TY  - JOUR
ID  - 2014-38600-003
AN  - 2014-38600-003
AU  - Stark, Stephen
AU  - Chernyshenko, Oleksandr S.
AU  - Drasgow, Fritz
AU  - Nye, Christopher D.
AU  - White, Leonard A.
AU  - Heffner, Tonia
AU  - Farmer, William L.
T1  - From ABLE to TAPAS: A new generation of personality tests to support military selection and classification decisions
T3  - Selected New Developments in Military Enlistment Testing
JF  - Military Psychology
JO  - Military Psychology
JA  - Mil Psychol
Y1  - 2014/05//
VL  - 26
IS  - 3
SP  - 153
EP  - 164
PB  - Educational Publishing Foundation
SN  - 0899-5605
SN  - 1532-7876
SN  - 1-4338-1903-1
AD  - Stark, Stephen, University of South Florida, Department of Psychology, PCD 4118G, Tampa, FL, US, 33620
N1  - Accession Number: 2014-38600-003. Partial author list: First Author & Affiliation: Stark, Stephen; University of South Florida, Department of Psychology, Tampa, FL, US. Other Publishers: Lawrence Erlbaum; Taylor & Francis. Release Date: 20140922. Publication Type: Journal (0100), Peer Reviewed Journal (0110). Format Covered: Electronic. Document Type: Journal Article. ISBN: 1-4338-1903-1. Language: EnglishMajor Descriptor: Military Personnel; Personality Measures; Personnel Selection. Minor Descriptor: Adaptive Testing; Computer Assisted Testing; Scoring (Testing); Test Construction. Classification: Tests & Testing (2220); Military Psychology (3800). Population: Human (10). Location: US. Tests & Measures: Assessment of Individual Motivation; Navy Computerized Adaptive Personality Scales; Tailored Adaptive Personality Assessment System. References Available: Y. Page Count: 12. Issue Publication Date: May, 2014. Copyright Statement: American Psychological Association. 2014. 
AB  - This article discusses 3 modern forced-choice personality tests developed for the U.S Armed Services to provide resistance to faking and other forms of response distortion: the Assessment of Individual Motivation, the Navy Computerized Adaptive Personality Scales, and the Tailored Adaptive Personality Assessment System. These tests represent the transition from Likert to forced-choice formats and from static to computerized adaptive item selection to meet the challenges of large-scale, high-stakes testing environments. For each test, we briefly describe the personality constructs that are assessed, the response format and scoring methods, and selected ongoing research and development efforts. We also highlight the potential of these tests for personnel selection, classification, and diagnostic screening purposes. (PsycINFO Database Record (c) 2016 APA, all rights reserved)
KW  - personality
KW  - computerized adaptive testing
KW  - forced choice
KW  - ideal point
KW  - narrow factors
KW  - personality measures
KW  - Assessment of Individual Motivation
KW  - Navy Computerized Adaptive Personality Scales
KW  - Tailored Adaptive Personality Assessment System
KW  - test development
KW  - format
KW  - scoring
KW  - personnel selection
KW  - classification
KW  - Military Personnel
KW  - Personality Measures
KW  - Personnel Selection
KW  - Adaptive Testing
KW  - Computer Assisted Testing
KW  - Scoring (Testing)
KW  - Test Construction
DO  - 10.1037/mil0000044
UR  - https://search.ebscohost.com/login.aspx?direct=true&db=psyh&AN=2014-38600-003&lang=de&site=ehost-live
UR  - sestark@usf.edu
DP  - EBSCOhost
DB  - APA PsycInfo
ER  - 

TY  - JOUR
ID  - 2015-36449-001
AN  - 2015-36449-001
AU  - McGrory, Sarah
AU  - Austin, Elizabeth J.
AU  - Shenkin, Susan D.
AU  - Starr, John M.
AU  - Deary, Ian J.
T1  - From 'aisle' to 'labile': A hierarchical National Adult Reading Test scale revealed by Mokken scaling
JF  - Psychological Assessment
JO  - Psychological Assessment
JA  - Psychol Assess
Y1  - 2015/09//
VL  - 27
IS  - 3
SP  - 932
EP  - 943
PB  - American Psychological Association
SN  - 1040-3590
SN  - 1939-134X
AD  - McGrory, Sarah, Alzheimer Scotland Dementia Research Centre, University of Edinburgh, 7 George Square, Edinburgh, United Kingdom, EH8 9JZ
N1  - Accession Number: 2015-36449-001. PMID: 26302224 Other Journal Title: Psychological Assessment: A Journal of Consulting and Clinical Psychology. Partial author list: First Author & Affiliation: McGrory, Sarah; Alzheimer Scotland Dementia Research Centre, University of Edinburgh, Edinburgh, United Kingdom. Release Date: 20150810. Correction Date: 20200504. Publication Type: Journal (0100), Peer Reviewed Journal (0110). Format Covered: Electronic. Document Type: Journal Article. Language: EnglishGrant Information: McGrory, Sarah. Major Descriptor: Cognitive Ability; Dementia; Intelligence Measures; Premorbidity; Reading. Minor Descriptor: Item Response Theory; Testing. Classification: Clinical Psychological Testing (2224); Neurological Disorders & Brain Damage (3297). Population: Human (10); Male (30); Female (40). Location: United Kingdom. Age Group: Childhood (birth-12 yrs) (100); School Age (6-12 yrs) (180); Adulthood (18 yrs & older) (300); Aged (65 yrs & older) (380). Tests & Measures: Scottish Mental Survey; Community Health Index; Mini-National Adult Reading Test; Moray House Test; Mini Mental State Examination; Wechsler Adult Intelligence Scale; National Adult Reading Test [Appended]. Methodology: Empirical Study; Longitudinal Study; Quantitative Study. References Available: Y. Page Count: 12. Issue Publication Date: Sep, 2015. Publication History: First Posted Date: Aug 10, 2015; Accepted Date: Dec 22, 2014; Revised Date: Nov 17, 2014; First Submitted Date: Aug 8, 2014. Copyright Statement: Author(s) grant(s) the American Psychological Association the exclusive right to publish the article and identify itself as the original publisher. the Author(s). 2015. 
AB  - Decline in cognitive ability is a core diagnostic criterion for dementia. Knowing the extent of decline requires a baseline score from which change can be reckoned. In the absence of prior cognitive ability scores, vocabulary-based cognitive tests are used to estimate premorbid cognitive ability. It is important that such tests are short yet informative, to maximize information and practicability. The National Adult Reading Test (NART) is commonly used to estimate premorbid intelligence. People are asked to pronounce 50 words ranging from easy to difficult but whether its words conform to a hierarchy is unknown. Five hundred eighty-seven healthy community-dwelling older people with known age 11 IQ scores completed the NART as part of the Lothian Birth Cohort 1936 study. Mokken analysis was used to explore item responses for unidimensional, ordinal, and hierarchical scales. A strong hierarchical scale ('mini-NART') of 23 of the 50 items was identified. These items are invariantly ordered across all ability levels. The validity of the interpretation of this briefer scale’s score as an estimate of premorbid ability was examined using the actual age 11 IQ score. The mini-NART accounted for a similar amount of the variance in age 11 IQ as the full NART (NART = 46.5%, mini-NART = 44.8%). The mini-NART is proposed as a useful short clinical tool to estimate prior cognitive ability. The mini-NART has clinical relevance, comprising highly discriminatory, invariantly ordered items allowing for sensitive measurement, and adaptive testing, reducing test administration time, and patient stress. (PsycInfo Database Record (c) 2020 APA, all rights reserved)
KW  - NART
KW  - Mokken scaling
KW  - hierarchical scales
KW  - item response theory
KW  - premorbid cognitive ability
KW  - Aged
KW  - Cohort Studies
KW  - Dementia
KW  - Female
KW  - Humans
KW  - Intelligence
KW  - Intelligence Tests
KW  - Male
KW  - Reading
KW  - Reproducibility of Results
KW  - Cognitive Ability
KW  - Dementia
KW  - Intelligence Measures
KW  - Premorbidity
KW  - Reading
KW  - Item Response Theory
KW  - Testing
U1  - Sponsor: Age United Kingdom. Other Details: LBC1936 data collection was supported by the Disconnected Mind project funded by Age United Kingdom. Recipients: No recipient indicated
U1  - Sponsor: Sponsor name not included. Grant: MR/K026992/1. Other Details: The work was undertaken by The University of Edinburgh Centre for Cognitive Ageing and Cognitive Epidemiology, part of the cross council Lifelong Health and Wellbeing Initiative. Recipients: No recipient indicated
U1  - Sponsor: Biotechnology and Biological Sciences Research Council. Recipients: No recipient indicated
U1  - Sponsor: Medical Research Council. Recipients: No recipient indicated
U1  - Sponsor: Alzheimer Scotland, United Kingdom. Other Details: PhD studentship. Recipients: McGrory, Sarah
DO  - 10.1037/pas0000091
UR  - https://search.ebscohost.com/login.aspx?direct=true&db=psyh&AN=2015-36449-001&lang=de&site=ehost-live
UR  - ORCID: 0000-0002-1733-263X
UR  - ORCID: 0000-0001-7375-4776
UR  - s.mcgrory@sms.ed.ac.uk
DP  - EBSCOhost
DB  - APA PsycInfo
ER  - 

TY  - JOUR
ID  - 2014-55811-001
AN  - 2014-55811-001
AU  - Allen, Diane D.
AU  - Talavera, Carolina
AU  - Baxter, Stephen
AU  - Topp, Kimberly
T1  - Gaps between patients’ reported current and preferred abilities versus clinicians’ emphases during an episode of care: Any agreement?
JF  - Quality of Life Research: An International Journal of Quality of Life Aspects of Treatment, Care & Rehabilitation
JO  - Quality of Life Research: An International Journal of Quality of Life Aspects of Treatment, Care & Rehabilitation
JA  - Qual Life Res
Y1  - 2015/05//
VL  - 24
IS  - 5
SP  - 1137
EP  - 1143
PB  - Springer
SN  - 0962-9343
SN  - 1573-2649
AD  - Allen, Diane D., Graduate Program in Physical Therapy, University of California San Francisco, 1600 Holloway Avenue, San Francisco, CA, US, 94132
N1  - Accession Number: 2014-55811-001. PMID: 25502091 Partial author list: First Author & Affiliation: Allen, Diane D.; Graduate Program in Physical Therapy, University of California San Francisco, San Francisco, CA, US. Release Date: 20141222. Correction Date: 20150518. Publication Type: Journal (0100), Peer Reviewed Journal (0110). Format Covered: Electronic. Document Type: Journal Article. Language: EnglishMajor Descriptor: Ability Level; Client Attitudes; Motor Processes; Physical Therapy; Therapeutic Processes. Minor Descriptor: Adaptive Testing; Clinicians. Classification: Rehabilitation (3380). Population: Human (10). Age Group: Adulthood (18 yrs & older) (300). Tests & Measures: Movement Ability Measure-Computer Adaptive Test Version. Methodology: Empirical Study; Quantitative Study. Page Count: 7. Issue Publication Date: May, 2015. Publication History: First Posted Date: Dec 13, 2014; Accepted Date: Dec 5, 2014. Copyright Statement: Springer International Publishing Switzerland. 2014. 
AB  - Purpose: To be patient-centered, assessment must extract what patients prefer to be able to do along with what they can do now so health care can specifically address the gap between current and preferred abilities. In this project, we compared patient-perceived current–preferred gaps with the assessments and interventions reported by clinicians in a rehabilitation clinic. Methods: Sixty-two patients in outpatient physical therapy completed a computer-adaptive test version of the patient-reported Movement Ability Measure (MAM-CAT) at initial visit and discharge. The MAM-CAT calculated the gaps between the movement patients perceived that they could do 'Now' and what movement ability they 'Would Like' to have across six dimensions of movement: flexibility, strength, accuracy, speed, adaptability, and endurance. Physical therapists’ notes regarding assessments and interventions were categorized based on these same six dimensions of movement. Frequency of agreement between the largest patient-perceived gaps and clinician-documented emphases was recorded (kappa analyses), along with MAM-CAT changes at discharge (paired t tests).Results: Although patient progress was noted in both the MAM-CAT and the clinician notes (p < .05), comparison showed poor or slight agreement (kappa < .05) between the specific movement dimensions patients regarded as having the largest gaps and the dimensions on which clinicians focused.Conclusion: The MAM-CAT facilitated direct comparison of patients’ current–preferred gaps at initiation and discharge with clinicians’ emphases during episodes of care. While interventions were perceived as effective, collaboration between patients and clinicians using gap data could increase alignment between patient priorities and clinician emphases, potentially resulting in improved patient engagement and rehabilitative outcomes. (PsycINFO Database Record (c) 2016 APA, all rights reserved)
KW  - Patient-reported outcomes
KW  - Computer-adaptive test
KW  - Movement Ability Measure
KW  - Physical therapy
KW  - Patient engagement
KW  - Adult
KW  - Aged
KW  - Episode of Care
KW  - Female
KW  - Humans
KW  - Male
KW  - Movement
KW  - Patient Discharge
KW  - Patient-Centered Care
KW  - Physical Therapy Modalities
KW  - Quality of Life
KW  - Self Concept
KW  - Self Report
KW  - Treatment Outcome
KW  - Ability Level
KW  - Client Attitudes
KW  - Motor Processes
KW  - Physical Therapy
KW  - Therapeutic Processes
KW  - Adaptive Testing
KW  - Clinicians
U1  - Sponsor: Patient-Centered Outcomes Research Institute (PCORI). Grant: R120190. Recipients: No recipient indicated
DO  - 10.1007/s11136-014-0888-0
UR  - https://search.ebscohost.com/login.aspx?direct=true&db=psyh&AN=2014-55811-001&lang=de&site=ehost-live
UR  - ddallen@sfsu.edu
DP  - EBSCOhost
DB  - APA PsycInfo
ER  - 

TY  - JOUR
ID  - 2015-07948-017
AN  - 2015-07948-017
AU  - Keutmann, Michael K.
AU  - Moore, Samantha L.
AU  - Savitt, Adam
AU  - Gur, Ruben C.
T1  - Generating an item pool for translational social cognition research: Methodology and initial validation
JF  - Behavior Research Methods
JO  - Behavior Research Methods
JA  - Behav Res Methods
Y1  - 2015/03//
VL  - 47
IS  - 1
SP  - 228
EP  - 234
PB  - Springer
SN  - 1554-351X
SN  - 1554-3528
AD  - Keutmann, Michael K., Department of Psychology, University of Illinois at Chicago, 1007 W. Harrison St (M/C 285), Chicago, IL, US, 60607
N1  - Accession Number: 2015-07948-017. PMID: 24719265 Other Journal Title: Behavior Research Methods & Instrumentation; Behavior Research Methods, Instruments & Computers. Partial author list: First Author & Affiliation: Keutmann, Michael K.; University of Illinois, Chicago, IL, US. Other Publishers: Psychonomic Society. Release Date: 20150309. Publication Type: Journal (0100), Peer Reviewed Journal (0110). Format Covered: Electronic. Document Type: Journal Article. Language: EnglishMajor Descriptor: Experimental Methods; Social Cognition; Test Construction; Test Items. Classification: Research Methods & Experimental Design (2260). Population: Human (10); Male (30); Female (40). Age Group: Childhood (birth-12 yrs) (100); Preschool Age (2-5 yrs) (160); School Age (6-12 yrs) (180); Adolescence (13-17 yrs) (200); Adulthood (18 yrs & older) (300); Young Adulthood (18-29 yrs) (320); Thirties (30-39 yrs) (340); Middle Age (40-64 yrs) (360); Aged (65 yrs & older) (380). Methodology: Empirical Study; Quantitative Study. References Available: Y. Page Count: 7. Issue Publication Date: Mar, 2015. Publication History: First Posted Date: Apr 10, 2014. Copyright Statement: Psychonomic Society, Inc. 2014. 
AB  - Existing sets of social and emotional stimuli suitable for social cognition research are limited in many ways, including size, unimodal stimulus delivery, and restriction to major universal emotions. Existing measures of social cognition could be improved by taking advantage of item response theory and adaptive testing technology to develop instruments that obtain more efficient measures of multimodal social cognition. However, for this to be possible, large pools of emotional stimuli must be obtained and validated. We present the development of a large, high-quality multimedia stimulus set produced by professional adult and child actors (ages 5 to 74) containing both visual and vocal emotional expressions. We obtained over 74,000 audiovisual recordings of a wide array of emotional and social behaviors, including the main universal emotions (happiness, sadness, anger, fear, disgust, and surprise), as well as more complex social expressions (pride, affection, sarcasm, jealousy, and shame). The actors generated a high quantity of technically superior, ecologically valid stimuli that were digitized, archived, and rated for accuracy and intensity of expressions. A subset of these facial and vocal expressions of emotion and social behavior were submitted for quantitative ratings to generate parameters for validity and discriminability. These stimuli are suitable for affective neuroscience-based psychometric tests, functional neuroimaging, and social cognitive rehabilitation programs. The purposes of this report are to describe the method of obtaining and validating this database and to make it accessible to the scientific community. We invite all those interested in participating in the use and validation of these stimuli to access them at www.med.upenn.edu/bbl/actors/index.shtml . (PsycINFO Database Record (c) 2016 APA, all rights reserved)
KW  - Social cognition
KW  - Emotion
KW  - Affect
KW  - Stimuli
KW  - Faces
KW  - Audiovisual
KW  - Recordings
KW  - Adult
KW  - Aged
KW  - Audiovisual Aids
KW  - Behavioral Research
KW  - Child
KW  - Cognition
KW  - Databases, Factual
KW  - Expressed Emotion
KW  - Humans
KW  - Neuropsychological Tests
KW  - Psychometrics
KW  - Reproducibility of Results
KW  - Social Behavior
KW  - Experimental Methods
KW  - Social Cognition
KW  - Test Construction
KW  - Test Items
U1  - Sponsor: NIH. Grant: NIH R01-MH060722 and NIH R01-MH084856. Recipients: No recipient indicated
DO  - 10.3758/s13428-014-0464-0
UR  - https://search.ebscohost.com/login.aspx?direct=true&db=psyh&AN=2015-07948-017&lang=de&site=ehost-live
UR  - mkeutm2@uic.edu
DP  - EBSCOhost
DB  - APA PsycInfo
ER  - 

TY  - JOUR
ID  - 2013-30598-001
AN  - 2013-30598-001
AU  - Ortner, Tuulia M.
AU  - Weißkopf, Eva
AU  - Koch, Tobias
T1  - I will probably fail: Higher ability students’ motivational experiences during adaptive achievement testing
JF  - European Journal of Psychological Assessment
JO  - European Journal of Psychological Assessment
JA  - Eur J Psychol Assess
Y1  - 2014///
VL  - 30
IS  - 1
SP  - 48
EP  - 56
PB  - Hogrefe Publishing
SN  - 1015-5759
SN  - 2151-2426
AD  - Ortner, Tuulia M., Department of Psychology, Division for Psychological Assessment University of Salzburg, Heilbrunnerstrasse 34, 5020, Salzburg, Austria
N1  - Accession Number: 2013-30598-001. Other Journal Title: Evaluación Psicológica. Partial author list: First Author & Affiliation: Ortner, Tuulia M.; University of Salzburg, Salzburg, Austria. Other Publishers: Hogrefe & Huber Publishers. Release Date: 20130826. Correction Date: 20190211. Publication Type: Journal (0100), Peer Reviewed Journal (0110). Format Covered: Electronic. Document Type: Journal Article. Language: EnglishMajor Descriptor: Achievement Measures; Adaptive Testing; Motivation; Test Taking. Minor Descriptor: Failure; Fear; Probability; Reasoning; Computerized Assessment. Classification: Educational Measurement (2227); Academic Learning & Achievement (3550). Population: Human (10); Male (30); Female (40). Location: Germany. Age Group: Adolescence (13-17 yrs) (200); Adulthood (18 yrs & older) (300); Young Adulthood (18-29 yrs) (320). Tests & Measures: Questionnaire on Current Motivation--German Version; AMT Adaptive Matrices Test DOI: 10.1037/t10889-000; Flow Short Scale DOI: 10.1037/t47787-000. Methodology: Empirical Study; Quantitative Study. References Available: Y. Page Count: 9. Issue Publication Date: 2014. Publication History: First Posted Date: May 21, 2013; Accepted Date: Feb 19, 2013. Copyright Statement: Hogrefe Publishing. 2013. 
AB  - We investigated the effects of computerized adaptive testing (CAT) versus computerized fixed item testing (FIT) of reasoning ability on current motivation in terms of situational fear of failure and subjective probability of success, as well as flow. A group of 174 students (aged 15–21) from two German secondary schools was presented either a CAT or a FIT version of a matrices test; motivational variables were assessed during a short break in testing. More situational fear of failure and less subjective probability of success were reported using CAT compared to FIT. Self-reported flow did not differ between test mode conditions. When we addressed the hypothesis that adaptive testing is equally motivating for both high and lower performers, test performance appeared to moderate the relationship of test mode and subjective probability of success: Only during FIT was subjective probability of success higher with lower test performance. This moderation effect was also revealed for the relationship of test mode and flow. However, as average reported motivation was lower during CAT, results contradict assumptions of enhanced motivation during CAT. Results are discussed in relation to self-concept relevance of testing domains and with reference to test fairness. (PsycINFO Database Record (c) 2019 APA, all rights reserved)
KW  - CAT
KW  - computerized adaptive testing
KW  - fairness
KW  - motivation
KW  - reasoning
KW  - computerized fixed item testing
KW  - test taking motivation
KW  - fear of failure
KW  - probability of success
KW  - achievement testing
KW  - Achievement Measures
KW  - Adaptive Testing
KW  - Motivation
KW  - Test Taking
KW  - Failure
KW  - Fear
KW  - Probability
KW  - Reasoning
KW  - Computerized Assessment
DO  - 10.1027/1015-5759/a000168
UR  - https://search.ebscohost.com/login.aspx?direct=true&db=psyh&AN=2013-30598-001&lang=de&site=ehost-live
UR  - ORCID: 0000-0002-5703-6915
UR  - tuulia.ortner@sbg.ac.at
DP  - EBSCOhost
DB  - APA PsycInfo
ER  - 

TY  - JOUR
ID  - 2013-10016-007
AN  - 2013-10016-007
AU  - Gierl, Mark J.
AU  - Lai, Hollis
AU  - Li, Johnson
T1  - Identifying differential item functioning in multi-stage computer adaptive testing
JF  - Educational Research and Evaluation
JO  - Educational Research and Evaluation
JA  - Educ Res Eval
Y1  - 2013/04//
VL  - 19
IS  - 2-3
SP  - 188
EP  - 203
PB  - Taylor & Francis
SN  - 1380-3611
SN  - 1744-4187
AD  - Gierl, Mark J.
N1  - Accession Number: 2013-10016-007. Partial author list: First Author & Affiliation: Gierl, Mark J.; Centre for Research in Applied Measurement and Evaluation, University of Alberta, Edmonton, AB, Canada. Release Date: 20130812. Correction Date: 20190211. Publication Type: Journal (0100), Peer Reviewed Journal (0110). Format Covered: Electronic. Document Type: Journal Article. Language: EnglishMajor Descriptor: Adaptive Testing; Item Analysis (Test); Psychometrics; Differential Item Functioning. Minor Descriptor: Performance; Test Bias. Classification: Tests & Testing (2220). Population: Human (10). Tests & Measures: Computer Adaptive Testing Simultaneous Item Bias Test. Methodology: Quantitative Study; Scientific Simulation. References Available: Y. Page Count: 16. Issue Publication Date: Apr, 2013. Copyright Statement: Taylor & Francis. 2013. 
AB  - The purpose of this study is to evaluate the performance of CATSIB (Computer Adaptive Testing-Simultaneous Item Bias Test) for detecting differential item functioning (DIF) when items in the matching and studied subtest are administered adaptively in the context of a realistic multi-stage adaptive test (MST). MST was simulated using a 4-item module in a 7-panel administration. Three independent variables, expected to affect DIF detection rates, were manipulated: item difficulty, sample size, and balanced/unbalanced design. CATSIB met the acceptable criteria, meaning that the Type I error and power rates met 5% and 80%, respectively, for the large reference/moderate focal sample and the large reference/large focal sample conditions. These results indicate that CATSIB can be used to consistently and accurately detect DIF on an MST, but only with moderate to large samples. (PsycINFO Database Record (c) 2019 APA, all rights reserved)
KW  - multi stage adaptive testing
KW  - test design
KW  - adaptive tests
KW  - psychometrics
KW  - Computer Adaptive Testing Simultaneous Item Bias Test
KW  - differential item functioning
KW  - Adaptive Testing
KW  - Item Analysis (Test)
KW  - Psychometrics
KW  - Differential Item Functioning
KW  - Performance
KW  - Test Bias
DO  - 10.1080/13803611.2013.767622
UR  - https://search.ebscohost.com/login.aspx?direct=true&db=psyh&AN=2013-10016-007&lang=de&site=ehost-live
UR  - ORCID: 0000-0002-5531-6622
UR  - mark.gierl@ualberta.ca
DP  - EBSCOhost
DB  - APA PsycInfo
ER  - 

TY  - JOUR
ID  - 2021-89634-001
AN  - 2021-89634-001
AU  - Fenwick, Eva K.
AU  - Lee, Ester P. X.
AU  - Man, Ryan E. K.
AU  - Ho, Kam Chun
AU  - Najjar, Raymond P.
AU  - Milea, Dan
AU  - Teo, Kelvin Y. C.
AU  - Tan, Anna C. S.
AU  - Lee, Shu Yen
AU  - Yeo, Ian Yew San
AU  - Tan, Gavin S. W.
AU  - Mathur, Ranjana
AU  - Wong, Tien Yin
AU  - Cheung, Chui Ming Gemmy
AU  - Lamoureux, Ecosse L.
T1  - Identifying the content for an item bank and computerized adaptive testing system to measure the impact of age-related macular degeneration on health-related quality of life
JF  - Quality of Life Research: An International Journal of Quality of Life Aspects of Treatment, Care & Rehabilitation
JO  - Quality of Life Research: An International Journal of Quality of Life Aspects of Treatment, Care & Rehabilitation
JA  - Qual Life Res
Y1  - 2021/09/25/
PB  - Springer
SN  - 0962-9343
SN  - 1573-2649
AD  - Lamoureux, Ecosse L.
N1  - Accession Number: 2021-89634-001. PMID: 34562188 Partial author list: First Author & Affiliation: Fenwick, Eva K.; Singapore Eye Research Institute, Singapore National Eye Centre, Singapore, Singapore. Release Date: 20210930. Publication Type: Journal (0100), Peer Reviewed Journal (0110). Format Covered: Electronic. Document Type: Journal. Language: EnglishMajor Descriptor: No terms assigned. Classification: Psychological & Physical Disorders (3200). References Available: Y. Publication History: Accepted Date: Aug 29, 2021. Copyright Statement: The Author(s), under exclusive licence to Springer Nature Switzerland AG. 2021. 
AB  - PurposeWe are developing an age-related macular degeneration (AMD) health-related quality of life (HRQoL) item bank, applicable to Western and Asian populations. We report primarily on content generation and refinement, but also compare the HRQoL issues reported in our study with Western studies and current AMD-HRQoL questionnaires.MethodsIn this cross-sectional, qualitative study of AMD patients attending the Singapore National Eye Centre (May–December 2019), items/domains were generated from: (1) AMD-specific questionnaires; (2) published articles; (3) focus groups/semi-structured interviews with AMD patients (n = 27); and (4) written feedback from retinal experts. Following thematic analysis, items were systematically refined to a minimally representative set and pre-tested using cognitive interviews with 16 AMD patients.ResultsOf the 27 patients (mean ± standard deviation age 67.9 ± 7.0; 59.2% male), 18 (66.7%), two (7.4%), and seven (25.9%) had no, early-intermediate, and late/advanced AMD (better eye), respectively. Whilst some HRQoL issues, e.g. activity limitation, mobility, lighting, and concerns were similarly reported by Western patients and covered by other questionnaires, others like anxiety about intravitreal injections, work tasks, and financial dependency were novel. Overall, 462 items within seven independent HRQoL domains were identified: Activity limitation, Lighting, Mobility, Emotional, Concerns, AMD management, and Work. Following item refinement, items were reduced to 219, with 31 items undergoing amendment.ConclusionOur 7-domain, 219-item AMD-specific HRQoL instrument will undergo psychometric testing and calibration for computerized adaptive testing. The future instrument will enable users to precisely, rapidly, and comprehensively quantify the HRQoL impact of AMD and associated treatments, with item coverage relevant across several populations. (PsycInfo Database Record (c) 2021 APA, all rights reserved)
KW  - Age-related macular degeneration
KW  - Quality of life
KW  - Item bank
KW  - Computerized adaptive testing
KW  - Qualitative
KW  - No terms assigned
DO  - 10.1007/s11136-021-02989-w
UR  - https://search.ebscohost.com/login.aspx?direct=true&db=psyh&AN=2021-89634-001&lang=de&site=ehost-live
UR  - ORCID: 0000-0001-8674-5705
UR  - ecosse.lamoureux@duke-nus.edu.sg
DP  - EBSCOhost
DB  - APA PsycInfo
ER  - 

TY  - THES
ID  - 2011-99031-068
AN  - 2011-99031-068
AU  - McCoy, Keith Marcel
T1  - Impact of item parameter drift on examinee ability measures in a computer adaptive environment
JF  - Dissertation Abstracts International Section A: Humanities and Social Sciences
JO  - Dissertation Abstracts International Section A: Humanities and Social Sciences
Y1  - 2011///
VL  - 71
IS  - 8-A
SP  - 2861
EP  - 2861
PB  - ProQuest Information & Learning
SN  - 0419-4209
SN  - 978-1-124-14414-6
N1  - Accession Number: 2011-99031-068. Other Journal Title: Dissertation Abstracts International. Partial author list: First Author & Affiliation: McCoy, Keith Marcel; U Illinois at Chicago, US. Release Date: 20110425. Correction Date: 20221128. Publication Type: Dissertation Abstract (0400). Format Covered: Electronic. Document Type: Dissertation. Dissertation Number: AAI3417367. ISBN: 978-1-124-14414-6. Language: EnglishMajor Descriptor: Adaptive Testing; Educational Programs; Item Response Theory; Computerized Assessment. Minor Descriptor: Computers. Classification: Educational & School Psychology (3500). Population: Human (10). Methodology: Empirical Study; Quantitative Study. Page Count: 1. 
AB  - This dissertation investigated the impact of item parameter drift (IPD) on examinee ability measures, in the analysis of data generated from a computerized adaptive test (CAT) collected in year 2003 by the American Society for Clinical Pathology (ASCP), in a Medical Technologist (MT) examination. For the 2003 ASCP MT exam, the item bank included 1,270 items. Items were selected from seven subscales: (a) blood banking, (b) chemistry, (c) hematology, (d) immunology, (e) laboratory operations, (f) microbiology, and (g) urinalysis. ASCP administered this CAT to 2,555 examinees. The data were analyzed using a new hierarchical linear model (HLM), called the Rasch longitudinal model (RLM), which models interaction between the item and time, in addition to parameters of examinee ability and item difficulty (easiness). The analysis demonstrated how IPD within the item bank for the ASCP MT exam affected examinee ability measures, and the extent to which IPD altered certification or pass-fail decisions. The RLM identified items that exhibited significant IPD for each subscale and provided the magnitude (and direction) of IPD within the 2003 ASCP MT exam data. No one subscale was more sensitive to IPD than any other. Further, by including significant time effects (IPD), the RLM provided a better fit to the data than the Rasch model as an HLM. The RLM not only determined items exhibiting IPD but also compensated for the existence of IPD within the calculation of examinee abilities by using the drifted item difficulty value versus the anchored one. Consequently, the RLM provided more accurate measures of examinee ability and item difficulty than the Rasch model as analyzed using the WINSTEPS program As a result, using the RLM yielded a more efficient IPD detection method, one that identified items with IPD but also corrected the estimation of examinee abilities for the existence of IPD. (PsycInfo Database Record (c) 2022 APA, all rights reserved)
KW  - item parameter drift
KW  - examinee ability measures
KW  - computer adaptive environment
KW  - Adaptive Testing
KW  - Educational Programs
KW  - Item Response Theory
KW  - Computerized Assessment
KW  - Computers
UR  - https://search.ebscohost.com/login.aspx?direct=true&db=psyh&AN=2011-99031-068&lang=de&site=ehost-live
DP  - EBSCOhost
DB  - APA PsycInfo
ER  - 

TY  - JOUR
ID  - 2018-07616-018
AN  - 2018-07616-018
AU  - Marfeo, Elizabeth E.
AU  - Ni, Pengsheng
AU  - McDonough, Christine
AU  - Peterik, Kara
AU  - Marino, Molly
AU  - Meterko, Mark
AU  - Rasch, Elizabeth K.
AU  - Chan, Leighton
AU  - Brandt, Diane
AU  - Jette, Alan M.
T1  - Improving assessment of work related mental health function using the Work Disability Functional Assessment Battery (WD-FAB)
JF  - Journal of Occupational Rehabilitation
JO  - Journal of Occupational Rehabilitation
JA  - J Occup Rehabil
Y1  - 2018/03//
VL  - 28
IS  - 1
SP  - 190
EP  - 199
PB  - Springer
SN  - 1053-0487
SN  - 1573-3688
AD  - Marfeo, Elizabeth E., Department of Occupational Therapy, Tufts University, 574 Boston Ave. Suite 216G, Medford, MA, US, 02155
N1  - Accession Number: 2018-07616-018. PMID: 28477069 Partial author list: First Author & Affiliation: Marfeo, Elizabeth E.; Department of Occupational Therapy, Tufts University, Medford, MA, US. Release Date: 20181206. Correction Date: 20210719. Publication Type: Journal (0100), Peer Reviewed Journal (0110). Format Covered: Electronic. Document Type: Journal Article. Language: EnglishMajor Descriptor: Disability Evaluation; Mental Health; Psychological Assessment; Occupational Health. Minor Descriptor: Cognition; Communication; Psychometrics. Classification: Occupational & Employment Testing (2228); Occupational & Vocational Rehabilitation (3384). Population: Human (10); Male (30); Female (40). Location: US. Age Group: Adulthood (18 yrs & older) (300); Young Adulthood (18-29 yrs) (320); Thirties (30-39 yrs) (340); Middle Age (40-64 yrs) (360); Aged (65 yrs & older) (380). Tests & Measures: Work Disability Functional Assessment Battery. Methodology: Empirical Study; Quantitative Study. Supplemental Data: Tables and Figures Internet. References Available: Y. Page Count: 10. Issue Publication Date: Mar, 2018. Publication History: First Posted Date: May 5, 2017. Copyright Statement: Springer Science+Business Media New York. 2017. 
AB  - Purpose: To improve the mental health component of the Work Disability Functional Assessment Battery (WD-FAB), developed for the US Social Security Administration’s (SSA) disability determination process. Specifically our goal was to expand the WD-FAB scales of mood & emotions, resilience, social interactions, and behavioral control to improve the depth and breadth of the current scales and expand the content coverage to include aspects of cognition & communication function. Methods: Data were collected from a random, stratified sample of 1695 claimants applying for the SSA work disability benefits, and a general population sample of 2025 working age adults. 169 new items were developed to replenish the WD-FAB scales and analyzed using factor analysis and item response theory (IRT) analysis to construct unidimensional scales. We conducted computer adaptive test (CAT) simulations to examine the psychometric properties of the WD-FAB. Results: Analyses supported the inclusion of four mental health subdomains: Cognition & Communication (68 items), Self-Regulation (34 items), Resilience & Sociability (29 items) and Mood & Emotions (34 items). All scales yielded acceptable psychometric properties. Conclusions: IRT methods were effective in expanding the WD-FAB to assess mental health function. The WD-FAB has the potential to enhance work disability assessment both within the context of the SSA disability programs as well as other clinical and vocational rehabilitation settings. (PsycInfo Database Record (c) 2021 APA, all rights reserved)
KW  - Work Disability Functional Assessment Battery
KW  - Mental health
KW  - Disability assessment
KW  - Employment
KW  - Measurement
KW  - Cognition
KW  - Communication
KW  - Psychometric properties
KW  - Adult
KW  - Disabled Persons
KW  - Female
KW  - Humans
KW  - Male
KW  - Middle Aged
KW  - Quality Improvement
KW  - Reproducibility of Results
KW  - Social Security
KW  - United States
KW  - Work Capacity Evaluation
KW  - Disability Evaluation
KW  - Mental Health
KW  - Psychological Assessment
KW  - Occupational Health
KW  - Cognition
KW  - Communication
KW  - Psychometrics
U1  - Sponsor: National Institutes of Health, Social Security Administration, US. Grant: HHSN269200900004C; HHSN269201000011C; HHSN269201100009I; HHSN269201200005C. Other Details: Interagency Agreements. Recipients: No recipient indicated
U1  - Sponsor: National Institutes of Health, Intramural Research Program, US. Recipients: No recipient indicated
DO  - 10.1007/s10926-017-9710-5
UR  - https://search.ebscohost.com/login.aspx?direct=true&db=psyh&AN=2018-07616-018&lang=de&site=ehost-live
UR  - ORCID: 0000-0003-3848-6918
UR  - Elizabeth.marfeo@tufts.edu
DP  - EBSCOhost
DB  - APA PsycInfo
ER  - 

TY  - JOUR
ID  - 2020-87868-002
AN  - 2020-87868-002
AU  - Colman, Julia
AU  - Casto, Shelley Coleman
AU  - Wisner, Eliscia
AU  - Stanek, Joseph R.
AU  - Auletta, Jeffery J.
T1  - Improving occupational performance in pediatric hematopoietic cell transplant recipients
JF  - American Journal of Occupational Therapy
JO  - American Journal of Occupational Therapy
JA  - Am J Occup Ther
Y1  - 2020/09//Sep-Oct, 2020
VL  - 74
IS  - 5
SP  - 1
EP  - 11
PB  - American Occupational Therapy Assn
SN  - 0272-9490
SN  - 1943-7676
AD  - Colman, Julia
N1  - Accession Number: 2020-87868-002. Partial author list: First Author & Affiliation: Colman, Julia; Nationwide Children’s Hospital, Columbus, OH, US. Release Date: 20210114. Publication Type: Journal (0100), Peer Reviewed Journal (0110). Format Covered: Electronic. Document Type: Journal Article. Language: EnglishMajor Descriptor: Cells (Biology); Grasping; Intervention; Occupational Therapy; Pediatrics. Minor Descriptor: Blood and Lymphatic Disorders. Classification: Occupational & Vocational Rehabilitation (3384). Population: Human (10); Male (30); Female (40); Inpatient (50). Location: US. Age Group: Childhood (birth-12 yrs) (100); School Age (6-12 yrs) (180); Adolescence (13-17 yrs) (200); Adulthood (18 yrs & older) (300); Young Adulthood (18-29 yrs) (320). Tests & Measures: 9-Hole Peg Test; Pediatric Evaluation of Disability Inventory-Computer Adaptive Test; Activities of Daily Living Functional Measure. Methodology: Empirical Study; Longitudinal Study; Retrospective Study; Quantitative Study. Page Count: 11. Issue Publication Date: Sep-Oct, 2020. 
AB  - Importance: There is a critical gap in the literature regarding the efficacy of occupational therapy interventions for pediatric hematopoietic cell transplantation (HCT) patients. Objective: To demonstrate that occupational therapy 4-5×/wk during inpatient hospitalization positively affects strength, coordination, and independence in activities of daily living (ADLs) of pediatric patients during HCT. Design: Retrospective study. Setting: Inpatient bone marrow transplant unit at a children's hospital. Participants: Thirty-two pediatric patients admitted for HCT. Outcomes and measures: Patients were seen by an occupational therapist as part of an interdisciplinary program. Interventions included play and leisure engagement, upper extremity therapeutic exercises, fine motor activities, and ADL training. Strength, coordination, and daily living skills data were documented prospectively and analyzed retrospectively to compare differences between patients seen by occupational therapy at high versus low frequency. Results: For grip strength (dynamometer), fine motor dexterity (the 9-Hole Peg Test), and independence in ADLs (an ADL functional measure and the Pediatric Evaluation of Disability Inventory-Computer Adaptive Test Daily Activities), the high-frequency group had a significantly smaller decrease in performance from time of admission at pretransplant (baseline) to peak decline after transplant. Grip strength and ADL scores for the high-frequency group returned to baseline at time of discharge more readily than for the low-frequency group. Conclusions and relevance: Participation in occupational therapy 4-5×/wk had positive effects on strength, coordination, and independence in ADLs for patients undergoing HCT. What this article adds: This article provides evidence that occupational therapists are an important part of the interdisciplinary team treating pediatric bone marrow transplant patients. It also demonstrates that occupational therapy interventions delivered at a high frequency can have a positive impact on upper extremity strength and independence in ADLs. (PsycInfo Database Record (c) 2021 APA, all rights reserved)
KW  - hematopoietic cell transplant recipients
KW  - activities of daily living
KW  - occupational performance
KW  - pediatrics
KW  - Cells (Biology)
KW  - Grasping
KW  - Intervention
KW  - Occupational Therapy
KW  - Pediatrics
KW  - Blood and Lymphatic Disorders
DO  - 10.5014/ajot.2020.040543
UR  - https://search.ebscohost.com/login.aspx?direct=true&db=psyh&AN=2020-87868-002&lang=de&site=ehost-live
UR  - colmanjulia6@gmail.com
DP  - EBSCOhost
DB  - APA PsycInfo
ER  - 

TY  - JOUR
ID  - 2020-13436-010
AN  - 2020-13436-010
AU  - Gibbons, Robert D.
AU  - Smith, Justin D.
AU  - Brown, C. Hendricks
AU  - Sajdak, Mary
AU  - Tapia, Nneka Jones
AU  - Kulik, Andrew
AU  - Epperson, Matthew W.
AU  - Csernansky, John
T1  - Improving the evaluation of adult mental disorders in the criminal justice system with computerized adaptive testing
JF  - Psychiatric Services
JO  - Psychiatric Services
JA  - Psychiatr Serv
Y1  - 2019/11/01/
VL  - 70
IS  - 11
SP  - 1040
EP  - 1043
PB  - American Psychiatric Assn
SN  - 1075-2730
SN  - 1557-9700
AD  - Gibbons, Robert D.
N1  - Accession Number: 2020-13436-010. PMID: 31337321 Other Journal Title: Hospital & Community Psychiatry. Partial author list: First Author & Affiliation: Gibbons, Robert D.; Center for Health Statistics, University of Chicago, Chicago, IL, US. Release Date: 20200528. Correction Date: 20210715. Publication Type: Journal (0100), Peer Reviewed Journal (0110). Format Covered: Electronic. Document Type: Journal Article. Language: EnglishGrant Information: Gibbons, Robert D. Major Descriptor: Adaptive Testing; Criminal Justice; Defendants; Mental Disorders; Psychometrics. Minor Descriptor: Test Validity. Classification: Clinical Psychological Testing (2224); Psychological Disorders (3210). Population: Human (10); Male (30); Female (40). Location: US. Age Group: Adulthood (18 yrs & older) (300). Tests & Measures: Computerized Adaptive Test–Mental Health. Methodology: Empirical Study; Quantitative Study. References Available: Y. Page Count: 4. Issue Publication Date: Nov 1, 2019. Publication History: First Posted Date: Jul 24, 2019; Accepted Date: May 16, 2019; Revised Date: May 2, 2019; Mar 18, 2019; First Submitted Date: Jan 18, 2019. 
AB  - Objective: The authors sought to develop and validate a suite of dimensional measures of psychiatric syndromes for use in a criminal justice population. Methods: The previously validated Computerized Adaptive Test–Mental Health (CAT-MH) was administered to a sample of 475 defendants in the Cook County Bond Court. Item-level data were used to determine which test items exhibited differential item functioning in this population compared with the population used for the original calibration. Results: After removal of nine items that exhibited differential item functioning from the CAT-MH, correlations between scores based on the original calibration from a nonjustice-involved population and the newly computed scores based on a sample of bond court defendants showed a correlation coefficient of r = 0.96 to r = 0.99. Conclusions: With a slight modification of the original CAT-MH, the tool was successfully used to measure severity of depression, anxiety, mania and/or hypomania, suicidality, and substance use disorder in an English- and Spanish-speaking criminal justice population. (PsycInfo Database Record (c) 2021 APA, all rights reserved)
KW  - development
KW  - validation
KW  - Computerized Adaptive Test-Mental Health
KW  - psychiatric syndromes
KW  - criminal justice
KW  - defendants
KW  - Adult
KW  - Anxiety Disorders
KW  - Bipolar Disorder
KW  - Depressive Disorder
KW  - Diagnosis, Computer-Assisted
KW  - Female
KW  - Humans
KW  - Illinois
KW  - Male
KW  - Prisoners
KW  - Psychiatric Status Rating Scales
KW  - Severity of Illness Index
KW  - Substance-Related Disorders
KW  - Adaptive Testing
KW  - Criminal Justice
KW  - Defendants
KW  - Mental Disorders
KW  - Psychometrics
KW  - Test Validity
U1  - Sponsor: National Institutes of Health, US. Grant: R01-MH-100155. Other Details: “A New Statistical Paradigm for Measuring Psycho-pathology Dimensions in Youth”. Recipients: Gibbons, Robert D.
U1  - Sponsor: National Institute on Drug Abuse, US. Grant: P30DA027828. Other Details: Center for Prevention Implementation Methodology for Drug Abuse and HIV. Recipients: Brown, C. Hendricks
DO  - 10.1176/appi.ps.201900038
UR  - https://search.ebscohost.com/login.aspx?direct=true&db=psyh&AN=2020-13436-010&lang=de&site=ehost-live
UR  - rdg@uchicago.edu
DP  - EBSCOhost
DB  - APA PsycInfo
ER  - 

TY  - JOUR
ID  - 2009-20318-001
AN  - 2009-20318-001
AU  - Helbig-Lang, Sylvia
AU  - Petermann, Franz
T1  - Innovative ansätze in der klinischen angstdiagnostik = Innovative approaches in clinical anxiety assessment
JF  - Verhaltenstherapie
JO  - Verhaltenstherapie
Y1  - 2009/09//
VL  - 19
IS  - 3
SP  - 145
EP  - 151
PB  - Karger
SN  - 1016-6262
SN  - 1423-0402
AD  - Helbig-Lang, Sylvia, Zentrum fur Klinische Psychologie und Rehabilitation, Universitat Bremen, Grazer Str. 6, 28359, Bremen, Germany
N1  - Accession Number: 2009-20318-001. Translated Title: Innovative approaches in clinical anxiety assessment. Partial author list: First Author & Affiliation: Helbig-Lang, Sylvia; Zentrum fur Klinische Psychologie und Rehabilitation, Universitat Bremen, Bremen, Germany. Release Date: 20100208. Publication Type: Journal (0100), Peer Reviewed Journal (0110). Format Covered: Electronic. Document Type: Journal Article. Language: GermanMajor Descriptor: Anxiety; Anxiety Disorders; Innovation; Measurement; Nonverbal Communication. Classification: Affective Disorders (3211). Population: Human (10). References Available: Y. Page Count: 7. Issue Publication Date: Sep, 2009. Copyright Statement: S. Karger GmbH, Freiburg. 2009. 
AB  - The assessment of anxiety and its disorders is an essential challenge for clinical work and stems both from methodological issues and from recent research findings. Despite the availability of numerous anxiety-related questionnaires and assessment tools, crucial diagnostic problems remain unresolved. The present review summarizes recent theoretical and methodological requirements of clinical anxiety assessment. Virtual behavior tests, ambulatory assessment, nonverbal measurements and adaptive tests are new approaches to anxiety assessment. They are described and discussed regarding their feasibility for research and practice. (PsycINFO Database Record (c) 2016 APA, all rights reserved)
KW  - innovative approaches
KW  - clinical anxiety assessment
KW  - virtual behavior tests
KW  - ambulatory assessment
KW  - nonverbal measurements
KW  - adaptive tests
KW  - Anxiety
KW  - Anxiety Disorders
KW  - Innovation
KW  - Measurement
KW  - Nonverbal Communication
DO  - 10.1159/000228549
UR  - https://search.ebscohost.com/login.aspx?direct=true&db=psyh&AN=2009-20318-001&lang=de&site=ehost-live
UR  - shelbig@uni-bremen.de
DP  - EBSCOhost
DB  - APA PsycInfo
ER  - 

TY  - JOUR
ID  - 2019-23038-009
AN  - 2019-23038-009
AU  - Fox, Rina S.
AU  - Moreno, Patricia I.
AU  - Yanez, Betina
AU  - Estabrook, Ryne
AU  - Thomas, Jessica
AU  - Bouchard, Laura C.
AU  - McGinty, Heather L.
AU  - Mohr, David C.
AU  - Begale, Mark J.
AU  - Flury, Sarah C.
AU  - Perry, Kent T.
AU  - Kundu, Shilajit D.
AU  - Penedo, Frank J.
T1  - Integrating PROMIS® computerized adaptive tests into a web-based intervention for prostate cancer
T3  - PROMIS® Methods and Applications in Health Psychology and Behavioral Medicine Research
JF  - Health Psychology
JO  - Health Psychology
JA  - Health Psychol
Y1  - 2019/05//
VL  - 38
IS  - 5
SP  - 403
EP  - 409
PB  - American Psychological Association
SN  - 0278-6133
SN  - 1930-7810
SN  - 978-1-4338-9269-1
AD  - Penedo, Frank J., Department of Psychology, University of Miami, 5665 Ponce de Leon Boulevard, Flipse Building, 5th Floor, Coral Gables, FL, US, 33146
N1  - Accession Number: 2019-23038-009. PMID: 31045423 Partial author list: First Author & Affiliation: Fox, Rina S.; Department of Medical Social Sciences, Northwestern University Feinberg School of Medicine, Chicago, IL, US. Other Publishers: Lawrence Erlbaum Associates. Release Date: 20190502. Correction Date: 20201217. Publication Type: Journal (0100), Peer Reviewed Journal (0110). Format Covered: Electronic. Document Type: Journal Article. ISBN: 978-1-4338-9269-1. Language: EnglishGrant Information: Penedo, Frank J. Major Descriptor: Distress; Measurement; Neoplasms; Prostate; Digital Interventions. Minor Descriptor: Adaptive Testing; Anxiety; Fatigue; Major Depression; Pain; Psychosocial Factors. Classification: Health & Mental Health Treatment & Prevention (3300); Health Psychology Testing (2226). Population: Human (10); Male (30). Location: US. Age Group: Adulthood (18 yrs & older) (300); Middle Age (40-64 yrs) (360); Aged (65 yrs & older) (380). Tests & Measures: Patient-Reported Outcomes Measurement Information System (PROMIS) Anxiety Computerized Adaptive Test; Patient-Reported Outcomes Measurement Information System (PROMIS) Depression Computerized Adaptive Test; Patient-Reported Outcomes Measurement Information System (PROMIS) Fatigue Computerized Adaptive Test; Patient-Reported Outcomes Measurement Information System (PROMIS) Pain Interference Computerized Adaptive Test; Patient-Reported Outcomes Measurement Information System (PROMIS) Physical Function Computerized Adaptive Test. Methodology: Clinical Trial; Empirical Study; Quantitative Study. References Available: Y. Page Count: 7. Issue Publication Date: May, 2019. Publication History: Accepted Date: Jul 3, 2018; Revised Date: Jun 5, 2018; First Submitted Date: Oct 13, 2017. Copyright Statement: American Psychological Association. 2019. 
AB  - Objective: This study outlined the implementation and feasibility of delivering PROMIS® computer adaptive tests (CATs) using a web-based method to evaluate the impact of a technological adaptation of Cognitive-Behavioral Stress Management (CBSM) on the psychosocial functioning of men with advanced prostate cancer (APC) undergoing hormone therapy. Method: Patients were randomized to a CBSM group intervention (n = 95) or a health promotion (HP) attention-matched control condition (n = 97). Participants attended all sessions via video conference using tablets, and completed PROMIS® computer adaptive tests (CATs) assessing anxiety, depression, fatigue, pain interference, and physical function weekly during the 10-week intervention. Results: Assessment completion rates >50% at week 1 and week 10 demonstrated moderate feasibility of repeatedly administering PROMIS® CATs using a web-based method. Multilevel modeling demonstrated no significant group-by-time interactions from week 1 to week 10 for any of the assessed PROMIS® domains adjusting for sociodemographic and medical covariates. However, simple effects demonstrated decreases in PROMIS® anxiety scores from week 1 to 10 for both groups. Results also demonstrated significant relationships of medical variables to psychosocial functioning across time points. Conclusions: Results highlight the feasibility and benefits of utilizing PROMIS® CATs to repeatedly assess psychosocial functioning using a web-based method and indicate that web-based interventions may be effective for decreasing psychosocial distress and adverse symptoms among men with APC undergoing hormone therapy. (PsycInfo Database Record (c) 2020 APA, all rights reserved)
KW  - distress
KW  - eHealth
KW  - PROMIS®
KW  - psychosocial
KW  - prostate cancer
KW  - Aged
KW  - Humans
KW  - Internet
KW  - Male
KW  - Patient Reported Outcome Measures
KW  - Prostatic Neoplasms
KW  - Distress
KW  - Measurement
KW  - Neoplasms
KW  - Prostate
KW  - Digital Interventions
KW  - Adaptive Testing
KW  - Anxiety
KW  - Fatigue
KW  - Major Depression
KW  - Pain
KW  - Psychosocial Factors
U1  - Sponsor: National Cancer Institute, US. Grant: R01CA157809. Recipients: Penedo, Frank J.
U1  - Sponsor: National Cancer Institute, US. Other Details: diversity supplement. Recipients: Yanez, Betina
U1  - Sponsor: National Institutes of Health/National Cancer Institute, US. Grant: CA193193. Other Details: Training Grant. Recipients: Moreno, Patricia I.; Bouchard, Laura C.; Fox, Rina S.
U1  - Sponsor: National Institutes of Health, National Center for Advancing Translational Sciences, US. Grant: UL1TR001422; UL1TR000150. Recipients: No recipient indicated
DO  - 10.1037/hea0000672
UR  - https://search.ebscohost.com/login.aspx?direct=true&db=psyh&AN=2019-23038-009&lang=de&site=ehost-live
UR  - ORCID: 0000-0002-3083-6461
UR  - frank.penedo@miami.edu
DP  - EBSCOhost
DB  - APA PsycInfo
ER  - 

TY  - THES
ID  - 2020-86253-190
AN  - 2020-86253-190
AU  - Ma, Ye (Cheryl)
T1  - Investigating hybrid test designs in passage-based adaptive tests
JF  - Dissertation Abstracts International: Section B: The Sciences and Engineering
JO  - Dissertation Abstracts International: Section B: The Sciences and Engineering
Y1  - 2021///
VL  - 82
IS  - 4-B
PB  - ProQuest Information & Learning
SN  - 0419-4217
SN  - 979-8672162072
N1  - Accession Number: 2020-86253-190. Other Journal Title: Dissertation Abstracts International. Partial author list: First Author & Affiliation: Ma, Ye (Cheryl); The University of Iowa, Psychological & Quantitative Foundations, US. Release Date: 20210114. Correction Date: 20221128. Publication Type: Dissertation Abstract (0400). Format Covered: Electronic. Document Type: Dissertation. Dissertation Number: AAI27997446. ISBN: 979-8672162072. Language: EnglishMajor Descriptor: Adaptive Testing; Estimation; Item Content (Test); Simulation. Minor Descriptor: Ability Level; Knowledge Level; Test Administration. Classification: Educational & School Psychology (3500). Population: Human (10). Methodology: Empirical Study; Quantitative Study. 
AB  - Computerized adaptive testing (CAT) selects each item to match with an examinee's ability level during the test. In contrast, multistage adaptive testing (MST) administers a group of items instead of each individual item adaptively. A hybrid design combines CAT and MST borrowing strengths from both testing modes. However, existing research studies indicate that there is lack of knowledge on hybrid designs in passage-based adaptive testing, including situations where misrouting occurs. Misrouting refers to when examinees get routed to paths that don't match with their true ability levels. Therefore, the purpose of this study is to evaluate the proposed hybrid designs (HMCAT designs) with respect to ability estimation accuracy under different MST configurations and their performance when misrouting occurs.The HMCAT designs under passage-based adaptive testing control test adaptability at both passage/stimulus level and item level. Four proposed HMCAT designs differ by implementing the item level CAT at different stages of the MST. Specifically, among all four designs, the two most extreme designs are the one implementing CAT across all stages (CC) and the one not implementing CAT across any stage (PP). The two hybrid designs include implementing CAT only in the routing stage (CP) and implementing CAT only in the last stage (PC). The hybrid designs' performance compared to that of the CC design and the PP design when interacting with different MST configurations and when misrouting occurs are the two main research questions of the study. The three-phase simulation analysis indicates that (1) the PC design achieves more accurate final ability estimation results under the three-stage and the four-stage MST configurations; (2) although the CP design fails to perform effectively when misrouting occurs under the three-stage MST configuration, it is still able to achieve accurate final ability estimation results under both two-stage MST configurations; (3) the CC design and the PP design achieve similar and the most accurate final ability estimation results under all four MST configurations; (4) the PC design reaches similar accuracy as the CC design and the PP design under the three-stage and the four-stage MST configurations; (5) the CP design reaches a similar accuracy level as the CC design and the PP design under both two-stage MST configurations; (6) the CP design is affected the most when misrouting occurs in the three-stage MST configuration, whereas the other three HMCAT designs are not affected. In general, findings from simulation analyses suggest that the HMCAT designs are effective enough to use in passage/stimulus-based adaptive tests with appropriate MST configurations. They also contain advantages such as greater flexibility with respect to content balancing, test security and control on item administration. However, it is important to understand that the performance of HMCAT designs can heavily depend on the MST configuration, the item pool characteristics, the test design and components in the test administration algorithm, such as content balancing strategies. Results from the current study provide implications for practitioners on how to decide on an appropriate HMCAT design under real testing contexts and how to evaluate and maintain the selected HMCAT design to effectively implement the design in practice. (PsycInfo Database Record (c) 2022 APA, all rights reserved)
KW  - hybrid test designs
KW  - adaptive tests
KW  - simulation
KW  - Adaptive Testing
KW  - Estimation
KW  - Item Content (Test)
KW  - Simulation
KW  - Ability Level
KW  - Knowledge Level
KW  - Test Administration
UR  - https://search.ebscohost.com/login.aspx?direct=true&db=psyh&AN=2020-86253-190&lang=de&site=ehost-live
DP  - EBSCOhost
DB  - APA PsycInfo
ER  - 

TY  - JOUR
ID  - 2017-40913-001
AN  - 2017-40913-001
AU  - Ling, Guangming
AU  - Attali, Yigal
AU  - Finn, Bridgid
AU  - Stone, Elizabeth A.
T1  - Is a computerized adaptive test more motivating than a fixed-item test?
JF  - Applied Psychological Measurement
JO  - Applied Psychological Measurement
JA  - Appl Psychol Meas
Y1  - 2017/10//
VL  - 41
IS  - 7
SP  - 495
EP  - 511
PB  - Sage Publications
SN  - 0146-6216
SN  - 1552-3497
AD  - Ling, Guangming, Educational Testing Service, 660 Rosedale Road, MS 07-R, Princeton, NJ, US, 08541
N1  - Accession Number: 2017-40913-001. PMID: 29881102 Partial author list: First Author & Affiliation: Ling, Guangming; Educational Testing Service, Princeton, NJ, US. Release Date: 20170928. Correction Date: 20200416. Publication Type: Journal (0100), Peer Reviewed Journal (0110). Format Covered: Electronic. Document Type: Journal Article. Language: EnglishMajor Descriptor: Adaptive Testing; Educational Measurement; Item Analysis (Test); Mathematical Ability; Middle School Students. Minor Descriptor: Feedback; Motivation. Classification: Educational Measurement (2227); Academic Learning & Achievement (3550). Population: Human (10); Male (30); Female (40). Location: US. Age Group: Childhood (birth-12 yrs) (100); School Age (6-12 yrs) (180); Adolescence (13-17 yrs) (200). Tests & Measures: Quick Math; Florida Comprehensive Assessment Test; Engagement Questionnaire DOI: 10.1037/t75696-000; Anxiety Questionnaire DOI: 10.1037/t02010-000. Methodology: Empirical Study; Quantitative Study. References Available: Y. Page Count: 17. Issue Publication Date: Oct, 2017. Copyright Statement: The Author(s). 2017. 
AB  - Computer adaptive tests provide important measurement advantages over traditional fixed-item tests, but research on the psychological reactions of test takers to adaptive tests is lacking. In particular, it has been suggested that test-taker engagement, and possibly test performance as a consequence, could benefit from the control that adaptive tests have on the number of test items examinees answer correctly. However, previous research on this issue found little support for this possibility. This study expands on previous research by examining this issue in the context of a mathematical ability assessment and by considering the possible effect of immediate feedback of response correctness on test engagement, test anxiety, time on task, and test performance. Middle school students completed a mathematics assessment under one of three test type conditions (fixed, adaptive, or easier adaptive) and either with or without immediate feedback about the correctness of responses. Results showed little evidence for test type effects. The easier adaptive test resulted in higher engagement and lower anxiety than either the adaptive or fixed-item tests; however, no significant differences in performance were found across test types, although performance was significantly higher across all test types when students received immediate feedback. In addition, these effects were not related to ability level, as measured by the state assessment achievement levels. The possibility that test experiences in adaptive tests may not in practice be significantly different than in fixed-item tests is raised and discussed to explain the results of this and previous studies. (PsycInfo Database Record (c) 2020 APA, all rights reserved)
KW  - adaptive testing
KW  - feedback
KW  - motivation
KW  - anxiety
KW  - effort
KW  - low-stakes assessments
KW  - Adaptive Testing
KW  - Educational Measurement
KW  - Item Analysis (Test)
KW  - Mathematical Ability
KW  - Middle School Students
KW  - Feedback
KW  - Motivation
U1  - Sponsor: ETS Internal Research Program. Recipients: No recipient indicated
DO  - 10.1177/0146621617707556
UR  - https://search.ebscohost.com/login.aspx?direct=true&db=psyh&AN=2017-40913-001&lang=de&site=ehost-live
UR  - ORCID: 0000-0002-3907-247X
UR  - gling@ets.org
DP  - EBSCOhost
DB  - APA PsycInfo
ER  - 

TY  - JOUR
ID  - 2016-00755-001
AN  - 2016-00755-001
AU  - Sung, Vivian W.
AU  - Griffith, James W.
AU  - Rogers, Rebecca G.
AU  - Raker, Christina A.
AU  - Clark, Melissa A.
T1  - Item bank development, calibration and validation for patient-reported outcomes in female urinary incontinence
JF  - Quality of Life Research: An International Journal of Quality of Life Aspects of Treatment, Care & Rehabilitation
JO  - Quality of Life Research: An International Journal of Quality of Life Aspects of Treatment, Care & Rehabilitation
JA  - Qual Life Res
Y1  - 2016/07//
VL  - 25
IS  - 7
SP  - 1645
EP  - 1654
PB  - Springer
SN  - 0962-9343
SN  - 1573-2649
AD  - Sung, Vivian W., Division of Urogynecology and Reconstructive Pelvic Surgery, Department of Obstetrics and Gynecology, Women and Infants’ Hospital/Warren, Alpert Medical School, Brown University, 695 Eddy Street, Lower Level, Providence, RI, US, 02903
N1  - Accession Number: 2016-00755-001. PMID: 26732514 Partial author list: First Author & Affiliation: Sung, Vivian W.; Division of Urogynecology and Reconstructive Pelvic Surgery, Department of Obstetrics and Gynecology, Women and Infants’ Hospital/Warren, Alpert Medical School, Brown University, Providence, RI, US. Release Date: 20160111. Correction Date: 20221128. Publication Type: Journal (0100), Peer Reviewed Journal (0110). Format Covered: Electronic. Document Type: Journal Article. Language: EnglishGrant Information: Sung, Vivian W. Major Descriptor: Item Analysis (Test); Psychometrics; Test Construction; Test Validity; Urinary Incontinence. Minor Descriptor: Human Females; Quality of Life; Self-Report. Classification: Health Psychology Testing (2226); Physical & Somatic Disorders (3290). Population: Human (10); Female (40). Age Group: Adulthood (18 yrs & older) (300); Young Adulthood (18-29 yrs) (320); Thirties (30-39 yrs) (340); Middle Age (40-64 yrs) (360); Aged (65 yrs & older) (380); Very Old (85 yrs & older) (390). Tests & Measures: Incontinence Severity Index; Patient Global Impression of Severity; Patient Global Impression of Improvement; Incontinence Impact Questionnaire DOI: 10.1037/t83949-000; Urogenital Distress Inventory DOI: 10.1037/t85581-000. Methodology: Empirical Study; Field Study; Interview; Quantitative Study. References Available: Y. Page Count: 10. Issue Publication Date: Jul, 2016. Publication History: First Posted Date: Jan 6, 2016; Accepted Date: Dec 19, 2015. Copyright Statement: Springer International Publishing Switzerland. 2016. 
AB  - Purpose: Current patient-reported outcomes for female urinary incontinence (UI) are limited by their inability to be tailored. Our objective is to describe the development and field testing of seven item banks designed to measure domains identified as important UI in females (UIf). We also describe the calibration and validation properties of the UIf-item banks, which allow for more efficient computerized adaptive testing (CAT) in the future. Methods: The UIf-measures included 168 items covering seven domains: Stress UI (SUI), overactive bladder (OAB), urinary frequency, physical, social and emotional health impact and adaptation. Items underwent rigorous qualitative development and psychometric testing across two sites. Items were calibrated using item response theory and evaluated for internal consistency, construct validity and responsiveness. Results: A total of 750 women (249 SUI, 249 OAB and 252 mixed UI) participated. Mean age was 55 ± 14 years, and 23 % were Hispanic and 80 % white. In addition to face and content validity, the measures demonstrated good internal consistency (coefficient alpha 0.92–0.98) and unidimensionality. There was evidence for construct validity with moderate-to-strong correlations with the UDI (r’s ≥ 0.6) and IIQ (r’s = ≥0.6) scales. The measures were responsive to change for SUI treatment (paired t test p < .001, ES range 1.3–2.9; SRM range 1.3–2.5) and OAB treatment (paired t test p < .05 for all domains except social health impact and adaptation, ES range 0.3–1.5, SRM range 0.4–1.0). The measures were responsive based on concurrent changes with the UDI and IIQ (p < 0.05). CAT versions were developed and pilot-tested. Conclusions: The UIf-item banks demonstrate good psychometric characteristics and are a sufficiently valid set of customizable tools for measuring UI symptoms and life impact. (PsycInfo Database Record (c) 2022 APA, all rights reserved)
KW  - Urinary incontinence
KW  - Female
KW  - Quality of life
KW  - Patient-centered outcomes research
KW  - Patient outcome assessments
KW  - Psychometrics
KW  - Item response theory
KW  - Computerized adaptive test
KW  - Adult
KW  - Aged
KW  - Calibration
KW  - Databases, Factual
KW  - Female
KW  - Humans
KW  - Middle Aged
KW  - Patient Reported Outcome Measures
KW  - Psychometrics
KW  - Quality of Life
KW  - Reproducibility of Results
KW  - Surveys and Questionnaires
KW  - Urinary Incontinence
KW  - Young Adult
KW  - Item Analysis (Test)
KW  - Psychometrics
KW  - Test Construction
KW  - Test Validity
KW  - Urinary Incontinence
KW  - Human Females
KW  - Quality of Life
KW  - Self-Report
U1  - Sponsor: Eunice Kennedy Shriver National Institute of Child Health and Human Development, US. Grant: K23HD060665; R21HD069962. Recipients: Sung, Vivian W.
DO  - 10.1007/s11136-015-1222-1
UR  - https://search.ebscohost.com/login.aspx?direct=true&db=psyh&AN=2016-00755-001&lang=de&site=ehost-live
UR  - ORCID: 0000-0002-8715-3207
UR  - vsung@wihri.org
DP  - EBSCOhost
DB  - APA PsycInfo
ER  - 

TY  - CHAP
ID  - 2012-22485-010
AN  - 2012-22485-010
AU  - Weiss, David J.
ED  - Geisinger, Kurt F.
ED  - Bracken, Bruce A.
ED  - Carlson, Janet F.
ED  - Hansen, Jo-Ida C.
ED  - Kuncel, Nathan R.
ED  - Reise, Steven P.
ED  - Rodriguez, Michael C.
T1  - Item banking, test development, and test delivery
T2  - APA handbook of testing and assessment in psychology, Vol. 1: Test theory and testing and assessment in industrial and organizational psychology.
T3  - APA handbooks in psychology®
Y1  - 2013///
SP  - 185
EP  - 200
CY  - Washington, DC
PB  - American Psychological Association
SN  - 1-4338-1229-0
SN  - 978-1-4338-1229-3
SN  - 1-4338-1227-4
SN  - 978-1-4338-1227-9
N1  - Accession Number: 2012-22485-010. Partial author list: First Author & Affiliation: Weiss, David J.; Department of Psychology, University of Minnesota, Minneapolis, MN, US. Release Date: 20130218. Correction Date: 20230123. Publication Type: Book (0200), Edited Book (0280). Format Covered: Print. Document Type: Chapter. Book Type: Handbook/Manual. ISBN: 1-4338-1229-0, ISBN Hardcover; 978-1-4338-1229-3, ISBN Hardcover; 1-4338-1227-4, ISBN Set; 978-1-4338-1227-9, ISBN Set. Language: EnglishMajor Descriptor: Measurement; Test Administration; Test Construction; Test Items; Computerized Assessment. Minor Descriptor: History. Classification: Tests & Testing (2220). Population: Human (10). Intended Audience: Psychology: Professional & Research (PS). References Available: Y. Page Count: 16. 
AB  - The first 60 or so years of psychological, educational, and personnel testing were dominated by the paper-and-pencil (P&P) test. Item banking, test assembly, and test scoring were entirely manual procedures that were labor intensive, tedious, and prone to errors. Tests were highly standardized, as were conditions of administration. Changes began to occur with the introduction of electronic optical mark readers that reduced test scoring to a relatively accurate partially automated procedure that dominated standardized testing for many decades. The introduction of the PC in the mid-1980s, however, began a major evolution of testing away from the traditional way of building tests and delivering them. The years since 1985 have seen computers automate the processes of item banking, test assembly, test analysis, and test delivery. The PC allowed the development of new modes of test delivery—random, sequential, and adaptive—and new kinds of test items. The advent of the Internet extended test delivery to any computer that could connect to it, albeit not without some problems. The result of this evolution of testing is a set of processes that are considerably less labor intensive, more accurate, and more efficient. In the process of this ongoing conversion, numerous questions have arisen, some of which have not yet been satisfactorily studied or even addressed. Few research questions surround computerized item banking and test assembly. The major questions have risen in the context of computer-based testing (CBT). In the early days of CBT, it was natural to address the question of whether CBTs functioned the same as P&P tests. Generally, it was found that they did (Mead & Drasgow, 1993), although early research indicated some differences on reading comprehension tests (e.g., Kiely, Zara, & Weiss, 1986; Mazzeo & Harvey, 1988). As CBTs begin to be used to measure constructs that cannot be measured by P&P, however, comparability is no longer an issue, and CBTs will have to be validated on their own merits. Obviously, a host of questions exist about how to best implement computerized adaptive tests (CATs) and sequential tests that have resulted in substantial research over the past 20 or 30 years and will continue to do so. CBTs also raise a number of questions about the psychological environment of testing that have generally not been addressed. In the process of creating a test of appropriate difficulty for each examinee, CATs create a different psychological environment than do P&P tests. Does that difference affect examinee performance? PC-delivered tests have virtually no delays between items in comparison to Internet-delivered tests. Do the unpredictable delays in Internet-based testing affect examinees’ test anxiety and thereby influence test performance? Do the random variations in the testing environment that occur for unsupervised Internet-based testing affect test scores? Finally, as the Internet continues to pervade people’s activities through various electronic devices, some have suggested that certain kinds of psychological measurements (e.g., attitudes, personality variables) can be delivered by portable electronic devices such as PDAs and cellular phones. If these modes of test delivery are implemented, the usefulness of the resulting measurements will have to be carefully scrutinized because of the extremely variable testing conditions under which such measurements will be obtained. (PsycInfo Database Record (c) 2023 APA, all rights reserved)
KW  - item banking
KW  - test development
KW  - test delivery
KW  - history
KW  - pencil & paper tests
KW  - computer based testing
KW  - Measurement
KW  - Test Administration
KW  - Test Construction
KW  - Test Items
KW  - Computerized Assessment
KW  - History
DO  - 10.1037/14047-010
UR  - https://search.ebscohost.com/login.aspx?direct=true&db=psyh&AN=2012-22485-010&lang=de&site=ehost-live
DP  - EBSCOhost
DB  - APA PsycInfo
ER  - 

TY  - JOUR
ID  - 2022-02913-004
AN  - 2022-02913-004
AU  - Liao, Xiangyi
AU  - Bolt, Daniel M.
T1  - Item characteristic curve asymmetry: A better way to accommodate slips and guesses than a four-parameter model?
JF  - Journal of Educational and Behavioral Statistics
JO  - Journal of Educational and Behavioral Statistics
JA  - J Educ Behav Stat
Y1  - 2021/12//
VL  - 46
IS  - 6
SP  - 753
EP  - 775
PB  - Sage Publications
SN  - 1076-9986
SN  - 1935-1054
AD  - Liao, Xiangyi, University of Wisconsin–Madison, Department of Educational Psychology, 1025 W. Johnson St., Madison, WI, US, 53706
N1  - Accession Number: 2022-02913-004. Other Journal Title: Journal of Educational Statistics. Partial author list: First Author & Affiliation: Liao, Xiangyi; University of Wisconsin–Madison, Department of Educational Psychology, Madison, WI, US. Other Publishers: American Educational Research Assn. Release Date: 20211129. Publication Type: Journal (0100), Peer Reviewed Journal (0110). Format Covered: Electronic. Document Type: Journal Article. Language: EnglishMajor Descriptor: Item Response Theory; Mathematical Modeling; Mathematics Achievement; Mathematics Education; Theories of Education. Minor Descriptor: Adaptive Testing; Guessing; Measurement; Models; Multiple Choice (Testing Method); Test Validity. Classification: Educational Measurement (2227); Academic Learning & Achievement (3550). Population: Human (10). Methodology: Empirical Study; Mathematical Model; Quantitative Study; Scientific Simulation. Supplemental Data: Other Internet. References Available: Y. Page Count: 23. Issue Publication Date: Dec, 2021. Publication History: Accepted Date: Feb 27, 2021; Revised Date: Feb 22, 2021; Nov 17, 2020; First Submitted Date: Aug 4, 2020. Copyright Statement: AERA. 2021. 
AB  - Four-parameter models have received increasing psychometric attention in recent years, as a reduced upper asymptote for item characteristic curves can be appealing for measurement applications such as adaptive testing and person-fit assessment. However, applications can be challenging due to the large number of parameters in the model. In this article, we demonstrate in the context of mathematics assessments how the slip and guess parameters of a four-parameter model may often be empirically related. This observation also has a psychological explanation to the extent that both asymptote parameters may be manifestations of a single item complexity characteristic. The relationship between lower and upper asymptotes motivates the consideration of an asymmetric item response theory model as a three-parameter alternative to the four-parameter model. Using actual response data from mathematics multiple-choice tests, we demonstrate the empirical superiority of a three-parameter asymmetric model in several standardized tests of mathematics. To the extent that a model of asymmetry ultimately portrays slips and guesses not as purely random but rather as proficiency-related phenomena, we argue that the asymmetric approach may also have greater psychological plausibility. (PsycInfo Database Record (c) 2021 APA, all rights reserved)
KW  - item response theory
KW  - mathematics education
KW  - test theory/development
KW  - validity/reliability
KW  - Item Response Theory
KW  - Mathematical Modeling
KW  - Mathematics Achievement
KW  - Mathematics Education
KW  - Theories of Education
KW  - Adaptive Testing
KW  - Guessing
KW  - Measurement
KW  - Models
KW  - Multiple Choice (Testing Method)
KW  - Test Validity
DO  - 10.3102/10769986211003283
UR  - https://search.ebscohost.com/login.aspx?direct=true&db=psyh&AN=2022-02913-004&lang=de&site=ehost-live
UR  - dmbolt@wisc.edu
UR  - xliao36@wisc.edu
DP  - EBSCOhost
DB  - APA PsycInfo
ER  - 

TY  - JOUR
ID  - 1984-32817-001
AN  - 1984-32817-001
AU  - Kingston, Neal M.
AU  - Dorans, Neil J.
T1  - Item location effects and their implications for IRT equating and adaptive testing
JF  - Applied Psychological Measurement
JO  - Applied Psychological Measurement
JA  - Appl Psychol Meas
Y1  - 1984///Spr 1984
VL  - 8
IS  - 2
SP  - 147
EP  - 154
PB  - Sage Publications
SN  - 0146-6216
SN  - 1552-3497
N1  - Accession Number: 1984-32817-001. Partial author list: First Author & Affiliation: Kingston, Neal M.; Educational Testing Service, Princeton, NJ. Release Date: 19841201. Correction Date: 20121001. Publication Type: Journal (0100), Peer Reviewed Journal (0110). Format Covered: Print. Document Type: Journal Article. Language: EnglishMajor Descriptor: Graduate Record Examination; Item Analysis (Test). Classification: Educational Measurement (2227). Population: Human (10). Age Group: Adulthood (18 yrs & older) (300). Methodology: Empirical Study. Page Count: 8. Issue Publication Date: Spr 1984. 
AB  - Investigated the susceptibility to item location effects of 10 item types from the Graduate Record Examination General Test, which was administered to approximately 4,000 examinees in 1 of 2 operational forms, by comparing the item difficulty parameters of sets of items across intended usages to identify location effects as a form of multidimensionality. Results show that 2 of the 10 item types, analysis of explanations and logical diagrams, were clearly affected by item location in the population tested. A common item type, reading comprehension, was affected somewhat by item context. Examples of testing situations where location effects are important are described. It is recommended that these item types not be used in an adaptive testing program without assessing their susceptibility to location effects within the population (and subpopulations) of interest. (12 ref) (PsycINFO Database Record (c) 2016 APA, all rights reserved)
KW  - location effects of item types from Graduate Record Examination General Test
KW  - Graduate Record Examination
KW  - Item Analysis (Test)
DO  - 10.1177/014662168400800202
UR  - https://search.ebscohost.com/login.aspx?direct=true&db=psyh&AN=1984-32817-001&lang=de&site=ehost-live
DP  - EBSCOhost
DB  - APA PsycInfo
ER  - 

TY  - THES
ID  - 2014-99131-049
AN  - 2014-99131-049
AU  - Lin, Haiyan
T1  - Item selection methods in multidimensional computerized adaptive testing adopting polytomously-scored items under multidimensional generalized partial credit model
JF  - Dissertation Abstracts International Section A: Humanities and Social Sciences
JO  - Dissertation Abstracts International Section A: Humanities and Social Sciences
Y1  - 2014///
VL  - 75
IS  - 1-A(E)
PB  - ProQuest Information & Learning
SN  - 0419-4209
SN  - 978-1-303-50911-7
N1  - Accession Number: 2014-99131-049. Other Journal Title: Dissertation Abstracts International. Partial author list: First Author & Affiliation: Lin, Haiyan; U Illinois at Urbana-Champaign, US. Release Date: 20140721. Correction Date: 20221128. Publication Type: Dissertation Abstract (0400). Format Covered: Electronic. Document Type: Dissertation. Dissertation Number: AAI3600698. ISBN: 978-1-303-50911-7. Language: EnglishMajor Descriptor: Adaptive Testing; Formative Assessment. Minor Descriptor: Computerized Assessment. Classification: Educational & School Psychology (3500). Population: Human (10). Methodology: Empirical Study; Quantitative Study. 
AB  - Four item selection methods are compared and investigated under three test formats in the context of Multidimensional Computerized Adaptive Testing (MCAT) delivering polytomous items partially or completely in tests. Item selection methods examined include Fisher information based D-optimality (D-optimality), Kullback-Leibler information index (KI), mutual information (MI), and continuous entropy method (CEM). The three test formats considered are the POLYTYPE format that contains polytomous items with three response categories, the DPMIX format that delivers dichotomous items at the beginning and polytomous items at the final stage, and the PDMIX format that has the reverse order as DPMIX. In general, D-optimality shows the best estimation accuracy and conditional estimation accuracy. D-optimality, MI, and CEM are similar in terms of ability estimation accuracy and tendency in selecting items when the item bank size is large. For both dichotomous and polytomous items, KI is mostly outperformed by the other three methods in terms of ability estimation precision. When sub-thetas in both dimensions are equal, however, KI shows the best performance for polytomous items. In this study, which item type, dichotomous or polytomous, being administered first does not affect the estimation accuracy. However, if the test length is much longer or shorter than the test length of the current study, it is possible that the estimation accuracy could be affected by the order of delivering different item types. Both DPMIX and PDMIX formats yield similar conditional estimation accuracy pattern and precision. In addition, the item bank size does affect the estimation precision. These conclusions, however, might not be applied to MCAT testing with different test designs or item pool structures. More studies are needed in MCAT combining with polytomous items to further facilitate the development and improvement of the next-generation assessments such as formative assessment or testing for diagnosis. (PsycInfo Database Record (c) 2022 APA, all rights reserved)
KW  - ability estimation
KW  - ability estimation accuracy
KW  - ability estimation precision
KW  - computerized adaptive testing
KW  - conditional estimation accuracy
KW  - continuous entropy method
KW  - dichotomous items
KW  - estimation accuracy
KW  - estimation precision
KW  - final stage
KW  - formative assessment
KW  - generalized partial credit
KW  - Adaptive Testing
KW  - Formative Assessment
KW  - Computerized Assessment
UR  - https://search.ebscohost.com/login.aspx?direct=true&db=psyh&AN=2014-99131-049&lang=de&site=ehost-live
DP  - EBSCOhost
DB  - APA PsycInfo
ER  - 

TY  - THES
ID  - 2012-99130-528
AN  - 2012-99130-528
AU  - Ali, Usama Sayed Ahmed
T1  - Item selection methods in polytomous computerized adaptive testing
JF  - Dissertation Abstracts International Section A: Humanities and Social Sciences
JO  - Dissertation Abstracts International Section A: Humanities and Social Sciences
Y1  - 2012///
VL  - 73
IS  - 1-A
SP  - 165
EP  - 165
PB  - ProQuest Information & Learning
SN  - 0419-4209
SN  - 978-1-124-96332-7
N1  - Accession Number: 2012-99130-528. Other Journal Title: Dissertation Abstracts International. Partial author list: First Author & Affiliation: Ali, Usama Sayed Ahmed; U Illinois at Urbana-Champaign, US. Release Date: 20120917. Correction Date: 20221128. Publication Type: Dissertation Abstract (0400). Format Covered: Electronic. Document Type: Dissertation. Dissertation Number: AAI3478592. ISBN: 978-1-124-96332-7. Language: EnglishMajor Descriptor: Adaptive Testing; Item Response Theory; Computerized Assessment. Minor Descriptor: Computers; Technology. Classification: Educational & School Psychology (3500). Population: Human (10). Page Count: 1. 
AB  - Given the rapid advancement of computer technology, the importance of administering adaptive tests with polytomous items is in great need. With regard to the applicability of adaptive testing using polytomous IRT models, adaptive testing can use polytomous items of either rating scales, or in some testing situations of multiple choice. Additionally, the availability of computerized polytomous scoring of open-ended items enhances such applicability. This need promotes the research in polytomous adaptive testing (PAT). This dissertation is an effort to focus on item selection methods, as a major component, in polytomous computerized adaptive testing. So, it consists of five chapters that cover the following: Chapter 1 focuses on a thorough introduction to the item response theory (IRT) models and adaptive testing related to polytomous items. Such an important overview and introduction to basic concepts in test theory and mathematical models for polytomous items is needed for the flow of consequent chapters. Chapter 2 is devoted to the development of a central location index (LI) to uniquely represent the polytomous item with a scale value parameter using most commonly used polytomous models. The motivation and rationale to search for a central or an overall location parameter is twofold: (a) the confusion of multiple and different parameterizations for a polytomous item even for the same model, and (b) the unavailability of such single location parameter block the usage of certain item selection methods in adaptive testing. Two approaches are used to derive the proposed LIs, one is based on the item category response functions (ICRFs) and the other is based on the polytomous item response function (IRF). As a result, four LIs are proposed. Chapter 3 is particularly assigned to development of an item selection method based on the developed location index and primarily assess its performance in the PAT context relative to existing methods. This method belongs to the non-information based item selection methods and we referred it as Matching-LI method. The results support that this proposed method is promising and is capable to produce accurate ability estimates and successfully manage the item pool usage. Chapter 4 introduces new item selection methods taking in consideration the previous chapter's results. The new methods are the hybrid, stage-based information, polytomous  a-stratification methods. The first two methods try to merge more than one criterion for selecting items of each PAT (e.g., the hybrid method merges both the Matching-LI and maximum information (MI) methods). The last method uses Matching-LI method within each stratum. Chapter 5 provides discussion, conclusions, and limitations and future research directions with respect to important components of an adaptive testing program (i.e., item selection methods, item response models, item banks, and trait versus attribute estimation). (PsycInfo Database Record (c) 2022 APA, all rights reserved)
KW  - item selection methods
KW  - polytomous computerized adaptive testing
KW  - Adaptive Testing
KW  - Item Response Theory
KW  - Computerized Assessment
KW  - Computers
KW  - Technology
UR  - https://search.ebscohost.com/login.aspx?direct=true&db=psyh&AN=2012-99130-528&lang=de&site=ehost-live
DP  - EBSCOhost
DB  - APA PsycInfo
ER  - 

TY  - JOUR
ID  - 2006-09336-020
AN  - 2006-09336-020
AU  - Ping, Chen
AU  - Shuliang, Ding
AU  - Haijing, Lin
AU  - Jie, Zhou
T1  - Item Selection Strategies of Computerized Adaptive Testing based on Graded Response Model
JF  - Acta Psychologica Sinica
JO  - Acta Psychologica Sinica
JA  - Xin Li Xue Bao
Y1  - 2006/05//
VL  - 38
IS  - 3
SP  - 461
EP  - 467
PB  - Science Press
SN  - 0439-755X
AD  - Shuliang, Ding, Computer Information Engineering College, Jiangxi Normal University, Nanchang, China, 330027
N1  - Accession Number: 2006-09336-020. Partial author list: First Author & Affiliation: Ping, Chen; Computer Information Engineering College, Jiangxi Normal University, Nanchang, China. Release Date: 20070219. Correction Date: 20190211. Publication Type: Journal (0100), Peer Reviewed Journal (0110). Format Covered: Print. Document Type: Journal Article. Language: ChineseMajor Descriptor: Adaptive Testing; Item Response Theory; Strategies; Test Items; Computerized Assessment. Classification: Tests & Testing (2220). Methodology: Empirical Study; Mathematical Model; Quantitative Study. Page Count: 7. Issue Publication Date: May, 2006. 
AB  - Item selection strategy (ISS) is an important component of Computerized Adaptive Testing (CAT). Its performance directly affects the security, efficiency and precision of the test. Thus, ISS becomes one of the central issues in CATs based on the Graded Response Model (GRM). It is well known that the goal of IIS is to administer the next unused item remaining in the item bank that best fits the examinees current ability estimate. In dichotomous IRT models, every item has only one difficulty parameter and the item whose difficulty matches the examinee's current ability estimate is considered to be the best fitting item. However, in GRM, each item has more than two ordered categories and has no single value to represent the item difficulty. Consequently, some researchers have used to employ the average or the median difficulty value across categories as the difficulty estimate for the item. Using the average value and the median value in effect introduced two corresponding ISSs. In this study, we used computer simulation compare four ISSs based on GRM. We also discussed the effect of 'shadow pool' on the uniformity of pool usage as well as the influence of different item parameter distributions and different ability estimation methods on the evaluation criteria of CAT. In the simulation process, Monte Carlo method was adopted to simulate the entire CAT process; 1,000 examinees drawn from standard normal distribution and four 1,000-sized item pools of different item parameter distributions were also simulated. The assumption of the simulation is that a polytomous item is comprised of six ordered categories. In addition, ability estimates were derived using two methods. They were expected a posteriori Bayesian (EAP) and maximum likelihood estimation (MLE). In MLE, the Newton-Raphson iteration method and the Fisher Score iteration method were employed, respectively, to solve the likelihood equation. Moreover, the CAT process was simulated with each examinee 30 times to eliminate random error. The IISs were evaluated by four indices usually used in CAT from four aspects--the accuracy of ability estimation, the stability of IIS, the usage of item pool, and the test efficiency. Simulation results showed adequate evaluation of the ISS that matched the estimate of an examinee's current trait level with the difficulty values across categories. Setting 'shadow pool' in ISS was able to improve the uniformity of pool utilization. Finally, different distributions of the item parameter and different ability estimation methods affected the evaluation indices of CAT. (PsycINFO Database Record (c) 2019 APA, all rights reserved)
KW  - item selection strategy
KW  - computerized adaptive testing
KW  - Adaptive Testing
KW  - Item Response Theory
KW  - Strategies
KW  - Test Items
KW  - Computerized Assessment
UR  - https://search.ebscohost.com/login.aspx?direct=true&db=psyh&AN=2006-09336-020&lang=de&site=ehost-live
UR  - ding06026@163.com
DP  - EBSCOhost
DB  - APA PsycInfo
ER  - 

TY  - JOUR
ID  - 2021-32083-001
AN  - 2021-32083-001
AU  - Kaufman, Joan
AU  - Kobak, Ken
AU  - Birmaher, Boris
AU  - de Lacy, Nina
T1  - KSADS-COMP perspectives on child psychiatric diagnostic assessment and treatment planning
JF  - Journal of the American Academy of Child & Adolescent Psychiatry
JO  - Journal of the American Academy of Child & Adolescent Psychiatry
JA  - J Am Acad Child Adolesc Psychiatry
Y1  - 2021/05//
VL  - 60
IS  - 5
SP  - 540
EP  - 542
PB  - Elsevier Science
SN  - 0890-8567
SN  - 1527-5418
AD  - Kaufman, Joan, Center for Child and Family Traumatic Stress, Kennedy Krieger Institute, 1741 Ashland Avenue, Baltimore, MD, US, 21224
N1  - Accession Number: 2021-32083-001. PMID: 33385508 Other Journal Title: Journal of the American Academy of Child Psychiatry. Partial author list: First Author & Affiliation: Kaufman, Joan; Center for Child and Family Traumatic Stress, Kennedy Krieger Institute, Baltimore, MD, US. Other Publishers: Lippincott Williams & Wilkins. Release Date: 20210401. Correction Date: 20210520. Publication Type: Journal (0100), Peer Reviewed Journal (0110). Format Covered: Electronic. Document Type: Comment/Reply. Language: EnglishGrant Information: Kaufman, Joan. Major Descriptor: Adaptive Testing; Differential Diagnosis; Primates (Nonhuman); Psychiatric Evaluation; Treatment Planning. Minor Descriptor: Major Depression; Oppositional Defiant Disorder; Tracking. Classification: Health Psychology & Medicine (3360). Page Count: 3. Issue Publication Date: May, 2021. Publication History: Accepted Date: Dec 23, 2020. Copyright Statement: American Academy of Child and Adolescent Psychiatry. 2021. 
AB  - Comments on an article by Robert D. Gibbons et al. (see record [rid]2020-75129-001[/rid]). Gibbons et al. demonstrated the utility of computerized adaptive tests (CATs) based on multidimensional item response theory for the assessment of depression, anxiety, mania/hypomania, attention-deficit/hyperactivity disorder, conduct disorder, oppositional defiant disorder, and suicidality in children and adolescents. The web-based versions of the KSADS-COMP has many advantages over its predecessor, including streamlined assessment time, automated scoring, branching and tracking of supplements, and availability of symptom-level and diagnostic reports in real time. Both the self- and the clinician-administered versions of the KSADS-COMP are particularly useful when conducting telehealth assessments. The K-CAT and KSADS-COMP both leverage technology to make the diagnostic process more efficient. In addition to dimensional measures of psychopathology, the KSADS-COMP provides categorical diagnoses, the necessary information to facilitate differential diagnosis, and information about the context of the child’s life. (PsycInfo Database Record (c) 2021 APA, all rights reserved)
KW  - diagnostic assessment
KW  - treatment planning
KW  - child psychiatry
KW  - Adaptive Testing
KW  - Differential Diagnosis
KW  - Primates (Nonhuman)
KW  - Psychiatric Evaluation
KW  - Treatment Planning
KW  - Major Depression
KW  - Oppositional Defiant Disorder
KW  - Tracking
U1  - Sponsor: National Institutes of Health, National Institute on Drug Abuse, US. Grant: R44 MH094092. Recipients: Kaufman, Joan; Kobak, Ken
DO  - 10.1016/j.jaac.2020.08.470
UR  - https://search.ebscohost.com/login.aspx?direct=true&db=psyh&AN=2021-32083-001&lang=de&site=ehost-live
UR  - ORCID: 0000-0001-9299-6519
UR  - ORCID: 0000-0002-4075-3045
UR  - ORCID: 0000-0003-2676-2970
UR  - joan.kaufman@kennedykrieger.org
DP  - EBSCOhost
DB  - APA PsycInfo
ER  - 

TY  - JOUR
ID  - 2013-31869-039
AN  - 2013-31869-039
AU  - Ma, Xin-yi
AU  - Ling, Hui
AU  - Li, Xin-li
AU  - Wang, Meng-yi
T1  - Learning adaptability, mental health of pupils in boarding and nonboarding schools
JF  - Chinese Journal of Clinical Psychology
JO  - Chinese Journal of Clinical Psychology
Y1  - 2013/06//
VL  - 21
IS  - 3
SP  - 497
EP  - 499
PB  - Clinical Psychological Research Ctr
SN  - 1005-3611
AD  - Ma, Xin-yi, Yunfu High School, Yunfu, China, 527300
N1  - Accession Number: 2013-31869-039. Partial author list: First Author & Affiliation: Ma, Xin-yi; Yunfu High School, Yunfu, China. Release Date: 20131028. Publication Type: Journal (0100), Peer Reviewed Journal (0110). Format Covered: Print. Document Type: Journal Article. Language: ChineseMajor Descriptor: Academic Achievement; Boarding Schools; Learning Ability; Mental Health. Minor Descriptor: Student Attitudes. Classification: Academic Learning & Achievement (3550). Population: Human (10). Location: China. Methodology: Empirical Study; Quantitative Study. Page Count: 3. Issue Publication Date: Jun, 2013. 
AB  - Objective: To explore and compare the differences in learning adaptability, mental health and academic performance of the boarding and non boarding children. Methods: Cluster sample the upper-grade-students of Bo Cai Primary School boarding section and non-boarding section in Changsha as the research subjects. All the subjects were assessed with the academic adaptive test (AAT) and Mental Health Test (MHT), and then their school records of final Chinese and math marks were collected and analyzed. Results: (1) The boarding pupils' learning adaptability of independent dimension and the study anxiety and blame tendency, allergy tendency and physical symptoms were significantly higher than non-boarding pupils'. (2) The interaction of Boarding/non-boarding × grade was significant in family environment, terror tendency and impulse tendency. Conclusion: The learning adaptability of independent dimension of boarding pupils' is significantly better than non-boarding pupils, but their mental health level is lower than the latter's. (PsycINFO Database Record (c) 2016 APA, all rights reserved)
KW  - learning adaptability
KW  - mental health
KW  - boarding schools
KW  - students academic performance
KW  - Academic Achievement
KW  - Boarding Schools
KW  - Learning Ability
KW  - Mental Health
KW  - Student Attitudes
UR  - https://search.ebscohost.com/login.aspx?direct=true&db=psyh&AN=2013-31869-039&lang=de&site=ehost-live
DP  - EBSCOhost
DB  - APA PsycInfo
ER  - 

TY  - JOUR
ID  - 2014-23629-017
AN  - 2014-23629-017
AU  - Carroll, Bernard J.
T1  - Limitations of computerized adaptive testing for anxiety
JF  - The American Journal of Psychiatry
JO  - The American Journal of Psychiatry
JA  - Am J Psychiatry
Y1  - 2014/06//
VL  - 171
IS  - 6
SP  - 692
EP  - 692
PB  - American Psychiatric Assn
SN  - 0002-953X
SN  - 1535-7228
N1  - Accession Number: 2014-23629-017. PMID: 24880515 Other Journal Title: American Journal of Insanity. Partial author list: First Author & Affiliation: Carroll, Bernard J.; Pacific Behavioral Research Foundation, Carmel, CA, US. Release Date: 20140811. Correction Date: 20221128. Publication Type: Journal (0100), Peer Reviewed Journal (0110). Format Covered: Electronic. Document Type: Comment/Reply. Language: EnglishMajor Descriptor: Adaptive Testing; Generalized Anxiety Disorder; Major Depression; Psychometrics; Computerized Assessment. Minor Descriptor: Anxiety. Classification: Clinical Psychological Testing (2224); Anxiety Disorders (3215). Population: Human (10). Page Count: 1. Issue Publication Date: Jun, 2014. 
AB  - Comments on an article by RD. Gibbons et al. (see record [rid]2014-06810-010[/rid]). The Computerized Adaptive Testing–Anxiety Inventory (CAT-ANX) was introduced as a new 'test for anxiety,' with suggested large-scale screening uses and planned commercial availability. The pool of over 400 items has not been demonstrated to have construct validity or predictive validity. The resultant CAT-ANX was not validated against existing anxiety scales. Test-retest reliability was not demonstrated. The 'anxiety' of CAT-ANX was not defined. It is not coterminous with generalized anxiety disorder, although the article focused on this disorder. Operationally, CAT-ANX draws from many. The data in the article thus provide no sound basis to say that screening with CAT-ANX will allow confident, rapid, and accurate positive identification of key clinical anxiety diagnoses. (PsycInfo Database Record (c) 2022 APA, all rights reserved)
KW  - psychometrics
KW  - Computerized Adaptive Testing–Anxiety Inventory
KW  - generalized anxiety disorder
KW  - Anxiety Disorders
KW  - Diagnosis, Computer-Assisted
KW  - Female
KW  - Humans
KW  - Male
KW  - Psychiatric Status Rating Scales
KW  - Adaptive Testing
KW  - Generalized Anxiety Disorder
KW  - Major Depression
KW  - Psychometrics
KW  - Computerized Assessment
KW  - Anxiety
DO  - 10.1176/appi.ajp.2014.13091252
UR  - https://search.ebscohost.com/login.aspx?direct=true&db=psyh&AN=2014-23629-017&lang=de&site=ehost-live
DP  - EBSCOhost
DB  - APA PsycInfo
ER  - 

TY  - JOUR
ID  - 2022-73331-008
AN  - 2022-73331-008
AU  - Weiss, Margaret Danielle
AU  - Stein, Mark A.
T1  - Measurement informed care in attention-deficit/hyperactivity disorder (ADHD)
JF  - Child and Adolescent Psychiatric Clinics of North America
JO  - Child and Adolescent Psychiatric Clinics of North America
JA  - Child Adolesc Psychiatr Clin N Am
Y1  - 2022/07//
VL  - 31
IS  - 3
SP  - 363
EP  - 372
PB  - Elsevier Science
SN  - 1056-4993
SN  - 1558-0490
AD  - Weiss, Margaret Danielle, Department of Child Psychiatry, Cambridge Health Alliance, 1493 Cambridge Street, Cambridge, MA, US, 02139
N1  - Accession Number: 2022-73331-008. PMID: 35697390 Partial author list: First Author & Affiliation: Weiss, Margaret Danielle; Department of Child Psychiatry, Cambridge Health Alliance, Cambridge, MA, US. Release Date: 20220704. Publication Type: Journal (0100), Peer Reviewed Journal (0110). Format Covered: Electronic. Document Type: Journal Article. Language: EnglishMajor Descriptor: Attention Deficit Disorder with Hyperactivity; Health Care Services; Primary Health Care; Symptom Remission. Minor Descriptor: Diagnosis; Differential Diagnosis; Emotional Regulation; Measurement; Rating Scales; Symptoms. Classification: Health & Mental Health Services (3370). Population: Human (10). References Available: Y. Page Count: 10. Issue Publication Date: Jul, 2022. Copyright Statement: All rights reserved. Elsevier Inc. 2022. 
AB  - Measurement informed care is a cornerstone of evidence-based practice and shared decision-making. A structured diagnostic interview specific to ADHD provides a globally agreed-on standard of evaluation. These interviews are accessible in the public domain in multiple languages and are helpful to clinicians new to the diagnosis of ADHD. Broad-based rating scales looking at multiple domains of psychopathology are critical to assuring recognition of comorbid diagnoses, which might otherwise be missed, differential diagnoses, and identification of the most prominent or treatable diagnosis. Recent innovations in computerized adaptive testing have improved the efficiency and accuracy of diagnostic screening. Rating scales specific to ADHD and disruptive behavior disorders establish the severity of the disorder and response to intervention. Age- and gender-normed symptom rating scales for ADHD capture clinically salient differences between what is normative in different demographic groups. An evaluation of functional impairment in ADHD has been critical to understanding the patient’s perspective of the presenting problem. Best practice care for ADHD treatment goes beyond improvement to well-defined standards for both symptom and functional remission. Studies of executive function, emotional regulation, mindwandering, and sluggish cognitive tempo have led to a richer understanding of the breadth and depth of associated deficits commonly experienced by ADHD patients. Psychometrically validated tools are available to complement every aspect of ADHD care and provide global standards for research. (PsycInfo Database Record (c) 2022 APA, all rights reserved)
KW  - ADHD
KW  - Measurement
KW  - Outcome
KW  - Scales
KW  - Symptoms
KW  - Function
KW  - Improvement
KW  - Remission
KW  - Attention Deficit Disorder with Hyperactivity
KW  - Attention Deficit and Disruptive Behavior Disorders
KW  - Comorbidity
KW  - Executive Function
KW  - Humans
KW  - Psychiatric Status Rating Scales
KW  - Attention Deficit Disorder with Hyperactivity
KW  - Health Care Services
KW  - Primary Health Care
KW  - Symptom Remission
KW  - Diagnosis
KW  - Differential Diagnosis
KW  - Emotional Regulation
KW  - Measurement
KW  - Rating Scales
KW  - Symptoms
DO  - 10.1016/j.chc.2022.03.010
UR  - https://search.ebscohost.com/login.aspx?direct=true&db=psyh&AN=2022-73331-008&lang=de&site=ehost-live
UR  - Margaret.weiss@icloud.com
DP  - EBSCOhost
DB  - APA PsycInfo
ER  - 

TY  - THES
ID  - 2006-99011-117
AN  - 2006-99011-117
AU  - Yang, Tae-Kyoung
T1  - Measurement of korean efl college students' foreign language classroom speaking anxiety: Evidence of psychometric properties and accuracy of a computerized adaptive test (CAT) with dichotomously scored items using a cat simulation
JF  - Dissertation Abstracts International Section A: Humanities and Social Sciences
JO  - Dissertation Abstracts International Section A: Humanities and Social Sciences
Y1  - 2006///
VL  - 66
IS  - 12-A
SP  - 4365
EP  - 4365
PB  - ProQuest Information & Learning
SN  - 0419-4209
N1  - Accession Number: 2006-99011-117. Other Journal Title: Dissertation Abstracts International. Partial author list: First Author & Affiliation: Yang, Tae-Kyoung; U Texas At Austin, US. Release Date: 20060918. Correction Date: 20221128. Publication Type: Dissertation Abstract (0400). Format Covered: Electronic. Document Type: Dissertation. Language: EnglishMajor Descriptor: Anxiety; College Students; Foreign Languages; Psychometrics; Computerized Assessment. Minor Descriptor: Item Response Theory. Classification: Educational & School Psychology (3500). Population: Human (10). Location: Korea. Age Group: Adulthood (18 yrs & older) (300). Methodology: Empirical Study; Quantitative Study; Scientific Simulation. Page Count: 1. 
AB  - Assessment of foreign language speaking anxiety is considered pertinent to assisting practitioners to reduce learners' speaking anxiety. Computerized adaptive testing (CAT) of severity of speaking anxiety has potential advantages over conventional paper-and-pencil (P&P) tests in terms of efficiency, precision, adaptiveness and real-time severity monitoring. Item response theory (IRT) was applied to obtain item characteristics (i.e. anxiety severity and discrimination parameter) and develop an item pool that can be used for a computerized adaptive test to measure severity of speaking anxiety. Based on two-parameter logistic IRT model, the study analyzed responses to a newly constructed English speaking anxiety inventory from a sample of 949 Korean EFL undergraduate students. The study used Principal Component Analysis and DIMTEST in a confirmatory mode to account for construct validity and conducted a computer simulation of CAT to investigate whether a CAT or a P&P test can more accurately estimate test-takers' severity levels of English speaking anxiety, conditional on their true severity of speaking anxiety. Examining construct validity, the results indicated that a principal component analysis (PCA) of the data revealed that 23 percent of the total variation in the data was accounted for by the first component, which exceeds the 20 percent criterion established by Reckase (1979) for assuming unidimensionality and that Cognitive Speaking Anxiety (CSA) items were not dimensionally separable from the Psychosomatic Speaking Anxiety (PSA) items. The study established a pool of 142 items that can be used for a computerized adaptive speaking anxiety severity test. Results of a CAT simulation indicate that a 20-item simulated fixed-length CAT provides better accuracy than that of a P&P test. (PsycInfo Database Record (c) 2022 APA, all rights reserved)
KW  - korea
KW  - foreign language speaking anxiety
KW  - psychometrics
KW  - computerized adaptive test
KW  - college students
KW  - item response theory
KW  - Anxiety
KW  - College Students
KW  - Foreign Languages
KW  - Psychometrics
KW  - Computerized Assessment
KW  - Item Response Theory
UR  - https://search.ebscohost.com/login.aspx?direct=true&db=psyh&AN=2006-99011-117&lang=de&site=ehost-live
DP  - EBSCOhost
DB  - APA PsycInfo
ER  - 

TY  - CHAP
ID  - 2012-23566-010
AN  - 2012-23566-010
AU  - Pouwer, Frans
AU  - Atlantis, Evan
ED  - Lloyd, Cathy E.
ED  - Pouwer, Frans
ED  - Hermanns, Norbert
T1  - Measuring and assessing depression in people with diabetes: Implications for clinical practice
T2  - Screening for depression and other psychological problems in diabetes: A practical guide.
Y1  - 2013///
SP  - 199
EP  - 209
CY  - New York, NY
PB  - Springer-Verlag Publishing
SN  - 978-0-85729-750-1
SN  - 978-0-85729-751-8
AD  - Atlantis, Evan, Family & Community Health Research Group, School of Nursing and Midwifery, University of Western Sydney, Locked Bag 1797, Penrith, NSW, Australia, 2751
N1  - Accession Number: 2012-23566-010. Partial author list: First Author & Affiliation: Pouwer, Frans; Center of Research on Psychology in Somatic diseases (CoRPS) FSW, Tilburg University, Tilburg, Netherlands. Release Date: 20121001. Correction Date: 20160616. Publication Type: Book (0200), Edited Book (0280). Format Covered: Print. Document Type: Chapter. ISBN: 978-0-85729-750-1, ISBN Paperback; 978-0-85729-751-8, ISBN PDF. Language: EnglishMajor Descriptor: Clinical Practice; Comorbidity; Diabetes; Major Depression; Screening. Minor Descriptor: Anxiety; Diagnosis; Measurement; Psychodiagnostic Typologies. Classification: Clinical Psychological Testing (2224); Psychological & Physical Disorders (3200). Population: Human (10). Intended Audience: Psychology: Professional & Research (PS). Tests & Measures: Well-Being Questionnaire; Diabetes Treatment Satisfaction Questionnaire; Beck Depression Inventory–II DOI: 10.1037/t00742-000; Beck Depression Inventory DOI: 10.1037/t00741-000; Center for Epidemiologic Studies Depression Scale; Patient Health Questionnaire-9 DOI: 10.1037/t06165-000. References Available: Y. Page Count: 11. 
AB  - Diabetes is an increasing global health and economic burden that is frequently associated with and worsened by clinically significant depression or anxiety. This chapter focuses on the history and evolution of the concept of depression and will also describe some of the uncertainties and controversy surrounding psychiatric nosology. Depression as a syndrome of pathological emotions should be diagnosed using operationalized criteria; however, at the same time, depression should not be reified as a discrete construct with absolute boundaries. Depression is often comorbid with anxiety and other mental disorders, which could be clinically important for diabetes. As with other medical syndromes, there is substantial heterogeneity between patients within psychiatric diagnostic groups. Clinically significant depression is defined using cutoff scores for self-rated symptoms using various psychometrics, which correlate reasonably well with operationalized diagnoses. Both methods classify persons into clinically significant dichotomies of depression for diabetes. This chapter considers the implications of these diagnoses for clinical practice and makes suggestions as to the way forward both in terms of the identification of people with comorbid depression and diabetes and for treatment and care. Computerized assessment, for example, with computerized adaptive testing (CAT), can facilitate time-efficient depression screening in busy clinic settings. (PsycINFO Database Record (c) 2016 APA, all rights reserved)
KW  - depression
KW  - diabetes
KW  - anxiety
KW  - screening
KW  - diagnosis
KW  - clinical practice
KW  - comorbid
KW  - psychiatric nosology
KW  - measurement
KW  - Clinical Practice
KW  - Comorbidity
KW  - Diabetes
KW  - Major Depression
KW  - Screening
KW  - Anxiety
KW  - Diagnosis
KW  - Measurement
KW  - Psychodiagnostic Typologies
DO  - 10.1007/978-0-85729-751-8_10
UR  - https://search.ebscohost.com/login.aspx?direct=true&db=psyh&AN=2012-23566-010&lang=de&site=ehost-live
UR  - e.atlantis@uws.edu.au
DP  - EBSCOhost
DB  - APA PsycInfo
ER  - 

TY  - JOUR
ID  - 2019-58840-001
AN  - 2019-58840-001
AU  - Carlozzi, Noelle E.
AU  - Kallen, Michael A.
AU  - Brickell, Tracey A.
AU  - Lange, Rael T.
AU  - Boileau, Nicholas R.
AU  - Tulsky, David
AU  - Hanks, Robin A.
AU  - Massengale, Jill P.
AU  - Nakase-Richardson, Risa
AU  - Ianni, Phillip A.
AU  - Miner, Jennifer A.
AU  - French, Louis M.
AU  - Sander, Angelle M.
T1  - Measuring emotional suppression in caregivers of adults with traumatic brain injury
T3  - Caregivers of Service Members/Veterans and Civilians With Traumatic Brain Injury
JF  - Rehabilitation Psychology
JO  - Rehabilitation Psychology
JA  - Rehabil Psychol
Y1  - 2020/11//
VL  - 65
IS  - 4
SP  - 455
EP  - 470
PB  - American Psychological Association
SN  - 0090-5550
SN  - 1939-1544
SN  - 978-1-4338-9417-6
AD  - Carlozzi, Noelle E., Department of Physical Medicine and Rehabilitation, University of Michigan, North Campus Research Complex, 2800 Plymouth Road, Building NCRC B14, Room G216, Ann Arbor, MI, US, 48109-2800
N1  - Accession Number: 2019-58840-001. PMID: 31580109 Other Journal Title: Psychological Aspects of Disability. Partial author list: First Author & Affiliation: Carlozzi, Noelle E.; Department of Physical Medicine and Rehabilitation, University of Michigan, Ann Arbor, MI, US. Other Publishers: Division 22 of the American Psychological Association; Educational Publishing Foundation; Springer Publishing. Release Date: 20191003. Correction Date: 20201231. Publication Type: Journal (0100), Peer Reviewed Journal (0110). Format Covered: Electronic. Document Type: Journal Article. ISBN: 978-1-4338-9417-6. Language: EnglishMajor Descriptor: Adaptive Testing; Caregivers; Emotions; Suppression (Defense Mechanism); Traumatic Brain Injury. Minor Descriptor: Confirmatory Factor Analysis; Emotional Adjustment; Test Construction; Test Items; Test Reliability; Test Validity; Patient Reported Outcome Measures; Health Related Quality of Life. Classification: Tests & Testing (2220); Home Care & Hospice (3375). Population: Human (10); Male (30); Female (40). Location: US. Age Group: Adulthood (18 yrs & older) (300). Tests & Measures: Patient Reported Outcomes Measurement Information; Caregiver Appraisal Scale; RAND-12-item Health Survey; Mayo Portland Adaptability Inventory-4th Edition; TBI-CareQOL Emotional Suppression Measure. Methodology: Empirical Study; Quantitative Study. Page Count: 16. Issue Publication Date: Nov, 2020. Publication History: First Posted Date: Oct 3, 2019; Accepted Date: Aug 6, 2019; Revised Date: Aug 5, 2019; First Submitted Date: Mar 18, 2019. Copyright Statement: American Psychological Association. 2019. 
AB  - Objective: Caregivers of individuals with traumatic brain injury (TBI) often feel pressure to maintain the appearance that they are emotionally well adjusted, despite feelings to the contrary. Because there are currently no measures examining this construct, this article focuses on the development of a new measure that is specific to caregivers of people with TBI. Design: A total of 533 caregivers of civilians with TBI (n = 218) or service members/veterans (SMVs) with TBI (n = 315) completed 43 emotional suppression items, as well as other patient-reported outcomes and an estimate of the functional ability of the person with TBI. Results: Exploratory and confirmatory factor analyses supported the retention of 25 items. Graded response model (GRM) analyses and differential item functioning (DIF) studies supported the retention of 21 items in the final measure. Expert review and GRM calibration data were used to develop a 6-item static short form (SF) and program a computer adaptive test (CAT). Internal consistency was excellent for both the CAT and SF (reliabilities ≥ 0.91); 3-week test–retest stability was good (all intraclass correlations ≥ 0.89). Convergent validity was supported by moderate associations between TBI-CareQOL Emotional Suppression and related measures (rs from 0.47 to 0.59); discriminant validity was supported by small correlations between Emotional Suppression and positive aspects of caregiving and physical health (rs from 0.14 to 0.28). Known-groups validity was also supported. Conclusions: The new TBI-CareQOL Emotional Suppression CAT and 6-item short form is the first self-report measure of this construct in this population. Our findings suggest this new measure has strong psychometric properties. (PsycInfo Database Record (c) 2020 APA, all rights reserved)
KW  - TBI-CareQOL
KW  - caregiver
KW  - health-related quality of life
KW  - traumatic brain injury
KW  - patient-reported outcome
KW  - Adaptive Testing
KW  - Caregivers
KW  - Emotions
KW  - Suppression (Defense Mechanism)
KW  - Traumatic Brain Injury
KW  - Confirmatory Factor Analysis
KW  - Emotional Adjustment
KW  - Test Construction
KW  - Test Items
KW  - Test Reliability
KW  - Test Validity
KW  - Patient Reported Outcome Measures
KW  - Health Related Quality of Life
U1  - Sponsor: National Institutes of Health, National Institute of Nursing Research, US. Grant: R01NR013658. Recipients: No recipient indicated
U1  - Sponsor: National Center for Advancing Translational Sciences, US. Grant: UL1TR000433. Recipients: No recipient indicated
U1  - Sponsor: General Dynamics Information Technology, Inc.. Grant: DVBIC-SC-14-003; W91YTZ-13-C-0015. Recipients: No recipient indicated
DO  - 10.1037/rep0000291
UR  - https://search.ebscohost.com/login.aspx?direct=true&db=psyh&AN=2019-58840-001&lang=de&site=ehost-live
UR  - ORCID: 0000-0002-9451-0604
UR  - ORCID: 0000-0002-4056-4404
UR  - ORCID: 0000-0003-0439-9429
UR  - carlozzi@med.umich.edu
DP  - EBSCOhost
DB  - APA PsycInfo
ER  - 

TY  - JOUR
ID  - 2022-72948-005
AN  - 2022-72948-005
AU  - Lai, Jin-Shei
AU  - Blackwell, Courtney K.
AU  - Tucker, Carole A.
AU  - Jensen, Sally E.
AU  - Cella, David
T1  - Measuring PROMIS® physical activity and sleep problems in early childhood
JF  - Journal of Pediatric Psychology
JO  - Journal of Pediatric Psychology
JA  - J Pediatr Psychol
Y1  - 2022/06//
VL  - 47
IS  - 5
SP  - 534
EP  - 546
PB  - Oxford University Press
SN  - 0146-8693
SN  - 1465-735X
AD  - Lai, Jin-Shei, Department of Medical Social Sciences, Northwestern University Feinberg School of Medicine, 625 N. Michigan Ave., 21st Floor, Chicago, IL, US, 60611
N1  - Accession Number: 2022-72948-005. PMID: 35552434 Partial author list: First Author & Affiliation: Lai, Jin-Shei; Department of Medical Social Sciences, Northwestern University Feinberg School of Medicine, Chicago, IL, US. Release Date: 20220711. Correction Date: 20221128. Publication Type: Journal (0100), Peer Reviewed Journal (0110). Format Covered: Electronic. Document Type: Journal Article. Language: EnglishMajor Descriptor: Early Childhood Development; Health Behavior; Physical Activity; Sleep; Sleep Wake Disorders. Minor Descriptor: Quality of Life; Test Validity. Classification: Health Psychology Testing (2226); Physical & Somatic Disorders (3290). Population: Human (10); Male (30); Female (40). Location: US. Age Group: Childhood (birth-12 yrs) (100); Preschool Age (2-5 yrs) (160). Tests & Measures: Physical Activity Calibrated Scale; Patient-Reported Outcomes Measurement Information System DOI: 10.1037/t06780-000. Methodology: Empirical Study; Quantitative Study. References Available: Y. Page Count: 13. Issue Publication Date: Jun, 2022. Publication History: First Posted Date: Apr 15, 2022; Accepted Date: Mar 4, 2022; Revised Date: Feb 27, 2022; First Submitted Date: Jun 23, 2021. Copyright Statement: Published by Oxford University Press on behalf of the Society of Pediatric Psychology. All rights reserved. The Author(s). 2022. 
AB  - Objective: Physical activity (PA) and sleep are leading health indicators for individuals of all ages. Monitoring young children’s PA and sleep using psychometrically sound instruments could help facilitate timely interventions to promote healthy development. This article describes the development of the PROMIS® Early Childhood (EC) Parent Report Physical Activity (PA) and Sleep Problems (SP) measures for children aged 1–5 years. Methods: Item pools were generated by interviewing parents, input from content experts, and literature review. Data from a U.S. general population sample were used to determine factor structures of item pools via factor analytic approaches, estimate item parameters via item response theory (IRT) models, and establish norms. Pearson correlations were used to evaluate across-domain associations. Analysis of variance was used and known-groups’ validity of PA and SP by comparing their scores to PROMIS EC Parent Report Global Health: child’s physical, emotional, and mental conditions. Results: Initial item pools consisted of 19 and 26 items for PA and SP, respectively. Factor analyses’ results supported unidimensionality of 5 and 16 items measuring PA and SP, respectively, which were then calibrated using IRT. Norms were established by centering to a probability-based U.S. general population. Computerized adaptive testing algorithms were established. Some analyses supported initial measure validity. Conclusions: The PROMIS EC PA calibrated scale and SP item banks are user-friendly and brief, yet produce precise scores. Both measures enable psychometrically sound assessment of PA behavior and sleep problems. Future studies to comprehensively evaluate the validity of these two measures are warranted. (PsycInfo Database Record (c) 2022 APA, all rights reserved)
KW  - Patient-Reported Outcome Measurement Information System
KW  - health behavior
KW  - infancy and early childhood
KW  - measure validation
KW  - preschool children
KW  - quality of life
KW  - sleep
KW  - Early Childhood Development
KW  - Health Behavior
KW  - Physical Activity
KW  - Sleep
KW  - Sleep Wake Disorders
KW  - Quality of Life
KW  - Test Validity
U1  - Sponsor: National Institutes of Health, Office of The Director, Environmental influences on Child Health Outcomes (ECHO) Program, US. Grant: U24OD023319. Recipients: No recipient indicated
U1  - Sponsor: Office of Behavioral and Social Sciences Research (OBSSR). Recipients: No recipient indicated
DO  - 10.1093/jpepsy/jsac028
UR  - https://search.ebscohost.com/login.aspx?direct=true&db=psyh&AN=2022-72948-005&lang=de&site=ehost-live
UR  - ORCID: 0000-0003-0934-0207
UR  - js-lai@northwestern.edu
DP  - EBSCOhost
DB  - APA PsycInfo
ER  - 

TY  - JOUR
ID  - 2012-04418-001
AN  - 2012-04418-001
AU  - Awad, A. George
AU  - Voruganti, Lakshmi N. P.
T1  - Measuring quality of life in patients with schizophrenia: An update
JF  - PharmacoEconomics
JO  - PharmacoEconomics
JA  - Pharmacoeconomics
Y1  - 2012/03/01/
VL  - 30
IS  - 3
SP  - 183
EP  - 195
PB  - Adis International
SN  - 1170-7690
SN  - 1179-2027
AD  - Awad, A. George, Department of Psychiatry, University of Toronto, 2175 Keele Street, Room 243A, Toronto, ON, Canada, M6M 3Z4
N1  - Accession Number: 2012-04418-001. PMID: 22263841 Partial author list: First Author & Affiliation: Awad, A. George; Department of Psychiatry, University of Toronto, Toronto, ON, Canada. Other Publishers: Springer. Release Date: 20120521. Correction Date: 20180517. Publication Type: Journal (0100), Peer Reviewed Journal (0110). Format Covered: Electronic. Document Type: Journal Article. Language: EnglishMajor Descriptor: Quality of Life; Schizophrenia. Minor Descriptor: Measurement. Classification: Schizophrenia & Psychotic States (3213). Population: Human (10). Tests & Measures: Community Adjustment Form; Satisfaction with Life Domains Scale; Social Performance Schedule; Wisconsin Quality of Life Index; Global Scale of Adaptive Function; Global Health and Quality of Life Measure; Modified Sickness Impact Profile; McMaster Health Index Questionnaire; Social Integration Survey; Personal Evaluation of Transitions in Treatment Scale; Subjective Quality of Life Analysis Questionnaire; Quality of Life Questionnaire—Autonomy Subscale; Life Skills Profile DOI: 10.1037/t12714-000; Quality of Life Checklist DOI: 10.1037/t12943-000; Quality of Life Scale; Drug Attitude Inventory DOI: 10.1037/t12633-000; Schizophrenia Quality of Life Scale DOI: 10.1037/t45282-000; Impact of Weight on Quality of Life--Lite DOI: 10.1037/t54236-000; Quality of Life Questionnaire DOI: 10.1037/t06842-000; SF-36 Health Survey; Quality of Life Interview DOI: 10.1037/t02516-000; Manchester Short Assessment of Quality of Life DOI: 10.1037/t10432-000. References Available: Y. Page Count: 13. Issue Publication Date: Mar 1, 2012. Copyright Statement: All rights reserved. Adis Data Information BV. 2012. 
AB  - In 1997, we published a review in PharmacoEconomics about quality of life (QOL) measurement in patients with schizophrenia. The objective of this article is to provide an update, as well as to revisit the development of the construct of QOL and its measurement as applied to schizophrenia. Since our previous article, there has been significant growth in the number of publications about QOL in schizophrenia. Unfortunately, alongside this significant increase in research interest, a number of concerns have also risen about the limitations and lack of impact the concept of QOL has on clinical care and health-policy decision making. A number of concerns previously outlined (such as lack of consensus on a uniform definition of QOL) continue to be an issue. However, we believe that a uniform definition may not be possible, and instead, it may be preferable to have several definitions, which may enrich the concept and broaden its usefulness. Some of the scales we reviewed in 1997 continue to be in use, while others are now rarely or never used. New scales with better psychometrics have been introduced, but most are without theoretical or conceptual foundation. On the other hand, the field of scaling in general has been changing over the past few years and is moving towards a new approach for scale development, based on item response theory, item banks and computer adaptive testing. Unfortunately, this has not extended to QOL in schizophrenia. There continues to be a dearth of theoretical and conceptual models for QOL in schizophrenia, which seems to create the perception that the construct lacks a good theoretical and scientific foundation. One of the major gaps identified in this review is the recognized lack of impact of QOL measurements on clinical management or policy decision making. The majority of publications continue to focus on measurement rather than what to do with the data. The lack of strategies to integrate QOL data in clinical care, as well as the failure to contribute to policy decisions, particularly in cost analysis or resource allocations, has created the perception that the construct of QOL in schizophrenia is not that useful. It is evident that, for QOL in schizophrenia to regain its promise, researchers must take the ultimate next step beyond measurement: to develop credible strategies for integrating QOL data in clinical practice. Additionally, more focused research is needed to demonstrate the role of QOL, not only as an outcome in itself but also as a contributor to other outcomes, such as adherence to medications, more satisfaction, less resource utilization and so on. Since self-appraisal of QOL does not happen in a vacuum but relates to the total human experience in all its biological, psychosocial and environmental aspects, particular attention must also be focused on important neurobiological dimensions such as affect and cognition. Both are significantly affected by the illness itself and its treatment. (PsycINFO Database Record (c) 2018 APA, all rights reserved)
KW  - quality of life
KW  - schizophrenia
KW  - measurement
KW  - Costs and Cost Analysis
KW  - Delivery of Health Care
KW  - Health Policy
KW  - Humans
KW  - Policy Making
KW  - Psychometrics
KW  - Quality of Life
KW  - Resource Allocation
KW  - Schizophrenia
KW  - Schizophrenic Psychology
KW  - Quality of Life
KW  - Schizophrenia
KW  - Measurement
DO  - 10.2165/11594470-000000000-00000
UR  - https://search.ebscohost.com/login.aspx?direct=true&db=psyh&AN=2012-04418-001&lang=de&site=ehost-live
UR  - gawad@hrrh.on.ca
DP  - EBSCOhost
DB  - APA PsycInfo
ER  - 

TY  - JOUR
ID  - 2012-03055-003
AN  - 2012-03055-003
AU  - Cokely, Edward T.
AU  - Galesic, Mirta
AU  - Schulz, Eric
AU  - Ghazal, Saima
AU  - Garcia-Retamero, Rocio
T1  - Measuring risk literacy: The Berlin Numeracy Test
JF  - Judgment and Decision Making
JO  - Judgment and Decision Making
JA  - Judgm Decis Mak
Y1  - 2012/01//
VL  - 7
IS  - 1
SP  - 25
EP  - 47
PB  - Society for Judgment and Decision Making
SN  - 1930-2975
AD  - Cokely, Edward T.
N1  - Accession Number: 2012-03055-003. Partial author list: First Author & Affiliation: Cokely, Edward T.; Department of Cognitive and Learning Sciences, Michigan Technological University, Houghton, MI, US. Release Date: 20120312. Correction Date: 20211011. Publication Type: Journal (0100), Peer Reviewed Journal (0110). Format Covered: Electronic. Document Type: Journal Article. Language: EnglishMajor Descriptor: Literacy; Mathematical Ability; Psychometrics; Risk Taking; Test Validity. Minor Descriptor: Comprehension. Classification: Tests & Testing (2220); Cognitive Processes (2340). Population: Human (10); Male (30); Female (40). Location: Germany. Age Group: Adulthood (18 yrs & older) (300); Young Adulthood (18-29 yrs) (320); Thirties (30-39 yrs) (340); Middle Age (40-64 yrs) (360). Tests & Measures: Raven's Advanced Progressive Matrices; Spot-a-Word German Vocabulary Test; Achievement Motivation Scale-Revised; Berlin Numeracy Test [Appended] DOI: 10.1037/t45862-000; Cognitive Reflection Test DOI: 10.1037/t66183-000; Test Anxiety Inventory. Methodology: Empirical Study; Quantitative Study. References Available: Y. Page Count: 23. Issue Publication Date: Jan, 2012. 
AB  - We introduce the Berlin Numeracy Test, a new psychometrically sound instrument that quickly assesses statistical numeracy and risk literacy. We present 21 studies (n = 5336) showing robust psychometric discriminability across 15 countries (e.g., Germany, Pakistan, Japan, USA) and diverse samples (e.g., medical professionals, general populations, Mechanical Turk web panels). Analyses demonstrate desirable patterns of convergent validity (e.g., numeracy, general cognitive abilities), discriminant validity (e.g., personality, motivation), and criterion validity (e.g., numerical and non-numerical questions about risk). The Berlin Numeracy Test was found to be the strongest predictor of comprehension of everyday risks (e.g., evaluating claims about products and treatments; interpreting forecasts), doubling the predictive power of other numeracy instruments and accounting for unique variance beyond other cognitive tests (e.g., cognitive reflection, working memory, intelligence). The Berlin Numeracy Test typically takes about three minutes to complete and is available in multiple languages and formats, including a computer adaptive test that automatically scores and reports data to researchers (www.riskliteracy.org). The online forum also provides interactive content for public outreach and education, and offers a recommendation system for test format selection. Discussion centers on construct validity of numeracy for risk literacy, underlying cognitive mechanisms, and applications in adaptive decision support. (PsycInfo Database Record (c) 2021 APA, all rights reserved)
KW  - risk literacy
KW  - Berlin Numeracy Test
KW  - comprehension
KW  - psychometrics
KW  - test validity
KW  - statistical numeracy
KW  - Literacy
KW  - Mathematical Ability
KW  - Psychometrics
KW  - Risk Taking
KW  - Test Validity
KW  - Comprehension
U1  - Sponsor: Ministerio de Ciencia e Innovación, Spain. Grant: PSI2008–02019. Other Details: How to Improve Understanding of Risks about Health. Recipients: No recipient indicated
U1  - Sponsor: Ministerio de Ciencia e Innovación, Spain. Grant: PSI2011–22954. Other Details: Helping Doctors and Their Patients Make Decisions about Health. Recipients: No recipient indicated
UR  - https://search.ebscohost.com/login.aspx?direct=true&db=psyh&AN=2012-03055-003&lang=de&site=ehost-live
UR  - ecokely@mtu.edu
DP  - EBSCOhost
DB  - APA PsycInfo
ER  - 

TY  - JOUR
ID  - 2010-06653-001
AN  - 2010-06653-001
AU  - Hahn, Elizabeth A.
AU  - Cella, David
AU  - Bode, Rita K.
AU  - Hanrahan, Rachel T.
T1  - Measuring social well-being in people with chronic illness
JF  - Social Indicators Research
JO  - Social Indicators Research
JA  - Soc Indic Res
Y1  - 2010/05//
VL  - 96
IS  - 3
SP  - 381
EP  - 401
PB  - Springer
SN  - 0303-8300
SN  - 1573-0921
AD  - Hahn, Elizabeth A., Department of Medical Social Sciences, Feinberg School of Medicine, Northwestern University, 750 N. Lake Shore Drive, Rubloff Building, 9th floor, Chicago, IL, US, 60611
N1  - Accession Number: 2010-06653-001. Partial author list: First Author & Affiliation: Hahn, Elizabeth A.; Department of Medical Social Sciences, Feinberg School of Medicine, Northwestern University, Chicago, IL, US. Release Date: 20100705. Publication Type: Journal (0100), Peer Reviewed Journal (0110). Format Covered: Electronic. Document Type: Journal Article. Language: EnglishConference Information: Annual Conference of the International Society for Quality of Life Research, 12th, Oct, 2005, San Francisco, CA, US. Grant Information: Cella, David. Conference Note: This research was presented in part at the aforementioned conference. Major Descriptor: Chronic Illness; Health; Quality of Life; Social Support; Well Being. Classification: Psychological & Physical Disorders (3200). Population: Human (10). Methodology: Literature Review. References Available: Y. Page Count: 21. Issue Publication Date: May, 2010. Publication History: First Posted Date: May 24, 2009; Accepted Date: May 3, 2009. Copyright Statement: Springer Science+Business Media B.V. 2009. 
AB  - Although social well-being (SWB) is recognized as an integral component of health, it is rarely included in health-related quality of life (HRQL) instruments. Two SWB dimensions were identified by literature review: social support (SWB-SS) and social function (SWB-SF). As part of a larger project to develop item response theory-derived item banks and computerized adaptive testing, we developed and tested items for the SWB banks. Item ratings of three large (n > 600) datasets were conducted by 15 reviewers. Rasch measurement analyses were conducted to initially define item hierarchies. Out of 83 total items, 8 were removed due to model misfit and 8 were removed because of overlapping item content. We then wrote 11 new SWB-SS and 16 new SWB-SF items to fill content gaps, and edited items to improve comprehension and consistency. A total of 94 items (65 SWB-SS, 29 SWB-SF) was administered by computer to 202 cancer patients. Confirmatory factor analyses, Rasch analyses, and evaluations of construct validity were performed. Patients commented favorably on the content of the items and expressed appreciation for attention to this aspect of their HRQL. Using current psychometric standards for unidimensionality, reliability, and content and construct validity, we derived six preliminary item banks for social support (instrumental support, informational support, positive and negative emotional support, positive and negative social companionship) and two for social function (limitations and satisfaction). The empirical construct hierarchy was consistent with clinical observations; e.g., hobbies and leisure activities tended to reflect more limitations, while meeting the needs of family and friends tended to reflect fewer limitations. Optimal care for patients with cancer or other chronic illnesses includes obtaining a complete picture of patients’ physical and psychosocial health status. SWB measures are important since diseases like cancer and their treatment can affect quality of relationships, parental responsibilities, work abilities and social activities. With properly calibrated item banks, it will be possible to precisely and efficiently measure and monitor multiple HRQL dimensions in individual patients, and use their responses to inform care. Qualitative patient feedback and quantitative analyses suggest that it is possible and desirable to include SWB measures in HRQL assessment. (PsycINFO Database Record (c) 2016 APA, all rights reserved)
KW  - social well being
KW  - chronic illness
KW  - health related quality of life
KW  - social support
KW  - Chronic Illness
KW  - Health
KW  - Quality of Life
KW  - Social Support
KW  - Well Being
U1  - Sponsor: National Cancer Institute, US. Grant: R01-CA060068. Recipients: Cella, David (Prin Inv)
DO  - 10.1007/s11205-009-9484-z
UR  - https://search.ebscohost.com/login.aspx?direct=true&db=psyh&AN=2010-06653-001&lang=de&site=ehost-live
UR  - e-hahn@northwestern.edu
DP  - EBSCOhost
DB  - APA PsycInfo
ER  - 

TY  - JOUR
ID  - 2018-60664-004
AN  - 2018-60664-004
AU  - Schwartz, Justin
AU  - Huntington, Noelle
AU  - Toomey, Marisa
AU  - Laverdiere, Michele
AU  - Bevans, Katherine
AU  - Blum, Nathan
AU  - Bridgemohan, Carolyn
T1  - Measuring the involvement in family life of children with autism spectrum disorder: A DBPNet study
JF  - Research in Developmental Disabilities
JO  - Research in Developmental Disabilities
JA  - Res Dev Disabil
Y1  - 2018/12//
VL  - 83
SP  - 18
EP  - 27
PB  - Elsevier Science
SN  - 0891-4222
SN  - 1873-3379
AD  - Schwartz, Justin, 1600 7th Avenue South, Dearth Tower Suite 5602, Birmingham, AL, US, 35233
N1  - Accession Number: 2018-60664-004. PMID: 30092382 Partial author list: First Author & Affiliation: Schwartz, Justin; Division of Developmental Medicine, Boston Children’s Hospital, Boston, MA, US. Release Date: 20190117. Correction Date: 20221128. Publication Type: Journal (0100), Peer Reviewed Journal (0110). Format Covered: Electronic. Document Type: Journal Article. Language: EnglishMajor Descriptor: Autism Spectrum Disorders; Family; Involvement; Measurement; Test Validity. Classification: Developmental Scales & Schedules (2222); Neurodevelopmental & Autism Spectrum Disorders (3250). Population: Human (10); Male (30); Female (40). Location: US. Age Group: Childhood (birth-12 yrs) (100); Preschool Age (2-5 yrs) (160); School Age (6-12 yrs) (180). Tests & Measures: Bayley Scales of Infant and Toddler Development; PROMIS Adult Depression Measure; Differential Ability Scales; Autism Diagnostic Observation Schedule DOI: 10.1037/t54175-000; Childhood Autism Rating Scale DOI: 10.1037/t49458-000; Social Responsiveness Scale DOI: 10.1037/t17260-000; Stanford-Binet Intelligence Scale; Parenting Stress Index DOI: 10.1037/t02445-000; Wechsler Intelligence Scale for Children. Methodology: Empirical Study; Quantitative Study. Supplemental Data: Other Internet. References Available: Y. Page Count: 10. Issue Publication Date: Dec, 2018. Publication History: First Posted Date: Aug 6, 2018; Accepted Date: Jul 28, 2018; Revised Date: Jul 23, 2018; First Submitted Date: Jul 14, 2017. Copyright Statement: All rights reserved. Elsevier Ltd. 2018. 
AB  - Background: Children with Autism Spectrum Disorder (ASD) have social and communication deficits that impair their involvement in family life. No measures of child involvement in the family have been validated for the ASD population. Aim: To evaluate the validity of a measure of Family Involvement (FI) of children ages 5–12 with ASD. Method: Parents of children ages 5–12 with ASD (n = 114) completed FI items from the PROMIS® pediatric Family Relationships item bank in computerized adaptive testing (CAT) format, as well as measures of ASD symptom burden, parenting stress, and parental depression. Medical record review provided child intelligence or developmental quotient. A reference sample (n = 236) closely matching the ASD sample in age and gender was created from the national standardization sample, and underwent a simulated CAT. Results: The CAT precisely and efficiently measured parent-reported FI of children with ASD. Average FI scores were lower among children with ASD (M = 46.3, SD = 7.1) than children in the reference sample (M = 52.5, SD = 9.1). A 'dose response' decrease in FI was observed as ASD severity increased. Increased parenting stress was associated with lower FI. No relationship between FI and child IQ was found. Conclusion: The FI items captured FI among children ages 5–12 with ASD with acceptable precision. Reduced FI among children with ASD, particularly those with higher symptom severity, suggests validity of the items in this population. (PsycInfo Database Record (c) 2022 APA, all rights reserved)
KW  - family involvement
KW  - autism spectrum disorder: validity
KW  - measures
KW  - Autism Spectrum Disorder
KW  - Child
KW  - Depression
KW  - Family Health
KW  - Family Relations
KW  - Female
KW  - Humans
KW  - Intelligence Tests
KW  - Interpersonal Relations
KW  - Male
KW  - Outcome Assessment, Health Care
KW  - Parenting
KW  - Parents
KW  - Quality of Life
KW  - Reproducibility of Results
KW  - Stress, Psychological
KW  - Autism Spectrum Disorders
KW  - Family
KW  - Involvement
KW  - Measurement
KW  - Test Validity
U1  - Sponsor: Sponsor name not included. Grant: UA3MC20218. Other Details: cooperative agreement. Recipients: No recipient indicated
U1  - Sponsor: US Department of Health and Human Services, Health Resources and Services Administration, Maternal and Child Health Bureau, US. Grant: T77MC00012; T77MC00011. Recipients: No recipient indicated
DO  - 10.1016/j.ridd.2018.07.012
UR  - https://search.ebscohost.com/login.aspx?direct=true&db=psyh&AN=2018-60664-004&lang=de&site=ehost-live
UR  - jschwartz@peds.uab.edu
DP  - EBSCOhost
DB  - APA PsycInfo
ER  - 

TY  - JOUR
ID  - 2018-27729-004
AN  - 2018-27729-004
AU  - Gonthier, Corentin
AU  - Aubry, Alexandre
AU  - Bourdin, Béatrice
T1  - Measuring working memory capacity in children using adaptive tasks: Example validation of an adaptive complex span
JF  - Behavior Research Methods
JO  - Behavior Research Methods
JA  - Behav Res Methods
Y1  - 2018/06//
VL  - 50
IS  - 3
SP  - 910
EP  - 921
PB  - Springer
SN  - 1554-351X
SN  - 1554-3528
AD  - Gonthier, Corentin, Universite Rennes 2, CRPCC EA 1285, Rennes, France
N1  - Accession Number: 2018-27729-004. PMID: 28643158 Other Journal Title: Behavior Research Methods & Instrumentation; Behavior Research Methods, Instruments & Computers. Partial author list: First Author & Affiliation: Gonthier, Corentin; Universite Rennes 2, CRPCC EA 1285, Rennes, France. Other Publishers: Psychonomic Society. Release Date: 20180705. Correction Date: 20180913. Publication Type: Journal (0100), Peer Reviewed Journal (0110). Format Covered: Electronic. Document Type: Journal Article. Language: EnglishMajor Descriptor: Adaptive Testing; Childhood Development; Cognition; Short Term Memory; Test Validity. Classification: Tests & Testing (2220); Cognitive & Perceptual Development (2820). Population: Human (10); Male (30); Female (40). Location: France. Age Group: Childhood (birth-12 yrs) (100); School Age (6-12 yrs) (180); Adolescence (13-17 yrs) (200). Tests & Measures: Adaptive Composite Complex Span DOI: 10.1037/t68261-000. Methodology: Empirical Study; Quantitative Study. References Available: Y. Page Count: 12. Issue Publication Date: Jun, 2018. Publication History: First Posted Date: Jun 22, 2017. Copyright Statement: Psychonomic Society, Inc. 2017. 
AB  - Working memory tasks designed for children usually present trials in order of ascending difficulty, with testing discontinued when the child fails a particular level. Unfortunately, this procedure comes with a number of issues, such as decreased engagement from high-ability children, vulnerability of the scores to temporary mind-wandering, and large between-subjects variations in number of trials, testing time, and proactive interference. To circumvent these problems, the goal of the present study was to demonstrate the feasibility of assessing working memory using an adaptive testing procedure. The principle of adaptive testing is to dynamically adjust the level of difficulty as the task progresses to match the participant's ability. We used this method to develop an adaptive complex span task (the ACCES) comprising verbal and visuo-spatial subtests. The task presents a fixed number of trials to all participants, allows for partial credit scoring, and can be used with children regardless of ability level. The ACCES demonstrated satisfying psychometric properties in a sample of 268 children aged 8–13 years, confirming the feasibility of using adaptive tasks to measure working memory capacity in children. A free-to-use implementation of the ACCES is provided. (PsycINFO Database Record (c) 2018 APA, all rights reserved)
KW  - Adaptive Composite Complex Span
KW  - Working memory capacity
KW  - Complex span task
KW  - Computerized adaptive testing
KW  - French validation
KW  - Child cognition
KW  - Adaptive Testing
KW  - Childhood Development
KW  - Cognition
KW  - Short Term Memory
KW  - Test Validity
U1  - Sponsor: Regional Council of Picardie (RCP). Recipients: No recipient indicated
U1  - Sponsor: European Regional Development Fund (ERDF), Europe. Recipients: No recipient indicated
DO  - 10.3758/s13428-017-0916-4
UR  - https://search.ebscohost.com/login.aspx?direct=true&db=psyh&AN=2018-27729-004&lang=de&site=ehost-live
UR  - ORCID: 0000-0002-4162-2557
UR  - corentin.gonthier@univ-rennes2.fr
DP  - EBSCOhost
DB  - APA PsycInfo
ER  - 

TY  - JOUR
ID  - 2018-56404-008
AN  - 2018-56404-008
AU  - Wang, Chao
AU  - Lu, Hong
T1  - Mediating effects of individuals' ability levels on the relationship of reflective-impulsive cognitive style and item response time in CAT
JF  - Journal of Educational Technology & Society
JO  - Journal of Educational Technology & Society
JA  - J Educ Techno Soc
Y1  - 2018/10//
VL  - 21
IS  - 4
SP  - 89
EP  - 99
PB  - International Forum of Educational Technology & Society
SN  - 1176-3647
SN  - 1436-4522
AD  - Lu, Hong
N1  - Accession Number: 2018-56404-008. Partial author list: First Author & Affiliation: Wang, Chao; Department of Educational Technology, Shandong Normal University, China. Release Date: 20190121. Publication Type: Journal (0100), Peer Reviewed Journal (0110). Format Covered: Electronic. Document Type: Journal Article. Language: EnglishMajor Descriptor: Adaptive Testing; Cognitive Ability; Cognitive Style; Item Response Theory. Minor Descriptor: Technology. Classification: Classroom Dynamics & Student Adjustment & Attitudes (3560). Population: Human (10); Male (30); Female (40). Location: China. Age Group: Adulthood (18 yrs & older) (300); Young Adulthood (18-29 yrs) (320). Tests & Measures: Matching Familiar Figures Test-20 DOI: 10.1037/t04276-000. Methodology: Empirical Study; Quantitative Study. References Available: Y. Page Count: 11. Issue Publication Date: Oct, 2018. Publication History: Accepted Date: Apr 17, 2017; Revised Date: Apr 13, 2017; First Submitted Date: Dec 9, 2016. 
AB  - This study focused on the effect of examinees’ ability levels on the relationship between Reflective-Impulsive (RI) cognitive style and item response time in computerized adaptive testing (CAT). The total of 56 students majoring in Educational Technology from Shandong Normal University participated in this study, and their RI cognitive styles were diagnosed using the Matching Familiar Figures Test-20 (MFFT-20). Examinees’ ability values and average item response time were recorded by the computerized adaptive testing system. Then mediation analysis was implemented and the findings revealed that there was direct and indirect effect between RI cognitive style and item response time in CAT. What’s more, RI cognitive style also directly affected the ability levels, and then the ability levels impacted on item response time. So, examinee’s ability level was a partly mediator between RI cognitive style and item response time. Furthermore, RI cognitive style of the examinees might also be diagnosed according to ability values and average item response time. The relevant research and implications were further discussed. (PsycINFO Database Record (c) 2019 APA, all rights reserved)
KW  - cognitive style
KW  - computerized adaptive testing
KW  - item response time
KW  - technology
KW  - Adaptive Testing
KW  - Cognitive Ability
KW  - Cognitive Style
KW  - Item Response Theory
KW  - Technology
U1  - Sponsor: Chinese National Education Science Foundation, China. Grant: BBA170069. Recipients: No recipient indicated
UR  - https://search.ebscohost.com/login.aspx?direct=true&db=psyh&AN=2018-56404-008&lang=de&site=ehost-live
UR  - sdnulh@163.com
UR  - wxcstudent@126.com
DP  - EBSCOhost
DB  - APA PsycInfo
ER  - 

TY  - JOUR
ID  - 2015-10018-006
AN  - 2015-10018-006
AU  - Buckley, Peter F.
T1  - Mental health in the medical setting: Service delivery, workforce needs, and emerging best practices
JF  - Psychiatric Clinics of North America
JO  - Psychiatric Clinics of North America
JA  - Psychiatr Clin North Am
Y1  - 2015/03//
VL  - 38
IS  - 1
SP  - xi
EP  - xii
PB  - Elsevier Science
SN  - 0193-953X
SN  - 1558-3147
AD  - Buckley, Peter F., Medical College of Georgia, Georgia Regents University, 1120 15th Street, AA-1002, Augusta, GA, US, 30912
N1  - Accession Number: 2015-10018-006. PMID: 25725576 Partial author list: First Author & Affiliation: Buckley, Peter F.; Medical College of Georgia, Georgia Regents University, Augusta, GA, US. Release Date: 20150330. Correction Date: 20170403. Publication Type: Journal (0100), Peer Reviewed Journal (0110). Format Covered: Electronic. Document Type: Journal Article. Language: EnglishMajor Descriptor: Anxiety Disorders; Health Care Costs; Health Screening; Major Depression; Psychological Assessment. Minor Descriptor: Costs and Cost Analysis; Health Care Delivery; Health Care Services; Health Service Needs; Best Practices. Classification: Clinical Psychological Testing (2224). Population: Human (10). Page Count: 2. Issue Publication Date: Mar, 2015. Copyright Statement: All rights reserved. Elsevier Inc. 2015. 
AB  - Depression and anxiety disorders are common conditions with significant morbidity. Many screening tools of varying length have been well validated for these conditions in the office-based setting. Novel instruments, including Internet-based and computerized adaptive testing, may be promising tools in the future. The best evidence for cost-effectiveness currently is for screening of major depression linked with the collaborative care model for treatment. Data are not conclusive regarding comparative cost-effectiveness of screening for multiple conditions at once or for other conditions. This article reviews screening tools for depression and anxiety disorders in the ambulatory setting. (PsycINFO Database Record (c) 2017 APA, all rights reserved)
KW  - major depression
KW  - anxiety disorders
KW  - screening tools
KW  - collaborative care model
KW  - Health Services Accessibility
KW  - Humans
KW  - Mental Health Services
KW  - Practice Guidelines as Topic
KW  - Anxiety Disorders
KW  - Health Care Costs
KW  - Health Screening
KW  - Major Depression
KW  - Psychological Assessment
KW  - Costs and Cost Analysis
KW  - Health Care Delivery
KW  - Health Care Services
KW  - Health Service Needs
KW  - Best Practices
DO  - 10.1016/j.psc.2014.11.010
UR  - https://search.ebscohost.com/login.aspx?direct=true&db=psyh&AN=2015-10018-006&lang=de&site=ehost-live
UR  - PBuckley@gru.edu
DP  - EBSCOhost
DB  - APA PsycInfo
ER  - 

TY  - JOUR
ID  - 2019-08366-001
AN  - 2019-08366-001
AU  - Wang, Chun
AU  - Weiss, David J.
AU  - Su, Shiyang
T1  - Modeling response time and responses in multidimensional health measurement
JF  - Frontiers in Psychology
JO  - Frontiers in Psychology
JA  - Front Psychol
Y1  - 2019/01/29/
VL  - 10
PB  - Frontiers Media S.A.
SN  - 1664-1078
AD  - Wang, Chun
N1  - Accession Number: 2019-08366-001. PMID: 30761036 Partial author list: First Author & Affiliation: Wang, Chun; College of Education, University of Washington, Seattle, WA, US. Other Publishers: Frontiers Research Foundation. Release Date: 20190228. Publication Type: Journal (0100), Peer Reviewed Journal (0110). Format Covered: Electronic. Document Type: Journal Article. Language: EnglishMajor Descriptor: Health Behavior; Reaction Time; Physical Health Assessment; Measurement Models. Minor Descriptor: Item Response Theory. Classification: Health Psychology Testing (2226); Health Psychology & Medicine (3360). Population: Human (10); Male (30); Female (40); Inpatient (50). Age Group: Adulthood (18 yrs & older) (300); Young Adulthood (18-29 yrs) (320); Thirties (30-39 yrs) (340); Middle Age (40-64 yrs) (360); Aged (65 yrs & older) (380). Tests & Measures: Activity Measure for Post-Acute Care; Functional Independence Measure DOI: 10.1037/t02229-000. Methodology: Empirical Study; Mathematical Model; Quantitative Study. Supplemental Data: Other Internet. References Available: Y. ArtID: 51. Issue Publication Date: Jan 29, 2019. Publication History: First Posted Date: Jan 29, 2019; Accepted Date: Jan 9, 2019; First Submitted Date: Aug 22, 2018. Copyright Statement: This is an open-access article distributed under the terms of the Creative Commons Attribution License (CC BY). The use, distribution or reproduction in other forums is permitted, provided the original author(s) and the copyright owner(s) are credited and that the original publication in this journal is cited, in accordance with accepted academic practice. No use, distribution or reproduction is permitted which does not comply with these terms. Wang, Weiss and Su. 2019. 
AB  - This study explored calibrating a large item bank for use in multidimensional health measurement with computerized adaptive testing, using both item responses and response time (RT) information. The Activity Measure for Post-Acute Care is a patient-reported outcomes measure comprised of three correlated scales (Applied Cognition, Daily Activities, and Mobility). All items from each scale are Likert type, so that a respondent chooses a response from an ordered set of four response options. The most appropriate item response theory model for analyzing and scoring these items is the multidimensional graded response model (MGRM). During the field testing of the items, an interviewer read each item to a patient and recorded, on a tablet computer, the patient's responses and the software recorded RTs. Due to the large item bank with over 300 items, data collection was conducted in four batches with a common set of anchor items to link the scale. van der Linden's (2007) hierarchical modeling framework was adopted. Several models, with or without interviewer as a covariate and with or without interaction between interviewer and items, were compared for each batch of data. It was found that the model with the interaction between interviewer and item, when the interaction effect was constrained to be proportional, fit the data best. Therefore, the final hierarchical model with a lognormal model for RT and the MGRM for response data was fitted to all batches of data via a concurrent calibration. Evaluation of parameter estimates revealed that (1) adding response time information did not affect the item parameter estimates and their standard errors significantly; (2) adding response time information helped reduce the standard error of patients' multidimensional latent trait estimates, but adding interviewer as a covariate did not result in further improvement. Implications of the findings for follow up adaptive test delivery design are discussed. (PsycINFO Database Record (c) 2019 APA, all rights reserved)
KW  - response time
KW  - hierarchical model
KW  - health measurement
KW  - multidimensional graded response model
KW  - item response theory (IRT)
KW  - Health Behavior
KW  - Reaction Time
KW  - Physical Health Assessment
KW  - Measurement Models
KW  - Item Response Theory
U1  - Sponsor: National Institutes of Health, Eunice Kennedy Shriver National Institute of Child Health and Human Development, US. Grant: R01HD079439. Other Details: Mayo Clinic in Rochester, Minnesota through a subcontract to the University of Minnesota. Recipients: No recipient indicated
DO  - 10.3389/fpsyg.2019.00051
UR  - https://search.ebscohost.com/login.aspx?direct=true&db=psyh&AN=2019-08366-001&lang=de&site=ehost-live
UR  - wang4066@uw.edu
DP  - EBSCOhost
DB  - APA PsycInfo
ER  - 

TY  - THES
ID  - 2001-95011-131
AN  - 2001-95011-131
AU  - Tseng, Fen-Lan
T1  - Multidimensional adaptive testing using the weighted likelihood estimation
JF  - Dissertation Abstracts International Section A: Humanities and Social Sciences
JO  - Dissertation Abstracts International Section A: Humanities and Social Sciences
Y1  - 2001/06//
VL  - 61
IS  - 12-A
SP  - 4746
EP  - 4746
PB  - ProQuest Information & Learning
SN  - 0419-4209
N1  - Accession Number: 2001-95011-131. Other Journal Title: Dissertation Abstracts International. Partial author list: First Author & Affiliation: Tseng, Fen-Lan; U Pittsburgh, US. Release Date: 20011114. Correction Date: 20221128. Publication Type: Dissertation Abstract (0400). Format Covered: Electronic. Document Type: Dissertation. Language: EnglishMajor Descriptor: Adaptive Testing; Statistical Estimation. Classification: Educational & School Psychology (3500). Population: Human (10). Age Group: Adulthood (18 yrs & older) (300). Methodology: Empirical Study. Page Count: 1. 
AB  - This study extended Warm's (1989) weighted likelihood estimation (WLE) to a multidimensional computerized adaptive test (MCAT) setting. WLE was compared with the maximum likelihood estimation (MLE), expected a posteriori (EAP), and maximum a posteriori (MAP) using a three-dimensional 3PL IRT model under a variety of computerized adaptive testing conditions. The dependent variables included bias, standard error of ability estimates (SE), square root of mean square error (RMSE), and test information. The independent variables were ability estimation methods, intercorrelation levels between dimensions, multidimensional structures, and ability combinations. Simulation results were presented in terms of descriptive statistics, such as figures and tables. In addition, inferential procedures were used to analyze bias by conceptualizing this Monte Carlo study as a statistical sampling experiment. The results of this study indicate that WLE and the other three estimation methods yield significantly more accurate ability estimates under an approximate simple test structure with one dominant dimension and several secondary dimensions. All four estimation methods, especially WLE, yield very large SEs when a three equally dominant multidimensional structure was employed. Consistent with previous findings based on unidimensional IRT model, MLE and WLE are less biased in the extreme of the ability scale; MLE and WLE yield larger SEs than the Bayesian methods; test information-based SEs underestimate actual SEs for both MLE and WLE in MCAT situations, especially at shorter test lengths; WLE reduced the bias of MLE under the approximate simple structure; test information-based SEs underestimates the actual SEs of MLE and WLE estimators in the MCAT conditions, similar to the findings of Warm (1989) in the unidimensional case. The results from the MCAT simulations did show some advantages of WLE in reducing the bias of MLE under the approximate simple structure with a fixed test length of 50 items, which was consistent with the previous research findings based on different unidimensional models. It is clear from the current results that all four methods perform very poorly when the multidimensional structures with multiple dominant factors were employed. More research efforts are urged to investigate systematically how different multidimensional structures affect the accuracy and reliability of ability estimation. Based on the simulated results in this study, there is no significant effect found on the ability estimation from the intercorrelation between dimensions. (PsycInfo Database Record (c) 2022 APA, all rights reserved)
KW  - nultidimensional adaptive testing
KW  - weighted likelihood estimation
KW  - Adaptive Testing
KW  - Statistical Estimation
UR  - https://search.ebscohost.com/login.aspx?direct=true&db=psyh&AN=2001-95011-131&lang=de&site=ehost-live
DP  - EBSCOhost
DB  - APA PsycInfo
ER  - 

TY  - JOUR
ID  - 2006-11439-001
AN  - 2006-11439-001
AU  - Petersen, Morten Aa.
AU  - Groenvold, Mogens
AU  - Aaronson, Neil
AU  - Fayers, Peter
AU  - Sprangers, Mirjam
AU  - Bjorner, Jakob B.
T1  - Multidimensional computerized adaptive testing of the EORTC QLQ-C30: Basic developments and evaluations
JF  - Quality of Life Research: An International Journal of Quality of Life Aspects of Treatment, Care & Rehabilitation
JO  - Quality of Life Research: An International Journal of Quality of Life Aspects of Treatment, Care & Rehabilitation
JA  - Qual Life Res
Y1  - 2006/04//
VL  - 15
IS  - 3
SP  - 315
EP  - 329
PB  - Springer
SN  - 0962-9343
SN  - 1573-2649
AD  - Petersen, Morten Aa., Research Unit, Department of Palliative Medicine, Bispebjerg Hospital, NV, DK-2400, Copenhagen, Denmark
N1  - Accession Number: 2006-11439-001. PMID: 16547770 Partial author list: First Author & Affiliation: Petersen, Morten Aa.; Research Unit, Department of Palliative Medicine, Bispebjerg Hospital, Copenhagen, Denmark. Institutional Authors: The European Organisation for Research and Treatment of Cancer Quality of Life Group. Release Date: 20061106. Correction Date: 20221128. Publication Type: Journal (0100), Peer Reviewed Journal (0110). Format Covered: Electronic. Document Type: Journal Article. Language: EnglishMajor Descriptor: Adaptive Testing; Multidimensional Scaling; Neoplasms; Quality of Life. Minor Descriptor: Fatigue; Physical Agility. Classification: Clinical Psychological Testing (2224); Physical & Somatic Disorders (3290). Population: Human (10); Male (30); Female (40). Age Group: Adulthood (18 yrs & older) (300); Young Adulthood (18-29 yrs) (320); Thirties (30-39 yrs) (340); Middle Age (40-64 yrs) (360); Aged (65 yrs & older) (380). Tests & Measures: European Organization for Research and Treatment of Cancer Quality of Life Questionnaire. Methodology: Empirical Study; Quantitative Study. References Available: Y. Page Count: 15. Issue Publication Date: Apr, 2006. 
AB  - Objective: Self-report questionnaires are widely used to measure health-related quality of life (HRQOL). Ideally, such questionnaires should be adapted to the individual patient and at the same time scores should be directly comparable across patients. This may be achieved using computerized adaptive testing (CAT). Usually, CAT is carried out for a single domain at a time. However, many HRQOL domains are highly correlated. Multidimensional CAT may utilize these correlations to improve measurement efficiency. We investigated the possible advantages and difficulties of multidimensional CAT. Study Design and Setting: We evaluated multidimensional CAT of three scales from the EORTC QLQ-C30: the physical functioning, emotional functioning, and fatigue scales. Analyses utilised a database with 2958 European cancer patients. Results: It was possible to obtain scores for the three domains with five to seven items administered using multidimensional CAT that were very close to the scores obtained using all 12 items and with no or little loss of measurement precision. Conclusion: The findings suggest that multidimensional CAT may significantly improve measurement precision and efficiency and encourage further research into multidimensional CAT. Particularly, the estimation of the model underlying the multidimensional CAT and the conceptual aspects need further investigations. (PsycInfo Database Record (c) 2022 APA, all rights reserved)
KW  - multidimensional computerized adaptive testing
KW  - health related quality of life
KW  - cancer patients
KW  - physical functioning scale
KW  - emotional functioning scale
KW  - fatigue scale
KW  - Adult
KW  - Female
KW  - Health Status
KW  - Humans
KW  - Male
KW  - Middle Aged
KW  - Quality of Life
KW  - Self Disclosure
KW  - Surveys and Questionnaires
KW  - User-Computer Interface
KW  - Adaptive Testing
KW  - Multidimensional Scaling
KW  - Neoplasms
KW  - Quality of Life
KW  - Fatigue
KW  - Physical Agility
U1  - Sponsor: European Organisation for Research and Treatment of Cancer Quality of Life Group, Europe. Recipients: No recipient indicated
DO  - 10.1007/s11136-005-3214-z
UR  - https://search.ebscohost.com/login.aspx?direct=true&db=psyh&AN=2006-11439-001&lang=de&site=ehost-live
UR  - ORCID: 0000-0003-4778-1513
UR  - map01@bbh.hosp.dk
DP  - EBSCOhost
DB  - APA PsycInfo
ER  - 

TY  - JOUR
ID  - 2013-37745-006
AN  - 2013-37745-006
AU  - Wang, Chun
T1  - Mutual information item selection method in cognitive diagnostic computerized adaptive testing with short test length
JF  - Educational and Psychological Measurement
JO  - Educational and Psychological Measurement
JA  - Educ Psychol Meas
Y1  - 2013/12//
VL  - 73
IS  - 6
SP  - 1017
EP  - 1035
PB  - Sage Publications
SN  - 0013-1644
SN  - 1552-3888
AD  - Wang, Chun, Department of Psychology, University of Minnesota, 75 East River Road, Minneapolis, MN, US, 55455
N1  - Accession Number: 2013-37745-006. Partial author list: First Author & Affiliation: Wang, Chun; University of Minnesota, Minneapolis, MN, US. Release Date: 20131111. Correction Date: 20190211. Publication Type: Journal (0100), Peer Reviewed Journal (0110). Format Covered: Electronic. Document Type: Journal Article. Language: EnglishMajor Descriptor: Cognition; Computerized Assessment. Minor Descriptor: Algorithms. Classification: Psychometrics & Statistics & Methodology (2200). Population: Human (10). Methodology: Empirical Study; Mathematical Model; Quantitative Study; Scientific Simulation. References Available: Y. Page Count: 19. Issue Publication Date: Dec, 2013. Copyright Statement: The Author(s). 2013. 
AB  - Cognitive diagnostic computerized adaptive testing (CD-CAT) purports to combine the strengths of both CAT and cognitive diagnosis. Cognitive diagnosis models aim at classifying examinees into the correct mastery profile group so as to pinpoint the strengths and weakness of each examinee whereas CAT algorithms choose items to determine those strengths and weakness as efficiently as possible. Most of the existing CD-CAT item selection algorithms are evaluated when test length is relatively long whereas several applications of CD-CAT, such as in interim assessment, require an item selection algorithm that is able to accurately recover examinees’ mastery profile with short test length. In this article, we introduce the mutual information item selection method in the context of CD-CAT and then provide a computationally easier formula to make the method more amenable in real time. Mutual information is then evaluated against common item selection methods, such as Kullback–Leibler information, posterior weighted Kullback–Leibler information, and Shannon entropy. Based on our simulations, mutual information consistently results in nearly the highest attribute and pattern recovery rate in more than half of the conditions.We conclude by discussing how the number of attributes, Q-matrix structure, correlations among the attributes, and item quality affect estimation accuracy. (PsycINFO Database Record (c) 2019 APA, all rights reserved)
KW  - item selection
KW  - cognitive diagnostic computerized adaptive testing
KW  - algorithms
KW  - Cognition
KW  - Computerized Assessment
KW  - Algorithms
DO  - 10.1177/0013164413498256
UR  - https://search.ebscohost.com/login.aspx?direct=true&db=psyh&AN=2013-37745-006&lang=de&site=ehost-live
UR  - wang4066@umn.edu
DP  - EBSCOhost
DB  - APA PsycInfo
ER  - 

TY  - JOUR
ID  - 2020-25148-012
AN  - 2020-25148-012
AU  - Surkar, Swati M.
T1  - Neural activation within the sensorimotor cortices during bimanual tasks in children with unilateral cerebral palsy
JF  - Developmental Medicine & Child Neurology
JO  - Developmental Medicine & Child Neurology
JA  - Dev Med Child Neurol
Y1  - 2020/05//
VL  - 62
IS  - 5
SP  - 547
EP  - 548
PB  - Wiley-Blackwell Publishing Ltd.
SN  - 0012-1622
SN  - 1469-8749
N1  - Accession Number: 2020-25148-012. PMID: 32100282 Partial author list: First Author & Affiliation: Surkar, Swati M.; Department of Physical Therapy, East Carolina University, Greenville, NC, US. Other Publishers: Mac Keith Press. Release Date: 20200423. Publication Type: Journal (0100), Peer Reviewed Journal (0110). Format Covered: Electronic. Document Type: Comment/Reply. Language: EnglishMajor Descriptor: Cerebral Palsy; Childhood Development; Electromyography; Muscles; Spectroscopy. Minor Descriptor: Experimentation; Hemiplegia; Pediatrics. Classification: Neurological Disorders & Brain Damage (3297). Page Count: 2. Issue Publication Date: May, 2020. Copyright Statement: Mac Keith Press. 2019. 
AB  - Comments on an article by A. C. de Campos et al. (see record [rid]2020-09950-001[/rid]). Campos et al. investigated the sensorimotor cortical activation during symmetric and asymmetric bimanual tasks in children with unilateral cerebral palsy (CP) using functional near-infrared spectroscopy (fNIRS). The main finding of the study suggests greater activation in bilateral sensorimotor cortices in children with unilateral CP compared to age-matched typically developing children. Moreover, the sensorimotor cortical activation was higher during asymmetric versus symmetric tasks. The finding of the study reveals correlation between greater activation within the sensorimotor cortices and synchronized electromyography (EMG) activity of flexor carpi radialis muscle. More synchronized EMG activity during asymmetric tasks potentially indicates reduced selective control of the arm; however, further investigation should consider the severity of hemiplegia and presence or absence of mirror movements, since these factors might significantly affect the correlation. Third, the study results indicate strong correlation between laterality index shifted to non-lesioned hemisphere and better function on the Pediatric Evaluation of Disability Inventory computer adaptive test (PEDI-CAT). These results contradict the evidence that collectively shows contra-lesional shift of laterality index as a marker of poor hand function. (PsycInfo Database Record (c) 2020 APA, all rights reserved)
KW  - neural activation
KW  - sensorimotor cortices
KW  - bimanual tasks
KW  - children
KW  - unilateral cerebral palsy
KW  - Cerebral Palsy
KW  - Childhood Development
KW  - Electromyography
KW  - Muscles
KW  - Spectroscopy
KW  - Experimentation
KW  - Hemiplegia
KW  - Pediatrics
DO  - 10.1111/dmcn.14505
UR  - https://search.ebscohost.com/login.aspx?direct=true&db=psyh&AN=2020-25148-012&lang=de&site=ehost-live
UR  - ORCID: 0000-0002-3334-4892
DP  - EBSCOhost
DB  - APA PsycInfo
ER  - 

TY  - JOUR
ID  - 2014-01828-001
AN  - 2014-01828-001
AU  - Hahn, Elizabeth A.
AU  - DeWalt, Darren A.
AU  - Bode, Rita K.
AU  - Garcia, Sofia F.
AU  - DeVellis, Robert F.
AU  - Correia, Helena
AU  - Cella, David
T1  - New English and Spanish social health measures will facilitate evaluating health determinants
JF  - Health Psychology
JO  - Health Psychology
JA  - Health Psychol
Y1  - 2014/05//
VL  - 33
IS  - 5
SP  - 490
EP  - 499
PB  - American Psychological Association
SN  - 0278-6133
SN  - 1930-7810
AD  - Hahn, Elizabeth A., Department of Medical Social Sciences, Northwestern University Feinberg School of Medicine, 633 N. St. Clair St., Suite, 1900, Chicago, IL, US, 60611
N1  - Accession Number: 2014-01828-001. PMID: 24447188 Partial author list: First Author & Affiliation: Hahn, Elizabeth A.; Department of Medical Social Sciences, Northwestern University Feinberg School of Medicine, Chicago, IL, US. Institutional Authors: PROMIS Cooperative Group. Other Publishers: Lawrence Erlbaum Associates. Release Date: 20140120. Correction Date: 20200713. Publication Type: Journal (0100), Peer Reviewed Journal (0110). Format Covered: Electronic. Document Type: Journal Article. Language: EnglishGrant Information: Cella, David. Major Descriptor: Health; Psychometrics; Self-Report; Social Interaction; Latinos/Latinas. Minor Descriptor: Interpersonal Relationships; Social Health. Classification: Tests & Testing (2220); Group & Interpersonal Processes (3020). Population: Human (10). Location: US. Tests & Measures: FACT-GP, Version 4; SF-36 Health Survey; Short Acculturation Scale for Hispanics DOI: 10.1037/t03842-000; Patient-Reported Outcomes Measurement Information System DOI: 10.1037/t06780-000. Methodology: Empirical Study; Qualitative Study; Quantitative Study. Supplemental Data: Tables and Figures Internet. References Available: Y. Page Count: 10. Issue Publication Date: May, 2014. Publication History: First Posted Date: Jan 20, 2014; Accepted Date: Nov 15, 2013; Revised Date: Nov 15, 2013; First Submitted Date: Jan 25, 2013. Copyright Statement: American Psychological Association. 2014. 
AB  - Objective: To develop psychometrically sound, culturally relevant, and linguistically equivalent English and Spanish self-report measures of social health guided by a comprehensive conceptual model and applicable across chronic illnesses. Methods: The Patient-Reported Outcomes Measurement Information System (PROMIS) Social Health Workgroup implemented a mixed methods approach to evaluate earlier results (v1.0); expand and refine domain definitions and items; translate items into Spanish; and obtain qualitative feedback. Computer-based and paper/pencil questionnaire administration was conducted with a variety of U.S. respondent samples during 2009–2012. Analyses included exploratory factor analysis (EFA), confirmatory factor analysis (CFA), two-parameter logistic item response theory (IRT) modeling, evaluation of differential item functioning (DIF), and evaluation of criterion and construct validity. Results: Qualitative feedback supported the conceptualization of the Social Health domain framework (Social Function and Social Relationships subcomponents). Validation testing participants (n = 2,208 English; n = 644 Spanish) were diverse in terms of gender, age, education, and ethnicity/race. EFA, CFA, and IRT identified 7 unidimensional factors with good model fit. There was no DIF by language, and good evidence of criterion and construct validity. Conclusions: PROMIS English and Spanish language instruments (v2.0), including computer-adaptive tests and fixed-length short forms, are publicly available for assessment of Social Function (Ability to Participate in Social Roles and Activities, and Satisfaction with Social Roles and Activities) and Social Relationships (Companionship; Emotional, Informational and Instrumental Support; and Social Isolation). Measures of social health will play a key role in applications that use ecologic (or determinants of health) models that emphasize how patients’ social environments influence their health. (PsycInfo Database Record (c) 2020 APA, all rights reserved)
KW  - Hispanic Americans
KW  - patient-reported outcomes
KW  - psychometrics
KW  - social function
KW  - social health
KW  - social relationships
KW  - Patient-Reported Outcomes Measurement Information System
KW  - Adolescent
KW  - Adult
KW  - Aged
KW  - Chronic Disease
KW  - Cultural Competency
KW  - Factor Analysis, Statistical
KW  - Female
KW  - Humans
KW  - Language
KW  - Male
KW  - Middle Aged
KW  - Models, Theoretical
KW  - Psychometrics
KW  - Qualitative Research
KW  - Reproducibility of Results
KW  - Self Report
KW  - Social Determinants of Health
KW  - Young Adult
KW  - Health
KW  - Psychometrics
KW  - Self-Report
KW  - Social Interaction
KW  - Latinos/Latinas
KW  - Interpersonal Relationships
KW  - Social Health
U1  - Sponsor: National Institutes of Health, US. Grant: U01AR52177. Other Details: Funding for PROMIS; cooperative agreement to Statistical Coordinating Center (Evanston Northwestern Healthcare).. Recipients: Cella, David (Prin Inv)
U1  - Sponsor: National Institutes of Health, US. Grant: U01AR52186. Other Details: Funding for PROMIS; cooperative agreement to Primary Research Site (Duke University).. Recipients: Weinfurt, Kevin (Prin Inv)
U1  - Sponsor: National Institutes of Health, US. Grant: U01AR52181. Other Details: Funding for PROMIS; cooperative agreement to Primary Research Site (University of North Carolina).. Recipients: DeWalt, Darren (Prin Inv)
U1  - Sponsor: National Institutes of Health, US. Grant: U01AR52155. Other Details: Funding for PROMIS; cooperative agreement to Primary Research Site (University of Pittsburgh).. Recipients: Pilkonis, Paul A. (Prin Inv)
U1  - Sponsor: National Institutes of Health, US. Grant: U01AR52158. Other Details: Funding for PROMIS; cooperative agreement to Primary Research Site (Stanford University).. Recipients: Fries, James (Prin Inv)
U1  - Sponsor: National Institutes of Health, US. Grant: U01AR52170. Other Details: Funding for PROMIS; cooperative agreement to Primary Research Site (Stony Brook University).. Recipients: Stone, Arthur (Prin Inv)
U1  - Sponsor: National Institutes of Health, US. Grant: U01AR52171. Other Details: Funding for PROMIS; cooperative agreement to Primary Research Site (University of Washington).. Recipients: Amtmann, Dagmar (Prin Inv)
DO  - 10.1037/hea0000055
L3  - 10.1037/hea0000055.supp (Supplemental)
UR  - https://search.ebscohost.com/login.aspx?direct=true&db=psyh&AN=2014-01828-001&lang=de&site=ehost-live
UR  - ORCID: 0000-0003-2270-751X
UR  - e-hahn@northwestern.edu
DP  - EBSCOhost
DB  - APA PsycInfo
ER  - 

TY  - JOUR
ID  - 2016-45506-013
AN  - 2016-45506-013
AU  - Guo, Lei
AU  - Zheng, Chanjin
AU  - Bian, Yufang
AU  - Song, Naiqing
AU  - Xia, Lingxiang
T1  - New item selection methods in cognitive diagnostic computerized adaptive testing: Combining item discrimination indices
JF  - Acta Psychologica Sinica
JO  - Acta Psychologica Sinica
JA  - Xin Li Xue Bao
Y1  - 2016/07//
VL  - 48
IS  - 7
SP  - 903
EP  - 914
PB  - Science Press
SN  - 0439-755X
N1  - Accession Number: 2016-45506-013. Partial author list: First Author & Affiliation: Guo, Lei; Faculty of Psychology, Southwest University, Chongqing, China. Release Date: 20170427. Correction Date: 20190211. Publication Type: Journal (0100), Peer Reviewed Journal (0110). Format Covered: Print. Document Type: Journal Article. Language: ChineseMajor Descriptor: Adaptive Testing; Algorithms; Computer Simulation; Computerized Assessment. Classification: Artificial Intelligence & Expert Systems (4120). Population: Human (10). Methodology: Empirical Study; Quantitative Study; Scientific Simulation. Page Count: 12. Issue Publication Date: Jul, 2016. 
AB  - Interest in developing computerized adaptive testing (CAT) under cognitive diagnostic models has increased recently. Cognitive diagnostic computerized adaptive testing (CD-CAT) attempt to classify examinees into the correct latent class profile so as to pinpoint the strengths and weaknesses of each examinee whereas CAT algorithms choose items from the item bank to achieve that goal as efficiently as possible. Most of the research in CD-CAT uses the posterior-weighted Kullback-Leibler (PWKL) index due to its high efficiency. The PWKL index integrated the posterior probabilities of examinees' latent class profiles into the KL information, and thus improved item selection efficiency considerably. However, the PWKL index only used examinee-based information to assess the relative importance of each latent class profile. The current study attempted to take advantage of not only the examinee-base information but also the item-based information that could be readily obtained from items. In a sense, the PWKL index should be regarded as single-source index. This paper introduced four new multiple-source item selection methods, GIDPWKL, AIDPWKL, CIDPWKL, and KLEDPWKL respectively, which can be modified from the PWKL index by combining the item discrimination information. Two simulation studies were conducted to evaluate the new methods' efficiency against the PWKL index and mutual information (MI) index in the DINA model with the exposure control. The effects of different factors were investigated: the Q matrix structure (simple vs. complex), item quality (high vs. low) and test length (moderate vs. short). Simulation results indicated that: (1) In most cases, the shorter the test length was. the higher AACCR and PCCR values the four new methods would have in the fix-length test. The GIDPWKL index had the highest average attribute correct classification rate and pattern correct classification rate among the six methods, and followed by AIDPWKL index. The performance among the CIDPWKL. KLEDPWKL. and MI depends on the experimental conditions. (2) In most cases, the higher the item quality was. the more advantage the four new methods would have in the fix-length test. (3) The structure of the Q matrix affected the performance of different item selection methods. (4) In the variable-length test, the mean of test length across all examinees for the four new methods and MI method were all smaller than those in the PWKL method. As a whole, the performance of the GIDPWKL index was the best, and should be recommended in practice where had the similar testing scenarios. (PsycINFO Database Record (c) 2019 APA, all rights reserved)
KW  - cognitive diagnostic computerized adaptive testing
KW  - item selection strategy
KW  - item discrimination
KW  - exposure control
KW  - Adaptive Testing
KW  - Algorithms
KW  - Computer Simulation
KW  - Computerized Assessment
DO  - 10.3724/SP.J.1041.2016.00903
UR  - https://search.ebscohost.com/login.aspx?direct=true&db=psyh&AN=2016-45506-013&lang=de&site=ehost-live
DP  - EBSCOhost
DB  - APA PsycInfo
ER  - 

TY  - JOUR
ID  - 2014-57659-001
AN  - 2014-57659-001
AU  - Narayana, Sirisha
AU  - Wong, Christopher J.
T1  - Office-based screening of common psychiatric conditions
JF  - Psychiatric Clinics of North America
JO  - Psychiatric Clinics of North America
JA  - Psychiatr Clin North Am
Y1  - 2015/03//
VL  - 38
IS  - 1
SP  - 1
EP  - 22
PB  - Elsevier Science
SN  - 0193-953X
SN  - 1558-3147
AD  - Wong, Christopher J., Division of General Internal Medicine, Department of Medicine, University of Washington, 4245 Roosevelt Way Northeast, Box 354760, Seattle, WA, US, 98105
N1  - Accession Number: 2014-57659-001. PMID: 25725566 Partial author list: First Author & Affiliation: Narayana, Sirisha; Department of Medicine, UCSF, School of Medicine, San Francisco, CA, US. Release Date: 20150105. Correction Date: 20210712. Publication Type: Journal (0100), Peer Reviewed Journal (0110). Format Covered: Electronic. Document Type: Journal Article. Language: EnglishMajor Descriptor: Anxiety Disorders; Major Depression; Mental Disorders; Screening; Mental Health Screening. Minor Descriptor: Mental Health. Classification: Clinical Psychological Testing (2224); Psychological Disorders (3210). Population: Human (10). Tests & Measures: Geriatric Depression Scale DOI: 10.1037/t00930-000. Methodology: Literature Review. References Available: Y. Page Count: 22. Issue Publication Date: Mar, 2015. Copyright Statement: All rights reserved. Elsevier Inc. 2015. 
AB  - This article presents a literature review of office-based screening of common psychiatric conditions. Depression and anxiety disorders remain significant conditions in the primary care setting and in the general population. Screening tools for depression and anxiety disorders are freely available with acceptable sensitivity and specificity. Novel screening methods, including Internet-based and computerized adaptive testing, are in development and may be promising tools in the future. Despite the availability of these tools and a need to improve the mental health of patients, the utility of widespread use of screening for depression and anxiety disorders in the primary care, office-based setting is uncertain, and guidelines have reached different conclusions. The best evidence for cost effectiveness currently is for screening of major depression as part of the collaborative care model for treatment. Targeted screening is another reasonable approach in patients with comorbid psychiatric conditions or certain medical conditions. Despite unanswered questions, with further research, a growing literature, and increased awareness of mental health, there is every reason for optimism for the future of mental health screening. (PsycInfo Database Record (c) 2021 APA, all rights reserved)
KW  - anxiety disorders
KW  - psychiatric conditions
KW  - office-based screening
KW  - mental health screening
KW  - Anxiety Disorders
KW  - Major Depression
KW  - Mental Disorders
KW  - Screening
KW  - Mental Health Screening
KW  - Mental Health
DO  - 10.1016/j.psc.2014.11.005
UR  - https://search.ebscohost.com/login.aspx?direct=true&db=psyh&AN=2014-57659-001&lang=de&site=ehost-live
UR  - cjwong@uw.edu
DP  - EBSCOhost
DB  - APA PsycInfo
ER  - 

TY  - JOUR
ID  - 2020-09205-001
AN  - 2020-09205-001
AU  - Xiong, Jianhua
AU  - Ding, Shuliang
AU  - Luo, Fen
AU  - Luo, Zhaosheng
T1  - Online calibration of polytomous items under the graded response model
JF  - Frontiers in Psychology
JO  - Frontiers in Psychology
JA  - Front Psychol
Y1  - 2020/01/23/
VL  - 10
PB  - Frontiers Media S.A.
SN  - 1664-1078
AD  - Xiong, Jianhua
N1  - Accession Number: 2020-09205-001. PMID: 32038427 Partial author list: First Author & Affiliation: Xiong, Jianhua; School of Psychology, Jiangxi Normal University, Nanchang, China. Other Publishers: Frontiers Research Foundation. Release Date: 20200827. Publication Type: Journal (0100), Peer Reviewed Journal (0110). Format Covered: Electronic. Document Type: Journal Article. Language: EnglishMajor Descriptor: Adaptive Testing; Estimation; Item Response Theory. Classification: Statistics & Mathematics (2240). Population: Human (10). Methodology: Empirical Study; Quantitative Study; Scientific Simulation. ArtID: 3085. Issue Publication Date: Jan 23, 2020. Publication History: First Posted Date: Jan 23, 2020; Accepted Date: Dec 30, 2019; First Submitted Date: Sep 3, 2019. Copyright Statement: This is an open-access article distributed under the terms of the Creative Commons Attribution License (CC BY). The use, distribution or reproduction in other forums is permitted, provided the original author(s) and the copyright owner(s) are credited and that the original publication in this journal is cited, in accordance with accepted academic practice. No use, distribution or reproduction is permitted which does not comply with these terms. Xiong, Ding, Luo and Luo. 2020. 
AB  - Computerized adaptive testing (CAT) is an efficient testing mode, which allows each examinee to answer appropriate items according his or her latent trait level. The implementation of CAT requires a large-scale item pool, and item pool needs to be frequently replenished with new items to ensure test validity and security. Online calibration is a technique to calibrate the parameters of new items in CAT, which seeds new items in the process of answering operational items, and estimates the parameters of new items through the response data of examinees on new items. The most popular estimation methods include one EM cycle method (OEM) and multiple EM cycle method (MEM) under dichotomous item response theory models. This paper extends OEM and MEM to the graded response model (GRM), a popular model for polytomous data with ordered categories. Two simulation studies were carried out to explore online calibration under a variety of conditions, including calibration design, initial item parameter calculation methods, calibration methods, calibration sample size and the number of categories. Results show that the calibration accuracy of new items were acceptable, and which were affected by the interaction of some factors, therefore some conclusions were given. (PsycInfo Database Record (c) 2020 APA, all rights reserved)
KW  - online calibration
KW  - computerized adaptive testing
KW  - graded response model
KW  - squeezing average method
KW  - one EM cycle method
KW  - multiple EM cycle method
KW  - Adaptive Testing
KW  - Estimation
KW  - Item Response Theory
U1  - Sponsor: National Natural Science Foundation of China, China. Grant: 61967009; 31360237; 61877031; 31660279. Recipients: No recipient indicated
U1  - Sponsor: Jiangxi Education Science Foundation, China. Grant: GJJ160282. Recipients: No recipient indicated
U1  - Sponsor: Jiangxi Normal University, China. Grant: YC2019-B055. Other Details: Postgraduate Innovation Fund Project. Recipients: No recipient indicated
U1  - Sponsor: Chinese Test International Research Fund Project, China. Grant: CTI2017B06. Recipients: No recipient indicated
U1  - Sponsor: Collaborative Innovation Center for Teacher Quality Monitoring, Evaluation and Service in Jiangxi Province, China. Grant: JXJSZLC05. Recipients: No recipient indicated
DO  - 10.3389/fpsyg.2019.03085
UR  - https://search.ebscohost.com/login.aspx?direct=true&db=psyh&AN=2020-09205-001&lang=de&site=ehost-live
UR  - luozs@126.com
UR  - 002279@jxnu.edu.cn
DP  - EBSCOhost
DB  - APA PsycInfo
ER  - 

TY  - JOUR
ID  - 2010-11797-002
AN  - 2010-11797-002
AU  - Dennick, Reg
AU  - Wilkinson, Simon
AU  - Purcell, Nigel
T1  - Online eAssessment: AMEE guide no 39
JF  - Medical Teacher
JO  - Medical Teacher
JA  - Med Teach
Y1  - 2009///
VL  - 31
IS  - 3
SP  - 192
EP  - 206
PB  - Informa Healthcare
SN  - 0142-159X
SN  - 1466-187X
AD  - Dennick, Reg, University of Nottingham, Queen’s Medical Centre, Nottingham, United Kingdom, NG7 2UH
N1  - Accession Number: 2010-11797-002. PMID: 19811115 Partial author list: First Author & Affiliation: Dennick, Reg; University of Nottingham Medical School, Nottingham, United Kingdom. Other Publishers: Taylor & Francis. Release Date: 20110214. Correction Date: 20190211. Publication Type: Journal (0100), Peer Reviewed Journal (0110). Format Covered: Electronic. Document Type: Journal Article. Language: EnglishMajor Descriptor: Educational Measurement; Professional Examinations; Computerized Assessment. Minor Descriptor: Adaptive Testing; Costs and Cost Analysis; Medical Education. Classification: Professional Education & Training (3410). Population: Human (10). References Available: Y. Page Count: 15. Issue Publication Date: 2009. Copyright Statement: Informa Healthcare Ltd. 2009. 
AB  - In this guide, the authors outline the advantages of online eAssessment and examine the intellectual, technical, legal and cost issues that arise from its use. This guide outlines the major assessment types that are suitable for online assessment and makes a key distinction between formative and summative assessment. The focus is primarily on the latter since that is where the difficulties are most acute and robust systems most critical. A range of practical issues relating to the key stages in running a summative e-exam are explored and advice given on system requirements and on how to ensure that the exam runs smoothly when you 'go live'. This section includes consideration of the way that using eAssessment might affect the standard setting and results analysis process. The section on future trends in online assessment explores possibilities such as computer adaptive testing and the automated assessment of free text answers. Finally, there is a consideration of the implications of these trends for management. (PsycINFO Database Record (c) 2019 APA, all rights reserved)
KW  - online e-Assessment: AMEE guide
KW  - cost analysis
KW  - computer adaptive testing
KW  - Education, Medical
KW  - Educational Measurement
KW  - Great Britain
KW  - Guidelines as Topic
KW  - Internet
KW  - Educational Measurement
KW  - Professional Examinations
KW  - Computerized Assessment
KW  - Adaptive Testing
KW  - Costs and Cost Analysis
KW  - Medical Education
DO  - 10.1080/01421590902792406
UR  - https://search.ebscohost.com/login.aspx?direct=true&db=psyh&AN=2010-11797-002&lang=de&site=ehost-live
UR  - reg.dennick@nottingham.ac.uk
DP  - EBSCOhost
DB  - APA PsycInfo
ER  - 

TY  - THES
ID  - 2018-09132-166
AN  - 2018-09132-166
AU  - Steglitz, Jeremy
T1  - Online tools for evidence-based behavioral practice: A mixed-methods evaluation among graduate students and clinicians
JF  - Dissertation Abstracts International: Section B: The Sciences and Engineering
JO  - Dissertation Abstracts International: Section B: The Sciences and Engineering
Y1  - 2018///
VL  - 79
IS  - 3-B(E)
PB  - ProQuest Information & Learning
SN  - 0419-4217
SN  - 978-0355297348
N1  - Accession Number: 2018-09132-166. Other Journal Title: Dissertation Abstracts International. Partial author list: First Author & Affiliation: Steglitz, Jeremy; Northwestern University, Clinical Psychology, US. Release Date: 20180412. Correction Date: 20221128. Publication Type: Dissertation Abstract (0400). Format Covered: Electronic. Document Type: Dissertation. Dissertation Number: AAI10607607. ISBN: 978-0355297348. Language: EnglishMajor Descriptor: Educational Psychology; Policy Making; School Based Intervention. Classification: Health & Mental Health Treatment & Prevention (3300); Educational & School Psychology (3500). Population: Human (10); Male (30); Female (40). Location: US. Age Group: Adulthood (18 yrs & older) (300). Methodology: Empirical Study; Interview; Qualitative Study; Quantitative Study. 
AB  - Evidence-Based Behavioral Practice (EBBP) is a transdisciplinary systematic approach that emphasizes the use of best evidence in combination with clinical expertise, as well as patient preferences and values, to make clinically-informed decisions about care and treatment (Spring, 2007). Despite numerous benefits and strong policy support of EBBP, findings indicate that clinicians generally fail to implement EBBP in standard clinical practice due to a lack of knowledge, negative attitudes, and low self-efficacy associated with an absence of resources. To address the need for EBBP resources and tools, Bonnie Spring, PhD and colleagues developed a theory-driven website for graduate students and clinicians. The website is primarily comprised of two components: nine interactive training modules and a novel assessment portal that utilizes patient-reported computer adaptive tests (CATs). To date, the training modules and assessment portal have not been evaluated, and there is currently a critical need to incorporate mixed methods evaluations into EBBP implementation science. The goal of this dissertation is to contribute to the ongoing evaluation and optimization of the EBBP Project by conducting summative and implementation-focused formative evaluations of its training modules and assessment portal. Aim 1 evaluated changes in user knowledge, attitudes, and self-efficacy for each EBBP module and determined whether these outcomes varied by user characteristics, including degree, professional discipline, and number of completed modules. Together, findings suggested heightened interest and use of asynchronous, web-based training modules about EBBP in social work and psychology, and that these modules can produce short-term improvements in knowledge, attitudes, and self-efficacy about EBBP. Overall, however, attrition continues to be a challenge. To better understand the attrition rates of the EBBP training modules and web-based modules, in general, Aim 2 conducted implementation-focused formative evaluations to assess individual-level and intervention-level characteristics related to module engagement. Overall, findings from quantitative and qualitative analyses (i.e., mixed method analyses) revealed that specific instructional guidance and interactive features, coupled with user technical knowledge, are particularly important characteristics that may improve engagement in asynchronous web-based training modules. Given the novelty of the EBBP assessment portal, Aim 3 conducted implementation-focused formative evaluations to assess individual-level and intervention-level characteristics associated with its engagement and implementation. Results from mixed method analyses suggested that user knowledge, attitudes, and self-efficacy, coupled with programmatic functions to ensure privacy and confidentiality of patient information, may be important characteristics to improving engagement with a CATs-based assessment portal. In conclusion, the present study provides valuable information on the short-term effectiveness of asynchronous web-based modules, and factors that may contribute to engagement and implementation of online training and clinical tools for EBBP. (PsycInfo Database Record (c) 2022 APA, all rights reserved)
KW  - evidence based behavioral practice
KW  - policy support
KW  - educational psychology
KW  - Educational Psychology
KW  - Policy Making
KW  - School Based Intervention
UR  - https://search.ebscohost.com/login.aspx?direct=true&db=psyh&AN=2018-09132-166&lang=de&site=ehost-live
DP  - EBSCOhost
DB  - APA PsycInfo
ER  - 

TY  - JOUR
ID  - 2019-68577-001
AN  - 2019-68577-001
AU  - Fenwick, Eva K.
AU  - Loe, Bao Sheng
AU  - Khadka, Jyoti
AU  - Man, Ryan E. K.
AU  - Rees, Gwyn
AU  - Lamoureux, Ecosse L.
T1  - Optimizing measurement of vision-related quality of life: A computerized adaptive test for the Impact of Vision Impairment Questionnaire (IVI‑CAT)
JF  - Quality of Life Research: An International Journal of Quality of Life Aspects of Treatment, Care & Rehabilitation
JO  - Quality of Life Research: An International Journal of Quality of Life Aspects of Treatment, Care & Rehabilitation
JA  - Qual Life Res
Y1  - 2020/03//
VL  - 29
IS  - 3
SP  - 765
EP  - 774
PB  - Springer
SN  - 0962-9343
SN  - 1573-2649
AD  - Lamoureux, Ecosse L.
N1  - Accession Number: 2019-68577-001. PMID: 31707693 Partial author list: First Author & Affiliation: Fenwick, Eva K.; Centre for Eye Research Australia, Royal Victorian Eye and Ear Hospital, University of Melbourne, Melbourne, VIC, Australia. Release Date: 20191114. Correction Date: 20220217. Publication Type: Journal (0100), Peer Reviewed Journal (0110). Format Covered: Electronic. Document Type: Journal Article. Language: EnglishGrant Information: Lamoureux, Ecosse L. Major Descriptor: Adaptive Testing; Psychometrics; Quality of Life; Vision Disorders. Minor Descriptor: Questionnaires. Classification: Neuropsychological Assessment (2225); Vision & Hearing & Sensory Disorders (3299). Population: Human (10); Male (30); Female (40). Location: Australia; Singapore. Age Group: Adulthood (18 yrs & older) (300). Tests & Measures: Impact of Vision Impairment Questionnaire. Methodology: Empirical Study; Quantitative Study. Supplemental Data: Other Internet. References Available: Y. Page Count: 10. Issue Publication Date: Mar, 2020. Publication History: First Posted Date: Nov 9, 2019; Accepted Date: Oct 29, 2019. Copyright Statement: Springer Nature Switzerland AG. 2019. 
AB  - Purpose: To compare the results from a simulated computerized adaptive test (CAT) for the 28-item Impact of Vision Impairment (IVI) questionnaire and the original paper–pencil version in terms of efficiency (main outcome), defined as percentage item reduction. Methods: Using paper–pencil IVI data from 832 participants across the spectrum of vision impairment, item calibrations of the 28-item IVI instrument and its associated 20-item vision-specific functioning (VSF) and 8-item emotional well-being (EWB) subscales were generated with Rasch analysis. Based on these calibrations, CAT simulations were conducted on 1000 cases, with ‘high’ and ‘moderate’ precision stopping rules (standard error of measurement [SEM] 0.387 and 0.521, respectively). We examined the average number of items needed to satisfy the stopping rules and the corresponding percentage item reduction, level of agreement between person measures estimated from the full IVI item bank and from the CAT simulations, and item exposure rates (IER). Results: For the overall IVI-CAT, 5 or 9.7 items were required, on average, to obtain moderate or high precision estimates of vision-related quality of life, corresponding to 82.1 and 65.4% item reductions compared to the paper–pencil IVI. Agreement was high between the person measures generated from the full IVI item bank and the IVI-CAT for both the high precision simulation (mean bias, − 0.004 logits; 95% LOA − 0.594 to 0.587) and moderate precision simulation (mean bias, 0.014 logits; 95% LOA − 0.828 to 0.855). The IER for the IVI-CAT in the moderate precision simulation was skewed, with six EWB items used > 40% of the time. Conclusion: Compared to the paper–pencil IVI instrument, the IVI-CATs required fewer items without loss of measurement precision, making them potentially attractive outcome instruments for implementation into clinical trials, healthcare, and research. Final versions of the IVI-CATs are available. (PsycInfo Database Record (c) 2022 APA, all rights reserved)
KW  - Vision-related quality of life
KW  - Vision impairment
KW  - Computerized adaptive testing
KW  - Item bank
KW  - Impact of Vision Impairment Questionnaire
KW  - Computers
KW  - Female
KW  - Humans
KW  - Male
KW  - Mental Health
KW  - Middle Aged
KW  - Psychometrics
KW  - Quality of Life
KW  - Reproducibility of Results
KW  - Surveys and Questionnaires
KW  - Vision, Low
KW  - Young Adult
KW  - Adaptive Testing
KW  - Psychometrics
KW  - Quality of Life
KW  - Vision Disorders
KW  - Questionnaires
U1  - Sponsor: Australian National Health and Medical Research Council (NHMRC), Australia. Grant: #1045280. Other Details: Senior Research Fellowship. Recipients: Lamoureux, Ecosse L.
U1  - Sponsor: National Health and Medical Research Council. Grant: #1061801. Other Details: Career Development Fellowship. Recipients: Rees, Gwyn
DO  - 10.1007/s11136-019-02354-y
UR  - https://search.ebscohost.com/login.aspx?direct=true&db=psyh&AN=2019-68577-001&lang=de&site=ehost-live
UR  - ORCID: 0000-0001-8674-5705
UR  - ORCID: 0000-0003-1012-2119
UR  - ORCID: 0000-0003-0417-2048
UR  - ecosse.lamoureux@seri.com.sg
DP  - EBSCOhost
DB  - APA PsycInfo
ER  - 

TY  - JOUR
ID  - 2010-18788-009
AN  - 2010-18788-009
AU  - You, Xiao-Feng
AU  - Ding, Shu-Liang
AU  - Liu, Hong-Yun
T1  - Parameter estimation of the raw item in Computerized Adaptive Testing
JF  - Acta Psychologica Sinica
JO  - Acta Psychologica Sinica
JA  - Xin Li Xue Bao
Y1  - 2010/07//
VL  - 42
IS  - 7
SP  - 813
EP  - 820
PB  - Science Press
SN  - 0439-755X
AD  - Ding, Shu-Liang, Jiangxi Normal University, Nanchang, China, 330027
N1  - Accession Number: 2010-18788-009. Partial author list: First Author & Affiliation: You, Xiao-Feng; Jiangxi Normal University, Nanchang, China. Release Date: 20110530. Correction Date: 20190211. Publication Type: Journal (0100), Peer Reviewed Journal (0110). Format Covered: Print. Document Type: Journal Article. Language: ChineseMajor Descriptor: Adaptive Testing; Item Response Theory; Statistical Estimation; Test Items; Computerized Assessment. Classification: Statistics & Mathematics (2240). Population: Human (10). Methodology: Empirical Study; Mathematical Model; Quantitative Study. References Available: Y. Page Count: 8. Issue Publication Date: Jul, 2010. 
AB  - Along with the development of computer technology and increasing needs of individual learning, Computerized Adaptive Testing (CAT) has received more and more attention. However, the problem of the test security is becoming a new challenge to the CAT, for example, excessive exposure of the test items might weaken the efficiency and equity of the CAT. Therefore, constructing a large-scale and high-quality CAT item bank is highly demanded The traditional method of constructing a CAT item bank involves four steps: first, the items are developed by the experts according to the test specification and blueprint; second, a representative sample of examinees is recruited to take the test; Then, the calibration of item parameters and testing of the model-data fit will be conducted afterward. Finally, items of high-quality and its item parameters will be added into the item bank based on the pre-analysis. However, several problems exist in the above traditional method. First, a large number of test takers are needed, which is time-consuming and expensive. Second, the security of the items cannot be guaranteed in the pilot test. Third, complicated techniques of equating may have to be used to construct a good item bank, which may also affect the security of the items, especially when the anchor items are repeatedly used in the test equating process. Thus, if the raw items can be seeded in the CAT process and the item parameters combined with the examinee abilities estimated at the same time, it will be significant for the construction of CAT item bank. The research in this area has not been widely conducted and reported in the domestic journals, although this issue has been a big topic for foreign researchers. Thus, This study aims to explore how to insert raw items and estimate the item parameters in CAT and investigate the efficient strategies with high security for enlarging the item bank. A new online calibration method is proposed based on the principle of adaption in CAT, provided there is a small-scale item pool. And the formula of initial value during the stage of the iteration is determined. When the ability parameter has been estimated, the conditional maximum likelihood estimation (CMLE) will be implemented to estimate item parameters in the raw items. First, the difficulty parameter could be obtained through CMLE. Second, the discrimination parameter could be estimated with the abilities and difficulty parameter and the initial value of the discrimination parameter could be gained through CMLE. Third, the difficulty parameter could be estimated when the discrimination parameter and abilities are known. Repeat the second and the third steps until the stop condition is satisfied. Simulation of Monte Carlo has been employed to estimate the parameters of raw items with the one-parameter Logistic and two-parameter Logistic models in the study and good result has been gained. (PsycINFO Database Record (c) 2019 APA, all rights reserved)
KW  - parameter estimation
KW  - raw items
KW  - computerized adaptive testing
KW  - logistic models
KW  - Adaptive Testing
KW  - Item Response Theory
KW  - Statistical Estimation
KW  - Test Items
KW  - Computerized Assessment
DO  - 10.3724/SP.J.1041.2010.00813
UR  - https://search.ebscohost.com/login.aspx?direct=true&db=psyh&AN=2010-18788-009&lang=de&site=ehost-live
UR  - ding06026@163.com
DP  - EBSCOhost
DB  - APA PsycInfo
ER  - 

TY  - JOUR
ID  - 2021-33935-003
AN  - 2021-33935-003
AU  - Mustanski, Brian
AU  - Whitton, Sarah W.
AU  - Newcomb, Michael E.
AU  - Clifford, Antonia
AU  - Ryan, Daniel T.
AU  - Gibbons, Robert D.
T1  - Predicting suicidality using a computer adaptive test: Two longitudinal studies of sexual and gender minority youth
JF  - Journal of Consulting and Clinical Psychology
JO  - Journal of Consulting and Clinical Psychology
JA  - J Consult Clin Psychol
Y1  - 2021/03//
VL  - 89
IS  - 3
SP  - 166
EP  - 175
PB  - American Psychological Association
SN  - 0022-006X
SN  - 1939-2117
AD  - Mustanski, Brian, Northwestern University, 625 N Michigan Avenue Suite 14-061, Chicago, IL, US, 60611
N1  - Accession Number: 2021-33935-003. PMID: 33829805 Other Journal Title: Journal of Consulting Psychology. Partial author list: First Author & Affiliation: Mustanski, Brian; Institute for Sexual and Gender Minority Health and Wellbeing, Northwestern University, Chicago, IL, US. Other Publishers: American Association for Applied Psychology; Dentan Printing Company; Science Press Printing Company. Release Date: 20210408. Correction Date: 20220117. Publication Type: Journal (0100), Peer Reviewed Journal (0110). Format Covered: Electronic. Document Type: Journal Article. Language: EnglishGrant Information: Gibbons, Robert D. Major Descriptor: Adaptive Testing; Prediction; Suicide; Suicidality; Sexual Minority Groups. Minor Descriptor: Bisexuality; Homosexuality; Screening Tests; Suicide Prevention. Classification: Behavior Disorders & Antisocial Behavior (3230). Population: Human (10); Male (30); Transgender (35); Female (40). Location: US. Age Group: Adulthood (18 yrs & older) (300); Young Adulthood (18-29 yrs) (320); Thirties (30-39 yrs) (340). Tests & Measures: Computerized Adaptive Test for Suicide Risk. Methodology: Empirical Study; Longitudinal Study; Qualitative Study. References Available: Y. Page Count: 10. Issue Publication Date: Mar, 2021. Publication History: Accepted Date: Dec 17, 2020; Revised Date: Dec 6, 2020; First Submitted Date: Apr 22, 2020. Copyright Statement: American Psychological Association. 2021. 
AB  - Objective: Over the past decade, rates of death by suicide have increased among youth. Efficient and effective screening approaches are needed for suicide prevention. Sexual and gender minority youth (SGMY) experience profound disparities, but little is known about subgroups and risk assessments need to be validated. This study tested the psychometric properties and predictive value of a highly efficient computerized adaptive test for suicide risk (CAT-SS) among SGMY. Methods: Participants in two cohort studies of SGMY completed the CAT-SS and validated measures of suicidality in 2018 (n = 1,073) and at their follow-up visit 6 months later (n = 936). Tests of psychometrics and predictive validity were performed. Results: Younger, assigned female at birth, nonmonosexual (e.g., bisexual; relative to monosexual), and gender nonconforming or nongender binary (relative to cisgender and transgender) participants had significantly higher CAT-SS scores. None of the CAT-SS items met the threshold for differential item functioning. In longitudinal analyses, prediction of suicidality moved from poor to good accuracy once CAT-SS was included in the model. CAT-SS significantly improved prediction of suicidality over-and-above reported suicidality at a prior wave. Conclusions: The current study validated CAT-SS as a brief predictor of suicide risk in the disproportionately affected population of SGMY. Screening of SGMY in clinical and community settings using CAT-SS could allow for the identification of participants that need services to reduce their risk of future suicide. Results support the need for particular attention to suicide prevention among SGMY who are teenagers, assigned female at birth, nonmonosexual, and gender nonconforming or nongender binary. (PsycInfo Database Record (c) 2022 APA, all rights reserved)
AB  - Public Health Statement—Sexual and gender minority youth (SGMY) are at disproportionate risk for suicidal ideation, attempts, and death by suicide. This study demonstrated that among SGMY a very brief CAT was more predictive of future suicidality than self-report of prior suicidality. Screening of SGMY in clinical and community settings using this tool could allow for efficient identification of participants that need services to reduce their risk of future suicide. (PsycInfo Database Record (c) 2022 APA, all rights reserved)
KW  - suicide
KW  - lesbian
KW  - gay
KW  - bisexual
KW  - youth
KW  - Adaptive Testing
KW  - Prediction
KW  - Suicide
KW  - Suicidality
KW  - Sexual Minority Groups
KW  - Bisexuality
KW  - Homosexuality
KW  - Screening Tests
KW  - Suicide Prevention
U1  - Sponsor: National Institute of Mental Health, US. Grant: 3R01MH100155-05S1. Recipients: Gibbons, Robert D.
U1  - Sponsor: National Institute on Drug Abuse, US. Grant: U01DA036939. Recipients: Mustanski, Brian
U1  - Sponsor: Eunice Kennedy Shriver National Institute of Child Health and Human Development, US. Grant: R01HD08617. Recipients: No recipient indicated
U1  - Sponsor: National Institute on Drug Abuse, Center for Prevention Implementation Methodology, US. Grant: P30DA027828. Recipients: No recipient indicated
U1  - Sponsor: National Center for Advancing Translational Sciences, Northwestern University Clinical and Translational Sciences Institute, US. Grant: UL1TR001422. Recipients: No recipient indicated
DO  - 10.1037/ccp0000531
UR  - https://search.ebscohost.com/login.aspx?direct=true&db=psyh&AN=2021-33935-003&lang=de&site=ehost-live
UR  - ORCID: 0000-0001-9860-4555
UR  - ORCID: 0000-0002-6566-1219
UR  - ORCID: 0000-0001-9222-5116
UR  - brian@northwestern.edu
DP  - EBSCOhost
DB  - APA PsycInfo
ER  - 

TY  - JOUR
ID  - 2018-13207-001
AN  - 2018-13207-001
AU  - Barthel, D.
AU  - Ravens-Sieberer, U.
AU  - Nolte, S.
AU  - Thyen, U.
AU  - Klein, M.
AU  - Walter, O.
AU  - Meyrose, A.-K.
AU  - Rose, M.
AU  - Otto, C.
T1  - Predictors of health-related quality of life in chronically ill children and adolescents over time
JF  - Journal of Psychosomatic Research
JO  - Journal of Psychosomatic Research
JA  - J Psychosom Res
Y1  - 2018/06//
VL  - 109
SP  - 63
EP  - 70
PB  - Elsevier Science
SN  - 0022-3999
SN  - 1879-1360
AD  - Barthel, D., Department of Child and Adolescent Psychiatry, Psychotherapy, and Psychosomatics, Research Unit Child Public Health, Center for Psychosocial Medicine, University Medical Center Hamburg-Eppendorf, Martinistrasse 52, 20246, Hamburg, Germany
N1  - Accession Number: 2018-13207-001. PMID: 29580563 Partial author list: First Author & Affiliation: Barthel, D.; Department of Child and Adolescent Psychiatry, Psychotherapy, and Psychosomatics, Research Unit Child Public Health, Center for Psychosocial Medicine, University Medical Center Hamburg-Eppendorf, Hamburg, Germany. Release Date: 20180329. Correction Date: 20200806. Publication Type: Journal (0100), Peer Reviewed Journal (0110). Format Covered: Electronic. Document Type: Journal Article. Language: EnglishMajor Descriptor: Chronic Illness; Health; Pediatrics; Health Related Quality of Life. Classification: Psychological & Physical Disorders (3200); Health Psychology & Medicine (3360). Population: Human (10); Male (30); Female (40); Outpatient (60). Location: Germany. Age Group: Childhood (birth-12 yrs) (100); School Age (6-12 yrs) (180); Adolescence (13-17 yrs) (200). Tests & Measures: Short Form-12 Health Survey-German Version; Health Behavior in School-Aged Children– Symptom Checklist-German Version; Strengths and Difficulties Questionnaire--German Version DOI: 10.1037/t07042-000; Kids-CAT DOI: 10.1037/t52793-000. Methodology: Empirical Study; Followup Study; Longitudinal Study; Prospective Study; Quantitative Study. Supplemental Data: Tables and Figures Internet. References Available: Y. Page Count: 8. Issue Publication Date: Jun, 2018. Publication History: Accepted Date: Mar 9, 2018; Revised Date: Feb 1, 2018; First Submitted Date: Nov 9, 2017. Copyright Statement: All rights reserved. Elsevier Inc. 2018. 
AB  - Objective: This study aims at identifying predictors of generic health-related quality of life (HRQoL) in chronically ill children and adolescents over time. The newly developed computer-adaptive test Kids-CAT was used to assess five dimensions of HRQoL. Methods: Longitudinal data from the Kids-CAT study on children and adolescents with asthma, diabetes and juvenile arthritis (n = 248; aged 7–17 years) were assessed at three measurement points over six months. Individual growth modeling served to investigate effects of sociodemographic, disease- and health-related as well as psychosocial factors on HRQoL dimensions Physical Well-Being (WB), Psychological WB, Parent Relations, Social Support & Peers, and School WB over time. Results: Besides effects of sociodemographic variables on HRQoL dimensions Social Support & Peers as well as School WB, we found that a longer duration of the disease was associated with better Physical WB. Lower scores were found for patients with juvenile arthritis compared to those with diabetes in HRQoL dimensions Physical WB and Social Support & Peers. Disease control was positively related to Physical and Psychological WB over time. Mental health problems were negatively associated with four, and subjective health complaints with all five HRQoL dimensions over time. Parental mental health was positively related to the patients' HRQoL score in Parent Relations over time. Conclusions: HRQoL as a multidimensional construct is associated with a wide range of different factors. Pediatricians should consider potential mental health problems and subjective health complaints in their patients. Finally, parental HRQoL can affect HRQoL in chronically ill children and adolescents. (PsycInfo Database Record (c) 2020 APA, all rights reserved)
KW  - Quality of life
KW  - Chronic disease
KW  - Children
KW  - Adolescents
KW  - Self-report
KW  - Computer-adaptive testing
KW  - Longitudinal research
KW  - Adolescent
KW  - Child
KW  - Chronic Disease
KW  - Female
KW  - Humans
KW  - Male
KW  - Quality of Life
KW  - Surveys and Questionnaires
KW  - Time Factors
KW  - Chronic Illness
KW  - Health
KW  - Pediatrics
KW  - Health Related Quality of Life
U1  - Sponsor: Federal Ministry of Education and Research, Germany. Grant: 0010-01GY1111. Other Details: Kids-CAT study. Recipients: No recipient indicated
DO  - 10.1016/j.jpsychores.2018.03.005
UR  - https://search.ebscohost.com/login.aspx?direct=true&db=psyh&AN=2018-13207-001&lang=de&site=ehost-live
UR  - ORCID: 0000-0002-7004-7854
UR  - ORCID: 0000-0003-3056-9794
UR  - ORCID: 0000-0001-6185-9423
UR  - ravens-sieberer@uke.de
UR  - d.barthel@uke.de
DP  - EBSCOhost
DB  - APA PsycInfo
ER  - 

TY  - JOUR
ID  - 2018-05960-001
AN  - 2018-05960-001
AU  - Clover, Kerrie
AU  - Lambert, Sylvie D.
AU  - Oldmeadow, Christopher
AU  - Britton, Benjamin
AU  - King, Madeleine T.
AU  - Mitchell, Alex J.
AU  - Carter, Gregory
T1  - PROMIS depression measures perform similarly to legacy measures relative to a structured diagnostic interview for depression in cancer patients
JF  - Quality of Life Research: An International Journal of Quality of Life Aspects of Treatment, Care & Rehabilitation
JO  - Quality of Life Research: An International Journal of Quality of Life Aspects of Treatment, Care & Rehabilitation
JA  - Qual Life Res
Y1  - 2018/05//
VL  - 27
IS  - 5
SP  - 1357
EP  - 1367
PB  - Springer
SN  - 0962-9343
SN  - 1573-2649
AD  - Clover, Kerrie, Psycho-Oncology Service, Calvary Mater Newcastle, Newcastle, NSW, Australia, 2298
N1  - Accession Number: 2018-05960-001. PMID: 29423755 Partial author list: First Author & Affiliation: Clover, Kerrie; Psycho-Oncology Service, Calvary Mater Newcastle, Newcastle, NSW, Australia. Release Date: 20180212. Correction Date: 20190603. Publication Type: Journal (0100), Peer Reviewed Journal (0110). Format Covered: Electronic. Document Type: Journal Article. Language: EnglishGrant Information: King, Madeleine T. Major Descriptor: Convergent Validity; Measurement; Neoplasms; Psychodiagnostic Interview. Minor Descriptor: Psychometrics. Classification: Clinical Psychological Testing (2224); Cancer (3293). Population: Human (10); Male (30); Female (40); Outpatient (60). Location: Australia. Age Group: Adulthood (18 yrs & older) (300). Tests & Measures: PROMIS Depression Computer Adaptive Test; PROMIS Depression Short Form; Depression, Anxiety and Stress Scale; Structured Clinical Interview for DSM-IV-TR Axis I Disorders; Beck Depression Inventory DOI: 10.1037/t00741-000; Hospital Anxiety and Depression Scale DOI: 10.1037/t03589-000; Patient Health Questionnaire DOI: 10.1037/t02598-000; Center for Epidemiological Studies Depression Scale DOI: 10.1037/t02942-000. Methodology: Empirical Study; Interview; Quantitative Study. References Available: Y. Page Count: 11. Issue Publication Date: May, 2018. Publication History: First Posted Date: Feb 8, 2018; Accepted Date: Jan 29, 2018. Copyright Statement: Springer International Publishing AG, part of Springer Nature. 2018. 
AB  - Purpose: To assess the convergent validity of the Patient-Reported Outcomes Measurement Information System (PROMIS) depression measures relative to legacy measures and criterion validity against a structured diagnostic interview for depression in an oncology sample. Methods: 132 oncology/haematology outpatients completed the PROMIS Depression Computer Adaptive Test (PROMIS-D-CAT) and PROMIS Depression Short Form (PROMIS-D-SF) along with seven legacy measures: Beck Depression Inventory (BDI); Centre for Epidemiological Studies Depression (CES-D); Depression, Anxiety and Stress Scale; Hospital Anxiety and Depression Scale; Patient Health Questionnaire; Distress Thermometer and PSYCH-6. Correlations, area under the curve (AUC) and diagnostic accuracy statistics were calculated with Structured Clinical Interview as the gold standard. Results: Both PROMIS measures correlated with all legacy measures at p < .001 (ρ = 0.589–0.810) and all AUCs (> 0.800) were comparable. At the cut-off points for mild depression of 53, the PROMIS measures had sensitivity (0.83 for PROMIS-D-CAT and 0.80 for PROMIS-D-SF) similar to or better than 6/7 legacy measures with high negative predictive value (> 90%). At cut-off points of 60 for moderate depression, PROMIS measures had specificity > 90%, similar to or better than all legacy measures and positive predictive value ≥ 0.50 (similar to 5/7 legacy measures). Conclusions: The convergent and criterion validity of the PROMIS depression measures in cancer populations was confirmed, although the optimal cut-off points are not established. PROMIS measures were briefer than BDI-II and CES-D but do not offer any advance in terms of diagnostic accuracy, reduced response burden or cost over other legacy measures of depression in oncology patients. (PsycINFO Database Record (c) 2019 APA, all rights reserved)
KW  - Psycho-oncology
KW  - Depression
KW  - Questionnaire development
KW  - Cancer
KW  - psychometrics
KW  - Convergent Validity
KW  - Measurement
KW  - Neoplasms
KW  - Psychodiagnostic Interview
KW  - Psychometrics
U1  - Sponsor: Calvary Mater Newcastle, Australia. Grant: 11-09. Recipients: No recipient indicated
U1  - Sponsor: University of Newcastle, Centre for Translational Neuroscience and Mental Health, Australia. Recipients: No recipient indicated
U1  - Sponsor: Australian Government, Australia. Other Details: through Cancer Australia. Recipients: King, Madeleine T.
U1  - Sponsor: National Health and Medical Research Council. Grant: APP1012869. Other Details: Research Fellowship. Recipients: Lambert, Sylvie D.
U1  - Sponsor: Fonds de recherche du Québec – Santé, Canada. Other Details: Junior 1 Research Scholar Award subsequently. Recipients: No recipient indicated
DO  - 10.1007/s11136-018-1803-x
UR  - https://search.ebscohost.com/login.aspx?direct=true&db=psyh&AN=2018-05960-001&lang=de&site=ehost-live
UR  - ORCID: 0000-0001-6014-598X
UR  - ORCID: 0000-0001-6104-1322
UR  - ORCID: 0000-0001-8310-7003
UR  - kerrie.clover@calvarymater.org.au
DP  - EBSCOhost
DB  - APA PsycInfo
ER  - 

TY  - JOUR
ID  - 2005-08402-020
AN  - 2005-08402-020
AU  - Aguado, David
AU  - Rubio, Víctor J.
AU  - Hontangas, Pedro M.
AU  - Hernández, José Manuel
T1  - Propiedades psicométricas de un test Adaptativo Informatizado para la medición del ajuste emocional = Psychometric properties of an Emotional Adjustment Computerized Adaptive Test
JF  - Psicothema
JO  - Psicothema
JA  - Psicothema
Y1  - 2005/08//
VL  - 17
IS  - 3
SP  - 484
EP  - 491
PB  - Colegio Oficial de Psicólogos del Principado de Asturias
SN  - 0214-9915
SN  - 1886-144X
AD  - Aguado, David, Instituto de Ingenieria del Conocimiento, Universidad Autonoma de Madrid, 28049, Madrid, Spain
N1  - Accession Number: 2005-08402-020. Translated Title: Psychometric properties of an Emotional Adjustment Computerized Adaptive Test. Partial author list: First Author & Affiliation: Aguado, David; Instituto de Ingeniería del Conocimiento, Madrid, Spain. Release Date: 20060130. Correction Date: 20200917. Publication Type: Journal (0100), Peer Reviewed Journal (0110). Format Covered: Print. Document Type: Journal Article. Language: Spanish; CastilianMajor Descriptor: Emotional Adjustment; Item Response Theory; Personality Measures; Psychometrics; Computerized Assessment. Minor Descriptor: Test Validity. Classification: Personality Scales & Inventories (2223); Personality Traits & Processes (3120). Population: Human (10). Age Group: Adulthood (18 yrs & older) (300); Young Adulthood (18-29 yrs) (320); Thirties (30-39 yrs) (340); Middle Age (40-64 yrs) (360). Tests & Measures: Emotional Adjustment Computerized Adaptive Test; Eysenck Personality Inventory DOI: 10.1037/t02711-000; Big Five Questionnaire. Methodology: Empirical Study; Quantitative Study. References Available: Y. Page Count: 8. Issue Publication Date: Aug, 2005. 
AB  - In the present work the psychometric properties of an emotional adjustment computerized adaptive test are described. An examination of Item Response Theory (IRT) research literature indicates that IRT mainly has been used for assessing achievements and ability rather than personality factors. Nevertheless several studies in recent years have shown successful use of IRT with personality assessment instruments. Even so, a few works have inquired about the computerized adaptative test (CAT) features, based on IRT, for the measurement of a personality traits as emotional adjustment. Our results show CAT's efficiency for the assessment of emotional adjustment, so this provides a valid and accurate measurement and uses a smaller number of items in comparison with the emotional adjustment scales from the most strongly established questionnaires. (PsycInfo Database Record (c) 2020 APA, all rights reserved)
KW  - psychometric properties
KW  - computerized adaptive test
KW  - emotional adjustment
KW  - Item Response Theory
KW  - personality assessment
KW  - test validity
KW  - Emotional Adjustment
KW  - Item Response Theory
KW  - Personality Measures
KW  - Psychometrics
KW  - Computerized Assessment
KW  - Test Validity
UR  - https://search.ebscohost.com/login.aspx?direct=true&db=psyh&AN=2005-08402-020&lang=de&site=ehost-live
UR  - david.aguado@iic.uam.es
DP  - EBSCOhost
DB  - APA PsycInfo
ER  - 

TY  - JOUR
ID  - 2000-02497-009
AN  - 2000-02497-009
AU  - Olea, J.
AU  - Revuelta, J.
AU  - Ximénez, M. C.
AU  - Abad, F. J.
T1  - Psychometric and psychological effects of review on computerized fixed and adaptive tests
JF  - Psicológica
JO  - Psicológica
JA  - Psicologica (Valencia)
Y1  - 2000///
VL  - 21
IS  - 1-2
SP  - 157
EP  - 173
PB  - Universitat de València
SN  - 0211-2159
SN  - 1576-8597
N1  - Accession Number: 2000-02497-009. Partial author list: First Author & Affiliation: Olea, J.; Autonoma U, Madrid, Spain. Other Publishers: Digital.CSIC; Walter de Gruyter. Release Date: 20001115. Correction Date: 20221128. Publication Type: Journal (0100), Peer Reviewed Journal (0110). Format Covered: Print. Document Type: Journal Article. Language: EnglishMajor Descriptor: Adaptive Testing; Foreign Language Translation; Item Content (Test); Psychological Reactance; Psychometrics. Minor Descriptor: Computers; Vocabulary. Classification: Cognitive Psychology & Intelligent Systems (4100). Population: Human (10). Age Group: Adulthood (18 yrs & older) (300); Young Adulthood (18-29 yrs) (320). Methodology: Empirical Study. Page Count: 17. Issue Publication Date: 2000. 
AB  - Two computerized versions of an English vocabulary test for Spanish speakers (an adaptive and a fixed one) were applied in a Spanish sample of first-year psychology undergraduate students. The effects of test type (computer-adaptive vs computerized-fixed) and review condition (allowed vs not allowed) on several psychological variables were examined. Within subject variables were measured both before and after review to study the effects of review on the psychological and psychometric variables for the review condition in both tests. Two major results were obtained after review: a) a significant increase of correct responses and estimated ability, and b) a decrease of the state-anxiety level. The differences were not significant for measurement error. Interaction effects (test type by moment) were not significant. These and other results concerning the assessment conditions established in this and previous papers are discussed. Finally, the implications that the results may have to establish review conditions in computerized adaptive tests are commented. (PsycInfo Database Record (c) 2022 APA, all rights reserved)
KW  - psychometrics & psychological effects of review
KW  - English vocabulary computerized fixed vs adaptive tests
KW  - Spanish speaking college students
KW  - Adaptive Testing
KW  - Foreign Language Translation
KW  - Item Content (Test)
KW  - Psychological Reactance
KW  - Psychometrics
KW  - Computers
KW  - Vocabulary
UR  - https://search.ebscohost.com/login.aspx?direct=true&db=psyh&AN=2000-02497-009&lang=de&site=ehost-live
DP  - EBSCOhost
DB  - APA PsycInfo
ER  - 

TY  - JOUR
ID  - 1998-12175-004
AN  - 1998-12175-004
AU  - Vispoel, Walter P.
T1  - Psychometric characteristics of computer-adaptive and self-adaptive vocabulary tests: The role of answer feedback and test anxiety
JF  - Journal of Educational Measurement
JO  - Journal of Educational Measurement
JA  - J Educ Meas
Y1  - 1998///Sum 1998
VL  - 35
IS  - 2
SP  - 155
EP  - 167
PB  - Blackwell Publishing
SN  - 0022-0655
SN  - 1745-3984
N1  - Accession Number: 1998-12175-004. Partial author list: First Author & Affiliation: Vispoel, Walter P.; U Iowa, Iowa City, IA, US. Other Publishers: Wiley-Blackwell Publishing Ltd. Release Date: 19981101. Correction Date: 20221128. Publication Type: Journal (0100), Peer Reviewed Journal (0110). Format Covered: Print. Document Type: Journal Article. Language: EnglishConference Information: Annual Meeting of the National Council on Measurement in Education, Apr, 1996, New York, NY, US. Major Descriptor: Feedback; Psychometrics; Test Administration; Test Anxiety; Test Taking. Minor Descriptor: Adaptive Testing; Vocabulary; Computerized Assessment. Classification: Tests & Testing (2220); Educational & School Psychology (3500). Population: Human (10); Male (30); Female (40). Location: US. Age Group: Adulthood (18 yrs & older) (300); Young Adulthood (18-29 yrs) (320). Methodology: Empirical Study. Page Count: 13. Issue Publication Date: Sum 1998. 
AB  - This study focused on the effects of administration mode (computer-adaptive test [CAT] versus self-adaptive test [SAT]), item-by-item answer feedback (present versus absent), and test anxiety on results obtained from computerized vocabulary tests. Examinees were assigned at random to 4 testing conditions (CAT with feedback, CAT without feedback, SAT with feedback, SAT without feedback). Examinees completed the Test Anxiety Inventory before taking their assigned computerized tests. Results show that the CATs were more reliable and took less time to complete than the SATs. Administration time for both the CATs and SATs was shorter when feedback was provided than when it was not, and this difference was most pronounced for examinees at medium to high levels of test anxiety. These results replicate prior findings regarding the precision and administrative efficiency of CATs and SATs but point to new possible benefits of including answer feedback on such tests. (PsycInfo Database Record (c) 2022 APA, all rights reserved)
KW  - psychometrics & item-by-item answer feedback & test anxiety
KW  - examinees administered computer-adaptive vs self-adaptive vocabulary tests
KW  - Feedback
KW  - Psychometrics
KW  - Test Administration
KW  - Test Anxiety
KW  - Test Taking
KW  - Adaptive Testing
KW  - Vocabulary
KW  - Computerized Assessment
DO  - 10.1111/j.1745-3984.1998.tb00532.x
UR  - https://search.ebscohost.com/login.aspx?direct=true&db=psyh&AN=1998-12175-004&lang=de&site=ehost-live
DP  - EBSCOhost
DB  - APA PsycInfo
ER  - 

TY  - JOUR
ID  - 2007-07407-003
AN  - 2007-07407-003
AU  - Reeve, Bryce B.
AU  - Hays, Ron D.
AU  - Bjorner, Jakob B.
AU  - Cook, Karon F.
AU  - Crane, Paul K.
AU  - Teresi, Jeanne A.
AU  - Thissen, David
AU  - Revicki, Dennis A.
AU  - Weiss, David J.
AU  - Hambleton, Ronald K.
AU  - Liu, Honghu
AU  - Gershon, Richard
AU  - Reise, Steven P.
AU  - Lai, Jin-shei
AU  - Cella, David
T1  - Psychometric evaluation and calibration of health-related quality of life item banks: Plans for the Patient-Reported Outcomes Measurement Information System (PROMIS)
JF  - Medical Care
JO  - Medical Care
JA  - Med Care
Y1  - 2007/05//
VL  - 45
IS  - 5,Suppl1
SP  - S22
EP  - S31
PB  - Lippincott Williams & Wilkins
SN  - 0025-7079
SN  - 1537-1948
AD  - Reeve, Bryce B., Outcomes Research Branch, National Cancer Institute, NIH, EPN 4005, 6130 Executive Blvd. MSC 7344, Bethesda, MD, US, 20892-7344
N1  - Accession Number: 2007-07407-003. Partial author list: First Author & Affiliation: Reeve, Bryce B.; National Cancer Institute, NIH, Bethesda, MD, US. Institutional Authors: PROMIS Cooperative Group. Release Date: 20071008. Correction Date: 20190211. Publication Type: Journal (0100), Peer Reviewed Journal (0110). Format Covered: Print. Document Type: Journal Article. Language: EnglishMajor Descriptor: Health; Information Systems; Measurement; Quality of Life; Treatment Outcomes. Minor Descriptor: Government Programs; Health Related Quality of Life. Classification: Health & Mental Health Treatment & Prevention (3300). Population: Human (10). Tests & Measures: SF-36 Health Survey; Quality of Life. References Available: Y. Page Count: 10. Issue Publication Date: May, 2007. 
AB  - Background: The construction and evaluation of item banks to measure unidimensional constructs of health-related quality of life (HRQOL) is a fundamental objective of the Patient-Reported Outcomes Measurement Information System (PROMIS) project. Objectives: Item banks will be used as the foundation for developing short-form instruments and enabling computerized adaptive testing. The PROMIS Steering Committee selected 5 HRQOL domains for initial focus: physical functioning, fatigue, pain, emotional distress, and social role participation. This report provides an overview of the methods used in the PROMIS item analyses and proposed calibration of item banks. Analyses: Analyses include evaluation of data quality (eg, logic and range checking, spread of response distribution within an item), descriptive statistics (eg, frequencies, means), item response theory model assumptions (unidimensionality, local independence, monotonicity), model fit, differential item functioning, and item calibration for banking. Recommendations: Summarized are key analytic issues; recommendations are provided for future evaluations of item banks in HRQOL assessment. (PsycINFO Database Record (c) 2019 APA, all rights reserved)
KW  - health related quality of life
KW  - Patient-Reported Outcomes Measurement Information System
KW  - Health
KW  - Information Systems
KW  - Measurement
KW  - Quality of Life
KW  - Treatment Outcomes
KW  - Government Programs
KW  - Health Related Quality of Life
U1  - Sponsor: National Institutes of Health, US. Grant: AG015815. Other Details: Through the NIH Roadmap for Medical Research,PROMIS Project. Recipients: No recipient indicated
DO  - 10.1097/01.mlr.0000250483.85507.04
UR  - https://search.ebscohost.com/login.aspx?direct=true&db=psyh&AN=2007-07407-003&lang=de&site=ehost-live
UR  - reeveb@mail.nih.gov
DP  - EBSCOhost
DB  - APA PsycInfo
ER  - 

TY  - JOUR
ID  - 2014-12154-001
AN  - 2014-12154-001
AU  - Vispoel, Walter P.
AU  - Kim, Han Yi
T1  - Psychometric properties for the Balanced Inventory of Desirable Responding: Dichotomous versus polytomous conventional and IRT scoring
JF  - Psychological Assessment
JO  - Psychological Assessment
JA  - Psychol Assess
Y1  - 2014/09//
VL  - 26
IS  - 3
SP  - 878
EP  - 891
PB  - American Psychological Association
SN  - 1040-3590
SN  - 1939-134X
AD  - Vispoel, Walter P., Department of Psychological and Quantitative Foundations, University of Iowa, 361 Lindquist Center, Iowa City, US, 52242-1529
N1  - Accession Number: 2014-12154-001. PMID: 24708082 Other Journal Title: Psychological Assessment: A Journal of Consulting and Clinical Psychology. Partial author list: First Author & Affiliation: Vispoel, Walter P.; Department of Psychological and Quantitative Foundations, University of Iowa, Iowa City, IA, US. Release Date: 20140407. Correction Date: 20140901. Publication Type: Journal (0100), Peer Reviewed Journal (0110). Format Covered: Electronic. Document Type: Journal Article. Language: EnglishMajor Descriptor: Impression Management; Inventories; Item Response Theory; Scoring (Testing). Minor Descriptor: Classical Test Theory; Psychometrics; Test Reliability; Test Validity. Classification: Tests & Testing (2220); Social Psychology (3000). Population: Human (10); Male (30); Female (40). Location: US. Age Group: Adulthood (18 yrs & older) (300); Young Adulthood (18-29 yrs) (320). Tests & Measures: Self-Perception Scale for College Students; Self-Description Questionnaire III DOI: 10.1037/t06009-000; Balanced Inventory of Desirable Responding DOI: 10.1037/t08059-000. Methodology: Empirical Study; Quantitative Study. References Available: Y. Page Count: 14. Issue Publication Date: Sep, 2014. Publication History: First Posted Date: Apr 7, 2014; Accepted Date: Feb 12, 2014; Revised Date: Nov 27, 2013; First Submitted Date: Jun 11, 2013. Copyright Statement: American Psychological Association. 2014. 
AB  - [Correction Notice: An Erratum for this article was reported in Vol 26(3) of Psychological Assessment (see record [rid]2014-16017-001[/rid]). The mean, standard deviation and alpha coefficient originally reported in Table 1 should be 74.317, 10.214 and .802, respectively. The validity coefficients in the last column of Table 4 are affected as well. Correcting this error did not change the substantive interpretations of the results, but did increase the mean, standard deviation, alpha coefficient, and validity coefficients reported for the Honesty subscale in the text and in Tables 1 and 4. The corrected versions of Tables 1 and Table 4 are shown in the erratum.] Item response theory (IRT) models were applied to dichotomous and polytomous scoring of the Self-Deceptive Enhancement and Impression Management subscales of the Balanced Inventory of Desirable Responding (Paulhus, 1991, 1999). Two dichotomous scoring methods reflecting exaggerated endorsement and exaggerated denial of socially desirable behaviors were examined. The 1- and 2-parameter logistic models (1PLM, 2PLM, respectively) were applied to dichotomous responses, and the partial credit model (PCM) and graded response model (GRM) were applied to polytomous responses. For both subscales, the 2PLM fit dichotomous responses better than did the 1PLM, and the GRM fit polytomous responses better than did the PCM. Polytomous GRM and raw scores for both subscales yielded higher test–retest and convergent validity coefficients than did PCM, 1PLM, 2PLM, and dichotomous raw scores. Information plots showed that the GRM provided consistently high measurement precision that was superior to that of all other IRT models over the full range of both construct continuums. Dichotomous scores reflecting exaggerated endorsement of socially desirable behaviors provided noticeably weak precision at low levels of the construct continuums, calling into question the use of such scores for detecting instances of 'faking bad.' Dichotomous models reflecting exaggerated denial of the same behaviors yielded much better precision at low levels of the constructs, but it was still less precision than that of the GRM. These results support polytomous over dichotomous scoring in general, alternative dichotomous scoring for detecting faking bad, and extension of GRM scoring to situations in which IRT offers additional practical advantages over classical test theory (adaptive testing, equating, linking, scaling, detecting differential item functioning, and so forth). (PsycINFO Database Record (c) 2016 APA, all rights reserved)
KW  - BIDR
KW  - polytomous IRT modeling
KW  - socially desirable responding
KW  - scoring methods
KW  - validity
KW  - reliability
KW  - Adolescent
KW  - Deception
KW  - Female
KW  - Humans
KW  - Male
KW  - Motivation
KW  - Psychological Theory
KW  - Psychometrics
KW  - Reproducibility of Results
KW  - Research Design
KW  - Social Desirability
KW  - Surveys and Questionnaires
KW  - Young Adult
KW  - Impression Management
KW  - Inventories
KW  - Item Response Theory
KW  - Scoring (Testing)
KW  - Classical Test Theory
KW  - Psychometrics
KW  - Test Reliability
KW  - Test Validity
DO  - 10.1037/a0036430
UR  - https://search.ebscohost.com/login.aspx?direct=true&db=psyh&AN=2014-12154-001&lang=de&site=ehost-live
UR  - walter-vispoel@uiowa.edu
DP  - EBSCOhost
DB  - APA PsycInfo
ER  - 

TY  - JOUR
ID  - 2014-16017-001
AN  - 2014-16017-001
T1  - 'Psychometric properties for the Balanced Inventory of Desirable Responding: Dichotomous versus polytomous conventional and IRT scoring': Correction to Vispoel and Kim (2014)
JF  - Psychological Assessment
JO  - Psychological Assessment
JA  - Psychol Assess
Y1  - 2014/09//
VL  - 26
IS  - 3
SP  - 1062
EP  - v
PB  - American Psychological Association
SN  - 1040-3590
SN  - 1939-134X
N1  - Accession Number: 2014-16017-001. PMID: 24796342 Other Journal Title: Psychological Assessment: A Journal of Consulting and Clinical Psychology. Partial author list: First Author & Affiliation: No authorship indicated. Release Date: 20140505. Correction Date: 20160218. Publication Type: Journal (0100), Peer Reviewed Journal (0110). Format Covered: Electronic. Document Type: Erratum/Correction. Language: EnglishMajor Descriptor: Impression Management; Inventories; Item Response Theory; Scoring (Testing). Minor Descriptor: Classical Test Theory; Psychometrics; Test Reliability; Test Validity. Classification: Tests & Testing (2220); Social Psychology (3000). Tests & Measures: Balanced Inventory of Desirable Responding DOI: 10.1037/t08059-000. Issue Publication Date: Sep, 2014. Publication History: First Posted Date: May 5, 2014. 
AB  - Reports an error in 'Psychometric properties for the Balanced Inventory of Desirable Responding: Dichotomous versus polytomous conventional and IRT scoring' by Walter P. Vispoel and Han Yi Kim (Psychological Assessment, Advanced Online Publication, Apr 7, 2014, np). The mean, standard deviation and alpha coefficient originally reported in Table 1 should be 74.317, 10.214 and .802, respectively. The validity coefficients in the last column of Table 4 are affected as well. Correcting this error did not change the substantive interpretations of the results, but did increase the mean, standard deviation, alpha coefficient, and validity coefficients reported for the Honesty subscale in the text and in Tables 1 and 4. The corrected versions of Tables 1 and Table 4 are shown in the erratum. (The following abstract of the original article appeared in record [rid]2014-12154-001[/rid].) Item response theory (IRT) models were applied to dichotomous and polytomous scoring of the Self-Deceptive Enhancement and Impression Management subscales of the Balanced Inventory of Desirable Responding (Paulhus, 1991, 1999). Two dichotomous scoring methods reflecting exaggerated endorsement and exaggerated denial of socially desirable behaviors were examined. The 1- and 2-parameter logistic models (1PLM, 2PLM, respectively) were applied to dichotomous responses, and the partial credit model (PCM) and graded response model (GRM) were applied to polytomous responses. For both subscales, the 2PLM fit dichotomous responses better than did the 1PLM, and the GRM fit polytomous responses better than did the PCM. Polytomous GRM and raw scores for both subscales yielded higher test–retest and convergent validity coefficients than did PCM, 1PLM, 2PLM, and dichotomous raw scores. Information plots showed that the GRM provided consistently high measurement precision that was superior to that of all other IRT models over the full range of both construct continuums. Dichotomous scores reflecting exaggerated endorsement of socially desirable behaviors provided noticeably weak precision at low levels of the construct continuums, calling into question the use of such scores for detecting instances of 'faking bad.' Dichotomous models reflecting exaggerated denial of the same behaviors yielded much better precision at low levels of the constructs, but it was still less precision than that of the GRM. These results support polytomous over dichotomous scoring in general, alternative dichotomous scoring for detecting faking bad, and extension of GRM scoring to situations in which IRT offers additional practical advantages over classical test theory (adaptive testing, equating, linking, scaling, detecting differential item functioning, and so forth). (PsycINFO Database Record (c) 2016 APA, all rights reserved)
KW  - BIDR
KW  - polytomous IRT modeling
KW  - socially desirable responding
KW  - scoring methods
KW  - validity
KW  - reliability
KW  - Impression Management
KW  - Inventories
KW  - Item Response Theory
KW  - Scoring (Testing)
KW  - Classical Test Theory
KW  - Psychometrics
KW  - Test Reliability
KW  - Test Validity
DO  - 10.1037/pas0000005
UR  - https://search.ebscohost.com/login.aspx?direct=true&db=psyh&AN=2014-16017-001&lang=de&site=ehost-live
DP  - EBSCOhost
DB  - APA PsycInfo
ER  - 

TY  - JOUR
ID  - 2007-01587-007
AN  - 2007-01587-007
AU  - Rubio, Víctor J.
AU  - Aguado, David
AU  - Hontangas, Pedro M.
AU  - Hernández, José M.
T1  - Psychometric properties of an emotional adjustment measure: An application of the graded response model
JF  - European Journal of Psychological Assessment
JO  - European Journal of Psychological Assessment
JA  - Eur J Psychol Assess
Y1  - 2007///
VL  - 23
IS  - 1
SP  - 39
EP  - 46
PB  - Hogrefe & Huber Publishers
SN  - 1015-5759
SN  - 2151-2426
AD  - Rubio, Víctor J., Department of Psicologia Biologica y de la Salud, Universidad Autonoma de Madrid, E-28049, Madrid, Spain
N1  - Accession Number: 2007-01587-007. Other Journal Title: Evaluación Psicológica. Partial author list: First Author & Affiliation: Rubio, Víctor J.; Department of Biological and Health Psychology, Autonoma University of Madrid, Madrid, Spain. Other Publishers: Hogrefe Publishing. Release Date: 20070212. Correction Date: 20120827. Publication Type: Journal (0100), Peer Reviewed Journal (0110). Format Covered: Electronic. Document Type: Journal Article. Language: EnglishMajor Descriptor: Emotional Adjustment; Emotional Stability; Item Response Theory; Personality Measures; Test Construction. Minor Descriptor: Adaptive Testing; Neuroticism; Psychometrics; Test Reliability; Test Validity. Classification: Personality Scales & Inventories (2223); Personality Traits & Processes (3120). Population: Human (10); Male (30); Female (40). Location: Spain. Age Group: Adulthood (18 yrs & older) (300); Young Adulthood (18-29 yrs) (320); Thirties (30-39 yrs) (340); Middle Age (40-64 yrs) (360). Tests & Measures: Eysenck Personality Inventory-Neuroticism scale; Big Five Questionnaire-Emotional Adjustment scale; Emotional Adjustment Bank DOI: 10.1037/t02863-000. Methodology: Empirical Study; Quantitative Study. References Available: Y. Page Count: 8. Issue Publication Date: 2007. Copyright Statement: Hogrefe & Huber Publishers. 2007. 
AB  - Item response theory (IRT) provides valuable methods for the analysis of the psychometric properties of a psychological measure. However, IRT has been mainly used for assessing achievements and ability rather than personality factors. This paper presents an application of the IRT to a personality measure. Thus, the psychometric properties of a new emotional adjustment measure that consists of a 28-six graded response items is shown. Classical test theory (CTT) analyses as well as IRT analyses are carried out. Samejima's (1969) graded-response model has been used for estimating item parameters. Results show that the bank of items fulfills model assumptions and fits the data reasonably well, demonstrating the suitability of the IRT models for the description and use of data originating from personality measures. In this sense, the model fulfills the expectations that IRT has undoubted advantages: (1) The invariance of the estimated parameters, (2) the treatment given to the standard error of measurement, and (3) the possibilities offered for the construction of computerized adaptive tests (CAT). The bank of items shows good reliability. It also shows convergent validity compared to the Eysenck Personality Inventory (EPQ-A; Eysenck & Eysenck, 1975) and the Big Five Questionnaire (BFQ; Caprara, Barbaranelli, & Borgogni, 1993). (PsycINFO Database Record (c) 2016 APA, all rights reserved)
KW  - emotional adjustment
KW  - item response theory
KW  - Samejima's graded response model
KW  - personnel recruitment
KW  - personality measures
KW  - psychometrics
KW  - test reliability
KW  - validity
KW  - computerized adaptive tests
KW  - Emotional Adjustment
KW  - Emotional Stability
KW  - Item Response Theory
KW  - Personality Measures
KW  - Test Construction
KW  - Adaptive Testing
KW  - Neuroticism
KW  - Psychometrics
KW  - Test Reliability
KW  - Test Validity
DO  - 10.1027/1015-5759.23.1.39
UR  - https://search.ebscohost.com/login.aspx?direct=true&db=psyh&AN=2007-01587-007&lang=de&site=ehost-live
UR  - ORCID: 0000-0001-9747-2167
UR  - victor.rubio@uam.es
DP  - EBSCOhost
DB  - APA PsycInfo
ER  - 

TY  - JOUR
ID  - 2021-46416-001
AN  - 2021-46416-001
AU  - Klaufus, L. H.
AU  - Luijten, M. A. J.
AU  - Verlinden, E.
AU  - van der Wal, M. F.
AU  - Haverman, L.
AU  - Cuijpers, P.
AU  - Chinapaw, M. J. M.
AU  - Terwee, C. B.
T1  - Psychometric properties of the Dutch‑Flemish PROMIS® pediatric item banks Anxiety and Depressive Symptoms in a general population
JF  - Quality of Life Research: An International Journal of Quality of Life Aspects of Treatment, Care & Rehabilitation
JO  - Quality of Life Research: An International Journal of Quality of Life Aspects of Treatment, Care & Rehabilitation
JA  - Qual Life Res
Y1  - 2021/09//
VL  - 30
IS  - 9
SP  - 2683
EP  - 2695
PB  - Springer
SN  - 0962-9343
SN  - 1573-2649
AD  - Klaufus, L. H., Department of Epidemiology, Health Promotion, and Health Care Innovation, Public Health Service Amsterdam, Nieuwe Achtergracht 100, Amsterdam, Netherlands
N1  - Accession Number: 2021-46416-001. PMID: 33983618 Partial author list: First Author & Affiliation: Klaufus, L. H.; Department of Epidemiology, Health Promotion, and Health Care Innovation, Public Health Service Amsterdam, Amsterdam, Netherlands. Release Date: 20210517. Correction Date: 20221201. Publication Type: Journal (0100), Peer Reviewed Journal (0110). Format Covered: Electronic. Document Type: Journal Article. Language: EnglishMajor Descriptor: Anxiety; Major Depression; Psychometrics; Test Validity; Patient Reported Outcome Measures. Minor Descriptor: Adaptive Testing; Pediatrics; Racial and Ethnic Groups; Symptoms; Differential Item Functioning. Classification: Clinical Psychological Testing (2224); Affective Disorders (3211). Population: Human (10); Male (30); Female (40). Location: Netherlands. Age Group: Childhood (birth-12 yrs) (100); School Age (6-12 yrs) (180); Adolescence (13-17 yrs) (200); Adulthood (18 yrs & older) (300); Young Adulthood (18-29 yrs) (320). Tests & Measures: PROMIS Pediatric Item Banks-V 2.0 and Short Forms 8-Anxiety and Depressive Symptoms; Child Anxiety and Depression Scale-Short Version-Revised. Methodology: Empirical Study; Quantitative Study. References Available: Y. Page Count: 13. Issue Publication Date: Sep, 2021. Publication History: First Posted Date: May 13, 2021; Accepted Date: Apr 17, 2021. Copyright Statement: The Author(s), under exclusive licence to Springer Nature Switzerland AG. 2021. 
AB  - Purpose: This study aims to validate the Dutch-Flemish PROMIS pediatric item banks v2.0 Anxiety and Depressive Symptoms, the short forms 8a, and computerized adaptive tests (CATs) in a general Dutch population and to provide reference data. Methods: Participants (N = 2,893, aged 8–18), recruited by two internet survey providers, completed both item banks. These item banks were assessed on unidimensionality, local independence, monotonicity, Graded Response Model (GRM) item fit, and differential item functioning (DIF) for gender, age group, region, ethnicity, and language. The short forms and CATs were assessed on reliability and construct validity compared to the Revised Child Anxiety and Depression Scale short version (RCADS-22) subscales. Reference scores were calculated. Results: Both item banks showed sufficient unidimensionality, local independence, monotonicity, and GRM item fit, except for three Depressive Symptoms items that showed insufficient GRM item fit. No DIF was found when using ordinal regression analyses, except for two Depressive Symptoms items that showed DIF for language; all items showed DIF for language when using IRT PRO, except for one Anxiety item. Both short forms and CATs revealed sufficient reliability for moderate and severe levels of anxiety and depression, as well as high positive correlations with corresponding RCADS-22 subscales and slightly lower correlations with non-corresponding RCADS-22 subscales. Conclusion: The Dutch-Flemish PROMIS pediatric item banks v2.0 Anxiety and Depressive Symptoms, the short forms 8a and CATs are useful to assess and monitor anxiety and depression in a general population. Reference data are presented. (PsycInfo Database Record (c) 2022 APA, all rights reserved)
KW  - Anxiety
KW  - Depression
KW  - Pediatric
KW  - PROMIS
KW  - IRT
KW  - Validation
KW  - Anxiety
KW  - Major Depression
KW  - Psychometrics
KW  - Test Validity
KW  - Patient Reported Outcome Measures
KW  - Adaptive Testing
KW  - Pediatrics
KW  - Racial and Ethnic Groups
KW  - Symptoms
KW  - Differential Item Functioning
U1  - Sponsor: Netherlands Organization for Health Research and Development, Netherlands. Grant: 729300104. Recipients: No recipient indicated
DO  - 10.1007/s11136-021-02852-y
UR  - https://search.ebscohost.com/login.aspx?direct=true&db=psyh&AN=2021-46416-001&lang=de&site=ehost-live
UR  - ORCID: 0000-0003-4570-2826
UR  - ORCID: 0000-0001-7849-0562
UR  - ORCID: 0000-0001-8016-2859
UR  - ORCID: 0000-0001-9878-3530
DP  - EBSCOhost
DB  - APA PsycInfo
ER  - 

TY  - JOUR
ID  - 2020-01760-001
AN  - 2020-01760-001
AU  - Mazefsky, Carla A.
AU  - Yu, Lan
AU  - Pilkonis, Paul A.
T1  - Psychometric properties of the Emotion Dysregulation Inventory in a nationally representative sample of youth
JF  - Journal of Clinical Child and Adolescent Psychology
JO  - Journal of Clinical Child and Adolescent Psychology
JA  - J Clin Child Adolesc Psychol
Y1  - 2021/09//Sep-Oct, 2021
VL  - 50
IS  - 5
SP  - 596
EP  - 608
PB  - Taylor & Francis
SN  - 1537-4416
SN  - 1537-4424
AD  - Mazefsky, Carla A., Department of Psychiatry, University of Pittsburgh School of Medicine, 3811 O’Hara Street, Webster Hall Suite 300, Pittsburgh, PA, US, 15213
N1  - Accession Number: 2020-01760-001. PMID: 31910035 Other Journal Title: Journal of Clinical Child Psychology. Partial author list: First Author & Affiliation: Mazefsky, Carla A.; Department of Psychiatry, University of Pittsburgh School of Medicine, Pittsburgh, PA, US. Other Publishers: Lawrence Erlbaum. Release Date: 20200109. Correction Date: 20211220. Publication Type: Journal (0100), Peer Reviewed Journal (0110). Format Covered: Electronic. Document Type: Journal Article. Language: EnglishMajor Descriptor: Emotional Disturbances; Emotional Regulation; Item Response Theory; Pediatrics; Psychometrics. Minor Descriptor: Inventories; Test Validity. Classification: Clinical Psychological Testing (2224); Affective Disorders (3211). Population: Human (10); Male (30); Female (40). Location: US. Age Group: Childhood (birth-12 yrs) (100); School Age (6-12 yrs) (180); Adolescence (13-17 yrs) (200). Tests & Measures: Emotion Dysregulation Inventory DOI: 10.1037/t71153-000; Achenbach Child Behavior Checklist; Emotion Regulation Checklist DOI: 10.1037/t08132-000; Patient-Reported Outcomes Measurement Information System DOI: 10.1037/t06780-000. Methodology: Empirical Study; Interview; Quantitative Study. References Available: Y. Page Count: 13. Issue Publication Date: Sep-Oct, 2021. Copyright Statement: Society of Clinical Child & Adolescent Psychology. 2020. 
AB  - Objective: The Emotion Dysregulation Inventory (EDI) is an informant questionnaire developed based on the Patient-Reported Outcomes Measurement Information System (PROMIS®) Scientific Standards and refined through factor analyses and item response theory (IRT) analyses. Although it was developed to improve measurement of emotion dysregulation in youth with autism spectrum disorder, emotion dysregulation has transdiagnostic significance. Therefore, the aim of this study was to evaluate the EDI’s psychometric properties and to establish IRT-based scores for a general population of youth. Methods: Data were collected from a sample of 1000 caregivers of 6- to 17-year-old youth matched to the US census on age, gender, race/ethnicity, years of education, and region. Confirmatory factor analyses and IRT analyses using the two-parameter graded response model were performed to evaluate the EDI’s structure and psychometric properties. Results: Analyses supported the original two-factor structure of the EDI, reflecting factors for Reactivity and Dysphoria. Simulations of computerized adaptive testing supported use of the same items for a Reactivity short form as those that emerged as most informative in the original autism psychometric analyses. IRT co-calibration with commonly used measures of emotion regulation and irritability in child clinical or community samples indicated the EDI scales provide more information across a wider range of emotion dysregulation. Validity was supported by moderate correlations with measures of related constructs and expected known-group differences. Conclusions: The EDI is an efficient and precise measure of emotion dysregulation for use in general community and clinical samples as well as samples of youth with ASD. (PsycInfo Database Record (c) 2021 APA, all rights reserved)
KW  - Emotion Dysregulation Inventory
KW  - psychometrics
KW  - test validity
KW  - factor analysis
KW  - item response theory
KW  - pediatrics
KW  - Adolescent
KW  - Autism Spectrum Disorder
KW  - Child
KW  - Emotional Regulation
KW  - Emotions
KW  - Humans
KW  - Psychometrics
KW  - Surveys and Questionnaires
KW  - Emotional Disturbances
KW  - Emotional Regulation
KW  - Item Response Theory
KW  - Pediatrics
KW  - Psychometrics
KW  - Inventories
KW  - Test Validity
U1  - Sponsor: Eunice Kennedy Shriver National Institute of Child Health and Human Development, US. Grant: R01 HD079512. Recipients: No recipient indicated
U1  - Sponsor: International Society for Autism Research/Slifka Foundation. Other Details: Slifka-Ritvo Award For Innovation in Autism Research. Recipients: No recipient indicated
DO  - 10.1080/15374416.2019.1703710
UR  - https://search.ebscohost.com/login.aspx?direct=true&db=psyh&AN=2020-01760-001&lang=de&site=ehost-live
UR  - mazefskyca@upmc.edu
DP  - EBSCOhost
DB  - APA PsycInfo
ER  - 

TY  - JOUR
ID  - 2021-61909-001
AN  - 2021-61909-001
AU  - van Muilekom, Maud M.
AU  - Luijten, Michiel A. J.
AU  - van Litsenburg, Raphaele R. L.
AU  - Grootenhuis, Martha A.
AU  - Terwee, Caroline B.
AU  - Haverman, Lotte
T1  - Psychometric properties of the Patient-Reported Outcomes Measurement Information System (PROMIS®) Pediatric Anger Scale in the Dutch general population
JF  - Psychological Assessment
JO  - Psychological Assessment
JA  - Psychol Assess
Y1  - 2021/12//
VL  - 33
IS  - 12
SP  - 1261
EP  - 1266
PB  - American Psychological Association
SN  - 1040-3590
SN  - 1939-134X
AD  - Haverman, Lotte, Department of Child and Adolescent Psychiatry & Psychosocial Care, Amsterdam Reproduction and Development, Amsterdam Public Health, Emma Children’s Hospital, Amsterdam UMC, University of Amsterdam, P.O. Box 22660, 1100 DD, Amsterdam, Netherlands
N1  - Accession Number: 2021-61909-001. PMID: 34197165 Other Journal Title: Psychological Assessment: A Journal of Consulting and Clinical Psychology. Partial author list: First Author & Affiliation: van Muilekom, Maud M.; Department of Child and Adolescent Psychiatry & Psychosocial Care, Amsterdam Reproduction and Development, Amsterdam Public Health, Emma Children’s Hospital, Amsterdam UMC, University of Amsterdam, Amsterdam, Netherlands. Release Date: 20210701. Correction Date: 20211230. Publication Type: Journal (0100), Peer Reviewed Journal (0110). Format Covered: Electronic. Document Type: Journal Article. Language: EnglishConference Information: annual conference of the International Society of Quality of Life Research (online), 27th, 2020. Conference Note: The abstract of this manuscript was selected for an oral presentation (Oral Session 120: Application of PROMIS) during the aforementioned conference. Major Descriptor: Adaptive Testing; Anger; Pediatrics; Test Validity; Patient Reported Outcome Measures. Minor Descriptor: Aggressive Behavior; Construct Validity; Item Response Theory; Test Forms; Test Reliability. Classification: Clinical Psychological Testing (2224); Health & Mental Health Treatment & Prevention (3300). Population: Human (10); Male (30); Female (40). Location: Netherlands. Age Group: Childhood (birth-12 yrs) (100); School Age (6-12 yrs) (180); Adolescence (13-17 yrs) (200); Adulthood (18 yrs & older) (300); Young Adulthood (18-29 yrs) (320). Tests & Measures: Pediatric Quality of Life InventoryTM(4.0); PROMIS Pediatric Anger 9a v2.0 Scale. Methodology: Empirical Study; Quantitative Study. References Available: Y. Page Count: 6. Issue Publication Date: Dec, 2021. Publication History: First Posted Date: Jul 1, 2021; Accepted Date: May 24, 2021; Revised Date: May 20, 2021; First Submitted Date: Mar 4, 2021. Copyright Statement: American Psychological Association. 2021. 
AB  - This study aimed to validate the PROMIS® pediatric v2.0 Anger scale in the Dutch general population, provide reference data, and compare reliability and relative efficiency between the full-length scale, its short-form, computerized adaptive test (CAT), and Pediatric Quality of Life Inventory (PedsQLTM) emotional functioning (EF) subscale scores. Children (N = 1,328), representative of the Dutch population, were asked to complete the PROMIS pediatric Anger scale (8–18 years) and PedsQLTM (8–17 years). A graded response model (GRM) was fit to the data. Structural validity was assessed by checking item-fit statistics (S−X2, p < .001 = misfit). For construct validity, a moderate correlation (Pearson’s r > 0.50) was expected between the Anger scale and PedsQLTM EF subscale score. Dutch mean T score based on the U.S. model was calculated to provide reference data and cut-offs. Standard error of measurement (SE(θ)) was used to assess reliability (SE(θ) < .32 = .90 reliability). Relative efficiency was calculated (1 − SE(θ)2/N items) to compare how good the measures performed relative to the amount of items administered. In total, 527 children completed the PROMIS pediatric Anger scale, of which 482 completed the PedsQLTM. Structural validity was sufficient as no items displayed misfit (S−X2 = 22.9–40.3, p > .001). The Anger scale score correlated moderately (Pearson’s r = .64) with the PedsQLTM EF subscale score. Dutch mean T score was 44.20 (SD = 11.39), with cut-offs of >52.2 for moderate and ≥62.3 for severe symptoms. Reliable measurements were obtained at the population mean and >2SD in the clinically relevant direction. CAT outperformed all other measures in efficiency. The PROMIS pediatric Anger scale displayed sufficient psychometric properties within the Dutch population and reference data are available. (PsycInfo Database Record (c) 2021 APA, all rights reserved)
AB  - Public Significance Statement—This study provides evidence that the PROMIS pediatric Anger scale has good psychometric properties in the Dutch general population. This scale can thus be implemented in clinical practice and for pediatric research in the Netherlands to solve the problem that validated Patient-Reported Outcome Measures (PROMs) assessing anger in a pediatric population are missing, improve interpretability and comparison of scores and reduce the burden of completing PROMs for pediatric patients. (PsycInfo Database Record (c) 2021 APA, all rights reserved)
KW  - validity
KW  - reliability
KW  - aggression
KW  - computerized adaptive testing
KW  - item response theory
KW  - Adaptive Testing
KW  - Anger
KW  - Pediatrics
KW  - Test Validity
KW  - Patient Reported Outcome Measures
KW  - Aggressive Behavior
KW  - Construct Validity
KW  - Item Response Theory
KW  - Test Forms
KW  - Test Reliability
DO  - 10.1037/pas0001051
UR  - https://search.ebscohost.com/login.aspx?direct=true&db=psyh&AN=2021-61909-001&lang=de&site=ehost-live
UR  - ORCID: 0000-0001-7849-0562
UR  - ORCID: 0000-0003-4570-2826
UR  - ORCID: 0000-0003-1779-6159
UR  - ORCID: 0000-0001-8016-2859
UR  - ORCID: 0000-0001-9540-6254
UR  - l.haverman@amsterdamumc.nl
DP  - EBSCOhost
DB  - APA PsycInfo
ER  - 

TY  - JOUR
ID  - 2014-10817-015
AN  - 2014-10817-015
AU  - Varni, James W.
AU  - Magnus, Brooke
AU  - Stucky, Brian D.
AU  - Liu, Yang
AU  - Quinn, Hally
AU  - Thissen, David
AU  - Gross, Heather E.
AU  - Huang, I-Chan
AU  - DeWalt, Darren A.
T1  - Psychometric properties of the PROMIS® pediatric scales: Precision, stability, and comparison of different scoring and administration options
JF  - Quality of Life Research: An International Journal of Quality of Life Aspects of Treatment, Care & Rehabilitation
JO  - Quality of Life Research: An International Journal of Quality of Life Aspects of Treatment, Care & Rehabilitation
JA  - Qual Life Res
Y1  - 2014/05//
VL  - 23
IS  - 4
SP  - 1233
EP  - 1243
PB  - Springer
SN  - 0962-9343
SN  - 1573-2649
AD  - Varni, James W., Department of Landscape Architecture and Urban Planning, College of Architecture, Texas A&M University, 3137 TAMU, College Station, TX, US, 77843-3137
N1  - Accession Number: 2014-10817-015. PMID: 24085345 Partial author list: First Author & Affiliation: Varni, James W.; Department of Pediatrics, College of Medicine, Texas A&M University, College Station, TX, US. Release Date: 20140512. Correction Date: 20190211. Publication Type: Journal (0100), Peer Reviewed Journal (0110). Format Covered: Electronic. Document Type: Journal Article. Language: EnglishMajor Descriptor: Psychometrics; Computerized Assessment. Minor Descriptor: Asthma; Major Depression; Test Reliability; Test Validity. Classification: Health Psychology Testing (2226); Health Psychology & Medicine (3360). Population: Human (10); Male (30); Female (40). Location: US. Age Group: Childhood (birth-12 yrs) (100); School Age (6-12 yrs) (180); Adolescence (13-17 yrs) (200). Tests & Measures: Asthma Control Test; Pediatric Asthma Quality of Life Questionnaire. Methodology: Empirical Study; Quantitative Study. References Available: Y. Page Count: 11. Issue Publication Date: May, 2014. Publication History: First Posted Date: Oct 2, 2013; Accepted Date: Sep 23, 2013. Copyright Statement: Springer Science+Business Media Dordrecht. 2013. 
AB  - Objectives: The objectives of the present study are to investigate the precision of static (fixed-length) short forms versus computerized adaptive testing (CAT) administration, response pattern scoring versus summed score conversion, and test–retest reliability (stability) of the Patient- Reported Outcomes Measurement Information System (PROMIS®) pediatric self-report scales measuring the latent constructs of depressive symptoms, anxiety, anger, pain interference, peer relationships, fatigue, mobility, upper extremity functioning, and asthma impact with polytomous items. Methods: Participants (N = 331) between the ages of 8 and 17 were recruited from outpatient general pediatrics and subspecialty clinics. Of the 331 participants, 137 were diagnosed with asthma. Three scores based on item response theory (IRT) were computed for each respondent: CAT response pattern expected a posteriori estimates, short-form response pattern expected a posteriori estimates, and short-form summed score expected a posteriori estimates. Scores were also compared between participants with and without asthma. To examine test–retest reliability, 54 children were selected for retesting approximately 2 weeks after the first assessment. Results: A short CAT (maximum 12 items with a standard error of 0.4) was found, on average, to be less precise than the static short forms. The CAT appears to have limited usefulness over and above what can be accomplished with the existing static short forms (8–10 items). Stability of the scale scores over a 2-week period was generally supported. Conclusion: The study provides further information on the psychometric properties of the PROMIS pediatric scales and extends the previous IRT analyses to include precision estimates of dynamic versus static administration, test– retest reliability, and validity of administration across groups. Both the positive and negative aspects of using CAT versus short forms are highlighted. (PsycINFO Database Record (c) 2019 APA, all rights reserved)
KW  - Computerized Adaptive Test
KW  - test reliability
KW  - test validity
KW  - information systems
KW  - depressive symptoms
KW  - asthma
KW  - Adolescent
KW  - Asthma
KW  - Child
KW  - Female
KW  - Humans
KW  - Information Systems
KW  - Male
KW  - Patient Outcome Assessment
KW  - Pediatrics
KW  - Psychometrics
KW  - Quality of Life
KW  - Reproducibility of Results
KW  - Self Report
KW  - Sensitivity and Specificity
KW  - Surveys and Questionnaires
KW  - Psychometrics
KW  - Computerized Assessment
KW  - Asthma
KW  - Major Depression
KW  - Test Reliability
KW  - Test Validity
U1  - Sponsor: National Institutes of Health, US. Grant: U01AR052181. Other Details: Through the NIH Roadmap for Medical Research. Recipients: No recipient indicated
DO  - 10.1007/s11136-013-0544-0
UR  - https://search.ebscohost.com/login.aspx?direct=true&db=psyh&AN=2014-10817-015&lang=de&site=ehost-live
UR  - ORCID: 0000-0003-2270-751X
UR  - jvarni@arch.tamu.edu
DP  - EBSCOhost
DB  - APA PsycInfo
ER  - 

TY  - JOUR
ID  - 2021-87976-001
AN  - 2021-87976-001
AU  - Kuzu, Duygu
AU  - Kallen, Michael A.
AU  - Kalpakjian, Claire Z.
AU  - Kratz, Anna L.
T1  - Psychometric properties of the spinal cord injury-quality of life (sci-qol) resilience item bank in a sample with spinal cord injury and chronic pain
JF  - Quality of Life Research: An International Journal of Quality of Life Aspects of Treatment, Care & Rehabilitation
JO  - Quality of Life Research: An International Journal of Quality of Life Aspects of Treatment, Care & Rehabilitation
JA  - Qual Life Res
Y1  - 2021/09/20/
PB  - Springer
SN  - 0962-9343
SN  - 1573-2649
AD  - Kuzu, Duygu
N1  - Accession Number: 2021-87976-001. Partial author list: First Author & Affiliation: Kuzu, Duygu; Department of Physical Medicine and Rehabilitation, University of Michigan, Ann Arbor, MI, US. Release Date: 20210923. Publication Type: Journal (0100), Peer Reviewed Journal (0110). Format Covered: Electronic. Document Type: Journal. Language: EnglishMajor Descriptor: No terms assigned. Classification: Psychological & Physical Disorders (3200). Publication History: Accepted Date: Aug 17, 2021. Copyright Statement: The Author(s), under exclusive licence to Springer Nature Switzerland AG. 2021. 
AB  - PurposeTo describe the psychometric properties (e.g., data distribution characteristics, convergent/discriminant validity, internal consistency reliability, and test administration characteristics) of the spinal cord injury quality of life measurement system (SCI-QOL) Resilience item bank delivered as a computer adaptive test (CAT) in a sample of individuals with chronic pain and spinal cord injury (SCI).MethodsDescriptive statistics were calculated to investigate variable data distribution characteristics. Correlation analyses were conducted for convergent and discriminant validity. Item response theory-derived reliability was calculated for the SCI-QOL Resilience CAT.ResultOne hundred thirty-three adults with SCI (N = 133; 73.5% male, 26.5% female) were enrolled. Sample mean T score on the SCI-QOL Resilience measure was 48.40, SD = 8.60 (min = 29.4; max = 70.0). The CAT administered between 4 (most common, 41.4% of cases) and 12 (9% of cases) items with the Mean#items = 5.73, SD = 2.45. The SCI-QOL Resilience CAT scores were normally distributed, with very low ceiling (0%) and floor (3%) effects. The SCI-QOL Resilience CAT had a reliability of 0.89, and the mean length of time for respondents to complete the SCI-QOL Resilience CAT was 44.34 s. SCI-QOL Resilience CAT validity was supported by significant moderate correlations with pain acceptance, depressive symptoms, pain catastrophizing, positive affect and well-being, and pain interference (convergent validity) and small non-significant correlations with age, sex, injury level, pain intensity, mobility level, and years since injury (discriminant validity).ConclusionThe SCI-QOL Resilience CAT demonstrated good convergent and discriminant validity. The CAT administration characteristics were impressive: With few items (low response burden), the scale achieved good reliability. (PsycInfo Database Record (c) 2021 APA, all rights reserved)
KW  - Resilience
KW  - Computer adaptive test
KW  - Spinal cord injury
KW  - Psychometrics
KW  - No terms assigned
DO  - 10.1007/s11136-021-02981-4
UR  - https://search.ebscohost.com/login.aspx?direct=true&db=psyh&AN=2021-87976-001&lang=de&site=ehost-live
UR  - ORCID: 0000-0002-1134-1551
UR  - dkuzu@med.umich.edu
DP  - EBSCOhost
DB  - APA PsycInfo
ER  - 

TY  - JOUR
ID  - 2012-01571-005
AN  - 2012-01571-005
AU  - Lai, Jin-Shei
AU  - Nowinski, Cindy
AU  - Victorson, David
AU  - Bode, Rita
AU  - Podrabsky, Tracy
AU  - McKinney, Natalie
AU  - Straube, Don
AU  - Holmes, Gregory L.
AU  - McDonald, Craig M.
AU  - Henricson, Erik
AU  - Abresch, R. Ted
AU  - Moy, Claudia S.
AU  - Cella, David
T1  - Quality-of-life measures in children with neurological conditions: Pediatric Neuro-QOL
JF  - Neurorehabilitation and Neural Repair
JO  - Neurorehabilitation and Neural Repair
JA  - Neurorehabil Neural Repair
Y1  - 2012/01//
VL  - 26
IS  - 1
SP  - 36
EP  - 47
PB  - Sage Publications
SN  - 1545-9683
SN  - 1552-6844
AD  - Lai, Jin-Shei, Department of Medical Social Sciences, Feinberg School of Medicine, Northwestern University, 710 N Lake Shore Drive, #724, Chicago, IL, US, 60611
N1  - Accession Number: 2012-01571-005. PMID: 21788436 Other Journal Title: Journal of Neurologic Rehabilitation. Partial author list: First Author & Affiliation: Lai, Jin-Shei; Department of Medical Social Sciences, Feinberg School of Medicine, Northwestern University, Chicago, IL, US. Release Date: 20120227. Correction Date: 20191114. Publication Type: Journal (0100), Peer Reviewed Journal (0110). Format Covered: Electronic. Document Type: Journal Article. Language: EnglishGrant Information: Cella, David. Major Descriptor: Nervous System Disorders; Pediatrics; Psychometrics; Quality of Life; Test Construction. Minor Descriptor: Test Reliability; Test Validity; Quality of Life Measures. Classification: Tests & Testing (2220); Neurological Disorders & Brain Damage (3297). Population: Human (10); Male (30); Female (40). Location: US. Age Group: Childhood (birth-12 yrs) (100); School Age (6-12 yrs) (180); Adolescence (13-17 yrs) (200). Tests & Measures: Quality of Life in Neurological Disorders; Lower Extremity (Mobility) Scale; Upper Extremity (ADL) Scale; Quality of Life in Neurological Disorders-Pediatric Version; Health-Related Quality of Life Questionnaire DOI: 10.1037/t74001-000. Methodology: Empirical Study; Quantitative Study. References Available: Y. Page Count: 12. Issue Publication Date: Jan, 2012. Copyright Statement: The Author(s). 2012. 
AB  - Background: A comprehensive, reliable, and valid measurement system is needed to monitor changes in children with neurological conditions who experience lifelong functional limitations. Objective. This article describes the development and psychometric properties of the pediatric version of the Quality of Life in Neurological Disorders (Neuro-QOL) measurement system. Methods: The pediatric Neuro-QOL consists of generic and targeted measures. Literature review, focus groups, individual interviews, cognitive interviews of children and consensus meetings were used to identify and finalize relevant domains and item content. Testing was conducted on 1018 children aged 10 to 17 years drawn from the US general population for generic measures and 171 similarly aged children with muscular dystrophy or epilepsy for targeted measures. Dimensionality was evaluated using factor analytic methods. For unidimensional domains, item parameters were estimated using item response theory models. Measures with acceptable fit indices were calibrated as item banks; those without acceptable fit indices were treated as summary scales. Results: Ten measures were developed: 8 generic or targeted banks (anxiety, depression, anger, interaction with peers, fatigue, pain, applied cognition, and stigma) and 2 generic scales (upper and lower extremity function). The banks reliably (r > 0.90) measured 63.2% to 100% of the children tested. Conclusions: The pediatric Neuro-QOL is a comprehensive measurement system with acceptable psychometric properties that could be used in computerized adaptive testing. The next step is to validate these measures in various clinical populations. (PsycINFO Database Record (c) 2019 APA, all rights reserved)
KW  - Pediatric Neuro-Quality Of Life
KW  - psychometrics
KW  - test validity
KW  - test reliability
KW  - neurological conditions
KW  - Adolescent
KW  - Anger
KW  - Anxiety
KW  - Case-Control Studies
KW  - Child
KW  - Cognition
KW  - Depression
KW  - Epilepsy
KW  - Fatigue
KW  - Female
KW  - Humans
KW  - Interpersonal Relations
KW  - Male
KW  - Muscular Dystrophies
KW  - Pain
KW  - Psychometrics
KW  - Quality of Life
KW  - Reproducibility of Results
KW  - Social Stigma
KW  - Surveys and Questionnaires
KW  - Nervous System Disorders
KW  - Pediatrics
KW  - Psychometrics
KW  - Quality of Life
KW  - Test Construction
KW  - Test Reliability
KW  - Test Validity
KW  - Quality of Life Measures
U1  - Sponsor: National Institute of Neurological Disorders and Stroke, US. Grant: HHSN265200423601C. Recipients: Cella, David (Prin Inv)
DO  - 10.1177/1545968311412054
UR  - https://search.ebscohost.com/login.aspx?direct=true&db=psyh&AN=2012-01571-005&lang=de&site=ehost-live
UR  - ORCID: 0000-0002-8779-3220
UR  - js-lai@northwestern.edu
DP  - EBSCOhost
DB  - APA PsycInfo
ER  - 

TY  - JOUR
ID  - 2003-05968-013
AN  - 2003-05968-013
AU  - Wiechmann, Darin
AU  - Ryan, Ann Marie
T1  - Reactions to computerized testing in selection contexts
JF  - International Journal of Selection and Assessment
JO  - International Journal of Selection and Assessment
Y1  - 2003/06//Jun-Sep, 2003
VL  - 11
IS  - 2-3
SP  - 215
EP  - 229
PB  - Blackwell Publishing
SN  - 0965-075X
SN  - 1468-2389
AD  - Wiechmann, Darin, Michigan State U, 135 Snyder Hall, East Lansing, MI, US, 48824
N1  - Accession Number: 2003-05968-013. Partial author list: First Author & Affiliation: Wiechmann, Darin; Michigan State U, East Lansing, MI, US. Other Publishers: Wiley-Blackwell Publishing Ltd. Release Date: 20030825. Correction Date: 20190211. Publication Type: Journal (0100), Peer Reviewed Journal (0110). Format Covered: Print. Document Type: Journal Article. Language: EnglishMajor Descriptor: Computer Attitudes; Internet; Personnel Selection; Test Taking; Computerized Assessment. Minor Descriptor: Adaptive Testing; Computers; Human Computer Interaction; Intention; Job Experience Level; Organizations. Classification: Personnel Management & Selection & Training (3620); Engineering & Environmental Psychology (4000). Population: Human (10); Male (30); Female (40). Location: US. Age Group: Adulthood (18 yrs & older) (300). Methodology: Empirical Study. References Available: Y. Page Count: 15. Issue Publication Date: Jun-Sep, 2003. 
AB  - Organizations are increasingly using computerized tests (e.g., multimedia, web-based, computer adaptive testing) in selection systems. A 2 (mode of presentation: paper-and-pencil--computerized) × 2 (technical level of the job: high technical job--low technical job) × 2 (selection decision: rejected or selected) between subjects design was used to assess proposed relationships between reactions to tests, their antecedents, and their consequences. While test-takers' post-test perceptions did not significantly differ as a result of mode of administration, computer anxiety and experience with computing were important factors in performing successfully. Significant relationships were found between post-feedback reactions and test-takers' intentions. The discussion highlights implications for implementing computerized selection tools. (PsycINFO Database Record (c) 2019 APA, all rights reserved)
KW  - computerized tests
KW  - personnel selection
KW  - organizations
KW  - computer adaptive testing
KW  - web-based testing
KW  - job technical level
KW  - selection decision
KW  - test-taking reactions
KW  - perceptions
KW  - intentions
KW  - attitudes
KW  - Computer Attitudes
KW  - Internet
KW  - Personnel Selection
KW  - Test Taking
KW  - Computerized Assessment
KW  - Adaptive Testing
KW  - Computers
KW  - Human Computer Interaction
KW  - Intention
KW  - Job Experience Level
KW  - Organizations
DO  - 10.1111/1468-2389.00245
UR  - https://search.ebscohost.com/login.aspx?direct=true&db=psyh&AN=2003-05968-013&lang=de&site=ehost-live
UR  - wiechma2@msu.edu
DP  - EBSCOhost
DB  - APA PsycInfo
ER  - 

TY  - JOUR
ID  - 2005-06556-002
AN  - 2005-06556-002
AU  - Blasco, Ricardo D.
T1  - Reclutamiento, selección de personal y las tecnologías de la informatión y de la comunicación = Recruitment, personnel selection and information and communication technologies
JF  - Revista de Psicología del Trabajo y de las Organizaciones
JO  - Revista de Psicología del Trabajo y de las Organizaciones
Y1  - 2004///
VL  - 20
IS  - 2
SP  - 141
EP  - 167
PB  - Colegio Oficial de Psicólogos de Madrid
SN  - 1576-5962
SN  - 2174-0534
N1  - Accession Number: 2005-06556-002. Translated Title: Recruitment, personnel selection and information and communication technologies. Other Journal Title: Journal of Work and Organizational Psychology; Psicología del Trabajo y Organizaciones. Partial author list: First Author & Affiliation: Blasco, Ricardo D.; Departamento de Psicología Social, Facultad de Psicología, Universidad de Barcelona, Barcelona, Spain. Other Publishers: Elsevier Science. Release Date: 20050906. Correction Date: 20190211. Publication Type: Journal (0100), Peer Reviewed Journal (0110). Format Covered: Print. Document Type: Journal Article. Language: Spanish; CastilianMajor Descriptor: Communication Systems; Organizational Behavior; Personnel Selection; Psychological Assessment; Technology. Minor Descriptor: Computer Applications; Internet; Teleconferencing; Computerized Assessment; Videoconferencing. Classification: Personnel Management & Selection & Training (3620). Population: Human (10). Age Group: Adulthood (18 yrs & older) (300). References Available: Y. Page Count: 27. Issue Publication Date: 2004. 
AB  - In spite of the diversity of practices in personnel recruiting and selection (PRS), a tendency to use current technologies is becoming more and more a practice both for automation of psychological assessment actions and for off-site and on-request assessment (24×7). New tests embodying enriched items or virtual reality simulations together with classical methods such as adaptive tests, change practice scenarios and pose academic challenges. Videoconference interviews are of particular interest because of the ubiquity of interviews in personnel recruitment and selection. All these changes in techniques and methods need research and reflection. This paper presents a synopsis of current knowledge coming both from literature and from professional practice. It is a radical situation that affects by and large both strategies and relations with candidates. The kernel is to achieve the effectiveness of PRS processes within established ethical and deontological limits. (PsycINFO Database Record (c) 2019 APA, all rights reserved)
KW  - personnel selection
KW  - communication technologies
KW  - psychological assessment
KW  - professional practice
KW  - organizational behavior
KW  - videoconferencing
KW  - Communication Systems
KW  - Organizational Behavior
KW  - Personnel Selection
KW  - Psychological Assessment
KW  - Technology
KW  - Computer Applications
KW  - Internet
KW  - Teleconferencing
KW  - Computerized Assessment
KW  - Videoconferencing
UR  - https://search.ebscohost.com/login.aspx?direct=true&db=psyh&AN=2005-06556-002&lang=de&site=ehost-live
DP  - EBSCOhost
DB  - APA PsycInfo
ER  - 

TY  - JOUR
ID  - 2022-43740-001
AN  - 2022-43740-001
AU  - Forsyth, Rob J.
AU  - Roberts, Liz
AU  - Henderson, Rob
AU  - Wales, Lorna
T1  - Rehabilitation after paediatric acquired brain injury: Longitudinal change in content and effect on recovery
JF  - Developmental Medicine & Child Neurology
JO  - Developmental Medicine & Child Neurology
JA  - Dev Med Child Neurol
Y1  - 2022/09//
VL  - 64
IS  - 9
SP  - 1168
EP  - 1175
PB  - Wiley-Blackwell Publishing Ltd.
SN  - 0012-1622
SN  - 1469-8749
AD  - Forsyth, Rob J., Royal Victoria Infirmary, Sir James Spence Building, Newcastle upon Tyne, United Kingdom, NE1 4LP
N1  - Accession Number: 2022-43740-001. Partial author list: First Author & Affiliation: Forsyth, Rob J.; Translational and Clinical Research Institute, Newcastle University, Newcastle upon Tyne, United Kingdom. Other Publishers: Mac Keith Press. Release Date: 20220310. Correction Date: 20220822. Publication Type: Journal (0100), Peer Reviewed Journal (0110). Format Covered: Electronic. Document Type: Journal Article. Language: EnglishMajor Descriptor: Cognitive Rehabilitation; Pediatrics; Severity (Disorders); Neurorehabilitation; Brain Injuries. Minor Descriptor: Adaptive Testing; Adjustment; Multidimensional Scaling; Rehabilitation. Classification: Neurological Disorders & Brain Damage (3297); Rehabilitation (3380). Population: Human (10); Male (30); Female (40). Age Group: Childhood (birth-12 yrs) (100); Infancy (2-23 mo) (140); Preschool Age (2-5 yrs) (160); School Age (6-12 yrs) (180); Adolescence (13-17 yrs) (200). Tests & Measures: Pediatric Evaluation of Disability Inventory –Computer Adaptive Testing. Methodology: Empirical Study; Longitudinal Study; Quantitative Study. Supplemental Data: Appendixes Internet; Experimental Materials Internet. References Available: Y. Page Count: 8. Issue Publication Date: Sep, 2022. Publication History: Accepted Date: Feb 11, 2022; First Submitted Date: Sep 23, 2021. Copyright Statement: Published by John Wiley & Sons Ltd on behalf of Mac Keith Press. This is an open access article under the terms of the Creative Commons Attribution License, which permits use, distribution and reproduction in any medium, provided the original work is properly cited. The Authors—Developmental Medicine & Child Neurology. 2022. 
AB  - Aim: To describe cross‐sectional and longitudinal variation in neurorehabilitation content provided to young people after severe paediatric acquired brain injury (pABI) and to relate this to observed functional recovery. Method: This was an observational study in a cohort of admissions to a residential neurorehabilitation centre. Recovery was described using the Pediatric Evaluation of Disability—Computer Adaptive Testing instrument. Rehabilitation content was measured using the recently described Paediatric Rehabilitation Ingredients Measure (PRISM) and examined using multidimensional scaling. Results: The PRISM reveals wide variation in rehabilitation content between and during admissions primarily reflecting proportions of child active practice, child emotional support, and other management of body structure and function. Rehabilitation content is predicted by pre‐admission recovery, suggesting therapist decisions in designing rehabilitation programmes are shaped by their initial expectations of recovery. However, significant correlations persist between plausibly‐related aspects of delivered therapy and observed post‐admission recovery after adjusting for such effects. Interpretation: The PRISM approach to the analysis of rehabilitation content shows promise in that it demonstrates significant correlations between plausibly‐related aspects of delivered therapy and observed recovery that have been hard to identify with other approaches. However, rigorous, causal analysis will be required to truly understand the contributions of rehabilitation to recovery after pABI. (PsycInfo Database Record (c) 2022 APA, all rights reserved)
KW  - Neurorehabilitation
KW  - pediatrics
KW  - acquired brain injury
KW  - recovery
KW  - severity
KW  - Cognitive Rehabilitation
KW  - Pediatrics
KW  - Severity (Disorders)
KW  - Neurorehabilitation
KW  - Brain Injuries
KW  - Adaptive Testing
KW  - Adjustment
KW  - Multidimensional Scaling
KW  - Rehabilitation
DO  - 10.1111/dmcn.15199
UR  - https://search.ebscohost.com/login.aspx?direct=true&db=psyh&AN=2022-43740-001&lang=de&site=ehost-live
UR  - ORCID: 0000-0002-5657-4180
UR  - rob.forsyth@newcastle.ac.uk
DP  - EBSCOhost
DB  - APA PsycInfo
ER  - 

TY  - THES
ID  - 2008-99171-014
AN  - 2008-99171-014
AU  - Bremer, Coleen Ann
T1  - Relationships between NWEA-map score information and teachers' efficacy beliefs for student performance on the North Dakota state assessment
JF  - Dissertation Abstracts International Section A: Humanities and Social Sciences
JO  - Dissertation Abstracts International Section A: Humanities and Social Sciences
Y1  - 2008///
VL  - 69
IS  - 3-A
SP  - 871
EP  - 871
PB  - ProQuest Information & Learning
SN  - 0419-4209
N1  - Accession Number: 2008-99171-014. Other Journal Title: Dissertation Abstracts International. Partial author list: First Author & Affiliation: Bremer, Coleen Ann; North Dakota State U., US. Release Date: 20081013. Correction Date: 20221128. Publication Type: Dissertation Abstract (0400). Format Covered: Electronic. Document Type: Dissertation. Language: EnglishMajor Descriptor: Academic Achievement; Educational Programs; Self-Efficacy; Students; Teacher Attitudes. Minor Descriptor: Educational Measurement. Classification: Educational & School Psychology (3500). Population: Human (10). Age Group: Adulthood (18 yrs & older) (300). Methodology: Empirical Study; Quantitative Study. Page Count: 1. 
AB  - In the current environment of federally mandated high-stakes testing, teachers and schools are under great pressure to ensure that students demonstrate proficiency on state assessments. Without formative feedback that connects what a teacher did in the classroom with positive outcomes on the state assessment, a teacher may not feel that he or she can make a difference in how students fare on the state assessment. Teachers who experience low self-efficacy use less effective teaching techniques, and this affects student learning. Many schools are now using computer adaptive testing, such as the Northwest Evaluation Association Measures of Academic Progress (NWEA-MAP), to provide teachers with formative information to use for instructional improvement. When adjustments to instruction result in higher student achievement, it can provide the teacher with mastery experiences that increase the teacher's efficacy perceptions. The purpose of this study was to examine the relationships between teachers' efficacy beliefs for making a difference in their students' performances on the state assessment and teachers' uses of NWEA-MAP test results. Teachers were surveyed using four measures of teacher efficacy and collective teacher efficacy. Two measures that were created for this study demonstrated high reliability with the study sample and correlated well with the two existing scales. Data were also collected on the ways teachers used NWEA-MAP score information and the number of different ways they used score information. The results indicated that the number of ways teachers used NWEA-MAP score information made a significant difference in teachers' scores on the efficacy measures: Using score information in three or four different ways was associated with the highest scores on the efficacy scales. Also, using test score information to set goals with students and to individualize instruction predicted teachers' efficacy scores to a greater degree than did using scores to adjust curriculum and instruction. The current study, with its two new measures and its suggestions of relationships between variables, provides a starting point for new lines of research in sources of teacher efficacy beliefs. (PsycInfo Database Record (c) 2022 APA, all rights reserved)
KW  - Northwest Evaluation Association Measures of Academic Progress
KW  - teachers efficacy beliefs
KW  - student performance
KW  - North Dakota state assessment
KW  - Academic Achievement
KW  - Educational Programs
KW  - Self-Efficacy
KW  - Students
KW  - Teacher Attitudes
KW  - Educational Measurement
UR  - https://search.ebscohost.com/login.aspx?direct=true&db=psyh&AN=2008-99171-014&lang=de&site=ehost-live
DP  - EBSCOhost
DB  - APA PsycInfo
ER  - 

TY  - JOUR
ID  - 2019-75298-001
AN  - 2019-75298-001
AU  - Carlozzi, Noelle E.
AU  - Boileau, Nicholas R.
AU  - Kallen, Michael A.
AU  - Nakase-Richardson, Risa
AU  - Hahn, Elizabeth A.
AU  - Tulsky, David S.
AU  - Miner, Jennifer A.
AU  - Hanks, Robin A.
AU  - Massengale, Jill P.
AU  - Lange, Rael T.
AU  - Brickell, Tracey A.
AU  - French, Louis M.
AU  - Ianni, Phillip A.
AU  - Sander, Angelle M.
T1  - Reliability and validity data to support the clinical utility of the Traumatic Brain Injury Caregiver Quality of Life (TBI-CareQOL)
T3  - Caregivers of Service Members/Veterans and Civilians With Traumatic Brain Injury
JF  - Rehabilitation Psychology
JO  - Rehabilitation Psychology
JA  - Rehabil Psychol
Y1  - 2020/11//
VL  - 65
IS  - 4
SP  - 323
EP  - 336
PB  - American Psychological Association
SN  - 0090-5550
SN  - 1939-1544
SN  - 978-1-4338-9417-6
AD  - Carlozzi, Noelle E., Department of Physical Medicine and Rehabilitation, University of Michigan, North Campus Research Complex, 2800 Plymouth Road, Building NCRC B14, Room G216, Ann Arbor, MI, US, 48109-2800
N1  - Accession Number: 2019-75298-001. PMID: 31829641 Other Journal Title: Psychological Aspects of Disability. Partial author list: First Author & Affiliation: Carlozzi, Noelle E.; Department of Physical Medicine and Rehabilitation, University of Michigan, Ann Arbor, MI, US. Other Publishers: Division 22 of the American Psychological Association; Educational Publishing Foundation; Springer Publishing. Release Date: 20191212. Correction Date: 20201231. Publication Type: Journal (0100), Peer Reviewed Journal (0110). Format Covered: Electronic. Document Type: Journal Article. ISBN: 978-1-4338-9417-6. Language: EnglishMajor Descriptor: Caregiver Burden; Caregivers; Measurement; Traumatic Brain Injury; Health Related Quality of Life. Minor Descriptor: Attitudes; Quality of Life; Test Construction; Test Reliability; Test Validity; Quality of Life Measures. Classification: Health Psychology Testing (2226); Home Care & Hospice (3375). Population: Human (10); Male (30); Female (40). Location: US. Age Group: Adulthood (18 yrs & older) (300). Tests & Measures: Traumatic Brain Injury Caregiver Quality of Life Measure; Rand-12 Health Status Inventory; Caregiver Appraisal Scale; Patient-Reported Outcomes Measurement Information System-Sleep-Related Impairment-Short Form; Patient-Reported Outcomes Measurement Information System-Sleep-Related Impairment-Computer Adaptive Test; Neuro-QoL Positive Affect and Well-Being Scale; NIH Toolbox Perceived Stress; NIH Toolbox-General Life Satisfaction and Self Efficacy; Traumatic Brain Injury-Quality of Life Grief/Loss; Mayo-Portland Adaptability Inventory-4 DOI: 10.1037/t29117-000. Methodology: Empirical Study; Quantitative Study. Page Count: 14. Issue Publication Date: Nov, 2020. Publication History: First Posted Date: Dec 12, 2019; Accepted Date: Sep 27, 2019; Revised Date: Sep 27, 2019; First Submitted Date: Apr 22, 2019. Copyright Statement: American Psychological Association. 2019. 
AB  - Objective: The Traumatic Brain Injury Caregiver Quality of Life (TBI-CareQOL) is a patient-reported outcome measurement system that is specific to caregivers of civilians and service members/veterans (SMVs) with traumatic brain injury (TBI). This measurement system includes 26 item banks that represent both generic (i.e., borrowed from existing measurement systems) and caregiver-specific components of health-related quality of life (HRQOL). This report provides reliability and validity data for measures within the TBI-CareQOL that have not previously been reported (i.e., 4 caregiver-specific and 7 generic measures of HRQOL). Design: Three hundred eighty-five caregivers of persons with TBI completed caregiver-specific computer adaptive tests (CATs) for Feelings of Loss-Self, Caregiver Strain, Caregiver-Specific Anxiety, and Feeling Trapped, as well as generic measures of HRQOL from complementary measurement systems (i.e., Neuro-QoL Positive Affect and Well-Being; PROMIS Sleep-Related Impairment; NIH Toolbox Perceived Stress, General Life Satisfaction, and Self Efficacy; TBI-QOL Resilience and Grief/Loss). Caregivers also completed several additional measures to establish convergent and discriminant validity, as well as the Mayo Portland Adaptability Index, 4th ed. Results: Findings support the internal consistency reliability (all alphas > .85) and test-retest stability (all alphas >.73) of the TBI-CareQOL measures. Convergent validity was supported by moderate to high correlations between the TBI-CareQOL measures and related measures, whereas discriminant validity was supported by low correlations between the TBI-CareQOL measures and unrelated constructs. Known-groups validity was also supported. Conclusions: Findings support the reliability and validity of the item banks that comprise the TBI-CareQOL Measurement System. These measures should be considered for any standardized assessment of HRQOL in caregivers of civilians and SMVs with TBI. (PsycInfo Database Record (c) 2020 APA, all rights reserved)
KW  - caregiver
KW  - quality of life
KW  - traumatic brain injury
KW  - caregiver burden
KW  - patient-reported outcome
KW  - Caregiver Burden
KW  - Caregivers
KW  - Measurement
KW  - Traumatic Brain Injury
KW  - Health Related Quality of Life
KW  - Attitudes
KW  - Quality of Life
KW  - Test Construction
KW  - Test Reliability
KW  - Test Validity
KW  - Quality of Life Measures
U1  - Sponsor: National Institutes of Health, National Institute of Nursing Research, US. Grant: R01NR013658. Recipients: No recipient indicated
U1  - Sponsor: General Dynamics Information Technology, Inc.. Recipients: No recipient indicated
U1  - Sponsor: National Center for Advancing Translational Sciences, US. Grant: UL1TR000433. Recipients: No recipient indicated
DO  - 10.1037/rep0000295
UR  - https://search.ebscohost.com/login.aspx?direct=true&db=psyh&AN=2019-75298-001&lang=de&site=ehost-live
UR  - ORCID: 0000-0002-9451-0604
UR  - ORCID: 0000-0002-4056-4404
UR  - ORCID: 0000-0003-0439-9429
UR  - carlozzi@med.umich.edu
DP  - EBSCOhost
DB  - APA PsycInfo
ER  - 

TY  - JOUR
ID  - 2008-15125-013
AN  - 2008-15125-013
AU  - Chen, Ping
AU  - Ding, Shu-Liang
T1  - Research on computerized adaptive testing that allows reviewing and changing answers
JF  - Acta Psychologica Sinica
JO  - Acta Psychologica Sinica
JA  - Xin Li Xue Bao
Y1  - 2008///
VL  - 40
IS  - 6
SP  - 737
EP  - 747
PB  - Science Press
SN  - 0439-755X
AD  - Ding, Shu-Liang, Computer Information Engineering College, Jiangxi Normal University, Nanchang, China, 330027
N1  - Accession Number: 2008-15125-013. Partial author list: First Author & Affiliation: Chen, Ping; Developmental Psychology Institute, Beijing Normal University, Beijing, China. Release Date: 20091012. Publication Type: Journal (0100), Peer Reviewed Journal (0110). Format Covered: Print. Document Type: Journal Article. Language: ChineseMajor Descriptor: Adaptive Testing; Decision Making; Preferences. Minor Descriptor: Simulation. Classification: Cognitive Processes (2340). Population: Human (10). Methodology: Empirical Study; Quantitative Study; Scientific Simulation. References Available: Y. Page Count: 11. Issue Publication Date: 2008. 
AB  - In the past decade, some paper-and-pencil (P&P) tests have been replaced by computerized adaptive testing (CAT) within many large-scale standardized testing programs. However, many researches and applications on CAT had limitations because most of the CAT did not allow examinees to review and change their answers. Among the operative CAT applications, there was only one that incorporated item review. Not allowing examinees to review and change their answers would result in test pressure and affect their performances. Moreover, a majority of examinees manifested a clear preference for item review because they believed that the inclusion of item review made the test fairer and considered it to be a disadvantage if review was disallowed. The CAT test organizers did not allow examinees to review and change answers mainly because they were apprehensive that examinees would use the deceptive Wainer strategy in the review stage to obtain positively biased ability estimates, consequently affecting the fairness and precision of the test. If we could provide a solution that not only allowed examinees to review and change answers but that was also able to deal with the Wainer strategy, the meaning would be great for the development of CAT. Until now, there have been few relevant studies on this topic worldwide. Moreover, the previous studies had a nonnegligible disadvantage, in that the researchers only recorded the answers of the review stage and used them as a basis for scoring, without considering the answers of the adaptive stage. We assumed that comprehensively considering the answers before and after review could produce a more accurate ability estimation. Therefore, this paper employed a new scoring method and attempted to deal with the Wainer strategy: Vαj = Beta × Uαj + (1 − Beta) × Uα, j + m This study involved two experiments. Experiment 1 used the Monte Carlo method to simulate the entire process of CAT that allows the reviewing and changing of answers, with the aim of investigating the influence of different beta values on ability estimation. Experiment 2 used simulation data generated by the Monte Carlo method to evaluate the effectiveness of the Wainer strategy and attempted to deal with the strategy by using a new scoring method. The simulation results of Experiment 1 indicated the following. First, comprehensively considering the answers before and after review did produce a more accurate ability estimation, and the most accurate estimates occurred when beta = 0.50. Second, the share of examinees who changed their answers was 66.80%; further, 6.40% of the answers were changed, and 75% of the modified answers represented changes from incorrect to correct answers. Experiment 2 indicated the following: When using the new scoring method, the ability estimates generated by the CAT involving the use of the Wainer strategy obviously diverged from the true ability values. Moreover, the bias increased as the true ability value increased. The new scoring method employed in this study was not able to effectively deal with the Wainer strategy because of the abnormal ability estimates and abnormal estimated standard error. However, through a simulation experiment, we found the following: When beta = 0, comprehensively considering the expected a posteriori (EAP) and maximum likelihood estimation (MLE) ability estimates of CAT involving the use of the Wainer strategy succeeded in roughly dealing with the Wainer strategy. Our future task involves developing a more accurate method to deal with the Wainer Strategy. (PsycINFO Database Record (c) 2016 APA, all rights reserved)
KW  - computerized adaptive testing
KW  - simulation
KW  - decision making
KW  - preferences
KW  - Adaptive Testing
KW  - Decision Making
KW  - Preferences
KW  - Simulation
DO  - 10.3724/SP.J.1041.2008.00737
UR  - https://search.ebscohost.com/login.aspx?direct=true&db=psyh&AN=2008-15125-013&lang=de&site=ehost-live
UR  - cling06026@163.com
DP  - EBSCOhost
DB  - APA PsycInfo
ER  - 

TY  - JOUR
ID  - 2020-62552-001
AN  - 2020-62552-001
AU  - Carlozzi, Noelle E.
AU  - Boileau, Nicholas R.
AU  - Roché, Matthew W.
AU  - Ready, Rebecca E.
AU  - Perlmutter, Joel S.
AU  - Chou, Kelvin L.
AU  - Barton, Stacey K.
AU  - McCormack, Michael K.
AU  - Stout, Julie C.
AU  - Cella, David
AU  - Miner, Jennifer A.
AU  - Paulsen, Jane S.
T1  - Responsiveness to change over time and test-retest reliability of the PROMIS and Neuro-QoL mental health measures in persons with Huntington disease (HD)
JF  - Quality of Life Research: An International Journal of Quality of Life Aspects of Treatment, Care & Rehabilitation
JO  - Quality of Life Research: An International Journal of Quality of Life Aspects of Treatment, Care & Rehabilitation
JA  - Qual Life Res
Y1  - 2020/12//
VL  - 29
IS  - 12
SP  - 3419
EP  - 3439
PB  - Springer
SN  - 0962-9343
SN  - 1573-2649
AD  - Carlozzi, Noelle E., Department of Physical Medicine & Rehabilitation, University of Michigan, North Campus Research Complex, 2800 Plymouth Road, Building NCRC B14, Room G213, Ann Arbor, MI, US, 48109-2800
N1  - Accession Number: 2020-62552-001. PMID: 32813263 Partial author list: First Author & Affiliation: Carlozzi, Noelle E.; Department of Physical Medicine and Rehabilitation, University of Michigan, Ann Arbor, MI, US. Release Date: 20200824. Correction Date: 20221128. Publication Type: Journal (0100), Peer Reviewed Journal (0110). Format Covered: Electronic. Document Type: Journal Article. Language: EnglishMajor Descriptor: Huntingtons Disease; Mental Health; Psychometrics; Quality of Life Measures; Test-Retest Reliability. Minor Descriptor: Test Responsiveness. Classification: Health Psychology Testing (2226); Physical & Somatic Disorders (3290). Population: Human (10); Male (30); Female (40); Outpatient (60). Location: US. Age Group: Adulthood (18 yrs & older) (300). Tests & Measures: Unified Huntington’s Disease Rating Scales; Problem Behaviors Assessment—Short Form; Neuro-QoL Stigma Scale--Computer Adaptive Test; Neuro-QOL Depression Scale--Computer Adaptive Test; PROMIS-Depression--Short Form; Neuro-QoL Anxiety Scale--Computer Adaptive Test; PROMIS Anxiety--Short Form; PROMIS Anger--Computer Adaptive Test; PROMIS Anger--Short Form; Neuro-QoL Positive Affect and Well-Being Scale--Computer Adaptive Test; Neuro-QoL Positive Affect and Well-Being Scale--Short Form; Neuro-QoL Emotional & Behavioral Dyscontrol Scale--Computer Adaptive Test; Neuro-QoL Emotional and Behavioral Dyscontrol--Short Form; Neuro-QoL Stigma Scale--Short Form; Health-Related Quality of Life Questionnaire DOI: 10.1037/t74001-000. Methodology: Empirical Study; Followup Study; Longitudinal Study; Interview; Quantitative Study. Supplemental Data: Tables and Figures Internet. References Available: Y. Page Count: 21. Issue Publication Date: Dec, 2020. Publication History: First Posted Date: Aug 19, 2020; Accepted Date: Jul 28, 2020. Copyright Statement: Springer Nature Switzerland AG. 2020. 
AB  - Background: The majority of persons with Huntington disease (HD) experience mental health symptoms. Patient-reported outcome (PRO) measures are capable of capturing unobservable behaviors and feelings relating to mental health. The current study aimed to test the reliability and responsiveness to self-reported and clinician-rated change over time of Neuro-QoL and PROMIS mental health PROs over the course of a 24-month period. Methods: At baseline, 12-months, and 24-months, 362 participants with premanifest or manifest HD completed the Neuro-QoL Depression computer adaptive test (CAT), PROMIS Depression short form (SF), Neuro-QoL Anxiety CAT, PROMIS Anxiety SF, PROMIS Anger CAT and SF, Neuro-QoL Emotional/Behavioral Dyscontrol CAT and SF, Neuro-QoL Positive Affect and Well-Being CAT and SF, and Neuro-QoL Stigma CAT and SF. Participants completed several clinician-administered measures at each time point, as well as several global ratings of change at 12- and 24-months. Reliability (test-retest reliability and measurement error) and responsiveness (using standardized response means and general linear models) were assessed. Results: Test-retest reliability and measurement error were excellent for all PROs (all ICC ≥ .90 for test-retest reliability and all SEM percentages ≤ 6.82%). In addition, 12- and 24-month responsiveness were generally supported for the Neuro-QoL and PROMIS mental health PROs; findings relative to clinician-rated anchors of change (e.g., SRMs for the group with declines ranged from .38 to .91 for 24-month change and .09 to .45, with the majority above .25 for 12-month change) were generally more robust than those relative to self-reported anchors of change (e.g., SRMs for the group with declines ranged from .02 to .75, with the majority above .39 for 24-month change and .09 to .45, with the majority above .16 for 12-month change). Conclusions: The Neuro-QoL and PROMIS mental health PROs demonstrated strong psychometric reliability, as well as responsiveness to self-reported and clinician-rated change over time in people with HD. (PsycInfo Database Record (c) 2022 APA, all rights reserved)
KW  - Neuro-QoL
KW  - PROMIS
KW  - Emotion
KW  - Mental health
KW  - Validity
KW  - Reliability
KW  - Huntington disease
KW  - Adult
KW  - Female
KW  - Humans
KW  - Huntington Disease
KW  - Male
KW  - Mental Health
KW  - Middle Aged
KW  - Patient Reported Outcome Measures
KW  - Quality of Life
KW  - Reproducibility of Results
KW  - Huntingtons Disease
KW  - Mental Health
KW  - Psychometrics
KW  - Quality of Life Measures
KW  - Test-Retest Reliability
KW  - Test Responsiveness
U1  - Sponsor: National Institutes of Health, National Institute of Neurological Disorders and Stroke, US. Grant: R01NS077946. Recipients: No recipient indicated
U1  - Sponsor: National Center for Advancing Translational Sciences, US. Grant: UL1TR000433. Recipients: No recipient indicated
U1  - Sponsor: National Institutes of Health, National Institute of Neurological Disorders and Stroke, US. Grant: R01NS040068. Other Details: Predict-HD study. Recipients: No recipient indicated
U1  - Sponsor: National Institutes of Health, Center for Inherited Disease Research, US. Recipients: No recipient indicated
U1  - Sponsor: CHDI Foundation. Other Details: University of Iowa. Recipients: No recipient indicated
DO  - 10.1007/s11136-020-02596-1
UR  - https://search.ebscohost.com/login.aspx?direct=true&db=psyh&AN=2020-62552-001&lang=de&site=ehost-live
UR  - ORCID: 0000-0003-0439-9429
UR  - carlozzi@med.umich.edu
DP  - EBSCOhost
DB  - APA PsycInfo
ER  - 

TY  - JOUR
ID  - 2015-16006-007
AN  - 2015-16006-007
AU  - Curi, Mariana
T1  - Review of Computerized multistage testing: Theory and applications
JF  - Applied Psychological Measurement
JO  - Applied Psychological Measurement
JA  - Appl Psychol Meas
Y1  - 2015/05//
VL  - 39
IS  - 3
SP  - 245
EP  - 246
PB  - Sage Publications
SN  - 0146-6216
SN  - 1552-3497
N1  - Accession Number: 2015-16006-007. Partial author list: First Author & Affiliation: Curi, Mariana; University of Sao Paulo, Sao Carlos, Brazil. Release Date: 20150525. Correction Date: 20190211. Publication Type: Journal (0100), Peer Reviewed Journal (0110). Format Covered: Electronic. Document Type: Review-Book. Language: EnglishMajor Descriptor: Adaptive Testing; Decision Making; Computerized Assessment. Classification: Tests & Testing (2220). Population: Human (10). Reviewed Item: Yan, D.; von Davier, A. A.; Lewis, C. Computerized multistage testing: Theory and applications=Boca Raton, FL: Chapman & Hall. 492 pp. $90.00; 2014. References Available: Y. Page Count: 2. Issue Publication Date: May, 2015. Copyright Statement: The Author(s). 2014. 
AB  - Reviews the book, Computerized Multistage Testing: Theory and Applications by D. Yan, A. A. von Davier, and C. Lewis (2014). This book presents multistage testing (MST), a useful methodology for the design and administration of adaptive tests that exhibits several practical advantages over traditional computerized adaptive testing. The book mirrors the process of developing and implementing an MST from beginning to end. It takes into consideration the decisions that need to be made at each step including the questions that need to be considered in the decision-making process, as well as the various methodologies that exist to implement the MST design chosen. The authors succeed in merging theoretical concepts with the operational and implementation aspects of MST, making the reading fruitful and motivating. The strength of the book is that it addresses the needs of practitioners in the testing industry, while still remaining theoretical enough to be of interest to graduate students and scientific researchers. (PsycINFO Database Record (c) 2019 APA, all rights reserved)
KW  - computerized multistage testing
KW  - decision-making process
KW  - adaptive testing
KW  - Adaptive Testing
KW  - Decision Making
KW  - Computerized Assessment
U2  - Yan, D.; von Davier, A. A.; Lewis, C. (2014); Computerized multistage testing: Theory and applications; Boca Raton, FL: Chapman & Hall. 492 pp. $90.00; 978-1466505773.
DO  - 10.1177/0146621614559744
UR  - https://search.ebscohost.com/login.aspx?direct=true&db=psyh&AN=2015-16006-007&lang=de&site=ehost-live
DP  - EBSCOhost
DB  - APA PsycInfo
ER  - 

TY  - JOUR
ID  - 1999-10557-004
AN  - 1999-10557-004
AU  - Vispoel, Walter P.
T1  - Reviewing and changing answers on computer-adaptive and self-adaptive vocabulary tests
JF  - Journal of Educational Measurement
JO  - Journal of Educational Measurement
JA  - J Educ Meas
Y1  - 1998///Win 1998
VL  - 35
IS  - 4
SP  - 328
EP  - 345
PB  - Blackwell Publishing
SN  - 0022-0655
SN  - 1745-3984
N1  - Accession Number: 1999-10557-004. Partial author list: First Author & Affiliation: Vispoel, Walter P.; U Iowa, Iowa City, IA, US. Other Publishers: Wiley-Blackwell Publishing Ltd. Release Date: 19990501. Correction Date: 20221128. Publication Type: Journal (0100), Peer Reviewed Journal (0110). Format Covered: Print. Document Type: Journal Article. Language: EnglishConference Information: National Council on Measurement in Education, Apr, 1996, New York City, NY, US. Conference Note: The present article is an extension of a paper presented at the aforementioned conference. Major Descriptor: Adaptive Testing; Test Items; Vocabulary; Computerized Assessment. Classification: Educational & School Psychology (3500). Population: Human (10). Location: US. Age Group: Adulthood (18 yrs & older) (300); Young Adulthood (18-29 yrs) (320). Methodology: Empirical Study. Page Count: 18. Issue Publication Date: Win 1998. 
AB  - Compared results obtained from computer-adaptive (CAT) and self-adaptive tests (SAT) under conditions in which item review was permitted and not permitted. 379 college students completed a test anxiety inventory, one form of a computerized vocabulary test (CAT or SAT), and questionnaires assessing demographic information and attitudes about computerized tests. Comparisons of answers before and after review within the 'review' condition showed that a small percentage of answers was changed, that more answers were changed from wrong to right than from right to wrong, that most Ss changed answers to at least some questions, and that most Ss who changed answers improved their ability estimates by doing so. (PsycInfo Database Record (c) 2022 APA, all rights reserved)
KW  - reviewing & changing answers on computer-adaptive vs self-adaptive vocabulary tests
KW  - adults
KW  - Adaptive Testing
KW  - Test Items
KW  - Vocabulary
KW  - Computerized Assessment
DO  - 10.1111/j.1745-3984.1998.tb00542.x
UR  - https://search.ebscohost.com/login.aspx?direct=true&db=psyh&AN=1999-10557-004&lang=de&site=ehost-live
DP  - EBSCOhost
DB  - APA PsycInfo
ER  - 

TY  - JOUR
ID  - 2022-71415-015
AN  - 2022-71415-015
AU  - Spittle, Alicia J.
AU  - Olsen, Joy E.
AU  - FitzGerald, Tara L.
AU  - Cameron, Kate L.
AU  - Albesher, Reem A.
AU  - Mentiplay, Benjamin F.
AU  - Treyvaud, Karli
AU  - Burnett, Alice
AU  - Lee, Katherine J.
AU  - Pascoe, Leona
AU  - Roberts, Gehan
AU  - Doyle, Lex W.
AU  - Anderson, Peter
AU  - Cheong, Jeanie L. Y.
T1  - School readiness in children born &lt;30 weeks' gestation at risk for developmental coordination disorder: A prospective cohort study
JF  - Journal of Developmental and Behavioral Pediatrics
JO  - Journal of Developmental and Behavioral Pediatrics
JA  - J Dev Behav Pediatr
Y1  - 2022/06//Jun-Jul, 2022
VL  - 43
IS  - 5
SP  - e312
EP  - e319
PB  - Lippincott Williams & Wilkins
SN  - 0196-206X
SN  - 1536-7312
AD  - Spittle, Alicia J., Physiotherapy Department, University of Melbourne, 7th Floor Alan Gilbert Building, Grattan St., Parkville, VIC, Australia, 3052
N1  - Accession Number: 2022-71415-015. Partial author list: First Author & Affiliation: Spittle, Alicia J.; Department of Physiotherapy, University of Melbourne, Parkville, VIC, Australia. Release Date: 20220721. Correction Date: 20221128. Publication Type: Journal (0100), Peer Reviewed Journal (0110). Format Covered: Electronic. Document Type: Journal Article. Language: EnglishGrant Information: Anderson, Peter. Major Descriptor: Developmental Disabilities; Premature Birth; School Readiness; Dyspraxia. Minor Descriptor: Cognitive Ability; Language Delay; Motor Coordination; Socioemotional Functioning. Classification: Neurodevelopmental & Autism Spectrum Disorders (3250); Classroom Dynamics & Student Adjustment & Attitudes (3560). Population: Human (10); Male (30); Female (40). Location: Australia. Age Group: Childhood (birth-12 yrs) (100); Infancy (2-23 mo) (140); Preschool Age (2-5 yrs) (160). Tests & Measures: Movement Assessment Battery for Children–Second Edition; Wechsler Preschool and Primary Scale of Intelligence–Fourth Edition; Physical Health domain of Pediatric Quality of Life Inventory; Pediatric Evaluation of Disability Inventory Computer Adaptive Test; Pediatric Quality of Life Inventory 4.0; Little Developmental Coordination Disorder Questionnaire DOI: 10.1037/t72035-000; Strengths and Difficulties Questionnaire DOI: 10.1037/t00540-000. Methodology: Empirical Study; Longitudinal Study; Prospective Study; Quantitative Study. References Available: Y. Page Count: 8. Issue Publication Date: Jun-Jul, 2022. Copyright Statement: All rights reserved. Wolters Kluwer Health, Inc. 2021. 
AB  - Objective: The objective of this study was to determine whether school readiness differs between children born < 30 weeks' gestation who are classified as at risk for developmental coordination disorder (DCD) and those who are not. Methods: This study was a prospective cohort study of children born < 30 weeks' gestation. Children were classified as at risk for DCD at a corrected age of 4 to 5 years if they scored < 16th centile on the Movement Assessment Battery for Children–Second Edition (MABC-2), had a full scale IQ score of ≥ 80 on the Wechsler Preschool and Primary Scale of Intelligence–Fourth Edition (WPPSI-IV), and had no cerebral palsy. Children were assessed on 4 school readiness domains: (1) health/physical development [Physical Health domain of Pediatric Quality of Life Inventory (PedsQL), Pediatric Evaluation of Disability Inventory Computer Adaptive Test, and Little Developmental Coordination Disorder Questionnaire], (2) social-emotional development (Strengths and Difficulties Questionnaire and PedsQL psychosocial domains), (3) cognitive skills/general knowledge (WPPSI-IV), and (4) language skills (WPPSI-IV). Results: Of 123 children assessed, 16 were ineligible (IQ < 80 or cerebral palsy: n = 15; incomplete MABC-2: n = 1); 28 of 107 (26%) eligible children were at risk for DCD. Children at risk for DCD had poorer performance on all school readiness domains, with group differences of more than 0.4 SD in health/physical development, social-emotional development, and language skills and up to 0.8 SD for cognitive skills/general knowledge compared with those not at risk of DCD. Conclusion: Being at risk for DCD in children born < 30 weeks' gestation is associated with challenges in multiple school readiness domains, not only the health/physical domain. (PsycInfo Database Record (c) 2022 APA, all rights reserved)
KW  - developmental coordination disorder
KW  - school readiness
KW  - very preterm infants
KW  - Developmental Disabilities
KW  - Premature Birth
KW  - School Readiness
KW  - Dyspraxia
KW  - Cognitive Ability
KW  - Language Delay
KW  - Motor Coordination
KW  - Socioemotional Functioning
U1  - Sponsor: Australian National Health and Medical Council, Centre for Research Excellence, Australia. Grant: 1060733. Recipients: No recipient indicated
U1  - Sponsor: Australian National Health and Medical Council, Australia. Grant: 1101035; 1024516. Other Details: Project grants. Recipients: No recipient indicated
U1  - Sponsor: Australian National Health and Medical Council, Australia. Grant: 1081288. Other Details: Senior Research Fellowship. Recipients: Anderson, Peter
U1  - Sponsor: Australian National Health and Medical Council, Australia. Grant: 1108714. Other Details: Career Development Fellowships. Recipients: Spittle, Alicia J.
U1  - Sponsor: Australian National Health and Medical Council, Australia. Grant: 1141354. Recipients: Cheong, Jeanie L. Y.
U1  - Sponsor: Australian National Health and Medical Council, Australia. Grant: 1127984. Recipients: Lee, Katherine J.
U1  - Sponsor: Australian National Health and Medical Council, Australia. Grant: 1176077. Other Details: Investigator grant. Recipients: Anderson, Peter
U1  - Sponsor: Victorian Government, Operational Infrastructure Support Program, Australia. Recipients: No recipient indicated
U1  - Sponsor: Australian Government, Australia. Other Details: Research Training Program Scholarship. Recipients: FitzGerald, Tara L.; Cameron, Kate L.
U1  - Sponsor: Centre of Research Excellence in Newborn Medicine. Recipients: FitzGerald, Tara L.; Cameron, Kate L.; Albesher, Reem A.
U1  - Sponsor: Princess Nourah Bint Abdulrahman University, Saudi Arabia. Other Details: Scholarship. Recipients: Albesher, Reem A.
DO  - 10.1097/DBP.0000000000001031
UR  - https://search.ebscohost.com/login.aspx?direct=true&db=psyh&AN=2022-71415-015&lang=de&site=ehost-live
UR  - ORCID: 0000-0001-7430-868X
UR  - ORCID: 0000-0002-4360-8310
UR  - ORCID: 0000-0002-4575-7117
UR  - ORCID: 0000-0001-5447-594X
UR  - aspittle@unimelb.edu.au
DP  - EBSCOhost
DB  - APA PsycInfo
ER  - 

TY  - JOUR
ID  - 2019-21143-004
AN  - 2019-21143-004
AU  - Klein, Balázs
AU  - Raven, John
AU  - Fodor, Szilvia
T1  - Scrambled Adaptive Matrices (SAM)—A new test of eductive ability
JF  - Psychological Test and Assessment Modeling
JO  - Psychological Test and Assessment Modeling
JA  - Psychol Test Assess Model
Y1  - 2018///
VL  - 60
IS  - 4
SP  - 451
EP  - 492
PB  - Pabst Science Publishers
SN  - 2190-0493
SN  - 2190-0507
AD  - Klein, Balázs, University of Pecs, Research Center for Labor and Health Sciences, Pecs, Hungary
N1  - Accession Number: 2019-21143-004. Other Journal Title: Psychologische Beitrage; Psychology Science. Partial author list: First Author & Affiliation: Klein, Balázs; University of Pecs, Research Center for Labor and Health Sciences, Pecs, Hungary. Release Date: 20200113. Publication Type: Journal (0100), Peer Reviewed Journal (0110). Format Covered: Electronic. Document Type: Journal Article. Language: EnglishMajor Descriptor: Ability; Adaptive Testing; Aptitude Measures; Psychometrics; Test Validity. Minor Descriptor: Age Differences; Educational Measurement; Human Sex Differences; Test Reliability. Classification: Educational Measurement (2227); Academic Learning & Achievement (3550). Population: Human (10); Male (30); Female (40). Location: Hungary. Age Group: Childhood (birth-12 yrs) (100); School Age (6-12 yrs) (180). Tests & Measures: Scrambled Adaptive Matrices. Methodology: Empirical Study; Quantitative Study. Page Count: 42. Issue Publication Date: 2018. 
AB  - Inspired by the Raven’s Progressive Matrices a new, innovative, IRT-based online adaptive test, the Scrambled Adaptive Matrices (SAM) has been developed and used in talent identification projects both in educational and work settings with more than 15 000 participants. The current article introduces the test and shows results on reliability and validity as well as response time, motivational concerns, adaptivity, security and the effects of external variables such as socio-economic status, age, gender and pre-selection. The data show that the newly developed instrument is a feasible, reliable and valid tool for ability assessment and talent identification in projects of all sizes. (PsycINFO Database Record (c) 2020 APA, all rights reserved)
KW  - computerized adaptive testing
KW  - intelligence
KW  - item response theory
KW  - Raven Progressive Matrices
KW  - validity
KW  - Ability
KW  - Adaptive Testing
KW  - Aptitude Measures
KW  - Psychometrics
KW  - Test Validity
KW  - Age Differences
KW  - Educational Measurement
KW  - Human Sex Differences
KW  - Test Reliability
UR  - https://search.ebscohost.com/login.aspx?direct=true&db=psyh&AN=2019-21143-004&lang=de&site=ehost-live
UR  - balazs.klein@gmail.com
DP  - EBSCOhost
DB  - APA PsycInfo
ER  - 

TY  - JOUR
ID  - 2014-21778-020
AN  - 2014-21778-020
AU  - Fischer, H. Felix
AU  - Klug, Cassandra
AU  - Roeper, Koosje
AU  - Blozik, Eva
AU  - Edelmann, Frank
AU  - Eisele, Marion
AU  - Störk, Stefan
AU  - Wachter, Rolf
AU  - Scherer, Martin
AU  - Rose, Matthias
AU  - Herrmann-Lingen, Christoph
T1  - Screening for mental disorders in heart failure patients using computer-adaptive tests
JF  - Quality of Life Research: An International Journal of Quality of Life Aspects of Treatment, Care & Rehabilitation
JO  - Quality of Life Research: An International Journal of Quality of Life Aspects of Treatment, Care & Rehabilitation
JA  - Qual Life Res
Y1  - 2014/06//
VL  - 23
IS  - 5
SP  - 1609
EP  - 1618
PB  - Springer
SN  - 0962-9343
SN  - 1573-2649
AD  - Fischer, H. Felix, Department of Psychosomatic Medicine, Clinic for Internal Medicine, Charite – Universitatsmedizin, Berlin, Germany
N1  - Accession Number: 2014-21778-020. PMID: 24338104 Partial author list: First Author & Affiliation: Fischer, H. Felix; Department of Psychosomatic Medicine, Clinic for Internal Medicine, Charite – Universitatsmedizin, Berlin, Germany. Release Date: 20140901. Correction Date: 20210712. Publication Type: Journal (0100), Peer Reviewed Journal (0110). Format Covered: Electronic. Document Type: Journal Article. Language: EnglishMajor Descriptor: Mental Disorders; Psychometrics; Screening; Test Reliability; Test Validity. Minor Descriptor: Heart Disorders; Item Response Theory; Mental Health Screening. Classification: Clinical Psychological Testing (2224); Psychological & Physical Disorders (3200). Population: Human (10); Male (30); Female (40); Outpatient (60). Location: Germany. Age Group: Adulthood (18 yrs & older) (300); Aged (65 yrs & older) (380). Tests & Measures: Patient Health Questionnaire-9-German Version; Generalized Anxiety Disorder-7 Scale-German Version; Hospital Anxiety and Depression Scale-German Version; Computer-Adaptive Test; PROMIS-Depression Short Form 8a Scale; Structured Clinical Interview for DSM-IV. Methodology: Empirical Study; Interview; Quantitative Study. Supplemental Data: Tables and Figures Internet. References Available: Y. Page Count: 10. Issue Publication Date: Jun, 2014. Publication History: First Posted Date: Dec 14, 2013; Accepted Date: Dec 2, 2013. Copyright Statement: Springer Science+Business Media Dordrecht. 2013. 
AB  - Purpose: Item response theory is increasingly used in the development of psychometric tests. This paper evaluates whether these modern psychometric methods can improve self-reported screening for depression and anxiety in patients with heart failure. Methods: Themental health status of 194 patients with heart failure was assessed using six screening tools for depression (Patient Health Questionnaire -9 (9 items), Hospital Anxiety and Depression Scale (HADS) (7 items), PROMIS-Depression Short Form 8a (8 items)) and Anxiety (GAD-7 (7 items), Hospital Anxiety and Depression Scale (HADS) (7 items), PROMIS-Anxiety Short Form 8a (8 items)). An in-person structured clinical interview was used as the current gold standard to identify the presence of a mental disorder. The diagnostic accuracy of all static tools was compared when item response theory (IRT)-based person parameter were estimated instead of sum scores. Furthermore, we compared performance of static instruments with post hoc simulated individual-tailored computer-adaptive test (CATs) for both disorders and a common negative affect CAT. Results: In general, screening for depression was highly efficient and showed a better performance than screening for anxiety with only minimal differences among the assessed instruments. IRT-based person parameters yielded the same diagnostic accuracy as sum scores. CATs showed similar screening performance compared to legacy instruments but required significantly fewer items to identify patients without mental conditions. Ideal cutoffs varied between male and female samples. Conclusions: Overall, the diagnostic performance of all investigated instruments was similar, regardless of the methods being used.However,CATs can individually tailor the test to each patient, thus significantly decreasing the respondent burden for patients with and without mental conditions. Such approach could efficiently increase the acceptability ofmental health screening in clinical practice settings. (PsycInfo Database Record (c) 2021 APA, all rights reserved)
KW  - mental disorders
KW  - Mental Health Screening Tools
KW  - heart failure
KW  - test validity
KW  - test reliability
KW  - psychometrics
KW  - Aged
KW  - Anxiety
KW  - Depression
KW  - Diagnosis, Computer-Assisted
KW  - Female
KW  - Heart Failure
KW  - Hospitalization
KW  - Humans
KW  - Interviews as Topic
KW  - Male
KW  - Mass Screening
KW  - Mental Health
KW  - Middle Aged
KW  - Outcome Assessment (Health Care)
KW  - Psychiatric Status Rating Scales
KW  - Psychometrics
KW  - Quality of Life
KW  - Reproducibility of Results
KW  - Self Report
KW  - Surveys and Questionnaires
KW  - Mental Disorders
KW  - Psychometrics
KW  - Screening
KW  - Test Reliability
KW  - Test Validity
KW  - Heart Disorders
KW  - Item Response Theory
KW  - Mental Health Screening
U1  - Sponsor: German Federal Ministry for Education and Research, Germany. Grant: 01GY1150. Recipients: No recipient indicated
DO  - 10.1007/s11136-013-0599-y
UR  - https://search.ebscohost.com/login.aspx?direct=true&db=psyh&AN=2014-21778-020&lang=de&site=ehost-live
UR  - ORCID: 0000-0003-2231-2200
UR  - ORCID: 0000-0002-1771-7249
UR  - ORCID: 0000-0003-4401-5936
UR  - felix.fischer@charite.de
DP  - EBSCOhost
DB  - APA PsycInfo
ER  - 

TY  - JOUR
ID  - 1999-01326-001
AN  - 1999-01326-001
AU  - Rocklin, Thomas R.
T1  - Self-adapted testing
T3  - Self-adapted testing
JF  - Applied Measurement in Education
JO  - Applied Measurement in Education
Y1  - 1994///
VL  - 7
IS  - 1
SP  - 3
EP  - 14
PB  - Lawrence Erlbaum
SN  - 0895-7347
SN  - 1532-4818
N1  - Accession Number: 1999-01326-001. Partial author list: First Author & Affiliation: Rocklin, Thomas R.; U Iowa, Coll of Education, Iowa City, IA, US. Other Publishers: Taylor & Francis. Release Date: 19991201. Correction Date: 20190211. Publication Type: Journal (0100), Peer Reviewed Journal (0110). Format Covered: Print. Document Type: Journal Article. Language: EnglishMajor Descriptor: Adaptive Testing; Computerized Assessment. Minor Descriptor: Aptitude Measures; Estimation; Test Reliability; Test Validity. Classification: Educational Measurement (2227). Population: Human (10). Methodology: Literature Review. References Available: Y. Page Count: 12. Issue Publication Date: 1994. 
AB  - In self-adapted testing (SAT), examinees choose the difficulty of each item they attempt immediately before it is presented. In this article, the author reviews the rationale for investigating SAT and research exploring the effects of SAT on ability estimates, precision and efficiency, mechanisms underlying the effects associated with SAT, and examinee reactions to SAT. Some of the research demonstrates that SAT leads to higher ability estimates than computerized adaptive testing (CAT), whereas other research demonstrated no main effect of test administration mode but that SAT leads to ability estimates that are less influenced by extraneous attributes of the examinee (e.g., test anxiety). SAT is less efficient than CAT, but more efficient than fixed-item testing. Examinees strongly endorse the main elements of SAT. (PsycINFO Database Record (c) 2019 APA, all rights reserved)
KW  - ability estimates & precision & efficiency of & examinee reactions to self-adapted vs computerized-adaptive testing
KW  - Adaptive Testing
KW  - Computerized Assessment
KW  - Aptitude Measures
KW  - Estimation
KW  - Test Reliability
KW  - Test Validity
DO  - 10.1207/s15324818ame0701_2
UR  - https://search.ebscohost.com/login.aspx?direct=true&db=psyh&AN=1999-01326-001&lang=de&site=ehost-live
DP  - EBSCOhost
DB  - APA PsycInfo
ER  - 

TY  - JOUR
ID  - 1988-02954-001
AN  - 1988-02954-001
AU  - Rocklin, Thomas R.
AU  - O'Donnell, Angela M.
T1  - Self-adapted testing: A performance-improving variant of computerized adaptive testing
JF  - Journal of Educational Psychology
JO  - Journal of Educational Psychology
JA  - J Educ Psychol
Y1  - 1987/09//
VL  - 79
IS  - 3
SP  - 315
EP  - 319
PB  - American Psychological Association
SN  - 0022-0663
SN  - 1939-2176
N1  - Accession Number: 1988-02954-001. Partial author list: First Author & Affiliation: Rocklin, Thomas R.; U Iowa, Div of Psychological & Quantitative Foundations, Iowa City. Other Publishers: Warwick & York. Release Date: 19880101. Correction Date: 20190211. Publication Type: Journal (0100), Peer Reviewed Journal (0110). Format Covered: Print. Document Type: Journal Article. Language: EnglishMajor Descriptor: College Students; Test Anxiety; Computerized Assessment. Minor Descriptor: Test Performance. Classification: Educational Measurement (2227). Population: Human (10). Age Group: Adulthood (18 yrs & older) (300). Methodology: Empirical Study. References Available: Y. Page Count: 5. Issue Publication Date: Sep, 1987. Publication History: Accepted Date: Mar 29, 1987; Revised Date: Feb 23, 1987; First Submitted Date: Aug 18, 1986. Copyright Statement: American Psychological Association. 1987. 
AB  - We conducted an experiment that contrasted a variant of computerized adaptive testing, self-adapted testing, with two traditional tests, a relatively difficult one and a relatively easy one, that were constructed from the same bank of verbal ability items. In a self-adapted test, the examinee, rather than a computerized algorithm, chooses the difficulty of the next item to be presented. Participants completed a self-report of text anxiety and were randomly assigned to take one of the three tests of verbal ability. Analyses of variance using Rasch estimates of ability and the standard error of those estimates as dependent measures demonstrated that the self-adapted test led to higher ability estimates and minimized the effect of test anxiety without any overall loss of measurement precision. Analysis of the item choices in the self-adapted test suggested that, in general, participants chose more difficult items as the test progressed. Anxiety was negatively associated with the difficulty of the initial choice but not associated with the rate of progress to higher difficulty items. (PsycINFO Database Record (c) 2019 APA, all rights reserved)
KW  - computerized self adapted ability testing
KW  - test performance & test anxiety
KW  - college students
KW  - College Students
KW  - Test Anxiety
KW  - Computerized Assessment
KW  - Test Performance
DO  - 10.1037/0022-0663.79.3.315
UR  - https://search.ebscohost.com/login.aspx?direct=true&db=psyh&AN=1988-02954-001&lang=de&site=ehost-live
DP  - EBSCOhost
DB  - APA PsycInfo
ER  - 

TY  - CHAP
ID  - 2020-06442-019
AN  - 2020-06442-019
AU  - Sunderland, Matthew
AU  - Batterham, Philip
AU  - Calear, Alison
AU  - Carragher, Natacha
ED  - Sellbom, Martin
ED  - Suhr, Julie A.
T1  - Self-report scales for common mental disorders: An overview of current and emerging methods
T2  - The Cambridge handbook of clinical assessment and diagnosis.
Y1  - 2020///
SP  - 263
EP  - 277
CY  - New York, NY
PB  - Cambridge University Press
SN  - 9781108415910
SN  - 9781108402491
SN  - 9781108235433
N1  - Accession Number: 2020-06442-019. Partial author list: First Author & Affiliation: Sunderland, Matthew; University of Sydney, Matilda Centre for Research in Mental Health and Substance Use, Sydney, Australia. Release Date: 20200416. Correction Date: 20221128. Publication Type: Book (0200), Edited Book (0280). Format Covered: Print. Document Type: Chapter. ISBN: 9781108415910, ISBN Hardcover; 9781108402491, ISBN Paperback; 9781108235433, ISBN EPUB. Language: EnglishMajor Descriptor: Clinical Psychology; Major Depression; Mental Disorders; Psychiatry; Self-Report. Minor Descriptor: Adaptive Testing; Anxiety Disorders; Psychometrics; Score Equating. Classification: Psychological & Physical Disorders (3200). Population: Human (10). Intended Audience: Psychology: Professional & Research (PS). Tests & Measures: Montgomery-Asberg Depression Rating Scale Self Report; NEO Five-Factor Inventory; Beck Depression Inventory-II; Worry Behaviors Inventory; Symptoms of Depression Quesitonnaire; Generalized Anxiety Disorder-7 Scale; Externalizing Spectrum Inventory; Beck Scale for Suicide Ideation; Mood and Anxiety Symptom Questionnaire DOI: 10.1037/t13679-000; State Trait Anxiety Inventory; Beck Anxiety Inventory DOI: 10.1037/t02025-000; Hospital Anxiety and Depression Scale DOI: 10.1037/t03589-000; Center for Epidemiologic Studies Depression Scale; Penn State Worry Questionnaire DOI: 10.1037/t01760-000; Generalized Anxiety Disorder 7 DOI: 10.1037/t02591-000; Patient Health Questionnaire-9 DOI: 10.1037/t06165-000; Positive and Negative Affect Schedule DOI: 10.1037/t03592-000. Page Count: 15. 
AB  - Self-report scales that measure the severity of common mental disorders (e.g., unipolar depression and anxiety disorders) based on subjective signs and symptoms have formed the cornerstone of assessment in clinical psychology and psychiatry for many years. This chapter aims to provide a broad overview of existing, widely used, self-report scales for assessing the presence and frequency of symptoms of depression and anxiety. It focuses on some of the more widely used scales in research and clinical settings with specific reference to their psychometric properties and cross-cultural applicability. The chapter discusses emerging methods that apply modem psychometric techniques to develop the next generation of self-report scales, with the aim of improving the reliability, validity, and comparability of self-report data, while minimizing respondent burden and increasing efficiency via electronic administration. It focuses on three applications of modern test theory to the self-report assessment of mental disorders: item banking, adaptive testing and data-driven short scales, and scale equating. The chapter outlines the strengths and some current criticisms of these techniques and highlights future directions for research. (PsycInfo Database Record (c) 2022 APA, all rights reserved)
KW  - mental disorders
KW  - anxiety disorders
KW  - clinical psychology
KW  - psychometric techniques
KW  - psychiatry
KW  - scale equating
KW  - self-report scales
KW  - unipolar depression
KW  - Clinical Psychology
KW  - Major Depression
KW  - Mental Disorders
KW  - Psychiatry
KW  - Self-Report
KW  - Adaptive Testing
KW  - Anxiety Disorders
KW  - Psychometrics
KW  - Score Equating
DO  - 10.1017/9781108235433.019
UR  - https://search.ebscohost.com/login.aspx?direct=true&db=psyh&AN=2020-06442-019&lang=de&site=ehost-live
DP  - EBSCOhost
DB  - APA PsycInfo
ER  - 

TY  - JOUR
ID  - 2022-95680-001
AN  - 2022-95680-001
AU  - Xu, Cai
AU  - Smith, Grace L.
AU  - Chen, Ying-Shiuan
AU  - Checka, Cristina M.
AU  - Giordano, Sharon H.
AU  - Kaiser, Kelsey
AU  - Lowenstein, Lisa M.
AU  - Ma, Hilary
AU  - Mendoza, Tito R.
AU  - Peterson, Susan K.
AU  - Shih, Ya-Chen T.
AU  - Shete, Sanjay
AU  - Tang, Chad
AU  - Volk, Robert J.
AU  - Sidey-Gibbons, Chris
T1  - Short-form adaptive measure of financial toxicity from the Economic Strain and Resilience in Cancer (ENRICh) study: Derivation using modern psychometric techniques
JF  - PLoS ONE
JO  - PLoS ONE
JA  - PLoS One
Y1  - 2022/08/25/
VL  - 17
IS  - 8
PB  - Public Library of Science
SN  - 1932-6203
AD  - Sidey-Gibbons, Chris
N1  - Accession Number: 2022-95680-001. PMID: 36006909 Partial author list: First Author & Affiliation: Xu, Cai; University of Texas MD Anderson Cancer Center, Houston, TX, US. Release Date: 20221020. Publication Type: Journal (0100), Peer Reviewed Journal (0110). Format Covered: Electronic. Document Type: Journal Article. Language: EnglishGrant Information: Smith, Grace L. Major Descriptor: Adaptive Testing; Financial Strain; Neoplasms; Computerized Assessment; Personal Finance. Minor Descriptor: Factor Analysis; Psychometrics; Test Construction; Stress and Coping Measures. Classification: Health Psychology Testing (2226); Cancer (3293). Population: Human (10); Male (30); Female (40). Location: US. Age Group: Adulthood (18 yrs & older) (300). Tests & Measures: Graded Response Model; Computerized Adaptive Test. Methodology: Empirical Study; Quantitative Study. Supplemental Data: Tables and Figures Internet. References Available: Y. ArtID: e0272804. Issue Publication Date: Aug 25, 2022. Publication History: First Posted Date: Aug 25, 2022; Accepted Date: Jul 26, 2022; First Submitted Date: Nov 30, 2021. Copyright Statement: This is an open access article distributed under the terms of the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited. Xu et al. 2022. 
AB  - Objectives: This study sought to evaluate advanced psychometric properties of the 15-item Economic Strain and Resilience in Cancer (ENRICh) measure of financial toxicity for cancer patients. Methods: We surveyed 515 cancer patients in the greater Houston metropolitan area using ENRICh from March 2019 to March 2020. We conducted a series of factor analyses alongside parametric and non-parametric item response theory (IRT) assessments using Mokken analysis and the graded response model (GRM). We utilized parameters derived from the GRM to run a simulated computerized adaptive test (CAT) assessment. Results: Among participants, mean age was 58.49 years and 278 (54%) were female. The initial round factor analysis results suggested a one-factor scale structure. Negligible levels of differential item functioning (DIF) were evident between eight items. Three items were removed due to local interdependence (Q3>+0.4). The original 11-point numerical rating scale did not function well, and a new 3-point scoring system was implemented. The final 12-item ENRICh had acceptable fit to the GRM (p < 0.001; TLI = 0.94; CFI = 0.95; RMSEA = 0.09; RMSR = 0.06) as well as good scalability and dimensionality. We observed high correlation between CAT version scores and the 12-item measure (r = 0.98). During CAT, items 2 (money you owe) and 4 (stress level about finances) were most frequently administered, followed by items 1 (money in savings) and 5 (ability to pay bills). Scores from these four items alone were strongly correlated with that of the 12-item ENRICh (r = 0.96). Conclusion These CAT and 4-item versions provide options for quick screening in clinical practice and low-burden assessment in research. (PsycInfo Database Record (c) 2022 APA, all rights reserved)
KW  - financial toxicity
KW  - psychometric techniques
KW  - cancer patients
KW  - economic strain measure
KW  - Factor Analysis, Statistical
KW  - Female
KW  - Financial Stress
KW  - Humans
KW  - Male
KW  - Neoplasms
KW  - Psychometrics
KW  - Reproducibility of Results
KW  - Surveys and Questionnaires
KW  - Adaptive Testing
KW  - Financial Strain
KW  - Neoplasms
KW  - Computerized Assessment
KW  - Personal Finance
KW  - Factor Analysis
KW  - Psychometrics
KW  - Test Construction
KW  - Stress and Coping Measures
U1  - Sponsor: National Institutes of Health, National Cancer Institute, US. Grant: K07CA211804. Recipients: Smith, Grace L.
U1  - Sponsor: Sponsor name not included. Other Details: Andrew Sabin Family Fellowship. Recipients: Smith, Grace L.
U1  - Sponsor: MD Anderson Cancer Center. Grant: P30 CA016672. Recipients: No recipient indicated
DO  - 10.1371/journal.pone.0272804
UR  - https://search.ebscohost.com/login.aspx?direct=true&db=psyh&AN=2022-95680-001&lang=de&site=ehost-live
UR  - cgibbons@mdanderson.org
DP  - EBSCOhost
DB  - APA PsycInfo
ER  - 

TY  - JOUR
ID  - 2015-57071-001
AN  - 2015-57071-001
AU  - Flens, Gerard
AU  - Smits, Niels
AU  - Carlier, Ingrid
AU  - van Hemert, Albert M.
AU  - de Beurs, Edwin
T1  - Simulating computer adaptive testing with the Mood and Anxiety Symptom Questionnaire
JF  - Psychological Assessment
JO  - Psychological Assessment
JA  - Psychol Assess
Y1  - 2016/08//
VL  - 28
IS  - 8
SP  - 953
EP  - 962
PB  - American Psychological Association
SN  - 1040-3590
SN  - 1939-134X
AD  - Flens, Gerard, Stichting Benchmark GGZ (SBG), Rembrandtlaan 48, 3723 BK, Bilthoven, Netherlands
N1  - Accession Number: 2015-57071-001. PMID: 26691506 Other Journal Title: Psychological Assessment: A Journal of Consulting and Clinical Psychology. Partial author list: First Author & Affiliation: Flens, Gerard; Foundation for Benchmarking Mental Health Care, Bilthoven, Netherlands. Release Date: 20151221. Correction Date: 20200504. Publication Type: Journal (0100), Peer Reviewed Journal (0110). Format Covered: Electronic. Document Type: Journal Article. Language: EnglishMajor Descriptor: Anxiety Disorders; Major Depression; Questionnaires; Computerized Assessment. Minor Descriptor: Item Response Theory; Measurement; Symptoms. Classification: Clinical Psychological Testing (2224); Psychological Disorders (3210). Population: Human (10); Male (30); Female (40); Outpatient (60). Location: Netherlands. Age Group: Adulthood (18 yrs & older) (300). Tests & Measures: Mood and Anxiety Symptom Questionnaire DOI: 10.1037/t13679-000. Methodology: Empirical Study; Quantitative Study. References Available: Y. Page Count: 10. Issue Publication Date: Aug, 2016. Publication History: First Posted Date: Dec 21, 2015; Accepted Date: Sep 17, 2015; Revised Date: Sep 14, 2015; First Submitted Date: Mar 18, 2015. Copyright Statement: American Psychological Association. 2015. 
AB  - In a post hoc simulation study (N = 3,597 psychiatric outpatients), we investigated whether the efficiency of the 90-item Mood and Anxiety Symptom Questionnaire (MASQ) could be improved for assessing clinical subjects with computerized adaptive testing (CAT). A CAT simulation was performed on each of the 3 MASQ subscales (Positive Affect, Negative Affect, and Somatic Anxiety). With the CAT simulation’s stopping rule set at a high level of measurement precision, the results showed that patients’ test administration can be shortened substantially; the mean decrease in items used for the subscales ranged from 56% up to 74%. Furthermore, the predictive utility of the CAT simulations was sufficient for all MASQ scales. The findings reveal that developing a MASQ CAT for clinical subjects is useful as it leads to more efficient measurement without compromising the reliability of the test outcomes. (PsycInfo Database Record (c) 2020 APA, all rights reserved)
KW  - computer adaptive test
KW  - clinical assessment
KW  - Mood and Anxiety Symptom Questionnaire
KW  - item response theory
KW  - Adult
KW  - Affect
KW  - Anxiety
KW  - Computers
KW  - Depression
KW  - Diagnosis, Computer-Assisted
KW  - Female
KW  - Humans
KW  - Male
KW  - Middle Aged
KW  - Outpatients
KW  - Psychometrics
KW  - Reproducibility of Results
KW  - Surveys and Questionnaires
KW  - Anxiety Disorders
KW  - Major Depression
KW  - Questionnaires
KW  - Computerized Assessment
KW  - Item Response Theory
KW  - Measurement
KW  - Symptoms
DO  - 10.1037/pas0000240
UR  - https://search.ebscohost.com/login.aspx?direct=true&db=psyh&AN=2015-57071-001&lang=de&site=ehost-live
UR  - ORCID: 0000-0002-6683-4628
UR  - gerard.flens@sbggz.nl
DP  - EBSCOhost
DB  - APA PsycInfo
ER  - 

TY  - JOUR
ID  - 2006-21384-019
AN  - 2006-21384-019
AU  - Reeve, Bryce B.
T1  - Special Issues for Building Computerized-Adaptive Tests for Measuring Patient-Reported Outcomes: The National Institute of Health's Investment in New Technology
T3  - Measurement in a multi-ethnic society
JF  - Medical Care
JO  - Medical Care
JA  - Med Care
Y1  - 2006/11//
VL  - 44
IS  - 11, Suppl 3
SP  - S198
EP  - S204
PB  - Lippincott Williams & Wilkins
SN  - 0025-7079
SN  - 1537-1948
AD  - Reeve, Bryce B., Outcomes Research Branch, Applied Research Program, Division of Cancer Control and Population Sciences, National Cancer Institute, National Institutes of Health, 6130 Executive Boulevard, MSC 7344, EPN room 4005, Bethesda, MD, US, 20892-7344
N1  - Accession Number: 2006-21384-019. Partial author list: First Author & Affiliation: Reeve, Bryce B.; National Cancer Institute, National Institutes of Health, Bethesda, MD, US. Release Date: 20070604. Correction Date: 20190211. Publication Type: Journal (0100), Peer Reviewed Journal (0110). Format Covered: Print. Document Type: Journal Article. Language: EnglishMajor Descriptor: Health Care Delivery; Technology; Treatment Outcomes; Computerized Assessment. Minor Descriptor: Health; Quality of Life; Test Items. Classification: Research Methods & Experimental Design (2260); Health & Mental Health Treatment & Prevention (3300). Population: Human (10). References Available: Y. Page Count: 7. Issue Publication Date: Nov, 2006. 
AB  - Advances in statistical methods and in computer and Internet technology have provided the opportunity to develop new instruments or adapt existing instruments measuring patient-reported outcomes (PROs). This article reviews the National Institutes of Health (NIH) interest in supporting the development and application of these methods, the process for developing an item bank, the special issues and challenges faced in the creation of item banks and computerized-adaptive tests (CATs) measuring key symptom and health-related quality of life (HRQOL) domains affected by a disease or its treatment, and the importance of NIH's role in encouraging this research field. This commentary raised only a few of the many complicated and challenging issues that will need to be addressed as we move into the electronic age of healthcare delivery systems. While we may find some possible solutions from the field of educational testing where most of these methods were developed and used, an open and continued dialogue will need to take place about the unique issues we face in health outcomes measurement. With public and private sector interaction and close cooperation, we can meet these challenges and explore the benefits of item banks and CATs for improving healthcare delivery in a multi-ethnic society. (PsycINFO Database Record (c) 2019 APA, all rights reserved)
KW  - computerized-adaptive tests
KW  - National Institutes of Health
KW  - patient reported outcomes measurement
KW  - technology
KW  - healthcare delivery
KW  - health-related quality of life
KW  - item banks
KW  - Health Care Delivery
KW  - Technology
KW  - Treatment Outcomes
KW  - Computerized Assessment
KW  - Health
KW  - Quality of Life
KW  - Test Items
DO  - 10.1097/01.mlr.0000245146.77104.50
UR  - https://search.ebscohost.com/login.aspx?direct=true&db=psyh&AN=2006-21384-019&lang=de&site=ehost-live
UR  - reeveb@mail.nih.gov
DP  - EBSCOhost
DB  - APA PsycInfo
ER  - 

TY  - THES
ID  - 2010-99050-029
AN  - 2010-99050-029
AU  - Hui, Hing-fai
T1  - Stability and sensitivity of a model-based person-fit index in detecting item pre-knowledge in computerized adaptive test
JF  - Dissertation Abstracts International Section A: Humanities and Social Sciences
JO  - Dissertation Abstracts International Section A: Humanities and Social Sciences
Y1  - 2010///
VL  - 70
IS  - 9-A
SP  - 3433
EP  - 3433
PB  - ProQuest Information & Learning
SN  - 0419-4209
SN  - 978-1-109-40180-6
N1  - Accession Number: 2010-99050-029. Other Journal Title: Dissertation Abstracts International. Partial author list: First Author & Affiliation: Hui, Hing-fai; The Chinese U Hong Kong, Hong Kong. Release Date: 20100510. Correction Date: 20221128. Publication Type: Dissertation Abstract (0400). Format Covered: Electronic. Document Type: Dissertation. Dissertation Number: AAI3377981. ISBN: 978-1-109-40180-6. Language: EnglishMajor Descriptor: Adaptive Testing; Item Response Theory; Sensitivity (Personality); Computerized Assessment. Classification: Educational & School Psychology (3500). Population: Human (10). Methodology: Empirical Study; Quantitative Study. Page Count: 1. 
AB  - Item response theory is a modern test theory. It focuses on the performance of each item. Under this framework, the performance of test takers on a test item can be predicted by a set of abilities. The relationship between the test takers' item performances and the set of abilities underlying item performances can be described by a monotonically increasing function called an item characteristic curve. Due to various personal reasons, the performances of the test takers may depart from the response patterns predicted by the underlying test model. In order to calculate the extent of departure of these aberrant response patterns, a number of methods have been developed under the theme 'person-fit statistics'. The degree of aberration is calculated as an index called person-fit index. Inside the computerized adaptive testing (CAT), test takers with different abilities will answer different numbers of questions and the difficulties of the items administered to them are usually clustered at the abilities of the test takers. Due to this reason, the application of person-fit indices in the computerized adaptive testing environment to measure misfit is difficult. When the frequent accesses to the item bank has become feasible, test takers may memorize blocks of test items and share these items with future test takers. Individuals with prior knowledge of some items may use that information to get high scores, in the sense that their test scores have been artificially inflated. FLOR is an index of posterior log-odds ratio used for detecting the use of item pre-knowledge. It can be applied both in the fixed item, fixed length test and the CAT environment. It is a model-based index in which aberrant models are defined in the situation of item pre-knowledge. FLOR describes the likelihood that a response pattern arises from the aberrant models. The present study used the hf plot to access the sensitivity of the person-fit indices. hf plot is a plot of hit rate against false alarm rate. For a higher hit rate, usually a higher false alarm rate is followed. hf plot provides a good tools for comparison between indices by inspection of the speed of rise of the curves. A sensitive index should give a faster rise of the curve. In this study, sensitivity of an index was defined as the speed of rise of the hf plot, which is represented by a parameter hfτ estimated from the data obtained from hf plot. The present study assessed the stability of FLOR over other variables, which were unrelated to item pre-knowledge. It found that FLOR was stable over the discrimination and difficulty parameters of test items. It was also stable over positions of the exposed items in the test and the initial assignment of prior probability of item pre-knowledge. However, the asymptotes (guessing factor) and the probabilities of item exposure did affect the final values of FLOR seriously. The present study also found that FLOR has a much superior sensitivity over other indices in detecting item pre-knowledge. Concerning about the sensitivity over different abilities of test takers, it was found that the sensitivity of FLOR was the highest among low ability test takers and the weakest among strong ability test takers in the fixed length and fixed items tests. However, the sensitivities of FLOR became the same among different abilities of test takers if items with difficulties matching their abilities were used in the tests. The number of beneficiaries among the test takers did not affect the sensitivity of FLOR. Moreover, in a simulation to test the differentiating power of FLOR, it was found that FLOR could differentiate item pre-knowledge from other reasons of personal misfits (test anxiety, player, random response and challenger) effectively. After the stability and sensitivity of FLOR were investigated, the application of it in the CAT environment had become the main concern. (PsycInfo Database Record (c) 2022 APA, all rights reserved)
KW  - stability
KW  - sensitivity
KW  - model-based person-fit index
KW  - item pre-knowledge
KW  - computerized adaptive test
KW  - Adaptive Testing
KW  - Item Response Theory
KW  - Sensitivity (Personality)
KW  - Computerized Assessment
UR  - https://search.ebscohost.com/login.aspx?direct=true&db=psyh&AN=2010-99050-029&lang=de&site=ehost-live
DP  - EBSCOhost
DB  - APA PsycInfo
ER  - 

TY  - JOUR
ID  - 2015-45248-007
AN  - 2015-45248-007
AU  - Cheng, Ying
AU  - Liu, Cheng
AU  - Behrens, John
T1  - Standard error of ability estimates and the classification accuracy and consistency of binary decisions
JF  - Psychometrika
JO  - Psychometrika
JA  - Psychometrika
Y1  - 2015/09//
VL  - 80
IS  - 3
SP  - 645
EP  - 664
PB  - Springer
SN  - 0033-3123
SN  - 1860-0980
AD  - Cheng, Ying, Department of Psychology, University of Notre Dame, 118 Haggar Hall, Notre Dame, IN, US, 46556
N1  - Accession Number: 2015-45248-007. PMID: 25228494 Partial author list: First Author & Affiliation: Cheng, Ying; University of Notre Dame, Notre Dame, IN, US. Release Date: 20151012. Correction Date: 20190211. Publication Type: Journal (0100), Peer Reviewed Journal (0110). Format Covered: Electronic. Document Type: Journal Article. Language: EnglishMajor Descriptor: Adaptive Testing; Error of Measurement; Item Response Theory; Computerized Assessment. Minor Descriptor: Ability. Classification: Statistics & Mathematics (2240). Population: Human (10). Methodology: Empirical Study; Mathematical Model; Quantitative Study; Scientific Simulation. References Available: Y. Page Count: 20. Issue Publication Date: Sep, 2015. Publication History: First Posted Date: Sep 17, 2014; First Submitted Date: May 19, 2013. Copyright Statement: The Psychometric Society. 2014. 
AB  - While estimation bias is a primary concern in psychological and educational measurement, the standard error is of equal importance in linking key aspects of the assessment structure, especially when the assessment goal concerns the classification of individuals into categories (e.g., master/non-mastery). In this paper, we show analytically how standard error of ability estimates affects expected classification accuracy and consistency when the decision is binary. When standard error decreases, the conditional classification accuracy and consistency increase. Given an examinee population and a cut score, smaller standard error over the entire latent trait continuum guarantees higher overall expected classification accuracy and consistency. We were also able to show the interrelationship between standard error, the expected classification consistency, and reliability. Utilizing the relationship between standard error and expected classification accuracy and consistency, we derive the upper bounds of the overall expected classification accuracy and consistency of a fixed-length computerized adaptive test. The lower bound of the expected classification accuracy and consistency is also derived given a number of stopping rules of variable-length computerized adaptive testing. Implications of these analytical results on operational tests are discussed. (PsycINFO Database Record (c) 2019 APA, all rights reserved)
KW  - standard error
KW  - classification accuracy
KW  - classification consistency
KW  - reliability
KW  - computerized adaptive testing
KW  - Algorithms
KW  - Humans
KW  - Models, Statistical
KW  - Psychometrics
KW  - Reproducibility of Results
KW  - Adaptive Testing
KW  - Error of Measurement
KW  - Item Response Theory
KW  - Computerized Assessment
KW  - Ability
DO  - 10.1007/s11336-014-9407-z
UR  - https://search.ebscohost.com/login.aspx?direct=true&db=psyh&AN=2015-45248-007&lang=de&site=ehost-live
UR  - ycheng4@nd.edu
DP  - EBSCOhost
DB  - APA PsycInfo
ER  - 

TY  - THES
ID  - 2003-95024-368
AN  - 2003-95024-368
AU  - Feng, Xin
T1  - Statistical detection and estimation of differential item functioning in computerized adaptive testing
JF  - Dissertation Abstracts International: Section B: The Sciences and Engineering
JO  - Dissertation Abstracts International: Section B: The Sciences and Engineering
Y1  - 2003///
VL  - 64
IS  - 6-B
SP  - 2736
EP  - 2736
PB  - ProQuest Information & Learning
SN  - 0419-4217
N1  - Accession Number: 2003-95024-368. Other Journal Title: Dissertation Abstracts International. Partial author list: First Author & Affiliation: Feng, Xin; Columbia U., US. Release Date: 20040802. Correction Date: 20190211. Publication Type: Dissertation Abstract (0400). Format Covered: Electronic. Document Type: Dissertation. Dissertation Number: AAI3095579. Language: EnglishMajor Descriptor: Adaptive Testing; Computer Applications; Human Sex Differences; Racial and Ethnic Differences; Differential Item Functioning. Classification: Psychometrics & Statistics & Methodology (2200). Population: Human (10). Methodology: Empirical Study; Quantitative Study. Page Count: 1. 
AB  - Differential item functioning (DIF) is an important issue in large scale standardized testing. DIF refers to the unexpected difference in item performances among groups of equally proficient examinees, usually classified by ethnicity or gender. Its presence could seriously affect the validity of inferences drawn from a test. Various statistical methods have been proposed to detect and estimate DIF. This dissertation addresses DIF analysis in the context of computerized adaptive testing (CAT), whose item selection algorithm adapts to the ability level of each individual examinee. In a CAT, a DIF item may be more consequential and more detrimental be cause fewer items are administered in a CAT than in a traditional paper-and-pencil test and because the remaining sequence of items presented to examinees depends in part on their responses to the DIF item. Consequently, an efficient, stable and flexible method to detect and estimate CAT DIF becomes necessary and increasingly important. We propose simultaneous implementations of online calibration and DIF testing. The idea is to perform online calibration of an item of interest separately in the focal and reference groups. Under any specific parametric IRT model, we can use the (online) estimated latent traits as covariates and fit a nonlinear regression model to each of the two groups. Because of the use of the estimated, not the true , the regression fit has to adjust for the covariate 'measurement errors'. It turns out that this situation fits nicely into the framework of nonlinear error-in-variable modelling, which has been extensively studied in statistical literature. We develop two bias-correction methods using asymptotic expansion and conditional score theory. After correcting the bias caused by measurement error, one can perform a significance test to detect DIF with the parameter estimates for different groups. This dissertation also discusses some general techniques to handle measurement error modelling with different IRT models, including the three-parameter normal ogive model and polytomous response models. Several methods of estimating DIF are studied as well. Large sample properties are established to justify the proposed methods. Extensive simulation studies show that the resulting methods perform well in terms of Type-I error rate control, accuracy in estimating DIF and power against both unidirectional and crossing DIF. (PsycINFO Database Record (c) 2019 APA, all rights reserved)
KW  - differential item functioning
KW  - computerized adaptive testing
KW  - ethnicity
KW  - gender
KW  - Adaptive Testing
KW  - Computer Applications
KW  - Human Sex Differences
KW  - Racial and Ethnic Differences
KW  - Differential Item Functioning
UR  - https://search.ebscohost.com/login.aspx?direct=true&db=psyh&AN=2003-95024-368&lang=de&site=ehost-live
DP  - EBSCOhost
DB  - APA PsycInfo
ER  - 

TY  - JOUR
ID  - 2019-41681-001
AN  - 2019-41681-001
AU  - Morris, Scott B.
AU  - Bass, Michael
AU  - Howard, Elizabeth
AU  - Neapolitan, Richard E.
T1  - Stopping rules for computer adaptive testing when item banks have nonuniform information
JF  - International Journal of Testing
JO  - International Journal of Testing
Y1  - 2020/04//Apr-Jun, 2020
VL  - 20
IS  - 2
SP  - 146
EP  - 168
PB  - Taylor & Francis
SN  - 1530-5058
SN  - 1532-7574
AD  - Morris, Scott B., Department of Psychology, Illinois Institute of Technology, Tech Central Room 201, 3424 S. State St., Chicago, IL, US, 60616
N1  - Accession Number: 2019-41681-001. PMID: 32982603 Partial author list: First Author & Affiliation: Morris, Scott B.; Department of Psychology, Illinois Institute of Technology, Chicago, IL, US. Other Publishers: Lawrence Erlbaum. Release Date: 20190718. Correction Date: 20210520. Publication Type: Journal (0100), Peer Reviewed Journal (0110). Format Covered: Electronic. Document Type: Journal Article. Language: EnglishMajor Descriptor: Adaptive Testing; Algorithms; Information Systems; Item Response Theory; Measurement. Minor Descriptor: Computers; Patient Reported Outcome Measures. Classification: Statistics & Mathematics (2240); Health & Mental Health Treatment & Prevention (3300). Population: Human (10). Methodology: Empirical Study; Mathematical Model; Quantitative Study; Scientific Simulation. Page Count: 23. Issue Publication Date: Apr-Jun, 2020. Copyright Statement: International Test Commission. 2019. 
AB  - The standard error (SE) stopping rule, which terminates a computer adaptive test (CAT) when the SE is less than a threshold, is effective when there are informative questions for all trait levels. However, in domains such as patient-reported outcomes, the items in a bank might all target one end of the trait continuum (e.g., negative symptoms), and the bank may lack depth for many individuals. In such cases, the predicted standard error reduction (PSER) stopping rule will stop the CAT even if the SE threshold has not been reached and can avoid administering excessive questions that provide little additional information. By tuning the parameters of the PSER algorithm, a practitioner can specify a desired tradeoff between accuracy and efficiency. Using simulated data for the Patient-Reported Outcomes Measurement Information System Anxiety and Physical Function banks, we demonstrate that these parameters can substantially impact CAT performance. When the parameters were optimally tuned, the PSER stopping rule was found to outperform the SE stopping rule overall, particularly for individuals not targeted by the bank, and presented roughly the same number of items across the trait continuum. Therefore, the PSER stopping rule provides an effective method for balancing the precision and efficiency of a CAT. (PsycInfo Database Record (c) 2021 APA, all rights reserved)
KW  - computer adaptive testing
KW  - item response theory
KW  - patient-reported outcomes
KW  - stopping rule
KW  - Adaptive Testing
KW  - Algorithms
KW  - Information Systems
KW  - Item Response Theory
KW  - Measurement
KW  - Computers
KW  - Patient Reported Outcome Measures
U1  - Sponsor: National Library of Medicine. Grant: R01LM011962; R01LM011663. Recipients: No recipient indicated
DO  - 10.1080/15305058.2019.1635604
UR  - https://search.ebscohost.com/login.aspx?direct=true&db=psyh&AN=2019-41681-001&lang=de&site=ehost-live
UR  - scott.morris@iit.edu
DP  - EBSCOhost
DB  - APA PsycInfo
ER  - 

TY  - THES
ID  - 2015-99110-298
AN  - 2015-99110-298
AU  - Challenger, Kathryn Lynch
T1  - Student perceptions of barriers to success on the nursing exit exam
JF  - Dissertation Abstracts International Section A: Humanities and Social Sciences
JO  - Dissertation Abstracts International Section A: Humanities and Social Sciences
Y1  - 2015///
VL  - 75
IS  - 12-A(E)
PB  - ProQuest Information & Learning
SN  - 0419-4209
SN  - 978-1-321-10393-9
N1  - Accession Number: 2015-99110-298. Other Journal Title: Dissertation Abstracts International. Partial author list: First Author & Affiliation: Challenger, Kathryn Lynch; Walden U., US. Release Date: 20150525. Correction Date: 20221128. Publication Type: Dissertation Abstract (0400). Format Covered: Electronic. Document Type: Dissertation. Dissertation Number: AAI3631619. ISBN: 978-1-321-10393-9. Language: EnglishMajor Descriptor: Nurses; Nursing; Nursing Students; Student Attitudes. Minor Descriptor: Test Taking. Classification: Educational & School Psychology (3500). Population: Human (10). Methodology: Empirical Study; Quantitative Study. 
AB  - Nursing programs increasingly use exit exams to increase graduating nursing students' readiness for the nurse licensing examination. These exams prepare students for computer adaptive testing and identify areas where remediation is needed. This study examines an Associate Degree in Nursing program that has shown consistently low exit exam scores causing delays in student graduation, eligibility to sit for the licensing exam, and entry into the workforce. The study was crafted to investigate student perspectives of the barriers to student success on the Health Education Systems Incorporated (HESI) exit exam. It employed a quantitative one-shot survey design to evaluate factors related to performance on the HESI exit exam, including lack of study time, lack of knowledge despite passing nursing courses, lack of motivation to prepare for the exam, overwhelming stress, poor test-taking skills, and lack of confidence. Using multiple linear regression analysis, the results showed that students' self-perceptions of poor test-taking skills had a significant negative effect on HESI exit exam scores. These findings were used to design a project focused on teaching test-taking skills throughout the nursing program. The study and project collectively promote positive social change by providing a means to improve nursing program graduation rates, a tool of great importance in addressing the current shortage of nurses in the United States' workforce. (PsycInfo Database Record (c) 2022 APA, all rights reserved)
KW  - quantitative one-shot survey
KW  - positive social change
KW  - student success
KW  - study time
KW  - licensing exam
KW  - Nurses
KW  - Nursing
KW  - Nursing Students
KW  - Student Attitudes
KW  - Test Taking
UR  - https://search.ebscohost.com/login.aspx?direct=true&db=psyh&AN=2015-99110-298&lang=de&site=ehost-live
DP  - EBSCOhost
DB  - APA PsycInfo
ER  - 

TY  - JOUR
ID  - 2020-87206-001
AN  - 2020-87206-001
AU  - Ravens-Sieberer, Ulrike
AU  - Devine, Janine
AU  - Bevans, Katherine
AU  - Riley, Anne W.
AU  - Moon, JeanHee
AU  - Salsman, John M.
AU  - Forrest, Christopher B.
T1  - Subjective well-being measures for children were developed within the PROMIS project: Presentation of first results
JF  - Journal of Clinical Epidemiology
JO  - Journal of Clinical Epidemiology
JA  - J Clin Epidemiol
Y1  - 2014/02//
VL  - 67
IS  - 2
SP  - 12
EP  - 12
PB  - Elsevier Science
SN  - 0895-4356
SN  - 1878-5921
AD  - Ravens-Sieberer, Ulrike, Department of Child and Adolescent Psychiatry, Psychotherapy and Psychosomatics, University Medical Center Hamburg-Eppendorf, Martinistr. 52, 20246, Hamburg, Germany
N1  - Accession Number: 2020-87206-001. Other Journal Title: Journal of Chronic Diseases. Partial author list: First Author & Affiliation: Ravens-Sieberer, Ulrike; Department of Child and Adolescent Psychiatry, Psychotherapy and Psychosomatics, University Medical Center Hamburg-Eppendorf, Hamburg, Germany. Other Publishers: Pergamon Press. Release Date: 20201210. Correction Date: 20210715. Publication Type: Journal (0100), Peer Reviewed Journal (0110). Format Covered: Print. Document Type: Journal Article. Language: EnglishGrant Information: Forrest, Christopher B. Major Descriptor: Pediatrics; Well Being; Patient Reported Outcome Measures; Child Health. Minor Descriptor: Childhood Development; Test Validity. Classification: Health Psychology Testing (2226); Health Psychology & Medicine (3360). Population: Human (10); Male (30); Female (40). Age Group: Childhood (birth-12 yrs) (100); School Age (6-12 yrs) (180); Adolescence (13-17 yrs) (200). Tests & Measures: PROMIS Measure of Subjective Well-Being. Methodology: Empirical Study; Interview; Qualitative Study; Quantitative Study. References Available: Y. Page Count: 1. Issue Publication Date: Feb, 2014. Copyright Statement: Elsevier Inc. 2014. 
AB  - Objectives: The aims of this Patient Reported Outcome Measurement Information System (PROMIS) study were to (1) conceptualize children’s subjective well-being (SWB) and (2) produce item pools with excellent content validity for calibration and use in computerized adaptive testings (CATs). Study Design and Setting: Children’s SWB was defined through semistructured interviews with experts, children (aged 8e17 years), parents, and a systematic literature review to identify item concepts comprehensively covering the full spectrum of SWB. Item concepts were transformed into item expressions and evaluated for comprehensibility using cognitive interviews, reading level analysis, and translatability review. Results: Children’s SWB comprises affective (positive affect) and global evaluation components (life satisfaction). Input from experts, children, parents, and the literature indicated that the eudaimonic dimension of SWBdthat is, a sense of meaning and purposedcould be evaluated. Item pools for life satisfaction (56 items), positive affect (53 items), and meaning and purpose (55 items) were produced. Small differences in comprehensibility of some items were observed between children and adolescents. Conclusion: The SWB measures for children are the first to assess both the hedonic and eudaimonic aspects of SWB. Both children and youth seem to understand the concepts of a meaningful life, optimism, and goal orientation. (PsycInfo Database Record (c) 2021 APA, all rights reserved)
KW  - Child
KW  - Pediatric health
KW  - well-being
KW  - Validity
KW  - well-being measures
KW  - PROMIS
KW  - patient outcomes
KW  - Pediatrics
KW  - Well Being
KW  - Patient Reported Outcome Measures
KW  - Child Health
KW  - Childhood Development
KW  - Test Validity
U1  - Sponsor: National Institutes of Health. Grant: 1U01AR057956. Other Details: NIH Roadmap for Medical Research (PROMIS II). Recipients: Forrest, Christopher B. (Prin Inv)
U1  - Sponsor: Northwestern University, US. Grant: 1U54AR057951. Other Details: Funded PROMIS II. Recipients: Cella, David (Prin Inv)
U1  - Sponsor: Northwestern University, US. Grant: 1U54AR057943. Recipients: Gershon, Richard C. (Prin Inv)
U1  - Sponsor: American Institutes for Research, US. Grant: 1U54AR057926. Recipients: Keller, Susan (San) D. (Prin Inv)
DO  - 10.1016/j.jclinepi.2013.08.018
UR  - https://search.ebscohost.com/login.aspx?direct=true&db=psyh&AN=2020-87206-001&lang=de&site=ehost-live
UR  - ravens-sieberer@uke.de
DP  - EBSCOhost
DB  - APA PsycInfo
ER  - 

TY  - JOUR
ID  - 2018-24447-001
AN  - 2018-24447-001
AU  - Gibbons, Robert D.
AU  - Alegría, Margarita
AU  - Cai, Li
AU  - Herrera, Lizbeth
AU  - Markle, Sheri Lapatin
AU  - Collazos, Francisco
AU  - Baca-García, Enrique
T1  - Successful validation of the CAT-MH Scales in a sample of Latin American migrants in the United States and Spain
JF  - Psychological Assessment
JO  - Psychological Assessment
JA  - Psychol Assess
Y1  - 2018/10//
VL  - 30
IS  - 10
SP  - 1267
EP  - 1276
PB  - American Psychological Association
SN  - 1040-3590
SN  - 1939-134X
AD  - Gibbons, Robert D., Departments of Medicine and Public Health Sciences, University of Chicago, 5841 South Maryland Avenue, Room W260, MC2000, Chicago, IL, US, 60637
N1  - Accession Number: 2018-24447-001. PMID: 29792502 Other Journal Title: Psychological Assessment: A Journal of Consulting and Clinical Psychology. Partial author list: First Author & Affiliation: Gibbons, Robert D.; Departments of Medicine and Public Health Sciences, University of Chicago, Chicago, IL, US. Release Date: 20180524. Correction Date: 20201130. Publication Type: Journal (0100), Peer Reviewed Journal (0110). Format Covered: Electronic. Document Type: Journal Article. Language: EnglishMajor Descriptor: Adaptive Testing; Foreign Language Translation; Mental Health; Test Validity; Computerized Assessment. Minor Descriptor: Anxiety; Immigration; Item Response Theory; Major Depression; Mania; Test Construction; Latinos/Latinas. Classification: Tests & Testing (2220); Psychological & Physical Disorders (3200). Population: Human (10); Male (30); Female (40). Location: Spain; US. Age Group: Adulthood (18 yrs & older) (300); Young Adulthood (18-29 yrs) (320); Thirties (30-39 yrs) (340); Middle Age (40-64 yrs) (360); Aged (65 yrs & older) (380). Tests & Measures: Computerized Adaptive Tests for Mental Health; Computerized Adaptive Tests for Mental Health--Spanish Version DOI: 10.1037/t67539-000. Methodology: Empirical Study; Quantitative Study. References Available: Y. Page Count: 10. Issue Publication Date: Oct, 2018. Publication History: First Posted Date: May 24, 2018; Accepted Date: Dec 18, 2017; Revised Date: Oct 23, 2017; First Submitted Date: Jan 13, 2017. Copyright Statement: American Psychological Association. 2018. 
AB  - We examined cultural differences in the item characteristic functions of self-reported of symptoms of depression, anxiety, and mania−hypomania in a Latino population taking Computerized Adaptive Tests for Mental Health (CAT-MH) in Spanish versus a non-Latino sample taking the tests in English. We studied differential item functioning (DIF) of the most common adaptively administered symptom items out of a bank of 1,008 items between Latino (n = 1276) and non-Latino (n = 798) subjects. For depression, we identified 4 items with DIF that were good discriminators for non-Latinos but poor discriminators for Latinos. These items were related to cheerfulness, life satisfaction, concentration, and fatigue. The correlation between the original calibration and a Latino-only new calibration after eliminating these items was r = .990. For anxiety, no items with DIF were identified. The correlation between the original and new calibrations was r = .993. For mania−hypomania, we identified 4 items with differential item functioning that were good discriminators for non-Latinos but poor discriminators for Latinos. These items were related to risk-taking, self-assurance, and sexual activity. The correlation between the original and new calibration was r = .962. Once the identified items were removed, the correlation between the original calibration and a Latino-only calibration was r = .96 or greater. These findings reveal that the CAT-MH can be reliably used to measure depression, anxiety, and mania in Latinos taking these tests in Spanish. (PsycInfo Database Record (c) 2020 APA, all rights reserved)
AB  - Public Significance Statement—This is the first study to validate the use of computerized adaptive tests for depression, anxiety, and mania at the symptom level in Latinos taking computerized adaptive tests in Spanish. We determined which symptoms that were good discriminators of high and low severity in a non-Latino population taking the tests in English were poor discriminators in a Spanish-speaking Latino population. The findings provide relatively unbiased cross-cultural psychiatric comparisons. (PsycInfo Database Record (c) 2020 APA, all rights reserved)
KW  - depression
KW  - anxiety
KW  - mania
KW  - item response theory
KW  - computerized adaptive testing
KW  - Adolescent
KW  - Adult
KW  - Aged
KW  - Aged, 80 and over
KW  - Anxiety
KW  - Anxiety Disorders
KW  - Bipolar Disorder
KW  - Calibration
KW  - Depression
KW  - Depressive Disorder
KW  - Diagnosis, Computer-Assisted
KW  - Emigrants and Immigrants
KW  - Fatigue
KW  - Female
KW  - Hispanic Americans
KW  - Humans
KW  - Latin America
KW  - Male
KW  - Mental Health
KW  - Middle Aged
KW  - Personal Satisfaction
KW  - Psychiatric Status Rating Scales
KW  - Reproducibility of Results
KW  - Self Report
KW  - Spain
KW  - Transients and Migrants
KW  - United States
KW  - Young Adult
KW  - Adaptive Testing
KW  - Foreign Language Translation
KW  - Mental Health
KW  - Test Validity
KW  - Computerized Assessment
KW  - Anxiety
KW  - Immigration
KW  - Item Response Theory
KW  - Major Depression
KW  - Mania
KW  - Test Construction
KW  - Latinos/Latinas
U1  - Sponsor: National Institutes of Health, US. Grant: 3R01MH100155-01S1. Other Details: supported by an administrative supplement for New Statistical Paradigm for Measuring Psychopathology Dimensions in Youth Grant. Recipients: No recipient indicated
U1  - Sponsor: National Institutes of Health, National Institute on Drug Abuse, US. Grant: R01DA034952. Recipients: No recipient indicated
DO  - 10.1037/pas0000569
UR  - https://search.ebscohost.com/login.aspx?direct=true&db=psyh&AN=2018-24447-001&lang=de&site=ehost-live
UR  - ORCID: 0000-0002-1388-498X
UR  - rdg@uchicago.edu
DP  - EBSCOhost
DB  - APA PsycInfo
ER  - 

TY  - JOUR
ID  - 2020-00774-001
AN  - 2020-00774-001
AU  - Carlozzi, Noelle E.
AU  - Lange, Rael T.
AU  - French, Louis M.
AU  - Kallen, Michael A.
AU  - Boileau, Nicholas R.
AU  - Hanks, Robin A.
AU  - Nakase-Richardson, Risa
AU  - Massengale, Jill P.
AU  - Sander, Angelle M.
AU  - Hahn, Elizabeth A.
AU  - Miner, Jennifer A.
AU  - Brickell, Tracey A.
T1  - TBI-CareQOL military health care frustration in caregivers of service members/veterans with traumatic brain injury
T3  - Caregivers of Service Members/Veterans and Civilians With Traumatic Brain Injury
JF  - Rehabilitation Psychology
JO  - Rehabilitation Psychology
JA  - Rehabil Psychol
Y1  - 2020/11//
VL  - 65
IS  - 4
SP  - 360
EP  - 376
PB  - American Psychological Association
SN  - 0090-5550
SN  - 1939-1544
SN  - 978-1-4338-9417-6
AD  - Carlozzi, Noelle E., Department of Physical Medicine and Rehabilitation, University of Michigan, North Campus Research Complex, 2800 Plymouth Road, Building NCRC B14, Room G216, Ann Arbor, MI, US, 48109-2800
N1  - Accession Number: 2020-00774-001. PMID: 31916805 Other Journal Title: Psychological Aspects of Disability. Partial author list: First Author & Affiliation: Carlozzi, Noelle E.; Department of Physical Medicine and Rehabilitation, University of Michigan, Ann Arbor, MI, US. Other Publishers: Division 22 of the American Psychological Association; Educational Publishing Foundation; Springer Publishing. Release Date: 20200109. Correction Date: 20201231. Publication Type: Journal (0100), Peer Reviewed Journal (0110). Format Covered: Electronic. Document Type: Journal Article. ISBN: 978-1-4338-9417-6. Language: EnglishMajor Descriptor: Caregivers; Frustration; Health Promotion; Traumatic Brain Injury; Military Measures. Minor Descriptor: Adaptive Testing; Number Systems; Test Construction. Classification: Neurological Disorders & Brain Damage (3297). Population: Human (10); Male (30); Female (40). Location: US. Age Group: Adulthood (18 yrs & older) (300). Tests & Measures: TBICareQOL Military Health Care Frustration; Caregiver Appraisal Scale; Military Health Care Frustration Measure; Global Ratings of Frustration Measure; Patient-Reported Outcomes Measurement Information System DOI: 10.1037/t06780-000. Methodology: Empirical Study; Quantitative Study. Supplemental Data: Tables and Figures Internet. Page Count: 17. Issue Publication Date: Nov, 2020. Publication History: First Posted Date: Jan 9, 2020; Accepted Date: Nov 20, 2019; Revised Date: Nov 18, 2019; First Submitted Date: May 7, 2019. Copyright Statement: American Psychological Association. 2020. 
AB  - Purpose: Caregivers of service members/veterans (SMVs) encounter a number of barriers when navigating the military health care system. The purpose of this study was to develop a new measure to assess potential caregiver frustration with the systems of care and benefits in the United States Departments of Defense and Veterans Affairs. Method: The TBI-CareQOL Military Health Care Frustration measure was developed using data from 317 caregivers of SMVs with TBI who completed an item pool comprised of 64 questions pertaining to anger or frustration with accessing military health care services. Results: Exploratory and confirmatory factor analyses supported the retention of 58 items. Constrained graded response model (GRM) overall fit and item fit analyses and differential item functioning investigations of age and education factors supported the retention of 43 items in the final measure. Expert review and GRM item calibration products were used to inform the selection of two 6-item static short forms (TBI-CareQOL Military Health Care Frustration-Self; TBI-CareQOL Military Health Care Frustration-Person with TBI) and to program the TBI-CareQOL Military Health Care Frustration computer adaptive test (CAT). Preliminary data supported the reliability (i.e., internal consistency and test–retest reliability) as well as the validity (i.e., convergent, discriminant, and known-groups) of the new measure. Conclusions: The new TBI-CareQOL Military Health Care Frustration measure can be used to examine caregiver perceptions of and experience with the military health care system, to target improvements. (PsycInfo Database Record (c) 2020 APA, all rights reserved)
AB  - Impact and Implications: There is a need for a measure of caregiver-reported concerns with navigating the military health care system. Given that caregivers are an important part of the recovery process for persons with TBI, it is important to evaluate their perception of services that their loved ones may or may not receive. A new measure, the TBI-CareQOL Military Health Care Frustration item bank, was developed for this purpose. It includes a long form (43 items), two six-item short forms (a version focused on services for the caregiver and a version focused on services for the person with TBI in the military), and a computer-adaptive test. This measure can assist the military health care system to focus on targeting unmet needs of the caregivers of those with TBI. (PsycInfo Database Record (c) 2020 APA, all rights reserved)
KW  - caregivers
KW  - frustration
KW  - health care
KW  - patient-reported outcomes
KW  - traumatic brain injury
KW  - Caregivers
KW  - Frustration
KW  - Health Promotion
KW  - Traumatic Brain Injury
KW  - Military Measures
KW  - Adaptive Testing
KW  - Number Systems
KW  - Test Construction
U1  - Sponsor: National Institutes of Health, National Institute of Nursing Research, US. Grant: R01NR013658. Recipients: No recipient indicated
U1  - Sponsor: National Center for Advancing Translational Sciences, US. Grant: UL1TR000433. Recipients: No recipient indicated
U1  - Sponsor: General Dynamics Information Technology, Inc.. Grant: DVBIC-SC-14-003; W91YTZ-13-C-0015. Recipients: No recipient indicated
DO  - 10.1037/rep0000305
L3  - 10.1037/rep0000305.supp (Supplemental)
UR  - https://search.ebscohost.com/login.aspx?direct=true&db=psyh&AN=2020-00774-001&lang=de&site=ehost-live
UR  - ORCID: 0000-0002-4056-4404
UR  - ORCID: 0000-0002-9451-0604
UR  - ORCID: 0000-0003-0439-9429
UR  - carlozzi@med.umich.edu
DP  - EBSCOhost
DB  - APA PsycInfo
ER  - 

TY  - THES
ID  - 1992-75864-001
AN  - 1992-75864-001
AU  - Powell, Zen-Hsiu E.
T1  - Test anxiety and test performance under computerized adaptive testing methods
JF  - Dissertation Abstracts International Section A: Humanities and Social Sciences
JO  - Dissertation Abstracts International Section A: Humanities and Social Sciences
Y1  - 1992/01//
VL  - 52
IS  - 7-A
SP  - 2518
EP  - 2518
PB  - ProQuest Information & Learning
SN  - 0419-4209
N1  - Accession Number: 1992-75864-001. Other Journal Title: Dissertation Abstracts International. Partial author list: First Author & Affiliation: Powell, Zen-Hsiu E.; Indiana U, US. Release Date: 19920901. Correction Date: 20200109. Publication Type: Dissertation Abstract (0400). Format Covered: Electronic. Document Type: Dissertation. Language: EnglishMajor Descriptor: Achievement; Test Anxiety; Computerized Assessment. Minor Descriptor: Test Performance. Classification: Tests & Testing (2220); Personality Traits & Processes (3120). Population: Human (10). Age Group: Adulthood (18 yrs & older) (300). Methodology: Empirical Study. Page Count: 1. 
KW  - computerized adaptive testing
KW  - test anxiety & performance
KW  - graduate students
KW  - Achievement
KW  - Test Anxiety
KW  - Computerized Assessment
KW  - Test Performance
UR  - https://search.ebscohost.com/login.aspx?direct=true&db=psyh&AN=1992-75864-001&lang=de&site=ehost-live
DP  - EBSCOhost
DB  - APA PsycInfo
ER  - 

TY  - JOUR
ID  - 2016-58513-016
AN  - 2016-58513-016
AU  - Beiser, David
AU  - Vu, Milkie
AU  - Gibbons, Robert
T1  - Test-retest reliability of a computerized adaptive depression screener
JF  - Psychiatric Services
JO  - Psychiatric Services
JA  - Psychiatr Serv
Y1  - 2016/09/01/
VL  - 67
IS  - 9
SP  - 1039
EP  - 1041
PB  - American Psychiatric Assn
SN  - 1075-2730
SN  - 1557-9700
AD  - Gibbons, Robert
N1  - Accession Number: 2016-58513-016. Other Journal Title: Hospital & Community Psychiatry. Partial author list: First Author & Affiliation: Beiser, David; Section of Emergency Medicine, University of Chicago, Chicago, IL, US. Release Date: 20170109. Correction Date: 20190211. Publication Type: Journal (0100), Peer Reviewed Journal (0110). Format Covered: Electronic. Document Type: Journal Article. Language: EnglishMajor Descriptor: Adaptive Testing; Emergency Services; Major Depression; Psychometrics; Test Reliability. Minor Descriptor: Test-Retest Reliability. Classification: Clinical Psychological Testing (2224); Affective Disorders (3211). Population: Human (10). Location: US. Age Group: Adolescence (13-17 yrs) (200). Tests & Measures: Computerized Adaptive Test–Depression Inventory DOI: 10.1037/t30221-000. Methodology: Empirical Study; Quantitative Study. References Available: Y. Page Count: 3. Issue Publication Date: Sep 1, 2016. Publication History: First Posted Date: Apr 15, 2016; Accepted Date: Dec 18, 2015; Revised Date: Oct 23, 2015; First Submitted Date: Jul 29, 2015. 
AB  - Objective: Computerized adaptive testing (CAT) provides improved precision and decreased test burden compared with traditional, fixed-length tests. Concerns have been raised regarding reliability of CAT-based measurements because the items administered vary both between and within individuals over time. The study measured test-retest reliability of the CAT Depression Inventory (CAT-DI) for assessment of depression in a screening setting where most scores fall in the normal range. Methods: A random sample of adults (N = 101) at an academic emergency department (ED) was screened twice with the CAT-DI during their visit. Test-retest scores, bias, and reliability were assessed. Results: Fourteen percent of patients scored in the mild range for depression, 4% in the moderate range, and 3% in the severe range. Test-retest scores were without significant bias and had excellent reliability (r = .92). Conclusions: The CAT-DI provided reliable screening results among ED patients. Concerns about whether changes in item presentation during repeat testing would affect test-retest reliability were not supported. (PsycINFO Database Record (c) 2019 APA, all rights reserved)
KW  - psychometrics
KW  - test reliability
KW  - emergency department
KW  - Computerized Adaptive Test–Depression Inventory
KW  - Adaptive Testing
KW  - Emergency Services
KW  - Major Depression
KW  - Psychometrics
KW  - Test Reliability
KW  - Test-Retest Reliability
U1  - Sponsor: National Institute of Mental Health, US. Grant: R01 MH66302. Recipients: No recipient indicated
DO  - 10.1176/appi.ps.201500304
UR  - https://search.ebscohost.com/login.aspx?direct=true&db=psyh&AN=2016-58513-016&lang=de&site=ehost-live
UR  - ORCID: 0000-0003-0230-473X
UR  - ORCID: 0000-0001-9676-087X
UR  - rdg@uchicago.edu
DP  - EBSCOhost
DB  - APA PsycInfo
ER  - 

TY  - JOUR
ID  - 2016-11925-008
AN  - 2016-11925-008
AU  - Suárez-Álvarez, Javier
AU  - Pedrosa, Ignacio
T1  - The assessment of entrepreneurial personality: The current situation and future directions
JF  - Papeles del Psicólogo
JO  - Papeles del Psicólogo
Y1  - 2016/04//
VL  - 37
IS  - 1
SP  - 62
EP  - 68
PB  - Revista Del Colegio Oficial De Psicologos
SN  - 0214-7823
SN  - 1886-1415
AD  - Suárez-Álvarez, Javier, Departamento de Psicologia, Universidad de Oviedo, Plaza Feijoo, s/n, 33003, Oviedo, Spain
N1  - Accession Number: 2016-11925-008. Translated Serial Title: Psychologist Papers. Partial author list: First Author & Affiliation: Suárez-Álvarez, Javier; Universidad de Oviedo, Oviedo, Spain. Release Date: 20161215. Correction Date: 20201123. Publication Type: Journal (0100), Peer Reviewed Journal (0110). Format Covered: Print. Document Type: Journal Article. Language: EnglishMajor Descriptor: Measurement; Personality Traits. Minor Descriptor: Entrepreneurship. Classification: Personality Traits & Processes (3120). Population: Human (10). Tests & Measures: Implicit Association Test DOI: 10.1037/t03782-000. Methodology: Literature Review. References Available: Y. Page Count: 7. Issue Publication Date: Apr, 2016. 
AB  - Entrepreneurship is fundamental in modern society because it represents an important source of innovation, employment, productivity, and growth. While the first theoretical models arose from economic and sociological approaches, psychology provides models that integrate different aspects such as cognitions, attitudes and personality, which allow a more detailed study. The purpose of this paper is to show the main contributions of psychology to the assessment of the enterprising personality. For this purpose, the main models and instruments developed to date were reviewed. The results confirm that the enterprising personality has a multidimensional structure and eight personality traits can be highlighted: achievement motivation, risk-taking, autonomy, self-efficacy, stress tolerance, innovativeness, internal locus of control, and optimism. From a methodological point of view, Item Response Theory and Computerised Adaptive Tests represent the most advanced and modern methods for assessing enterprising personality. There are currently several measurement instruments available. Future areas of research should be directed at the construction of multidimensional models as well as providing alternatives that facilitate a reduction in social desirability and other biases inherent in self-reports. (PsycInfo Database Record (c) 2020 APA, all rights reserved)
KW  - enterprising personality
KW  - entrepreneurship
KW  - item response theory
KW  - computerised adaptive tests
KW  - self-report.
KW  - Measurement
KW  - Personality Traits
KW  - Entrepreneurship
U1  - Sponsor: Ministry of Economy and Competitiveness. Recipients: No recipient indicated
U1  - Sponsor: Ministry of Education, Culture and Sports. Grant: PSI2011-28638; BES2012-053488; AP2010-1999. Recipients: No recipient indicated
UR  - https://search.ebscohost.com/login.aspx?direct=true&db=psyh&AN=2016-11925-008&lang=de&site=ehost-live
UR  - suarezajavier@uniovi.es
DP  - EBSCOhost
DB  - APA PsycInfo
ER  - 

TY  - JOUR
ID  - 2015-47748-010
AN  - 2015-47748-010
AU  - Lin, Zhe
AU  - Chen, Pin
AU  - Xin, Tao
T1  - The block item pocket method to allow item review in CAT
JF  - Acta Psychologica Sinica
JO  - Acta Psychologica Sinica
JA  - Xin Li Xue Bao
Y1  - 2015/09//
VL  - 47
IS  - 9
SP  - 1188
EP  - 1198
PB  - Science Press
SN  - 0439-755X
AD  - Xin, Tao, Institute of Developmental Psychology, Beijing Normal University, Beijing, China, 100875
N1  - Accession Number: 2015-47748-010. Partial author list: First Author & Affiliation: Lin, Zhe; Institute of Developmental Psychology, Beijing Normal University, Beijing, China. Release Date: 20151116. Correction Date: 20190211. Publication Type: Journal (0100), Peer Reviewed Journal (0110). Format Covered: Print. Document Type: Journal Article. Language: ChineseMajor Descriptor: Adaptive Testing; Human Computer Interaction; Testing Methods; Computerized Assessment. Classification: Tests & Testing (2220). Population: Human (10). Methodology: Empirical Study; Quantitative Study. References Available: Y. Page Count: 11. Issue Publication Date: Sep, 2015. 
AB  - Most computerized adaptive testing (CAT) do not allow examinees to review items because it will drastically decrease measurement precision and bring about extra cheating strategies (Wainer, 1993; Wise, 1996). Allowing item review is essential to make CAT comparable with traditional tests. It also matters in application. Item review enables examinees to correct mistakes due to carelessness, which can further improve the precision of ability estimation. No such option may cause some negative consequences for their overall performance especially in high-stake examinations, such as tension or anxiety (Vispoel, Henderickson, & Bleiler, 2000). Therefore, it is worth trying if allowing item review could alleviate problems mentioned at the beginning (Wise, 1996; Vispoel, 2000, 2005). Several methods have been proposed, including the successive block method (Stocking, 1997) and the item pocket (IP) method (Han, 2013). However, both methods are limited in some ways. Stocking's method does not allow examinees to skip items and requires a large number of blocks which may bring about some extra adverse effects because of frequent decision to go to next block. Han's method can avoid limitations of Stocking's. But it requires an appropriate IP size and may result in high bias in large IP size situation. The present study proposed the block item pocket (BIP) method which sets fewer but larger blocks with a proper total IP size. This method keeps advantages of Stocking's and Han's and overcomes their disadvantages. Two simulation studies of two response strategies were conducted to evaluate validity of the BIP method. Item parameters were randomly drawn from uniform distribution (b ~ U (-3, 3)) and (a ~ U (0, 2)). Each examinee was administered a fixed-length CAT with 30 items. The initial item for each examinee was randomly drawn from 0 ~ U (-0.5, 0.5). For the CAT administration, the Maximum Fisher Information method was adopted to select items. The interim and final scores were estimated using MLE method in most conditions. When responses were less than 5 or when all answers were correct or wrong, EAP method was adopted. Each study contained five conditions: non-review, 1 blocks IP method, 2 blocks, 3 blocks and 6 blocks BIP method. Statistics like BIAS, MAEj and RMSE were used as evaluation criteria. Results indicated that: (1) BIP method had better estimate precision than IP method at low ability level under normal strategy; (2) When dealing with Wainer-like strategy, BIP method was far more precise than item pocket method at all ability levels; (3) As the number of blocks increased, estimate precision got closer to non-review condition. Advantages of this new method and future directions were discussed. (PsycINFO Database Record (c) 2019 APA, all rights reserved)
KW  - computerized adaptive testing
KW  - item review
KW  - item pocket method
KW  - answer change
KW  - block item pocket method
KW  - Adaptive Testing
KW  - Human Computer Interaction
KW  - Testing Methods
KW  - Computerized Assessment
DO  - 10.3724/SP.J.1041.2015.01188
UR  - https://search.ebscohost.com/login.aspx?direct=true&db=psyh&AN=2015-47748-010&lang=de&site=ehost-live
UR  - xintao@bnu.edu.cn
DP  - EBSCOhost
DB  - APA PsycInfo
ER  - 

TY  - JOUR
ID  - 2017-55848-001
AN  - 2017-55848-001
AU  - Tulsky, David S.
AU  - Heinemann, Allen W.
T1  - The clinical utility and construct validity of the NIH Toolbox Cognition Battery (NIHTB-CB) in individuals with disabilities
JF  - Rehabilitation Psychology
JO  - Rehabilitation Psychology
JA  - Rehabil Psychol
Y1  - 2017/11//
VL  - 62
IS  - 4
SP  - 409
EP  - 412
PB  - American Psychological Association
SN  - 0090-5550
SN  - 1939-1544
AD  - Tulsky, David S., Center for Health Assessment Research and Translation, University of Delaware, STAR Campus, 540 South College Avenue, Newark, DE, US, 19713
N1  - Accession Number: 2017-55848-001. PMID: 29265861 Other Journal Title: Psychological Aspects of Disability. Partial author list: First Author & Affiliation: Tulsky, David S.; Center for Health Assessment Research and Translation, University of Delaware, Newark, DE, US. Other Publishers: Division 22 of the American Psychological Association; Educational Publishing Foundation; Springer Publishing. Release Date: 20171221. Correction Date: 20190211. Publication Type: Journal (0100), Peer Reviewed Journal (0110). Format Covered: Electronic. Document Type: Journal Article. Language: EnglishGrant Information: Heinemann, Allen W. Major Descriptor: Cognition; Disabilities; Neuropsychological Assessment; Test Validity; Test Battery. Minor Descriptor: Construct Validity. Classification: Neuropsychological Assessment (2225); Neurological Disorders & Brain Damage (3297). Population: Human (10). References Available: Y. Page Count: 4. Issue Publication Date: Nov, 2017. Publication History: Accepted Date: Nov 1, 2017; First Submitted Date: Nov 1, 2017. Copyright Statement: American Psychological Association. 2017. 
AB  - A State-of-the-Science conference on measurement with disability populations recommended '...the development of cognitive and psychosocial outcome measures, using computer-adaptive testing...that are low in respondent burden and valid across patient populations,' (Clohan et al., 2007, p. 1537). Following this recommendation, the National Institute on Disability, Independent Living, and Rehabilitation Research (NIDILRR) prioritized the development of measures of cognitive functioning for individuals with disabilities, noting that measures of cognitive functioning 'have not been developed for systemic application in the field of medical rehabilitation. Cognition is both a rehabilitation outcome and a factor related to broader functional and community outcomes for individuals with a wide variety of disabling conditions' (Office of Special Education & Rehabilitation Services, 2009, p. 37193). From this came the NIH Toolbox for the Assessment of Neurological and Behavioral Function project (NIH Toolbox) which provides a comprehensive set of cognitive, motor, sensory, and emotional health and function measures for use in clinical, longitudinal, and epidemiological research. The nine papers comprising this special section of Rehabilitation Psychology reflect the sustained collaborative efforts of more than two dozen investigators working at six sites over the past 8 years. They are an initial attempt to validate the NIHTB-CB in disability samples, and they provide initial evidence that the NIHTB-CB can be used with individuals who have TBI, SCI, or stroke. The articles published here reflect the fulfillment of recommendations made during a state-of-the-science conference in 2007. (PsycINFO Database Record (c) 2019 APA, all rights reserved)
KW  - clinical utility
KW  - construct validity
KW  - NIH Toolbox for the Assessment of Neurological and Behavioral Function
KW  - NIH Toolbox Cognition Battery
KW  - disabilities
KW  - Cognition
KW  - Disabled Persons
KW  - Humans
KW  - National Institutes of Health (U.S.)
KW  - Neuropsychological Tests
KW  - Reproducibility of Results
KW  - United States
KW  - Cognition
KW  - Disabilities
KW  - Neuropsychological Assessment
KW  - Test Validity
KW  - Test Battery
KW  - Construct Validity
U1  - Sponsor: National Institute on Disability, Independent Living, and Research Rehabilitation, US. Grant: H133B090024. Recipients: Heinemann, Allen W. (Prin Inv)
U1  - Sponsor: Blueprint for Neuroscience Research, National Institutes of Health, Office of Behavioral and Social Sciences Research. Grant: HHS-N-260-2006-00007-C. Other Details: Health, supported data collection for the NIH Toolbox development and adult norming. Recipients: Gershon, Richard C. (Prin Inv)
DO  - 10.1037/rep0000201
UR  - https://search.ebscohost.com/login.aspx?direct=true&db=psyh&AN=2017-55848-001&lang=de&site=ehost-live
UR  - dtulsky@udel.edu
DP  - EBSCOhost
DB  - APA PsycInfo
ER  - 

TY  - CHAP
ID  - 1997-36257-007
AN  - 1997-36257-007
AU  - Sands, W. A.
AU  - Gade, Paul A.
AU  - Knapp, Deirdre J.
ED  - Sands, William A.
ED  - Waters, Brian K.
ED  - McBride, James R.
T1  - The Computerized Adaptive-Screening Test
T2  - Computerized adaptive testing:  From inquiry to operation.
Y1  - 1997///
SP  - 69
EP  - 80
CY  - Washington, DC
PB  - American Psychological Association
SN  - 1-55798-442-5
N1  - Accession Number: 1997-36257-007. Partial author list: First Author & Affiliation: Sands, W. A.; US Army Research Inst for the Behavioral & Social Sciences, Organization & Personnel Resources Research Unit, US. Release Date: 19980201. Correction Date: 20190225. Publication Type: Book (0200), Edited Book (0280). Format Covered: Print. Document Type: Chapter. ISBN: 1-55798-442-5, ISBN Paperback. Language: EnglishMajor Descriptor: Adaptive Testing; Aptitude Measures; Army Personnel; Personnel Selection; Computerized Assessment. Minor Descriptor: Armed Services Vocational Aptitude Battery; Prediction; Psychometrics; Test Construction; Test Forms. Classification: Occupational & Employment Testing (2228); Military Psychology (3800). Population: Human (10). Location: US. Age Group: Adolescence (13-17 yrs) (200); Adulthood (18 yrs & older) (300); Young Adulthood (18-29 yrs) (320). Intended Audience: Psychology: Professional & Research (PS). Tests & Measures: Armed Services Vocational Aptitude Battery DOI: 10.1037/t11801-000. Page Count: 12. 
AB  - The authors describe the first application of Computerized Adaptive Testing (CAT) to personnel assessment in the Department of Defense: The U.S. Army's Computerized Adaptive Screening Test, known as CAST. CAST was introduced in 1984 as a more efficient alternative to a paper-and-pencil test, the Enlistment Screening Test. The purpose of both tests was to give military recruiters a means to forecast a candidate's aptitude qualification for military service by predicting his or her score on the Armed Forces Qualification Test composite of the Armed Services Vocational Aptitude Battery. Knowing who is likely to qualify for enlistment results in more efficient use of recruiters' time and other resources. The chapter summarizes the motivation for developing CAST, as well as its design, development, functional features and validation. Finally, the authors cite CAST as an example of cooperation among the Services, and provide a glimpse of related future developments. (PsycINFO Database Record (c) 2019 APA, all rights reserved)
KW  - computerized adaptive testing
KW  - personnel assessment
KW  - army personnel
KW  - Computerized Adaptive Screening Test
KW  - Armed Forces Qualification Test
KW  - Armed Services Vocational Aptitude Battery
KW  - military service
KW  - Adaptive Testing
KW  - Aptitude Measures
KW  - Army Personnel
KW  - Personnel Selection
KW  - Computerized Assessment
KW  - Armed Services Vocational Aptitude Battery
KW  - Prediction
KW  - Psychometrics
KW  - Test Construction
KW  - Test Forms
DO  - 10.1037/10244-007
UR  - https://search.ebscohost.com/login.aspx?direct=true&db=psyh&AN=1997-36257-007&lang=de&site=ehost-live
DP  - EBSCOhost
DB  - APA PsycInfo
ER  - 

TY  - JOUR
ID  - 2016-32185-001
AN  - 2016-32185-001
AU  - Paap, Muirne C. S.
AU  - Lenferink, Lonneke I. M.
AU  - Herzog, Nadine
AU  - Kroeze, Karel A.
AU  - van der Palen, Job
T1  - The COPD-SIB: A newly developed disease-specific item bank to measure health-related quality of life in patients with chronic obstructive pulmonary disease
JF  - Health and Quality of Life Outcomes
JO  - Health and Quality of Life Outcomes
JA  - Health Qual Life Outcomes
Y1  - 2016/12//
VL  - 14
PB  - BioMed Central Limited
SN  - 1477-7525
AD  - Paap, Muirne C. S., Department of Research Methodology, Measurement, and Data-Analysis, University of Twente, P.O. Box 217, 7500 AE, Enschede, Netherlands
N1  - Accession Number: 2016-32185-001. PMID: 27349641 Partial author list: First Author & Affiliation: Paap, Muirne C. S.; Centre for Educational Measurement at the University of Oslo (CEMO), Faculty of Educational Sciences, University of Oslo, Oslo, Norway. Release Date: 20160721. Correction Date: 20190211. Publication Type: Journal (0100), Peer Reviewed Journal (0110). Format Covered: Electronic. Document Type: Journal Article. Language: EnglishMajor Descriptor: Quality of Life; Test Construction; Test Reliability; Test Validity; Chronic Obstructive Pulmonary Disease. Minor Descriptor: Psychometrics; Health Related Quality of Life. Classification: Health Psychology Testing (2226); Health Psychology & Medicine (3360). Population: Human (10). Location: Netherlands. Age Group: Adulthood (18 yrs & older) (300). Tests & Measures: St. George Respiratory Questionnaire; Quality of Life for Respiratory Illness Questionnaire; Maugeri Respiratory Failure Questionnaire Reduced Form; COPD Assessment Test; Three Step Test Interview; Chronic Obstructive Pulmonary Disease--Specific Item Bank DOI: 10.1037/t58075-000. Methodology: Empirical Study; Interview; Quantitative Study. Supplemental Data: Data Sets Internet. References Available: Y. ArtID: 97. Issue Publication Date: Dec, 2016. Publication History: First Posted Date: Jun 27, 2016; Accepted Date: Jun 20, 2016; First Submitted Date: Feb 5, 2016. Copyright Statement: Open Access: This article is distributed under the terms of the Creative Commons Attribution 4.0 International License (http://creativecommons.org/licenses/by/4.0/), which permits unrestricted use, distribution, and reproduction in any medium, provided you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons license, and indicate if changes were made. The Creative Commons Public Domain Dedication waiver (http://creativecommons.org/publicdomain/zero/1.0/) applies to the data made available in this article, unless otherwise stated. The Author(s). 2016. 
AB  - Background: Health-related quality of life (HRQoL) is widely used as an outcome measure in the evaluation of treatment interventions in patients with chronic obstructive pulmonary disease (COPD). In order to address challenges associated with existing fixed-length measures (e.g., too long to be used routinely, too short to ensure both content validity and reliability), a COPD-specific item bank (COPD-SIB) was developed. Methods: Items were selected based on literature review and interviews with Dutch COPD patients, with a strong focus on both content validity and item comprehension. The psychometric quality of the item bank was evaluated using Mokken Scale Analysis and parametric Item Response Theory, using data of 666 COPD patients. Results: The final item bank contains 46 items that form a strong scale, tapping into eight important themes that were identified based on literature review and patient interviews: Coping with disease/symptoms, adaptability; Autonomy; Anxiety about the course/end-state of the disease, hopelessness; Positive psychological functioning; Situations triggering or enhancing breathing problems; Symptoms; Activity; Impact. Conclusions: The 46-item COPD-SIB has good psychometric properties and content validity. Items are available in Dutch and English. The COPD-SIB can be used as a stand-alone instrument, or to inform computerised adaptive testing. (PsycINFO Database Record (c) 2019 APA, all rights reserved)
KW  - Item response theory
KW  - IRT
KW  - Patient perspective
KW  - Item bank
KW  - COPD
KW  - SGRQ-C
KW  - MRF-26
KW  - VQ11
KW  - QoL-RIQ
KW  - Adaptation, Psychological
KW  - Adult
KW  - Aged
KW  - Anxiety
KW  - Female
KW  - Health Status
KW  - Humans
KW  - Male
KW  - Middle Aged
KW  - Netherlands
KW  - Outcome Assessment (Health Care)
KW  - Psychometrics
KW  - Pulmonary Disease, Chronic Obstructive
KW  - Quality of Life
KW  - Reproducibility of Results
KW  - Surveys and Questionnaires
KW  - Quality of Life
KW  - Test Construction
KW  - Test Reliability
KW  - Test Validity
KW  - Chronic Obstructive Pulmonary Disease
KW  - Psychometrics
KW  - Health Related Quality of Life
U1  - Sponsor: Lung Foundation Netherlands, Netherlands. Grant: 3.4.11.004. Recipients: No recipient indicated
UR  - https://search.ebscohost.com/login.aspx?direct=true&db=psyh&AN=2016-32185-001&lang=de&site=ehost-live
UR  - ORCID: 0000-0002-1173-7070
UR  - j.vanderpalen@mst.nl
UR  - k.a.kroeze@utwente.nl
UR  - nadineher@web.de
UR  - l.i.m.lenferink@rug.nl
UR  - m.c.s.paap@cemo.uio.no
DP  - EBSCOhost
DB  - APA PsycInfo
ER  - 

TY  - JOUR
ID  - 2017-54618-003
AN  - 2017-54618-003
AU  - Sunderland, Matthew
AU  - Batterham, Philip J.
AU  - Calear, Alison L.
AU  - Carragher, Natacha
T1  - The development and validation of static and adaptive screeners to measure the severity of panic disorder, social anxiety disorder, and obsessive compulsive disorder
JF  - International Journal of Methods in Psychiatric Research
JO  - International Journal of Methods in Psychiatric Research
JA  - Int J Methods Psychiatr Res
Y1  - 2017/12//
VL  - 26
IS  - 4
SP  - 1
EP  - 9
PB  - John Wiley & Sons
SN  - 1049-8931
SN  - 1557-0657
AD  - Sunderland, Matthew, National Drug and Alcohol Research Centre, UNSW, Sydney, NSW, Australia, 2031
N1  - Accession Number: 2017-54618-003. Partial author list: First Author & Affiliation: Sunderland, Matthew; NHMRC Centre for Research Excellence in Mental Health and Substance Use, National Drug and Alcohol Research Centre, UNSW, Sydney, NSW, Australia. Release Date: 20180118. Correction Date: 20221128. Publication Type: Journal (0100), Peer Reviewed Journal (0110). Format Covered: Electronic. Document Type: Journal Article. Language: EnglishGrant Information: Sunderland, Matthew. Major Descriptor: Obsessive Compulsive Disorder; Panic Disorder; Psychometrics; Social Anxiety; Social Phobia. Minor Descriptor: Test Reliability; Test Validity. Classification: Clinical Psychological Testing (2224); Anxiety Disorders (3215). Population: Human (10); Male (30); Female (40). Location: Australia. Age Group: Adulthood (18 yrs & older) (300); Young Adulthood (18-29 yrs) (320); Thirties (30-39 yrs) (340); Middle Age (40-64 yrs) (360); Aged (65 yrs & older) (380). Tests & Measures: Social Phobia Screener; Short Obsessions and Compulsions Screener; Australian National Survey of Mental Health and Well‐being; Panic Disorder Screener DOI: 10.1037/t47047-000. Methodology: Empirical Study; Quantitative Study. References Available: Y. Page Count: 9. Issue Publication Date: Dec, 2017. Publication History: Accepted Date: Feb 10, 2017; Revised Date: Feb 8, 2017; First Submitted Date: Oct 26, 2016. Copyright Statement: John Wiley & Sons, Ltd. 2017. 
AB  - A series of static and adaptive screeners for panic disorder, social anxiety disorder (SAD), and obsessive compulsive disorder (OCD) were developed and compared using data‐driven methods to facilitate the measurement of each disorder in community samples. Data comprised 3175 respondents for the development sample and 3755 respondents for the validation sample, recruited independently using Facebook advertising. Item Response Theory (IRT) was utilized to develop static continuous screeners and to simulate computerized adaptive algorithms. The screeners consisted of a small subset of items from each bank (79% reduction in items for panic disorder, 85% reduction in items for SAD, and 84% reduction in items for OCD) that provided similar scores (r = 0.88–0.96). Both static and adaptive screeners were valid with respect to existing scales that purportedly measure similar constructs (r > 0.70 for panic disorder, r > 0.76 for SAD, and r > 0.68 for OCD). The adaptive scales were able to maintain a higher level of precision in comparison to the static scales and evidenced slightly higher concordance with scores generated by the full item banks. The screeners for panic disorder, SAD, and OCD could be used as a flexible approach to measure and monitor the severity of psychopathology in tailored treatment protocols. (PsycInfo Database Record (c) 2022 APA, all rights reserved)
KW  - adaptive testing
KW  - IRT
KW  - obsessive compulsive disorder
KW  - panic disorder
KW  - screening
KW  - social anxiety
KW  - Obsessive Compulsive Disorder
KW  - Panic Disorder
KW  - Psychometrics
KW  - Social Anxiety
KW  - Social Phobia
KW  - Test Reliability
KW  - Test Validity
U1  - Sponsor: National Health and Medical Research Council. Grant: 1052327; 1083311; 1013199. Recipients: Sunderland, Matthew; Batterham, Philip J.; Calear, Alison L.
U1  - Sponsor: National Health and Medical Research Council. Grant: 1043952. Recipients: No recipient indicated
DO  - 10.1002/mpr.1561
UR  - https://search.ebscohost.com/login.aspx?direct=true&db=psyh&AN=2017-54618-003&lang=de&site=ehost-live
UR  - ORCID: 0000-0002-4547-6876
UR  - ORCID: 0000-0001-8452-364X
UR  - matthews@unsw.edu.au
DP  - EBSCOhost
DB  - APA PsycInfo
ER  - 

TY  - CHAP
ID  - 1999-02356-001
AN  - 1999-02356-001
AU  - Zickar, Michael J.
AU  - Overton, Randall C.
AU  - Taylor, L. Rogers
AU  - Harms, Harvey J.
ED  - Drasgow, Fritz
ED  - Olson-Buchanan, Julie B.
T1  - The development of a computerized selection system for computer programmers in a financial services company
T2  - Innovations in computerized assessment.
Y1  - 1999///
SP  - 7
EP  - 33
CY  - Mahwah, NJ
PB  - Lawrence Erlbaum Associates Publishers
SN  - 0-8058-2876-1
SN  - 0-8058-2877-X
N1  - Accession Number: 1999-02356-001. Partial author list: First Author & Affiliation: Zickar, Michael J.; Bowling Green State U, Bowling Green, OH, US. Release Date: 19990601. Correction Date: 20200907. Publication Type: Book (0200), Edited Book (0280). Format Covered: Print. Document Type: Chapter. ISBN: 0-8058-2876-1, ISBN Hardcover; 0-8058-2877-X, ISBN Paperback. Language: EnglishMajor Descriptor: Adaptive Testing; Computer Programming; Financial Services; Personnel Selection; Computerized Assessment. Minor Descriptor: Business Organizations; Test Construction; Test Validity. Classification: Occupational & Employment Testing (2228); Personnel Management & Selection & Training (3620). Population: Human (10). Location: US. Age Group: Adulthood (18 yrs & older) (300). Intended Audience: Psychology: Professional & Research (PS). Page Count: 27. 
AB  - This chapter describes the development and validation of a system for selecting entry-level computer programmers at a large financial services corporation. An important component of the selection system is a computer adaptive test (CAT). The practical and psychometric reasons for choosing an adaptive testing format are described, the reasoning behind the choices is emphasized, and the extent to which the project's objectives were achieved is discussed. In the selection system, the adaptive test was combined with a biodata form and with a semistructured, behavior-oriented interview to maximize the criterion variance predicted. (PsycInfo Database Record (c) 2020 APA, all rights reserved)
AB  - A newly commissioned math reasoning ability test was used as the basis for the adaptive test. The chapter includes a discussion of how incumbent computer programmers were used to calibrate the item pool and to conduct a concurrent validity study. Given its intended use, the authors paid particular attention to issues relating to how the content of the test could be kept secure, how an adaptive test might affect applicants' reactions to the organization, and keeping the time needed to complete the test to an acceptable amount while providing examinees with enough time to answer. (PsycInfo Database Record (c) 2020 APA, all rights reserved)
KW  - development & validation of computer adaptive selection test for financial service corporation entry level computer programmers
KW  - Adaptive Testing
KW  - Computer Programming
KW  - Financial Services
KW  - Personnel Selection
KW  - Computerized Assessment
KW  - Business Organizations
KW  - Test Construction
KW  - Test Validity
UR  - https://search.ebscohost.com/login.aspx?direct=true&db=psyh&AN=1999-02356-001&lang=de&site=ehost-live
DP  - EBSCOhost
DB  - APA PsycInfo
ER  - 

TY  - THES
ID  - 2000-95009-146
AN  - 2000-95009-146
AU  - Strong, Shawn Driftmier
T1  - The development of a computerized version of Vandenberg's mental rotation test and the effect of visuo-spatial working memory loading
JF  - Dissertation Abstracts International Section A: Humanities and Social Sciences
JO  - Dissertation Abstracts International Section A: Humanities and Social Sciences
Y1  - 2000/05//
VL  - 60
IS  - 11-A
SP  - 3938
EP  - 3938
PB  - ProQuest Information & Learning
SN  - 0419-4209
N1  - Accession Number: 2000-95009-146. Other Journal Title: Dissertation Abstracts International. Partial author list: First Author & Affiliation: Strong, Shawn Driftmier; Iowa State U., US. Release Date: 20010228. Correction Date: 20190211. Publication Type: Dissertation Abstract (0400). Format Covered: Electronic. Document Type: Dissertation. Dissertation Number: AAI9950123. Language: EnglishMajor Descriptor: Mental Rotation; Test Construction; Test Validity; Visuospatial Memory; Computerized Assessment. Minor Descriptor: Short Term Memory. Classification: Psychometrics & Statistics & Methodology (2200). Population: Human (10). Age Group: Adulthood (18 yrs & older) (300); Young Adulthood (18-29 yrs) (320). Methodology: Empirical Study. Page Count: 1. 
AB  - This dissertation focused on the generation and evaluation of web-based versions of Vandenberg's Mental Rotation Test. Memory and spatial visualization theory were explored in relation to the addition of a visuo-spatial working memory component. Analysis of the data determined that there was a significant difference between scores on the MRT Computer and MRT Memory test. The addition of a visuo-spatial working memory component did significantly affect results at the .05 alpha level. Reliability and discrimination estimates were higher on the MRT Memory version. The computerization of the paper and pencil version on the MRT did not significantly effect scores but did effect the time required to complete the test. The population utilized in the quasi-experiment consisted of 107 university students from eight institutions in engineering graphics related courses. The subjects completed two researcher developed, Web-based versions of Vandenberg's Mental Rotation Test and the original paper and pencil version of the Mental Rotation Test. One version of the test included a visuo-spatial working memory loading. Significant contributions of this study included developing and evaluating computerized versions of Vandenberg's Mental Rotation Test. Previous versions of Vandenberg's Mental Rotation Test did not take advantage of the ability of the computer to incorporate an interaction factor, such as a visuo-spatial working memory loading, into the test. The addition of an interaction factor results in a more discriminate test which will lend itself well to computerized adaptive testing practices. Educators in engineering graphics related disciplines should strongly consider the use of spatial visualization tests to aid in establishing the effects of modern computer systems on fundamental design/drafting skills. Regular testing of spatial visualization skills will result assist in the creation of a more relevant curriculum. Computerized tests which are valid and reliable will assist in making this task feasible. (PsycINFO Database Record (c) 2019 APA, all rights reserved)
KW  - development & evaluation of computerized version of Vandenberg's Mental Rotation Test & effect of visuo-spatial working memory loading
KW  - university students in engineering graphics related courses
KW  - Mental Rotation
KW  - Test Construction
KW  - Test Validity
KW  - Visuospatial Memory
KW  - Computerized Assessment
KW  - Short Term Memory
UR  - https://search.ebscohost.com/login.aspx?direct=true&db=psyh&AN=2000-95009-146&lang=de&site=ehost-live
DP  - EBSCOhost
DB  - APA PsycInfo
ER  - 

TY  - THES
ID  - 1995-73132-001
AN  - 1995-73132-001
AU  - Jeng, Hi-Lian
T1  - The effect of adaptive testing item selection methods on the precision of ability estimation and the examinee's motivation
JF  - Dissertation Abstracts International Section A: Humanities and Social Sciences
JO  - Dissertation Abstracts International Section A: Humanities and Social Sciences
Y1  - 1993/09//
VL  - 54
IS  - 3-A
SP  - 905
EP  - 905
PB  - ProQuest Information & Learning
SN  - 0419-4209
N1  - Accession Number: 1995-73132-001. Other Journal Title: Dissertation Abstracts International. Partial author list: First Author & Affiliation: Jeng, Hi-Lian; U Pittsburgh, PA, US. Release Date: 19950901. Correction Date: 20200109. Publication Type: Dissertation Abstract (0400). Format Covered: Electronic. Document Type: Dissertation. Language: EnglishMajor Descriptor: Adaptive Testing; Item Content (Test); Scoring (Testing); Test Construction. Minor Descriptor: Motivation. Classification: Tests & Testing (2220). Population: Human (10). Age Group: Adulthood (18 yrs & older) (300). Methodology: Empirical Study. Page Count: 1. 
KW  - adaptive testing item selection methods
KW  - pecision of ability estimation & examinee motivation
KW  - adults
KW  - Adaptive Testing
KW  - Item Content (Test)
KW  - Scoring (Testing)
KW  - Test Construction
KW  - Motivation
UR  - https://search.ebscohost.com/login.aspx?direct=true&db=psyh&AN=1995-73132-001&lang=de&site=ehost-live
DP  - EBSCOhost
DB  - APA PsycInfo
ER  - 

TY  - THES
ID  - 1996-95023-083
AN  - 1996-95023-083
AU  - Gershon, Richard Carl
T1  - The effect of individual differences variables on the assessment of ability for Computerized Adaptive Testing
JF  - Dissertation Abstracts International: Section B: The Sciences and Engineering
JO  - Dissertation Abstracts International: Section B: The Sciences and Engineering
Y1  - 1996/12//
VL  - 57
IS  - 6-B
SP  - 4085
EP  - 4085
PB  - ProQuest Information & Learning
SN  - 0419-4217
N1  - Accession Number: 1996-95023-083. Other Journal Title: Dissertation Abstracts International. Partial author list: First Author & Affiliation: Gershon, Richard Carl; Northwestern U, US. Release Date: 19960101. Correction Date: 20190211. Publication Type: Dissertation Abstract (0400). Format Covered: Electronic. Document Type: Dissertation. Dissertation Number: AAM9632690. Language: EnglishMajor Descriptor: Individual Differences; Test Anxiety; Test Scores; Test Taking; Computerized Assessment. Minor Descriptor: Performance. Classification: Psychometrics & Statistics & Methodology (2200). Population: Human (10). Age Group: Adulthood (18 yrs & older) (300). Methodology: Empirical Study. Page Count: 1. 
AB  - Computerized Adaptive Testing (CAT) continues to gain momentum as the accepted testing modality for a growing number of certification, licensure, education, government and human resource applications. However, the developers of these tests have for the most part failed to adequately explore the impact of individual differences such as test anxiety on the adaptive testing process. It is widely accepted that non-cognitive individual differences variables interact with the assessment of ability when using written examinations. Logic would dictate that individual differences variables would equally affect CAT. Two studies were used to explore this premise. In the first study, 507 examinees were given a test anxiety survey prior to taking a high stakes certification exam using CAT or using a written format. All examinees had already completed their course of study, and the examination would be their last hurdle prior to being awarded certification. High test anxious examinees performed worse than their low anxious counterparts on both testing formats. The second study replicated the finding that anxiety depresses performance in CAT. It also addressed the differential effect of anxiety on within test performance. Examinees were candidates taking their final certification examination following a four year college program. Ability measures were calculated for each successive part of the test for 923 subjects. Within subject performance varied depending upon test position. High anxious examinees performed poorly at all points in the test, while low and medium anxious examinee performance peaked in the middle of the test. If test anxiety and performance measures were actually the same trait, then low anxious individuals should have performed equally well throughout the test. The observed interaction of test anxiety and time on task serves as strong evidence that test anxiety has motivationally mediated as well as cognitively mediated effects. The results of the studies are di (PsycINFO Database Record (c) 2019 APA, all rights reserved)
KW  - individual differences
KW  - performance in Computerized Adaptive Testing (CAT)
KW  - examinees
KW  - Individual Differences
KW  - Test Anxiety
KW  - Test Scores
KW  - Test Taking
KW  - Computerized Assessment
KW  - Performance
UR  - https://search.ebscohost.com/login.aspx?direct=true&db=psyh&AN=1996-95023-083&lang=de&site=ehost-live
DP  - EBSCOhost
DB  - APA PsycInfo
ER  - 

TY  - JOUR
ID  - 1999-01404-001
AN  - 1999-01404-001
AU  - Stone, Gregory Ethan
AU  - Lunz, Mary E.
T1  - The effect of review on the psychometric characterstics of computerized adaptive tests
JF  - Applied Measurement in Education
JO  - Applied Measurement in Education
Y1  - 1994///
VL  - 7
IS  - 3
SP  - 211
EP  - 222
PB  - Lawrence Erlbaum
SN  - 0895-7347
SN  - 1532-4818
N1  - Accession Number: 1999-01404-001. Partial author list: First Author & Affiliation: Stone, Gregory Ethan; American Society of Clinical Pathologists, Chicago, IL, US. Other Publishers: Taylor & Francis. Release Date: 19991201. Correction Date: 20190211. Publication Type: Journal (0100), Peer Reviewed Journal (0110). Format Covered: Print. Document Type: Journal Article. Language: EnglishMajor Descriptor: Adaptive Testing; Psychometrics; Test Taking; Computerized Assessment. Minor Descriptor: Educational Measurement; Professional Examinations. Classification: Educational Measurement (2227); Academic Learning & Achievement (3550). Population: Human (10). Methodology: Empirical Study. References Available: Y. Page Count: 12. Issue Publication Date: 1994. 
AB  - Explored the effect of reviewing items and altering responses on examinee ability estimates, test precision, test information, decision confidence, and pass/fail status for computerized adaptive tests. Two different populations of examinees took different computerized certification examinations. For purposes of analysis, each population was divided into 3 ability groups (high, medium, and low). Ability measures before and after review were highly correlated, but slightly lower decision confidence was found after review. Pass/fail status was most affected for examinees with estimates close to the pass point. Decisions remained the same for 94% of the examinees. Test precision is only slightly affected by review, and the average information loss can be recovered by the addition of one item. (PsycINFO Database Record (c) 2019 APA, all rights reserved)
KW  - effect of reviewing items & altering responses on psychometric characteristics of computerized adaptive tests
KW  - certification examinees
KW  - Adaptive Testing
KW  - Psychometrics
KW  - Test Taking
KW  - Computerized Assessment
KW  - Educational Measurement
KW  - Professional Examinations
DO  - 10.1207/s15324818ame0703_4
UR  - https://search.ebscohost.com/login.aspx?direct=true&db=psyh&AN=1999-01404-001&lang=de&site=ehost-live
DP  - EBSCOhost
DB  - APA PsycInfo
ER  - 

TY  - JOUR
ID  - 2008-07048-006
AN  - 2008-07048-006
AU  - Häusler, Joachim
AU  - Sommer, Markus
T1  - The effect of success probability on test economy and self-confidence in computerized adaptive tests
JF  - Psychology Science
JO  - Psychology Science
Y1  - 2008///
VL  - 50
IS  - 1
SP  - 75
EP  - 87
PB  - Pabst Science Publishers
SN  - 1614-9947
AD  - Häusler, Joachim, Hyrtlstrasse 45, 2340, Modling, Austria
N1  - Accession Number: 2008-07048-006. Other Journal Title: Psychological Test and Assessment Modeling; Psychologische Beitrage. Partial author list: First Author & Affiliation: Häusler, Joachim; Schuhfried GmbH, Austria. Release Date: 20081117. Correction Date: 20190211. Publication Type: Journal (0100), Peer Reviewed Journal (0110). Format Covered: Print. Document Type: Journal Article. Language: EnglishMajor Descriptor: Adaptive Testing; Algorithms; Probability; Self-Confidence; Computerized Assessment. Minor Descriptor: Motivation; Psychometrics. Classification: Psychometrics & Statistics & Methodology (2200); Personality Traits & Processes (3120). Population: Human (10); Male (30); Female (40). Age Group: Adulthood (18 yrs & older) (300); Young Adulthood (18-29 yrs) (320); Thirties (30-39 yrs) (340); Middle Age (40-64 yrs) (360). Tests & Measures: Lexical Knowledge Test. Methodology: Empirical Study; Quantitative Study. References Available: Y. Page Count: 13. Issue Publication Date: 2008. 
AB  - Recent research on the psychological effects of different design decisions in computerized adaptive tests indicates that the maximum-information item selection rule fails to optimize respondents' test-taking motivation. While several recent studies have investigated psychological reactions to computerized adaptive tests using a consistently higher base success rate, little research has so far been conducted on the psychometric (primarily test reliability and bias) and psychological effects (e.g. test-taking motivation, self-confidence) of using mixtures of highly informative (p = .50) and easier items (p = .80) in the item selection process. The present paper thus compares these modifications to item selection with a classical maximum-information algorithm. In a simulation study the effect of the different item selection algorithms on measurement precision and bias in the person parameter estimates is evaluated. To do so, the item pool of the Lexical Knowledge Test, measuring crystallized intelligence and self-confidence, is used. The study indicated that modifications using base success probabilities over p = .70 lead to reduced measurement accuracy and--more seriously--a bias in the person parameter estimates for higher ability respondents. However, this was not the case for the motivator item algorithm, occasionally administering easier items as well. The second study (n = 191) thus compared the unmodified maximum-information algorithm with two motivator item algorithms, which differed with regard to the percentage of motivator items presented. The results indicate that respondents yield higher self-confidence estimates under the motivator item conditions. Furthermore, the three conditions did not differ from each other with regard to the total test duration. It can be concluded that a small number of easier motivator items is sufficient to preserve test-taking motivation throughout the test without a loss of test economy. (PsycINFO Database Record (c) 2019 APA, all rights reserved)
KW  - success probability
KW  - self-confidence
KW  - computerized adaptive tests
KW  - item selection
KW  - test-taking motivation
KW  - psychometrics
KW  - Adaptive Testing
KW  - Algorithms
KW  - Probability
KW  - Self-Confidence
KW  - Computerized Assessment
KW  - Motivation
KW  - Psychometrics
UR  - https://search.ebscohost.com/login.aspx?direct=true&db=psyh&AN=2008-07048-006&lang=de&site=ehost-live
UR  - sommer@schuhfried.at
UR  - haeusler@schuhfried.at
DP  - EBSCOhost
DB  - APA PsycInfo
ER  - 

TY  - THES
ID  - 2002-95007-003
AN  - 2002-95007-003
AU  - Rizavi, Saba M.
T1  - The effect of test characteristics on aberrant response patterns in computer adaptive testing
JF  - Dissertation Abstracts International Section A: Humanities and Social Sciences
JO  - Dissertation Abstracts International Section A: Humanities and Social Sciences
Y1  - 2002/04//
VL  - 62
IS  - 10-A
SP  - 3363
EP  - 3363
PB  - ProQuest Information & Learning
SN  - 0419-4209
N1  - Accession Number: 2002-95007-003. Other Journal Title: Dissertation Abstracts International. Partial author list: First Author & Affiliation: Rizavi, Saba M.; U Massachusetts Amherst, US. Release Date: 20020821. Publication Type: Dissertation Abstract (0400). Format Covered: Electronic. Document Type: Dissertation. Dissertation Number: AAI3027247. Language: EnglishMajor Descriptor: Adaptive Testing; Test Administration. Classification: Psychometrics & Statistics & Methodology (2200). Population: Human (10). Age Group: Adulthood (18 yrs & older) (300). Methodology: Empirical Study. Page Count: 1. 
AB  - The advantages that computer adaptive testing offers over linear tests have been well documented. The Computer Adaptive Test (CAT) design is more efficient than the Linear test design as fewer items are needed to estimate an examinee's proficiency to a desired level of precision. In the ideal situation, a CAT will result in examinees answering different number of items according to the stopping rule employed. Unfortunately, the realities of testing conditions have necessitated the imposition of time and minimum test length limits on CATs. Such constraints might place a burden on the CAT test taker resulting in aberrant response behaviors by some examinees. Occurrence of such response patterns results in inaccurate estimation of examinee proficiency levels. This study examined the effects of test lengths, time limits and the interaction of these factors with the examinee proficiency levels on the occurrence of aberrant response patterns. The focus of the study was on the aberrant behaviors caused by rushed guessing due to restrictive time limits. Four different testing scenarios were examined; fixed length performance tests with and without content constraints, fixed length mastery tests and variable length mastery tests without content constraints. For each of these testing scenarios, the effect of two test lengths, five different timing conditions and the interaction between these factors with three ability levels on ability estimation were examined. For fixed and variable length mastery tests, decision accuracy was also looked at in addition to the estimation accuracy. Several indices were used to evaluate the estimation and decision accuracy for different testing conditions. The results showed that changing time limits had a significant impact on the occurrence of aberrant response patterns conditional on ability. Increasing test length had negligible if not negative effect on ability estimation when rushed guessing occured. In case of performance testing high ability examinees while in classification testing middle ability examinees suffered the most. The decision accuracy was considerably affected in case of variable length classification tests. (PsycINFO Database Record (c) 2016 APA, all rights reserved)
KW  - test characteristics
KW  - aberrant response
KW  - computer adaptive testing
KW  - Adaptive Testing
KW  - Test Administration
UR  - https://search.ebscohost.com/login.aspx?direct=true&db=psyh&AN=2002-95007-003&lang=de&site=ehost-live
DP  - EBSCOhost
DB  - APA PsycInfo
ER  - 

TY  - JOUR
ID  - 2016-28782-005
AN  - 2016-28782-005
AU  - Lu, Hong
AU  - Hu, Yi-ping
AU  - Gao, Jia-jia
AU  - Kinshuk
T1  - The effects of computer self-efficacy, training satisfaction and test anxiety on attitude and performance in computerized adaptive testing
JF  - Computers & Education
JO  - Computers & Education
JA  - Comput Educ
Y1  - 2016/09//
VL  - 100
SP  - 45
EP  - 55
PB  - Elsevier Science
SN  - 0360-1315
SN  - 1873-782X
AD  - Lu, Hong
N1  - Accession Number: 2016-28782-005. Partial author list: First Author & Affiliation: Lu, Hong; College of Communication, Shandong Normal University, China. Release Date: 20160630. Correction Date: 20170116. Publication Type: Journal (0100), Peer Reviewed Journal (0110). Format Covered: Electronic. Document Type: Journal Article. Language: EnglishMajor Descriptor: Adaptive Testing; Computer Assisted Instruction; Self-Efficacy; Test Anxiety. Classification: Curriculum & Programs & Teaching Methods (3530); Human Factors Engineering (4010). Population: Human (10). Location: China. Age Group: Adolescence (13-17 yrs) (200). Tests & Measures: Graduate Management Admission Test; Test Anxiety Inventory--Chinese Version DOI: 10.1037/t49166-000; Training Satisfaction Rating Scale DOI: 10.1037/t03682-000. Methodology: Empirical Study; Mathematical Model; Quantitative Study. References Available: Y. Page Count: 11. Issue Publication Date: Sep, 2016. Publication History: First Posted Date: Apr 30, 2016; Accepted Date: Apr 29, 2016; Revised Date: Apr 28, 2016; First Submitted Date: Oct 20, 2015. Copyright Statement: All rights reserved. Elsevier Ltd. 2016. 
AB  - This study focused on test-takers’ psychological effects on computerized adaptive testing (CAT). The development and implementation of CAT were based on item response theory (IRT), and two-parameter logistic model was chosen for the items. The total of 268 students from a high school in Jinan took part in the English adaptive test. A structural equation model was used to examine the potential connections among a series of individual variables (computer self-efficacy, training satisfaction, test anxiety, CAT attitude and CAT performance). The findings revealed significant positive paths from computer self-efficacy and training satisfaction to CAT attitude, as well as a negative path from test anxiety to CAT performance. Furthermore, there was significant correlation between the residual variances of CAT attitude and CAT performance. Thus, it could be seen CAT might produce an unfair disadvantage for test-takers with higher test anxiety. The relevant research and implications were further discussed. (PsycINFO Database Record (c) 2017 APA, all rights reserved)
KW  - Computer self-efficacy
KW  - Training satisfaction
KW  - Test anxiety
KW  - Computerized adaptive testing
KW  - Structural equation model
KW  - Adaptive Testing
KW  - Computer Assisted Instruction
KW  - Self-Efficacy
KW  - Test Anxiety
DO  - 10.1016/j.compedu.2016.04.012
UR  - https://search.ebscohost.com/login.aspx?direct=true&db=psyh&AN=2016-28782-005&lang=de&site=ehost-live
UR  - sdnulh@163.com
DP  - EBSCOhost
DB  - APA PsycInfo
ER  - 

TY  - THES
ID  - 2016-99020-246
AN  - 2016-99020-246
AU  - Gasperson, Sean Morgan
T1  - The effects of item-by-item feedback during a computer adaptive test
JF  - Dissertation Abstracts International: Section B: The Sciences and Engineering
JO  - Dissertation Abstracts International: Section B: The Sciences and Engineering
Y1  - 2016///
VL  - 76
IS  - 7-B(E)
PB  - ProQuest Information & Learning
SN  - 0419-4217
SN  - 978-1-321-58320-5
N1  - Accession Number: 2016-99020-246. Other Journal Title: Dissertation Abstracts International. Partial author list: First Author & Affiliation: Gasperson, Sean Morgan; North Carolina State U., US. Release Date: 20160411. Correction Date: 20190211. Publication Type: Dissertation Abstract (0400). Format Covered: Electronic. Document Type: Dissertation. Dissertation Number: AAI3690266. ISBN: 978-1-321-58320-5. Language: EnglishMajor Descriptor: Computers; Feedback; Goal Orientation; Computerized Assessment. Minor Descriptor: Adaptive Testing; Anxiety; Psychometrics; Fairness. Classification: Health & Mental Health Treatment & Prevention (3300); Psychometrics & Statistics & Methodology (2200). Population: Human (10). Methodology: Empirical Study; Quantitative Study. 
AB  - Decisions involved in designing high-stakes tests include whether to give examinees feedback and, if it is given, how to display the feedback to them. Researchers have yet to examine the effects of item response theory-related feedback on state test anxiety and examinees’ perceptions of fairness when said feedback is given on an item-by-item basis – such as during a computer adaptive test. To address these gaps in the literature, this experiment examined the effects of feedback on state test anxiety and perceived test fairness, accounting for individual differences in feedback acceptance, exam performance, and performance goal orientation. These effects were examined across three modes of item-by-item feedback (i.e., text feedback, graphical feedback, and text/graphical feedback together) during a computer adaptive test in an online sample (N = 338). A three-way interaction occurred between text and graphical feedback, test performance, and prove performance goal orientation. As expected, among those who received feedback, there was a significant, positive relationship between feedback acceptance and perceived test fairness. Limitations and future directions are discussed. (PsycINFO Database Record (c) 2019 APA, all rights reserved)
KW  - computer adaptive test
KW  - computer adaptive testing
KW  - performance goal orientation
KW  - graphical feedback
KW  - feedback acceptance
KW  - Computers
KW  - Feedback
KW  - Goal Orientation
KW  - Computerized Assessment
KW  - Adaptive Testing
KW  - Anxiety
KW  - Psychometrics
KW  - Fairness
UR  - https://search.ebscohost.com/login.aspx?direct=true&db=psyh&AN=2016-99020-246&lang=de&site=ehost-live
DP  - EBSCOhost
DB  - APA PsycInfo
ER  - 

TY  - JOUR
ID  - 1999-13053-004
AN  - 1999-13053-004
AU  - Ponsoda, Vicente
AU  - Olea, Julio
AU  - Rodriguez, Maria Soledad
AU  - Revuelta, Javier
T1  - The effects of test difficulty manipulation in computerized adaptive testing and self-adapted testing
JF  - Applied Measurement in Education
JO  - Applied Measurement in Education
Y1  - 1999///
VL  - 12
IS  - 2
SP  - 167
EP  - 184
PB  - Lawrence Erlbaum
SN  - 0895-7347
SN  - 1532-4818
N1  - Accession Number: 1999-13053-004. Partial author list: First Author & Affiliation: Ponsoda, Vicente; Autonomous U of Madrid, Dept of Social Psychology & Methods, Madrid, Spain. Other Publishers: Taylor & Francis. Release Date: 19990501. Correction Date: 20190211. Publication Type: Journal (0100), Peer Reviewed Journal (0110). Format Covered: Print. Document Type: Journal Article. Language: EnglishMajor Descriptor: Adaptive Testing; Difficulty Level (Test); Test Anxiety; Computerized Assessment. Minor Descriptor: Educational Measurement; High School Students. Classification: Educational Measurement (2227). Population: Human (10); Male (30); Female (40). Location: Spain. Age Group: Adolescence (13-17 yrs) (200); Adulthood (18 yrs & older) (300); Young Adulthood (18-29 yrs) (320). Methodology: Empirical Study. Page Count: 18. Issue Publication Date: 1999. 
AB  - Compared easy and difficult versions of self-adapted tests (SATs) and computerized adaptive tests (CATs) among a sample of 187 high school students (aged 17–19 yrs). No significant differences were found among the 4 tests (easy vs difficult SAT, easy vs difficult CAT) for either estimated ability or posttest state anxiety. Significant differences were found for the number of correct responses, testing time, anxiety change, and standard error of ability. The difficulty manipulation was successful as easy and difficult tests differed in the number of items passed. Conditions with high percentage of items passed produced less posttest than pretest anxiety for both SATs and the easy CATs. In the difficult CAT condition, the number of correct responses was lower and posttest anxiety exceeded pretest anxiety. Results suggest that SAT research should continue to take into account the variable 'number of items passed' as it may hide the effects on anxiety when CATs and SATs are compared. The 2 easy conditions show good psychometric and motivational characteristics. However, the easy CAT gave a higher precision than the easy SAT. (PsycINFO Database Record (c) 2019 APA, all rights reserved)
KW  - test difficulty manipulation
KW  - proficiency & test anxiety on computerized- vs self-adapted testing
KW  - 17–19 yr old high school students
KW  - Adaptive Testing
KW  - Difficulty Level (Test)
KW  - Test Anxiety
KW  - Computerized Assessment
KW  - Educational Measurement
KW  - High School Students
DO  - 10.1207/s15324818ame1202_4
UR  - https://search.ebscohost.com/login.aspx?direct=true&db=psyh&AN=1999-13053-004&lang=de&site=ehost-live
UR  - ORCID: 0000-0003-4705-6282
DP  - EBSCOhost
DB  - APA PsycInfo
ER  - 

TY  - JOUR
ID  - 2014-12106-006
AN  - 2014-12106-006
AU  - Gamper, Eva‐Maria
AU  - Groenvold, Mogens
AU  - Petersen, Morten Aa
AU  - Young, Teresa
AU  - Costantini, Anna
AU  - Aaronson, Neil
AU  - Giesinger, Johannes M.
AU  - Meraner, Verena
AU  - Kemmler, Georg
AU  - Holzner, Bernhard
T1  - The EORTC emotional functioning computerized adaptive test: Phases I–III of a cross‐cultural item bank development
JF  - Psycho-Oncology
JO  - Psycho-Oncology
JA  - Psychooncology
Y1  - 2014/04//
VL  - 23
IS  - 4
SP  - 397
EP  - 403
PB  - John Wiley & Sons
SN  - 1057-9249
SN  - 1099-1611
AD  - Gamper, Eva‐Maria, Department for Psychiatry and Psychotherapy, University Clinic for Biological Psychiatry, Anichstrasse 35, 6020, Innsbruck, Austria
N1  - Accession Number: 2014-12106-006. PMID: 24217943 Partial author list: First Author & Affiliation: Gamper, Eva‐Maria; Department for Psychiatry and Psychotherapy, University Clinic for Biological Psychiatry, Innsbruck, Austria. Institutional Authors: EORTC Quality of Life Group. Release Date: 20140901. Correction Date: 20190211. Publication Type: Journal (0100), Peer Reviewed Journal (0110). Format Covered: Electronic. Document Type: Journal Article. Language: EnglishGrant Information: Gamper, Eva‐Maria. Major Descriptor: Emotions; Neoplasms; Psychometrics; Test Construction; Test Validity. Minor Descriptor: Quality of Life; Questionnaires; Test Reliability; Computerized Assessment. Classification: Tests & Testing (2220); Cancer (3293). Population: Human (10); Male (30); Female (40). Location: Austria; Denmark; Italy; United Kingdom. Age Group: Adulthood (18 yrs & older) (300); Young Adulthood (18-29 yrs) (320); Thirties (30-39 yrs) (340); Middle Age (40-64 yrs) (360); Aged (65 yrs & older) (380). Tests & Measures: Patient Interview; European Organization for Research and Treatment of Cancer Quality of Life Questionnaire. Methodology: Empirical Study; Interview; Quantitative Study. References Available: Y. Page Count: 7. Issue Publication Date: Apr, 2014. Publication History: First Posted Date: Nov 11, 2013; Accepted Date: Sep 12, 2013; Revised Date: Aug 27, 2013; First Submitted Date: Apr 24, 2013. Copyright Statement: Psycho-Oncology published by John Wiley & Sons, Ltd. This is an open access article under the terms of the Creative Commons Attribution License, which permits use, distribution and reproduction in any medium, provided the original work is properly cited. The Authors. 2013. 
AB  - Abstract Background The European Organisation for Research and Treatment of Cancer (EORTC) Quality of Life Group is currently developing computerized adaptive testing measures for the Quality of Life Questionnaire Core‐30 (QLQ‐C30) scales. The work presented here describes the development of an EORTC item bank for emotional functioning (EF), which is one of the core domains of the QLQ‐C30. Methods According to the EORTC guidelines on module development, the development of the EF item bank comprised four phases, of which the phases I–III are reported in the present paper. Phase I involved defining the theoretical framework for the EF item bank and a literature search. Phase II included pre‐defined item selection steps and a multi‐stage expert review process. In phase III, feedback from cancer patients from different countries was obtained. Results On the basis of literature search in phase I, a list of 1750 items was generated. These were reviewed and further developed in phase II with a focus on relevance, redundancy, clarity, and difficulty. The development and selection steps led to a preliminary list of 41 items. In phase III, patient interviews (N = 41; Austria, Denmark, Italy, and the UK) were conducted with the preliminary item list, resulting in some minor changes to item wording. The final list comprised 38 items. Discussion The phases I–III of the developmental process have resulted in an EF item list that was well accepted by patients in several countries. The items will be subjected to larger‐scale field testing in order to establish their psychometric characteristics and their fit to an item response theory model. © 2013 The Authors. Psycho‐Oncology published by John Wiley & Sons, Ltd. (PsycINFO Database Record (c) 2019 APA, all rights reserved)
KW  - European Organization for Research and Treatment of Cancer Quality of Life Questionnaire
KW  - test development
KW  - psychometrics
KW  - test validity
KW  - test reliability
KW  - computerized adaptive testing
KW  - emotional functioning
KW  - Adult
KW  - Aged
KW  - Aged, 80 and over
KW  - Anxiety
KW  - Cross-Cultural Comparison
KW  - Cultural Competency
KW  - Databases as Topic
KW  - Depression
KW  - Emotions
KW  - Europe
KW  - Female
KW  - Health Status
KW  - Humans
KW  - Male
KW  - Middle Aged
KW  - Neoplasms
KW  - Personal Satisfaction
KW  - Psychometrics
KW  - Quality of Life
KW  - Reproducibility of Results
KW  - Stress, Psychological
KW  - Surveys and Questionnaires
KW  - Emotions
KW  - Neoplasms
KW  - Psychometrics
KW  - Test Construction
KW  - Test Validity
KW  - Quality of Life
KW  - Questionnaires
KW  - Test Reliability
KW  - Computerized Assessment
U1  - Sponsor: Austrian Science Fund, Austria. Grant: L502-B05. Other Details: FWF project. Recipients: Gamper, Eva‐Maria; Giesinger, Johannes M.
DO  - 10.1002/pon.3427
UR  - https://search.ebscohost.com/login.aspx?direct=true&db=psyh&AN=2014-12106-006&lang=de&site=ehost-live
UR  - ORCID: 0000-0003-2744-5950
UR  - eva.gamper@uki.at
DP  - EBSCOhost
DB  - APA PsycInfo
ER  - 

TY  - JOUR
ID  - 2016-24981-001
AN  - 2016-24981-001
AU  - Kim, J. Jo
AU  - Silver, Richard K.
AU  - Elue, Rita
AU  - Adams, Marci G.
AU  - La Porte, Laura M.
AU  - Cai, Li
AU  - Kim, Jong Bae
AU  - Gibbons, Robert D.
T1  - The experience of depression, anxiety, and mania among perinatal women
JF  - Archives of Women's Mental Health
JO  - Archives of Women's Mental Health
JA  - Arch Womens Ment Health
Y1  - 2016/10//
VL  - 19
IS  - 5
SP  - 883
EP  - 890
PB  - Springer
SN  - 1434-1816
SN  - 1435-1102
AD  - Kim, J. Jo, NorthShore University HealthSystem, 2650 Ridge Avenue, Walgreen Suite 1507, Evanston, IL, US, 60201
N1  - Accession Number: 2016-24981-001. Partial author list: First Author & Affiliation: Kim, J. Jo; NorthShore University HealthSystem, Evanston, IL, US. Release Date: 20160523. Correction Date: 20170420. Publication Type: Journal (0100), Peer Reviewed Journal (0110). Format Covered: Electronic. Document Type: Journal Article. Language: EnglishConference Information: Biennial Perinatal Mental Health Meeting, 2nd, Nov, 2015, Chicago, IL, US. Conference Note: This work was previously presented in part at the aforementioned conference and at the Biennial Meeting of the North American Society for Psychosocial Obstetrics and Gynecology, New York, NY, April 2016. Major Descriptor: Anxiety; Major Depression; Mania; Perinatal Period. Minor Descriptor: Human Females; Mental Disorders. Classification: Clinical Psychological Testing (2224); Psychological Disorders (3210). Population: Human (10); Male (30); Female (40); Outpatient (60). Location: US. Age Group: Adulthood (18 yrs & older) (300); Young Adulthood (18-29 yrs) (320); Thirties (30-39 yrs) (340); Middle Age (40-64 yrs) (360); Aged (65 yrs & older) (380). Tests & Measures: Edinburgh Postnatal Depression Scales; CAT-Mental Health Scales. Methodology: Empirical Study; Quantitative Study. References Available: Y. Page Count: 8. Issue Publication Date: Oct, 2016. Publication History: First Posted Date: May 17, 2016; Accepted Date: Apr 30, 2016; First Submitted Date: Mar 8, 2016. Copyright Statement: Springer-Verlag Wien. 2016. 
AB  - We assessed differential item functioning (DIF) based on computerized adaptive testing (CAT) to examine how perinatal mood disorders differ from adult psychiatric disorders. The CAT-Mental Health (CAT-MH) was administered to 1614 adult psychiatric outpatients and 419 perinatal women with IRB approval. We examined individual item-level differences using logistic regression and overall score differences by scoring the perinatal data using the original bifactor model calibration based on the psychiatric sample data and a new bifactor model calibration based on the perinatal data and computing their correlation. To examine convergent validity, we computed correlations of the CAT-MH with contemporaneously administered Edinburgh Postnatal Depression Scales (EPDS). The rate of major depression in the perinatal sample was 13 %. Rates of anxiety, mania, and suicide risk were 5, 6, and 0.4 %, respectively. One of 66 depression items, one of 69 anxiety items, and 15 of 53 mania items exhibited DIF (i.e., failure to discriminate between high and low levels of the disorder) in the perinatal sample based on the psychiatric sample calibration. Removal of these items resulted in correlations of the original and perinatal calibrations of r = 0.983 for depression, r = 0.986 for anxiety, and r = 0.932 for mania. The 91.3 % of cases were concordantly categorized as either 'at-risk' or 'low-risk' between the EPDS and the perinatal calibration of the CAT-MH. There was little evidence of DIF for depression and anxiety symptoms in perinatal women. This was not true for mania. Now calibrated for perinatal women, the CAT-MH can be evaluated for longitudinal symptom monitoring. (PsycINFO Database Record (c) 2017 APA, all rights reserved)
KW  - Computerized adaptive testing
KW  - Item response theory
KW  - Perinatal depression
KW  - Perinatal mania
KW  - Anxiety
KW  - Major Depression
KW  - Mania
KW  - Perinatal Period
KW  - Human Females
KW  - Mental Disorders
U1  - Sponsor: Satter Foundation. Recipients: No recipient indicated
DO  - 10.1007/s00737-016-0632-6
UR  - https://search.ebscohost.com/login.aspx?direct=true&db=psyh&AN=2016-24981-001&lang=de&site=ehost-live
UR  - jkim@northshore.org
DP  - EBSCOhost
DB  - APA PsycInfo
ER  - 

TY  - JOUR
ID  - 2020-59622-001
AN  - 2020-59622-001
AU  - Webster, Kimberly
AU  - Cella, David
AU  - Yost, Kathleen
T1  - The Functional Assessment of Chronic Illness Therapy (FACIT) Measurement System: properties, applications, and interpretation
JF  - Health and Quality of Life Outcomes
JO  - Health and Quality of Life Outcomes
JA  - Health Qual Life Outcomes
Y1  - 2013/12/16/
VL  - 1
PB  - BioMed Central Limited
SN  - 1477-7525
AD  - Cella, David
N1  - Accession Number: 2020-59622-001. Partial author list: First Author & Affiliation: Webster, Kimberly; Center on Outcomes, Research and Education (CORE), Evanston Northwestern Healthcare, Evanston, IL, US. Release Date: 20200903. Correction Date: 20220919. Publication Type: Journal (0100), Peer Reviewed Journal (0110). Format Covered: Electronic. Document Type: Journal Article. Language: EnglishMajor Descriptor: Chronic Illness; Quality of Life; Questionnaires; Test Reliability; Well Being. Minor Descriptor: Disease Management; Test Validity; Treatment. Classification: Clinical Psychological Testing (2224); Health & Mental Health Treatment & Prevention (3300). Population: Human (10); Male (30); Female (40). Location: US. Age Group: Adulthood (18 yrs & older) (300); Young Adulthood (18-29 yrs) (320); Thirties (30-39 yrs) (340); Middle Age (40-64 yrs) (360); Aged (65 yrs & older) (380); Very Old (85 yrs & older) (390). Tests & Measures: Functional Assessment of Chronic Illness Therapy Measurement. Methodology: Empirical Study; Quantitative Study. References Available: Y. ArtID: 79. Issue Publication Date: Dec 16, 2013. Publication History: First Posted Date: Dec 16, 2003; Accepted Date: Dec 16, 2003; First Submitted Date: Jul 30, 2003. Copyright Statement: This is an Open Access article: verbatim copying and redistribution of this article are permitted in all media for any purpose, provided this notice is preserved along with the article's original URL. Webster et al; licensee BioMed Central Ltd. 2003. 
AB  - The Functional Assessment of Chronic Illness Therapy (FACIT) Measurement System is a collection of health-related quality of life (HRQOL) questionnaires targeted to the management of chronic illness. The measurement system, under development since 1987, began with the creation of a generic CORE questionnaire called the Functional Assessment of Cancer Therapy-General (FACT-G). The FACT-G (now in Version 4) is a 27-item compilation of general questions divided into four primary QOL domains: Physical Well-Being, Social/Family Well-Being, Emotional Well-Being, and Functional Well-Being. It is appropriate for use with patients with any form of cancer, and extensions of it have been used and validated in other chronic illness condition (e.g., HIV/AIDS; multiple sclerosis; Parkinson's disease; rheumatoid arthritis), and in the general population. The FACIT Measurement System now includes over 400 questions, some of which have been translated into more than 45 languages. Assessment of any one patient is tailored so that the most-relevant questions are asked and administration time for any one assessment is usually less than 15 minutes. This is accomplished both by the use of specific subscales for relevant domains of HRQOL, or computerized adaptive testing (CAT) of selected symptoms and functional areas. FACIT questionnaires can be administered by self-report (paper or computer) or interview (face-to-face or telephone). Available scoring, normative data and information on meaningful change now allow one to interpret results in the context of a growing literature base. (PsycInfo Database Record (c) 2022 APA, all rights reserved)
KW  - The Functional Assessment of Chronic Illness Therapy (FACIT)
KW  - psychometrics
KW  - health-related quality of life
KW  - physical well-being
KW  - family well-being
KW  - emotional well-being
KW  - Chronic Illness
KW  - Quality of Life
KW  - Questionnaires
KW  - Test Reliability
KW  - Well Being
KW  - Disease Management
KW  - Test Validity
KW  - Treatment
DO  - 10.1186/1477-7525-1-79
UR  - https://search.ebscohost.com/login.aspx?direct=true&db=psyh&AN=2020-59622-001&lang=de&site=ehost-live
UR  - kyost@enh.org
UR  - d-cella@northwestern.edu
UR  - k-webster@northwestern.edu
DP  - EBSCOhost
DB  - APA PsycInfo
ER  - 

TY  - CHAP
ID  - 2020-32637-005
AN  - 2020-32637-005
AU  - Thompson, Nathan
ED  - Khine, Myint Swe
T1  - The future of assessment: How will AI, automation, and machine learning change how we develop and deliver assessments?
T2  - Contemporary perspectives on research in educational assessment.
Y1  - 2020///
SP  - 93
EP  - 105
CY  - Waxhaw, NC
PB  - Information Age Publishing, Inc.
SN  - 9781641139373
SN  - 978-1-64113-938-0
SN  - 978-1-64113-939-7
N1  - Accession Number: 2020-32637-005. Partial author list: First Author & Affiliation: Thompson, Nathan; Assessment Systems Corporation, US. Release Date: 20210913. Correction Date: 20221128. Publication Type: Book (0200), Edited Book (0280). Format Covered: Print. Document Type: Chapter. ISBN: 9781641139373, ISBN Paperback; 978-1-64113-938-0, ISBN Hardcover; 978-1-64113-939-7, ISBN Digital (undefined format). Language: EnglishMajor Descriptor: Adaptive Testing; Artificial Intelligence; Automation; Educational Measurement; Machine Learning. Minor Descriptor: Innovation; Learning; Psychometrics; Test Construction; Test Forms; Test Items; Computerized Assessment. Classification: Educational Measurement (2227); Cognitive Psychology & Intelligent Systems (4100). Population: Human (10). Intended Audience: Psychology: Professional & Research (PS). References Available: Y. Page Count: 13. 
AB  - Many organizations provide linear test forms of multiple-choice items or other traditional formats and score them with classical methods. While the advent of the computer has most definitely affected how tests are delivered, in many cases, it is still a traditional test with classical scoring, just shown on a computer screen rather than paper. With the advent of The Cloud, innovation is becoming stronger and faster, offering us more avenues to improving student assessment—and, therefore, student learning. The core of much innovation is the concept of automation; leveraging advances in fields such as artificial intelligence (AI) and machine learning (ML) to improve how assessments are developed, delivered, and analyzed. The rapid increase in computing power and availability of other technological resources is fueling this change, impacting nearly every aspect of assessment from the writing of the first item to the reporting of the final student score. This chapter discusses a number of these aspects and some of the innovations currently under way, including automated item generation, automated test assembly, computerized adaptive testing, and psychometric forensics. (PsycInfo Database Record (c) 2022 APA, all rights reserved)
KW  - artificial intelligence
KW  - automation
KW  - machine learning
KW  - innovation
KW  - test formats
KW  - student assessment
KW  - student learning
KW  - automated item generation
KW  - automated test assembly
KW  - computerized adaptive testing
KW  - psychometric forensics
KW  - Adaptive Testing
KW  - Artificial Intelligence
KW  - Automation
KW  - Educational Measurement
KW  - Machine Learning
KW  - Innovation
KW  - Learning
KW  - Psychometrics
KW  - Test Construction
KW  - Test Forms
KW  - Test Items
KW  - Computerized Assessment
UR  - https://search.ebscohost.com/login.aspx?direct=true&db=psyh&AN=2020-32637-005&lang=de&site=ehost-live
DP  - EBSCOhost
DB  - APA PsycInfo
ER  - 

TY  - JOUR
ID  - 2020-98034-001
AN  - 2020-98034-001
AU  - Sun, Xiaojian
AU  - Liu, Yanlou
AU  - Xin, Tao
AU  - Song, Naiqing
T1  - The impact of item calibration error on variable-length cognitive diagnostic computerized adaptive testing
JF  - Frontiers in Psychology
JO  - Frontiers in Psychology
JA  - Front Psychol
Y1  - 2020/12/02/
VL  - 11
PB  - Frontiers Media S.A.
SN  - 1664-1078
AD  - Sun, Xiaojian
N1  - Accession Number: 2020-98034-001. PMID: 33343450 Partial author list: First Author & Affiliation: Sun, Xiaojian; School of Mathematics and Statistics, Southwest University, Chongqing, China. Other Publishers: Frontiers Research Foundation. Release Date: 20220120. Publication Type: Journal (0100), Peer Reviewed Journal (0110). Format Covered: Electronic. Document Type: Journal Article. Language: EnglishMajor Descriptor: Adaptive Testing; Error of Measurement; Estimation; Measurement; Simulation. Minor Descriptor: Cognitive Assessment. Classification: Statistics & Mathematics (2240). Population: Human (10). Methodology: Empirical Study; Quantitative Study; Scientific Simulation. References Available: Y. ArtID: 575141. Issue Publication Date: Dec 2, 2020. Publication History: First Posted Date: Dec 2, 2020; Accepted Date: Nov 4, 2020; First Submitted Date: Jun 22, 2020. Copyright Statement: This is an open-access article distributed under the terms of the Creative Commons Attribution License (CC BY). The use, distribution or reproduction in other forums is permitted, provided the original author(s) and the copyright owner(s) are credited and that the original publication in this journal is cited, in accordance with accepted academic practice. No use, distribution or reproduction is permitted which does not comply with these terms. Sun, Liu, Xin and Song. 2020. 
AB  - Calibration errors are inevitable and should not be ignored during the estimation of item parameters. Items with calibration error can affect the measurement results of tests. One of the purposes of the current study is to investigate the impacts of the calibration errors during the estimation of item parameters on the measurement accuracy, average test length, and test efficiency for variable-length cognitive diagnostic computerized adaptive testing. The other purpose is to examine the methods for reducing the adverse effects of calibration errors. Simulation results show that (1) calibration error has negative effect on the measurement accuracy for the deterministic input, noisy 'and' gate (DINA) model, and the reduced reparameterized unified model; (2) the average test lengths is shorter, and the test efficiency is overestimated for items with calibration errors; (3) the compensatory reparameterized unified model (CRUM) is less affected by the calibration errors, and the classification accuracy, average test length, and test efficiency are slightly stable in the CRUM framework; (4) methods such as improving the quality of items, using large calibration sample to calibrate the parameters of items, as well as using cross-validation method can reduce the adverse effects of calibration errors on CD-CAT. (PsycInfo Database Record (c) 2022 APA, all rights reserved)
KW  - variable-length cognitive diagnostic computerized adaptive testing
KW  - calibration errors
KW  - classification accuracy
KW  - test efficiency
KW  - cognitive diagnosis assessment
KW  - Adaptive Testing
KW  - Error of Measurement
KW  - Estimation
KW  - Measurement
KW  - Simulation
KW  - Cognitive Assessment
U1  - Sponsor: Cultural Experts, and “Four Groups of Talented People” Foundation of China, China. Recipients: No recipient indicated
U1  - Sponsor: National Natural Science Foundation of China, China. Grant: 31900794; 32071093. Recipients: No recipient indicated
DO  - 10.3389/fpsyg.2020.575141
UR  - https://search.ebscohost.com/login.aspx?direct=true&db=psyh&AN=2020-98034-001&lang=de&site=ehost-live
UR  - xintao@bnu.edu.cn
UR  - sun.xiaojian@outlook.com
DP  - EBSCOhost
DB  - APA PsycInfo
ER  - 

TY  - THES
ID  - 2002-95018-112
AN  - 2002-95018-112
AU  - Grodenchik, Debra Joan
T1  - The implications of the use of non-optimal items in a Computer Adaptive Testing (CAT) environment
JF  - Dissertation Abstracts International: Section B: The Sciences and Engineering
JO  - Dissertation Abstracts International: Section B: The Sciences and Engineering
Y1  - 2002/09//
VL  - 63
IS  - 3-B
SP  - 1606
EP  - 1606
PB  - ProQuest Information & Learning
SN  - 0419-4217
N1  - Accession Number: 2002-95018-112. Other Journal Title: Dissertation Abstracts International. Partial author list: First Author & Affiliation: Grodenchik, Debra Joan; City U New York, US. Release Date: 20030728. Correction Date: 20190211. Publication Type: Dissertation Abstract (0400). Format Covered: Electronic. Document Type: Dissertation. Dissertation Number: AAI3047223. Language: EnglishMajor Descriptor: Adaptive Testing; Test Items; Computerized Assessment. Classification: Psychometrics & Statistics & Methodology (2200). Population: Human (10). Age Group: Adulthood (18 yrs & older) (300). Methodology: Empirical Study. Page Count: 1. 
AB  - This study describes the effects of manipulating item difficulty in a computer adaptive testing (CAT) environment. There are many potential benefits when using CATS as compared to traditional tests. These include increased security, shorter tests, and more precise measurement. According to IRT, the theory underlying CAT, as the computer continually recalculates ability, items that match that current estimate of ability are administered. Such items provide maximum information about examinees during the test. Herein, however, lies a potential problem. These optimal CAT items result in an examinee having only a 50% chance of a correct response. Some examinees may consider such items unduly challenging. Further, when test anxiety is a factor, it is possible that test scores may be negatively affected.  This research was undertaken to determine the effects of administering easier CAT items on ability estimation and test length using computer simulations. Also considered was the administration of different numbers of initial items prior to the start of the adaptive portion of the test, using three different levels of measurement precision. Results indicate that regardless of the number of initial items administered, the level of precision employed, or the modifications made to item difficulty, the approximation of estimated ability to true ability is good in all cases. Additionally, the standard deviations of the ability estimates closely approximate the theoretical levels of precision used as stopping rules for the simulated CATs.  Since optimal CAT items are not used, each item administered provides less information about examinees than optimal CAT items. This results in longer tests. Fortunately, using easier items that provide up to a 66.4% chance of a correct response results in tests that only modestly increase in length, across levels of precision. For larger standard errors, even easier items (up to a 73.5% chance of a correct response) result in only negligible to modest increases in test length.  Examinees who find optimal CAT items difficult or examinees with test anxiety may find CATs that implement easier items enhance the already existing benefits of CAT. (PsycINFO Database Record (c) 2019 APA, all rights reserved)
KW  - non-optimal items
KW  - computer adaptive testing
KW  - Adaptive Testing
KW  - Test Items
KW  - Computerized Assessment
UR  - https://search.ebscohost.com/login.aspx?direct=true&db=psyh&AN=2002-95018-112&lang=de&site=ehost-live
DP  - EBSCOhost
DB  - APA PsycInfo
ER  - 

TY  - JOUR
ID  - 2022-77406-001
AN  - 2022-77406-001
AU  - Versluijs, Yvonne
AU  - Bandell, David
AU  - Kortlever, Joost
AU  - Ring, David
T1  - The influence of symptoms of anger on pain intensity and activity intolerance
JF  - Journal of Clinical Psychology in Medical Settings
JO  - Journal of Clinical Psychology in Medical Settings
JA  - J Clin Psychol Med Settings
Y1  - 2022/06/25/
PB  - Springer
SN  - 1068-9583
SN  - 1573-3572
AD  - Ring, David
N1  - Accession Number: 2022-77406-001. PMID: 35750973 Partial author list: First Author & Affiliation: Versluijs, Yvonne; Department of Surgery and Perioperative Care, Dell Medical School—The University of Texas at Austin, Austin, TX, US. Release Date: 20220630. Publication Type: Journal (0100), Peer Reviewed Journal (0110). Format Covered: Electronic. Document Type: Journal. Language: EnglishMajor Descriptor: No terms assigned. Classification: Health Psychology & Medicine (3360). References Available: Y. Publication History: Accepted Date: Jun 1, 2022. Copyright Statement: The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature. 2022. 
AB  - This study assessed the association of anger, anxiety, and depression, and cognitive bias with pain and activity tolerance among patients with a musculoskeletal illness or injury expected to last more than a month. 102 Patients completed emotional thermometers to quantify symptoms of anger, anxiety, depression; the abbreviated Pain Catastrophizing Scale; a pain intensity scale; Patient-Reported Outcomes Measurement Information System (PROMIS) Physical Function Computer Adaptive Test; the Spielberger State-Trait Anxiety Inventory and demographic questionnaires. Controlling for potential confounding in multivariable analysis we found greater activity intolerance was associated with retired work-status and greater depressive symptoms, but not with greater symptoms of anger. In addition, greater pain intensity was associated with greater symptoms of depression and greater catastrophic thinking, but not with greater symptoms of anger. Anger emotions do not contribute to symptom intensity and activity intolerance in musculoskeletal illness. Attention can be directed at addressing psychological distress and cognitive bias. (PsycInfo Database Record (c) 2022 APA, all rights reserved)
KW  - Pain intensity
KW  - Anger
KW  - Depression
KW  - Anxiety
KW  - Orthopaedics
KW  - Level III
KW  - No terms assigned
DO  - 10.1007/s10880-022-09894-5
UR  - https://search.ebscohost.com/login.aspx?direct=true&db=psyh&AN=2022-77406-001&lang=de&site=ehost-live
UR  - ORCID: 0000-0002-6506-4879
UR  - ORCID: 0000-0002-3570-4011
UR  - david.ring@austin.utexas.edu
DP  - EBSCOhost
DB  - APA PsycInfo
ER  - 

TY  - JOUR
ID  - 2019-23370-014
AN  - 2019-23370-014
AU  - Larrouy-Maestri, Pauline
AU  - Harrison, Peter M. C.
AU  - Müllensiefen, Daniel
T1  - The mistuning perception test: A new measurement instrument
JF  - Behavior Research Methods
JO  - Behavior Research Methods
JA  - Behav Res Methods
Y1  - 2019/04/15/
VL  - 51
IS  - 2
SP  - 663
EP  - 675
PB  - Springer
SN  - 1554-351X
SN  - 1554-3528
AD  - Larrouy-Maestri, Pauline
N1  - Accession Number: 2019-23370-014. PMID: 30924106 Other Journal Title: Behavior Research Methods & Instrumentation; Behavior Research Methods, Instruments & Computers. Partial author list: First Author & Affiliation: Larrouy-Maestri, Pauline; Neurosciences Department, Max Planck Institute for Empirical Aesthetics, Frankfurt am Main, Germany. Other Publishers: Psychonomic Society. Release Date: 20190617. Correction Date: 20191212. Publication Type: Journal (0100), Peer Reviewed Journal (0110). Format Covered: Electronic. Document Type: Journal Article. Language: EnglishGrant Information: Larrouy-Maestri, Pauline. Major Descriptor: Musical Ability; Perceptual Measures; Pitch Perception; Test Construction. Minor Descriptor: Singing; Test Reliability; Test Validity. Classification: Tests & Testing (2220); Auditory & Speech Perception (2326). Population: Human (10); Male (30); Female (40). Age Group: Adulthood (18 yrs & older) (300); Young Adulthood (18-29 yrs) (320); Thirties (30-39 yrs) (340); Middle Age (40-64 yrs) (360); Aged (65 yrs & older) (380). Tests & Measures: Computerized Adaptive Beat Alignment Test; Profile of Music Perception Skills Battery; Adaptive Melodic Discrimination Test; Goldsmiths Musical Sophistication Index DOI: 10.1037/t42817-000; Mistuning Perception Test DOI: 10.1037/t72567-000. Methodology: Empirical Study; Quantitative Study. Supplemental Data: Tables and Figures Internet. References Available: Y. Page Count: 13. Issue Publication Date: Apr 15, 2019. Publication History: First Posted Date: Mar 28, 2019. Copyright Statement: Open Access. This article is distributed under the terms of the Creative Commons Attribution 4.0 International License (http://creativecommons.org/licenses/by/4.0/), which permits unrestricted use, distribution, and reproduction in any medium, provided you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons license, and indicate if changes were made. The Author(s). 2019. 
AB  - An important aspect of the perceived quality of vocal music is the degree to which the vocalist sings in tune. Although most listeners seem sensitive to vocal mistuning, little is known about the development of this perceptual ability or how it differs between listeners. Motivated by a lack of suitable preexisting measures, we introduce in this article an adaptive and ecologically valid test of mistuning perception ability. The stimulus material consisted of short excerpts (6 to 12 s in length) from pop music performances (obtained from MedleyDB; Bittner et al., 2014) for which the vocal track was pitch-shifted relative to the instrumental tracks. In a first experiment, 333 listeners were tested on a two-alternative forced choice task that tested discrimination between a pitch-shifted and an unaltered version of the same audio clip. Explanatory item response modeling was then used to calibrate an adaptive version of the test. A subsequent validation experiment applied this adaptive test to 66 participants with a broad range of musical expertise, producing evidence of the test’s reliability, convergent validity, and divergent validity. The test is ready to be deployed as an experimental tool and should make an important contribution to our understanding of the human ability to judge mistuning. (PsycINFO Database Record (c) 2019 APA, all rights reserved)
KW  - Pitch perception
KW  - Musical abilities
KW  - Pitch accuracy
KW  - Mistuning Perception Test
KW  - Vocal Mistuning
KW  - Reliability
KW  - Validity
KW  - Test development
KW  - Adolescent
KW  - Adult
KW  - Aged
KW  - Auditory Perception
KW  - Female
KW  - Humans
KW  - Male
KW  - Middle Aged
KW  - Music
KW  - Pitch Perception
KW  - Reproducibility of Results
KW  - Singing
KW  - Young Adult
KW  - Musical Ability
KW  - Perceptual Measures
KW  - Pitch Perception
KW  - Test Construction
KW  - Singing
KW  - Test Reliability
KW  - Test Validity
U1  - Sponsor: Max Planck Society. Recipients: Larrouy-Maestri, Pauline
U1  - Sponsor: Engineering and Physical Sciences Research Council. Recipients: Harrison, Peter M. C.
U1  - Sponsor: AHRC, Centre for Doctoral Training in Media and Arts Technology. Grant: EP/L01632X/1. Recipients: Harrison, Peter M. C.
U1  - Sponsor: Humboldt Foundation. Other Details: Anneliese Maier research prize. Recipients: Müllensiefen, Daniel
DO  - 10.3758/s13428-019-01225-1
UR  - https://search.ebscohost.com/login.aspx?direct=true&db=psyh&AN=2019-23370-014&lang=de&site=ehost-live
UR  - ORCID: 0000-0002-9851-9462
UR  - plm@aesthetics.mpg.de
DP  - EBSCOhost
DB  - APA PsycInfo
ER  - 

TY  - JOUR
ID  - 2020-60277-006
AN  - 2020-60277-006
AU  - Yang, Lihong
AU  - Reckase, Mark D.
T1  - The optimal item pool design in multistage computerized adaptive tests with the p-optimality method
JF  - Educational and Psychological Measurement
JO  - Educational and Psychological Measurement
JA  - Educ Psychol Meas
Y1  - 2020/10//
VL  - 80
IS  - 5
SP  - 955
EP  - 974
PB  - Sage Publications
SN  - 0013-1644
SN  - 1552-3888
AD  - Yang, Lihong, Shandong Jianzhu University, No. 1000 Fengming Road, Shandong, Jinan, China, 250101
N1  - Accession Number: 2020-60277-006. PMID: 32855566 Partial author list: First Author & Affiliation: Yang, Lihong; Shandong Jianzhu University, Jinan, China. Release Date: 20201102. Correction Date: 20220224. Publication Type: Journal (0100), Peer Reviewed Journal (0110). Format Covered: Electronic. Document Type: Journal Article. Language: EnglishMajor Descriptor: Adaptive Testing; Item Response Theory. Minor Descriptor: Test Construction. Classification: Statistics & Mathematics (2240). Population: Human (10). Methodology: Empirical Study; Mathematical Model; Quantitative Study; Scientific Simulation. References Available: Y. Page Count: 20. Issue Publication Date: Oct, 2020. Copyright Statement: The Author(s). 2020. 
AB  - [Correction Notice: An Erratum for this article was reported in Vol 82(2) of Educational and Psychological Measurement (see record [rid]2022-34416-009[/rid]). In the original article, funding information was missed to be added in this article. Please note the following funding information - Funding The authors disclosed receipt of the following financial support for the research, authorship, and/or publication of this article: This research was funded by Shandong Jianzhu University Doctoral Research Fund (Grant number: XNBS20126).] The present study extended the p-optimality method to the multistage computerized adaptive test (MST) context in developing optimal item pools to support different MST panel designs under different test configurations. Using the Rasch model, simulated optimal item pools were generated with and without practical constraints of exposure control. A total number of 72 simulated optimal item pools were generated and evaluated by an overall sample and conditional sample using various statistical measures. Results showed that the optimal item pools built with the p-optimality method provide sufficient measurement accuracy under all simulated MST panel designs. Exposure control affected the item pool size, but not the item distributions and item pool characteristics. This study demonstrated that the p-optimality method can adapt to MST item pool design, facilitate the MST assembly process, and improve its scoring accuracy. (PsycInfo Database Record (c) 2022 APA, all rights reserved)
KW  - item pool design
KW  - multistage computerized adaptive testing
KW  - item pool development
KW  - Adaptive Testing
KW  - Item Response Theory
KW  - Test Construction
DO  - 10.1177/0013164419901292
UR  - https://search.ebscohost.com/login.aspx?direct=true&db=psyh&AN=2020-60277-006&lang=de&site=ehost-live
UR  - ORCID: 0000-0002-3862-122X
UR  - ylh123@126.com
DP  - EBSCOhost
DB  - APA PsycInfo
ER  - 

TY  - JOUR
ID  - 2022-34416-009
AN  - 2022-34416-009
AU  - Yang, Lihong
AU  - Reckase, Mark D.
T1  - 'The optimal item pool design in multistage computerized adaptive tests with the p-optimality method': Corrigendum
JF  - Educational and Psychological Measurement
JO  - Educational and Psychological Measurement
JA  - Educ Psychol Meas
Y1  - 2022/04//
VL  - 82
IS  - 2
SP  - 404
EP  - 404
PB  - Sage Publications
SN  - 0013-1644
SN  - 1552-3888
AD  - Yang, Lihong, Shandong Jianzhu University, No. 1000 Fengming Road, Shandong, Jinan, China, 250101
N1  - Accession Number: 2022-34416-009. Partial author list: First Author & Affiliation: Yang, Lihong; Shandong Jianzhu University, Jinan, China. Release Date: 20220307. Publication Type: Journal (0100), Peer Reviewed Journal (0110). Format Covered: Electronic. Document Type: Erratum/Correction. Language: EnglishMajor Descriptor: Adaptive Testing; Item Response Theory. Minor Descriptor: Test Construction. Classification: Statistics & Mathematics (2240). Page Count: 1. Issue Publication Date: Apr, 2022. Copyright Statement: The Author(s). 2021. 
AB  - Reports an error in 'The optimal item pool design in multistage computerized adaptive tests with the p-optimality method' by Lihong Yang and Mark D. Reckase (Educational and Psychological Measurement, 2020[Oct], Vol 80[5], 955-974). In the original article, funding information was missed to be added in this article. Please note the following funding information - Funding The authors disclosed receipt of the following financial support for the research, authorship, and/or publication of this article: This research was funded by Shandong Jianzhu University Doctoral Research Fund (Grant number: XNBS20126). (The following abstract of the original article appeared in record [rid]2020-60277-006[/rid]). The present study extended the p-optimality method to the multistage computerized adaptive test (MST) context in developing optimal item pools to support different MST panel designs under different test configurations. Using the Rasch model, simulated optimal item pools were generated with and without practical constraints of exposure control. A total number of 72 simulated optimal item pools were generated and evaluated by an overall sample and conditional sample using various statistical measures. Results showed that the optimal item pools built with the p-optimality method provide sufficient measurement accuracy under all simulated MST panel designs. Exposure control affected the item pool size, but not the item distributions and item pool characteristics. This study demonstrated that the p-optimality method can adapt to MST item pool design, facilitate the MST assembly process, and improve its scoring accuracy. (PsycInfo Database Record (c) 2022 APA, all rights reserved)
KW  - item pool design
KW  - multistage computerized adaptive testing
KW  - item pool development
KW  - Adaptive Testing
KW  - Item Response Theory
KW  - Test Construction
U1  - Sponsor: Shandong Jianzhu University, China. Grant: XNBS20126. Other Details: Doctoral Research Fund. Recipients: No recipient indicated
DO  - 10.1177/00131644211056095
UR  - https://search.ebscohost.com/login.aspx?direct=true&db=psyh&AN=2022-34416-009&lang=de&site=ehost-live
UR  - ylh123@126.com
DP  - EBSCOhost
DB  - APA PsycInfo
ER  - 

TY  - JOUR
ID  - 2016-37161-017
AN  - 2016-37161-017
AU  - Stucky, Brian D.
AU  - Huang, Wenjing
AU  - Edelen, Maria Orlando
T1  - The psychometric performance of the PROMIS smoking assessment toolkit: Comparisons of real-data computer adaptive tests, short forms, and mode of administration
JF  - Nicotine & Tobacco Research
JO  - Nicotine & Tobacco Research
JA  - Nicotine Tob Res
Y1  - 2016/03//
VL  - 18
IS  - 3
SP  - 361
EP  - 365
PB  - Oxford University Press
SN  - 1462-2203
SN  - 1469-994X
AD  - Stucky, Brian D., Department of Health, RAND Corporation, 1776 Main Street, Santa Monica, CA, US, 90407-2138
N1  - Accession Number: 2016-37161-017. Partial author list: First Author & Affiliation: Stucky, Brian D.; Department of Health, RAND Corporation, Santa Monica, CA, US. Other Publishers: Taylor & Francis. Release Date: 20170501. Publication Type: Journal (0100), Peer Reviewed Journal (0110). Format Covered: Electronic. Document Type: Journal Article. Language: EnglishGrant Information: Edelen, Maria Orlando. Major Descriptor: Adaptive Testing; Psychometrics; Test Reliability; Tobacco Smoking. Classification: Health Psychology Testing (2226); Substance Abuse & Addiction (3233). Population: Human (10); Male (30); Female (40). Location: US. Age Group: Adulthood (18 yrs & older) (300). Tests & Measures: Patient Reported Outcomes Measurement Information System Smoking Assessment Toolkit-Computer Adaptive Test Version. Methodology: Empirical Study; Quantitative Study. References Available: Y. Page Count: 5. Issue Publication Date: Mar, 2016. Publication History: First Posted Date: Apr 8, 2015. Copyright Statement: Published by Oxford University Press on behalf of the Society for Research on Nicotine and Tobacco. All rights reserved. For permissions, please e-mail: journals.permissions@oup.com. The Author. 2015. 
AB  - Introduction: The PROMIS Smoking Initiative has developed six item banks for assessment related to cigarette smoking among adult smokers (Nicotine Dependence, Coping Expectancies, Emotional and Sensory Expectancies, Health Expectancies, Psychosocial Expectancies, and Social Motivations). This article evaluates the psychometric performance of the banks when administered via short form (SF), computer adaptive test (CAT), and by mode of administration (computer vs. paper-and-pencil). Methods: Data are from two sources: an internet sample (N = 491) of daily and nondaily smokers who completed both SFs and CATs via the web and a community sample (N = 369) that completed either paper-and-pencil or computer administration of the SFs at two time points. First a CAT version of the PROMIS Smoking Assessment Toolkit was evaluated by comparing item administration rates and scores to the SF administration. Next, we considered the effect of computer versus paper-and-pencil administration on scoring and test-retest reliability. Results: Across the domains approximately 5.4 to 10.3 items were administered on average for the CAT. SF and CAT item response theory-scores were correlated from 0.82 to 0.92 across the domains. Cronbach’s alpha for the four- to eight-item SFs among daily smokers ranged from .80 to .91 and .82 to .91 for paper-and-pencil and computer administrations, respectively. Test-retest reliability of the SFs ranged from 0.79 to 0.89 across mode of administration. Conclusions: Results indicate that the SF and CAT and computer and paper-and-pencil administrations provide highly comparable scores for daily and nondaily smokers, but preference for SF or CAT administration may vary by smoking domain. (PsycINFO Database Record (c) 2017 APA, all rights reserved)
KW  - psychometrics
KW  - PROMIS smoking assessment toolkit
KW  - computer adaptive test
KW  - test reliability
KW  - Adaptive Testing
KW  - Psychometrics
KW  - Test Reliability
KW  - Tobacco Smoking
U1  - Sponsor: National Institute on Drug Abuse, US. Grant: R01DA026943. Recipients: Edelen, Maria Orlando (Prin Inv)
DO  - 10.1093/ntr/ntv083
UR  - https://search.ebscohost.com/login.aspx?direct=true&db=psyh&AN=2016-37161-017&lang=de&site=ehost-live
UR  - bstucky@rand.org
DP  - EBSCOhost
DB  - APA PsycInfo
ER  - 

TY  - JOUR
ID  - 1999-01326-005
AN  - 1999-01326-005
AU  - Wise, Steven L.
AU  - Roos, Linda L.
AU  - Plake, Barbara S.
AU  - Nebelsick-Gullett, Lori J.
T1  - The relationship between examinee anxiety and preference for self-adapted testing
T3  - Self-adapted testing
JF  - Applied Measurement in Education
JO  - Applied Measurement in Education
Y1  - 1994///
VL  - 7
IS  - 1
SP  - 81
EP  - 91
PB  - Lawrence Erlbaum
SN  - 0895-7347
SN  - 1532-4818
N1  - Accession Number: 1999-01326-005. Partial author list: First Author & Affiliation: Wise, Steven L.; U Nebraska, Dept of Educational Psychology, Lincoln, NE, US. Other Publishers: Taylor & Francis. Release Date: 19991201. Correction Date: 20210712. Publication Type: Journal (0100), Peer Reviewed Journal (0110). Format Covered: Print. Document Type: Journal Article. Language: EnglishMajor Descriptor: Adaptive Testing; Test Administration; Test Anxiety; Test Taking; Computerized Assessment. Minor Descriptor: Preferences; Perceived Control. Classification: Educational Measurement (2227); Academic Learning & Achievement (3550). Population: Human (10); Male (30); Female (40). Location: US. Age Group: Adulthood (18 yrs & older) (300). Methodology: Empirical Study. References Available: Y. Page Count: 11. Issue Publication Date: 1994. 
AB  - Investigated the hypothesis that the previously found positive effects of self-adapted testing are attributable to examinees having an increased perception of control over a stressful testing situation. 377 students were randomly assigned to either (a) take a computerized adaptive test (CAT), (b) take a self-adapted test (SAT), or choose between taking a CAT or SAT. Results showed that the strongest preference for SAT was shown by examinees reporting high levels of math anxiety. Moreover, highly anxious examinees who were allowed to choose between the test types exhibited higher mean proficiency estimates than examinees who were assigned to test type. (PsycInfo Database Record (c) 2021 APA, all rights reserved)
KW  - performance & perceived control of & anxiety & preference for self-adapted vs computer adaptive testing
KW  - examinees
KW  - Adaptive Testing
KW  - Test Administration
KW  - Test Anxiety
KW  - Test Taking
KW  - Computerized Assessment
KW  - Preferences
KW  - Perceived Control
DO  - 10.1207/s15324818ame0701_6
UR  - https://search.ebscohost.com/login.aspx?direct=true&db=psyh&AN=1999-01326-005&lang=de&site=ehost-live
DP  - EBSCOhost
DB  - APA PsycInfo
ER  - 

TY  - JOUR
ID  - 1997-02424-005
AN  - 1997-02424-005
AU  - Roos, Linda L.
AU  - Wise, Steven L.
AU  - Plake, Barbara S.
T1  - The role of item feedback in self-adapted testing
JF  - Educational and Psychological Measurement
JO  - Educational and Psychological Measurement
JA  - Educ Psychol Meas
Y1  - 1997/02//
VL  - 57
IS  - 1
SP  - 85
EP  - 98
PB  - Sage Publications
SN  - 0013-1644
SN  - 1552-3888
N1  - Accession Number: 1997-02424-005. Partial author list: First Author & Affiliation: Roos, Linda L.; U Nebraska, Lincoln, NE, US. Release Date: 19970101. Correction Date: 20190211. Publication Type: Journal (0100), Peer Reviewed Journal (0110). Format Covered: Print. Document Type: Journal Article. Language: EnglishMajor Descriptor: Adaptive Testing; Feedback; Item Response Theory; Test Anxiety; Computerized Assessment. Minor Descriptor: Experimental Replication; Test Performance. Classification: Educational Measurement (2227); Academic Learning & Achievement (3550). Population: Human (10). Age Group: Adulthood (18 yrs & older) (300); Young Adulthood (18-29 yrs) (320). Methodology: Empirical Study; Experimental Replication. Page Count: 14. Issue Publication Date: Feb, 1997. 
AB  - Replicated and extended the S. L. Wise et al (see record [rid]1993-20102-001[/rid]) study on self-adapted testing as an alternative to computerized adaptive testing. The present study looked at the importance of item feedback in self-adapted testing. Computerized adaptive tests and self-adapted tests were compared under both feedback and no-feedback conditions, using 363 college students. Self-adapted tests yielded higher test performance and lower anxiety than the computerized adaptive tests regardless of the feedback condition. These findings indicate that item feedback is not necessary for the score differences between self-adapted testing and computerized adaptive testing to be realized. (PsycINFO Database Record (c) 2019 APA, all rights reserved)
KW  - item feedback in self-adapted vs computerized adaptive testing
KW  - test performance & anxiety
KW  - college students
KW  - application of item response theory
KW  - replication
KW  - Adaptive Testing
KW  - Feedback
KW  - Item Response Theory
KW  - Test Anxiety
KW  - Computerized Assessment
KW  - Experimental Replication
KW  - Test Performance
DO  - 10.1177/0013164497057001005
UR  - https://search.ebscohost.com/login.aspx?direct=true&db=psyh&AN=1997-02424-005&lang=de&site=ehost-live
DP  - EBSCOhost
DB  - APA PsycInfo
ER  - 

TY  - CHAP
ID  - 1998-06667-008
AN  - 1998-06667-008
AU  - Brown, Annie
AU  - Iwashita, Noriko
ED  - Kunnan, Antony John
T1  - The role of language background in the validation of a computer-adaptive test
T2  - Validation in language assessment: Selected papers from 17th Language Testing Research Colloquium, Long Beach.
T3  - Language Testing Research Colloquium (17th:  1995:  Long Beach, Calif.)
Y1  - 1998///
SP  - 195
EP  - 207
CY  - Mahwah, NJ
PB  - Lawrence Erlbaum Associates Publishers
SN  - 0-8058-2752-8
SN  - 0-8058-2753-6
N1  - Accession Number: 1998-06667-008. Partial author list: First Author & Affiliation: Brown, Annie; U Melbourne, National Language & Literacy Insts, Language Testing Research Ctr, Melbourne, VIC, Australia. Release Date: 19991001. Correction Date: 20190211. Publication Type: Book (0200), Edited Book (0280). Format Covered: Print. Document Type: Chapter. ISBN: 0-8058-2752-8, ISBN Hardcover; 0-8058-2753-6, ISBN Paperback. Language: EnglishMajor Descriptor: Cross Cultural Differences; Educational Measurement; Language Proficiency; Performance; Test Validity. Minor Descriptor: Educational Placement. Classification: Educational Measurement (2227); Academic Learning & Achievement (3550). Population: Human (10). Location: China; Japan; Korea. Age Group: Adulthood (18 yrs & older) (300). Intended Audience: Psychology: Professional & Research (PS). Methodology: Empirical Study. Page Count: 13. 
AB  - Examined the performance of learners of Japanese from different language backgrounds, using data from a computer-adaptive grammar test developed as a placement tool. The trial pen-and-paper test consisted of 225 multiple-choice items. In Australia, China, Japan, and Korea 1,600 students each completed 50 items. In this study, data are presented from 1,452 native speakers of English, Chinese, and Korean who had studied Japanese for 150–300 hrs. Item difficulties drawn from the trial testing were found to be quite different for the 3 groups of test-takers. This has implications for the validity of using computer-adaptive tests: When actual test-takers are from a background different from that of the trial population, not only may the test fail to measure such test-takers accurately in that unacceptable percentages of test-takers are found to misfit, but the measures of ability provided for each of the test-takers and their relative rankings may differ according to the set of item difficulties used. This will consequently affect decisions made about individual learners regarding placement or selection. (PsycINFO Database Record (c) 2019 APA, all rights reserved)
KW  - language background
KW  - performance on & validation of computer adaptive grammar test developed as placement tool
KW  - English & Chinese & Korean students
KW  - Cross Cultural Differences
KW  - Educational Measurement
KW  - Language Proficiency
KW  - Performance
KW  - Test Validity
KW  - Educational Placement
UR  - https://search.ebscohost.com/login.aspx?direct=true&db=psyh&AN=1998-06667-008&lang=de&site=ehost-live
DP  - EBSCOhost
DB  - APA PsycInfo
ER  - 

TY  - JOUR
ID  - 2015-38075-001
AN  - 2015-38075-001
AU  - Iwata, Noboru
AU  - Kikuchi, Kenichi
AU  - Fujihara, Yuya
T1  - The usability of CAT system for assessing the depressive level of Japanese—A study on psychometric properties and response behavior
JF  - International Journal of Behavioral Medicine
JO  - International Journal of Behavioral Medicine
JA  - Int J Behav Med
Y1  - 2016/08//
VL  - 23
IS  - 4
SP  - 427
EP  - 437
PB  - Springer
SN  - 1070-5503
SN  - 1532-7558
AD  - Iwata, Noboru, Department of Psychology, Hiroshima International University, 555-36, Kurose-Gakuendai, Higashi-Hiroshima, Hiroshima, Japan, 739-2695
N1  - Accession Number: 2015-38075-001. PMID: 26272358 Partial author list: First Author & Affiliation: Iwata, Noboru; Department of Psychology, Hiroshima International University, Hiroshima, Japan. Other Publishers: Lawrence Erlbaum; Taylor & Francis. Release Date: 20150817. Correction Date: 20160811. Publication Type: Journal (0100), Peer Reviewed Journal (0110). Format Covered: Electronic. Document Type: Journal Article. Language: EnglishGrant Information: Kawakami, Norito. Major Descriptor: Adaptive Testing; Health Care Psychology; Major Depression; Psychometrics; Test Construction. Minor Descriptor: Depression (Emotion); Item Response Theory; Reaction Time. Classification: Health Psychology Testing (2226); Health Psychology & Medicine (3360). Population: Human (10); Male (30); Female (40). Location: Japan. Age Group: Adulthood (18 yrs & older) (300). Tests & Measures: Center for Epidemiologic Studies Depression Scale-Japanese Version; Computerized Adaptive Testing Measure. Methodology: Empirical Study; Quantitative Study. Page Count: 11. Issue Publication Date: Aug, 2016. Publication History: First Posted Date: Aug 14, 2015. Copyright Statement: International Society of Behavioral Medicine. 2015. 
AB  - Background: An innovative measurement system using a computerized adaptive testing technique based on the item response theory (CAT) has been expanding to measure mental health status. However, little is known about details in its measurement properties based on the empirical data. Moreover, the response time (RT) data, which are not available by a paper-and-pencil measurement but available by a computerized measurement, would be worth investigating for exploring the response behavior. Purpose: We aimed at constructing the CAT to measure depressive symptomatology in a community population and exploring its measurement properties. Also, we examined the relationships between RTs, individual item responses, and depressive levels. Method: For constructing the CAT system, responses of 2061 workers and university students to 24 depression scale plus four negatively revised positive affect items were subjected to a polytomous IRT analysis. The stopping rule was set for standard error of estimation < 0.30 or the maximum 15 items displayed. The CAT and non-adaptive computer-based test (CBT) were administered to 209 undergraduates, and 168 of them administered again after 1 week. Results: On average, the CAT was converged by 10.4 items. The θ values estimated by CAT and CBT were highly correlated (r = 0.94 and 0.95 for the 1st and 2nd measurements) and with the traditional scoring procedures (r’s > 0.90). The test–retest reliability was at a satisfactory level (r = 0.86). RTs to some items significantly correlated with the θ estimates. The mean RT varied by the item contents and wording, i.e., the RT to positive affect items required additional 2 s or longer than the other subscale items. Conclusion: The CAT would be a reliable and practical measurement tool for various purposes including stress check at workplace. (PsycINFO Database Record (c) 2016 APA, all rights reserved)
KW  - Computerized adaptive testing
KW  - Computer based test
KW  - Depression
KW  - Measure
KW  - Response behavior
KW  - Japanese
KW  - Adaptive Testing
KW  - Health Care Psychology
KW  - Major Depression
KW  - Psychometrics
KW  - Test Construction
KW  - Depression (Emotion)
KW  - Item Response Theory
KW  - Reaction Time
U1  - Sponsor: Japan Ministry of Education, Culture, Sports, Science and Technology, Japan. Grant: 18590620. Other Details: Grand-in-Aid for Scientific Research (C). Recipients: No recipient indicated
U1  - Sponsor: Japan Ministry of Health, Labour and Welfare, Japan. Grant: H17-Rodo-Ippan-005. Other Details: Health and Labour Sciences Research Grants (Research on Occupational Safety and Health. Recipients: Kawakami, Norito (Prin Inv)
DO  - 10.1007/s12529-015-9503-1
UR  - https://search.ebscohost.com/login.aspx?direct=true&db=psyh&AN=2015-38075-001&lang=de&site=ehost-live
UR  - fujihara@yasuda-u.ac.jp
UR  - kikuchi@is.sci.toho-u.ac.jp
UR  - iwatan@he.hirokoku-u.ac.jp
DP  - EBSCOhost
DB  - APA PsycInfo
ER  - 

TY  - JOUR
ID  - 2013-18917-006
AN  - 2013-18917-006
AU  - Asseburg, Regine
AU  - Frey, Andreas
T1  - Too hard, too easy, or just right? The relationship between effort or boredom and ability-difficulty fit
JF  - Psychological Test and Assessment Modeling
JO  - Psychological Test and Assessment Modeling
JA  - Psychol Test Assess Model
Y1  - 2013///
VL  - 55
IS  - 1
SP  - 92
EP  - 104
PB  - Pabst Science Publishers
SN  - 2190-0493
SN  - 2190-0507
AD  - Asseburg, Regine, Leibniz Institute for Science and Mathematics Education, University of Kiel (IPN), Kiel, Germany
N1  - Accession Number: 2013-18917-006. Other Journal Title: Psychologische Beitrage; Psychology Science. Partial author list: First Author & Affiliation: Asseburg, Regine; Leibniz Institute for Science and Mathematics Education, University of Kiel (IPN), Kiel, Germany. Release Date: 20130930. Correction Date: 20190211. Publication Type: Journal (0100), Peer Reviewed Journal (0110). Format Covered: Print. Document Type: Journal Article. Language: EnglishMajor Descriptor: Adaptive Testing; Emotions; Mathematics; Motivation; Computerized Assessment. Minor Descriptor: Boredom. Classification: Psychometrics & Statistics & Methodology (2200). Population: Human (10); Male (30); Female (40). Location: Germany. Age Group: Adolescence (13-17 yrs) (200). Methodology: Empirical Study; Mathematical Model; Quantitative Study. References Available: Y. Page Count: 13. Issue Publication Date: 2013. 
AB  - Usually, it is assumed that achievement tests measure maximum performance. However, test performance is not only associated with ability but also with motivational and emotional aspects of test-taking. These aspects are influenced by individual success probability, which in turn depends on the ratio of individual ability to item difficulty (ability-difficulty fit). The impact of ability-difficulty fit on test-taking motivation and emotion is unknown and rarely considered when interpreting test results. N = 9,452 ninth-graders in Germany (PISA 2006) completed a mathematics test and a questionnaire on test-taking effort (motivation) and boredom/daydreaming (emotion). Overall, mean item difficulty exceeded individual ability. Ability-difficulty fit was positively linear related with effort and boredom/daydreaming. The results suggest that low ability students may not show maximum performance in a sequential achievement test. Thus, test score interpretation for this subsample may be invalid. As a solution to this problem the application of computerized adaptive testing is discussed. (PsycINFO Database Record (c) 2019 APA, all rights reserved)
KW  - boredom
KW  - adaptive testing
KW  - motivation
KW  - mathematics
KW  - emotions
KW  - computer assisted testing
KW  - Adaptive Testing
KW  - Emotions
KW  - Mathematics
KW  - Motivation
KW  - Computerized Assessment
KW  - Boredom
UR  - https://search.ebscohost.com/login.aspx?direct=true&db=psyh&AN=2013-18917-006&lang=de&site=ehost-live
UR  - asseburg@ipn.uni-kiel.de
DP  - EBSCOhost
DB  - APA PsycInfo
ER  - 

TY  - JOUR
ID  - 2012-15695-004
AN  - 2012-15695-004
AU  - Jin, Yan
AU  - Pang, Augustine
AU  - Cameron, Glen T.
T1  - Toward a publics-driven, emotion-based conceptualization in crisis communication: Unearthing dominant emotions in multi-staged testing of the Integrated Crisis Mapping (ICM) Model
JF  - Journal of Public Relations Research
JO  - Journal of Public Relations Research
Y1  - 2012/06//
VL  - 24
IS  - 3
SP  - 266
EP  - 298
PB  - Taylor & Francis
SN  - 1062-726X
SN  - 1532-754X
AD  - Jin, Yan, Virginia Commonwealth University, School of Mass Communications, 901 W. Main Street, Richmond, VA, US, 23284
N1  - Accession Number: 2012-15695-004. Partial author list: First Author & Affiliation: Jin, Yan; Virginia Commonwealth University, Richmond, VA, US. Other Publishers: Lawrence Erlbaum. Release Date: 20121119. Publication Type: Journal (0100), Peer Reviewed Journal (0110). Format Covered: Electronic. Document Type: Journal Article. Language: EnglishMajor Descriptor: Concept Formation; Coping Behavior; Crisis Intervention; Emotions; Public Relations. Classification: Personality Traits & Processes (3120); Social Structure & Organization (2910). Population: Human (10); Male (30); Female (40). Location: US. Age Group: Adulthood (18 yrs & older) (300). Methodology: Empirical Study; Nonclinical Case Study; Quantitative Study. References Available: Y. Page Count: 33. Issue Publication Date: Jun, 2012. Copyright Statement: Taylor & Francis Group, LLC
AB  - To better understand not only the minds, but also the hearts of key publics, we have developed a more systemic approach to understand the responses of audiences in crisis situations. The Integrated Crisis Mapping (ICM) model is based on a publics-based, emotion-driven perspective where the publics' responses to different crises are mapped on 2 continua, the organization's engagement in the crisis and primary publics' coping strategy. This multistage testing found evidence that anxiety was the default emotion that publics felt in crises. The subsequent emotions felt by the publics varied in different quadrants involving different crisis types. As far as coping strategies were concerned, conative coping was more evident than cognitive coping across the 4 quadrants. Evidence also suggested strong merit that conative coping was the external manifestation of the internal cognitive processing that had taken place. Cognitive coping was thus the antecedent of conative coping. Although both the publics and the organizations agreed that the crises were relevant to the organizations' goals, they differed on who should assume more responsibility. The findings, although still very much exploratory, suggest theoretical rigor in the model, with room for further refinements to generate what Yin (2003) termed 'analytic generalization' (p. 33) for the ICM model. (PsycINFO Database Record (c) 2016 APA, all rights reserved)
KW  - publics driven conceptualization
KW  - emotion based conceptualization
KW  - crisis communication
KW  - multi-staged testing
KW  - Integrated Crisis Mapping Model
KW  - coping strategies
KW  - Concept Formation
KW  - Coping Behavior
KW  - Crisis Intervention
KW  - Emotions
KW  - Public Relations
DO  - 10.1080/1062726X.2012.676747
UR  - https://search.ebscohost.com/login.aspx?direct=true&db=psyh&AN=2012-15695-004&lang=de&site=ehost-live
UR  - yjin@vcu.edu
DP  - EBSCOhost
DB  - APA PsycInfo
ER  - 

TY  - CHAP
ID  - 2012-13897-010
AN  - 2012-13897-010
AU  - Kingsbury, G. Gage
AU  - Wise, Steven L.
ED  - Lissitz, Robert W.
ED  - Jiao, Hong
T1  - Turning the page: How smarter testing, vertical scales, and understanding of student engagement may improve our tests
T2  - Computers and their impact on state assessments: Recent history and predictions for the future.
T3  - The MARCES book series
Y1  - 2012///
SP  - 245
EP  - 269
CY  - Charlotte, NC
PB  - IAP Information Age Publishing
SN  - 978-1-61735-725-1
SN  - 978-1-61735-726-8
SN  - 978-1-61735-727-5
N1  - Accession Number: 2012-13897-010. Partial author list: First Author & Affiliation: Kingsbury, G. Gage; Northwest Evaluation Association, Portland, OR, US. Release Date: 20120806. Correction Date: 20221128. Publication Type: Book (0200), Edited Book (0280). Format Covered: Print. Document Type: Chapter. Book Type: Textbook/Study Guide. ISBN: 978-1-61735-725-1, ISBN Paperback; 978-1-61735-726-8, ISBN Hardcover; 978-1-61735-727-5, ISBN PDF. Language: EnglishConference Information: Computers and Their Impact on State Assessment: Recent History and Predictions for the Future, 10, Oct, 2010, University of Maryland, College Park, MD, US. Conference Note: Portions of this research were presented at the aforementioned conference. Major Descriptor: Academic Achievement; Adaptive Testing; Student Engagement; Computerized Assessment. Minor Descriptor: Psychometrics. Classification: Educational Measurement (2227); Educational & School Psychology (3500). Population: Human (10). Location: US. Intended Audience: Psychology: Professional & Research (PS). References Available: Y. Page Count: 25. 
AB  - Building an adaptive testing system requires the practitioner to blend basic research (Weiss, 1980) with technology and logistics that fit the situations in which the tests will be delivered (Sands, Waters, & McBride, 1997). In this study, we will consider the research needs of an adaptive testing system that has been developed for use in a number of K-12 settings (MAP: Northwest Evaluation Association, 2009). While many of the operational questions associated with the initial development of this system have been described earlier (Kingsbury & Houser, 1993, 1998), this chapter focuses on issues that relate current research findings to ongoing adaptive test development and administration there is a category of questions that need to be answered through research and need to be answered within the context of K-12 education to be meaningful. We will discuss three questions in this chapter. The three questions that will be addressed are as follows: Can a stable vertical measurement scale be established that allows the measurement of student achievement levels and achievement growth between grades and across years? Can we identify students who are not giving full effort on a test, and how can we use the information if we can obtain it? Can we adapt our tests using information aside from the momentary achievement estimate for the student and the psychometric characteristics of the items? We have chosen to focus on these questions because they represent research questions in different states of done-ness. The question of vertical scales has been answered in some settings and the methodology exists to answer it in any setting. The question of student effort has been answered in some settings, but we know less about the processes for using the information. Finally the question of using additional information to adapt our tests has been asked, but it is far from answered. These questions suggest the range and the direction of future research that might be useful for additional development of K-12 adaptive tests. (PsycInfo Database Record (c) 2022 APA, all rights reserved)
KW  - smarter testing
KW  - vertical scales
KW  - student engagement
KW  - adaptive testing system
KW  - K-12 settings
KW  - student achievement
KW  - psychometrics
KW  - Academic Achievement
KW  - Adaptive Testing
KW  - Student Engagement
KW  - Computerized Assessment
KW  - Psychometrics
UR  - https://search.ebscohost.com/login.aspx?direct=true&db=psyh&AN=2012-13897-010&lang=de&site=ehost-live
DP  - EBSCOhost
DB  - APA PsycInfo
ER  - 

TY  - JOUR
ID  - 2023-14435-001
AN  - 2023-14435-001
AU  - Luo, Fen
AU  - Wang, Xiaoqing
AU  - Cai, Yan
AU  - Tu, Dongbo
T1  - Two efficient selection methods for high‐dimensional cd‐cat utilizing max‐marginals factor from map query and ensemble learning approach
JF  - British Journal of Mathematical and Statistical Psychology
JO  - British Journal of Mathematical and Statistical Psychology
JA  - Br J Math Stat Psychol
Y1  - 2022/10/26/
PB  - Wiley-Blackwell Publishing Ltd.
SN  - 0007-1102
SN  - 2044-8317
N1  - Accession Number: 2023-14435-001. PMID: 36289154 Partial author list: First Author & Affiliation: Luo, Fen; College of Computer Information Engineering, Jiangxi Normal University, Nanchang, China. Other Publishers: British Psychological Society. Release Date: 20221031. Publication Type: Journal (0100), Peer Reviewed Journal (0110). Format Covered: Electronic. Document Type: Journal. Language: EnglishMajor Descriptor: No terms assigned. Classification: Psychometrics & Statistics & Methodology (2200). Publication History: Accepted Date: Aug 26, 2022; First Submitted Date: Jun 15, 2021. Copyright Statement: British Psychological Society. 2022. 
AB  - Computerized adaptive testing for cognitive diagnosis (CD‐CAT) needs to be efficient and responsive in real time to meet practical applications' requirements. For high‐dimensional data, the number of categories to be recognized in a test grows exponentially as the number of attributes increases, which can easily cause system reaction time to be too long such that it adversely affects the examinees and thus seriously impacts the measurement efficiency. More importantly, the long‐time CPU operations and memory usage of item selection in CD‐CAT due to intensive computation are impractical and cannot wholly meet practice needs. This paper proposed two new efficient selection strategies (HIA and CEL) for high‐dimensional CD‐CAT to address this issue by incorporating the max‐marginals from the maximum a posteriori query and integrating the ensemble learning approach into the previous efficient selection methods, respectively. The performance of the proposed selection method was compared with the conventional selection method using simulated and real item pools. The results showed that the proposed methods could significantly improve the measurement efficiency with about 1/2–1/200 of the conventional methods' computation time while retaining similar measurement accuracy. With increasing number of attributes and size of the item pool, the computation time advantage of the proposed methods becomes more significant. (PsycInfo Database Record (c) 2022 APA, all rights reserved)
KW  - computerized adaptive testing for cognitive diagnosis
KW  - ensemble learning approach
KW  - item selection method
KW  - maximum a posteriori query
KW  - max‐marginals factor
KW  - real‐time response
KW  - No terms assigned
DO  - 10.1111/bmsp.12288
UR  - https://search.ebscohost.com/login.aspx?direct=true&db=psyh&AN=2023-14435-001&lang=de&site=ehost-live
UR  - ORCID: 0000-0001-8029-7905
UR  - tudongbo@aliyun.com
DP  - EBSCOhost
DB  - APA PsycInfo
ER  - 

TY  - JOUR
ID  - 2016-52745-012
AN  - 2016-52745-012
AU  - Chen, Ping
T1  - Two new online calibration methods for computerized adaptive testing
JF  - Acta Psychologica Sinica
JO  - Acta Psychologica Sinica
JA  - Xin Li Xue Bao
Y1  - 2016/09//
VL  - 48
IS  - 9
SP  - 1184
EP  - 1198
PB  - Science Press
SN  - 0439-755X
AD  - Chen, Ping, Collaborative Innovation Center of Assessment toward Basic Education Quality, Beijing Normal University, Beijing, China, 100875
N1  - Accession Number: 2016-52745-012. Partial author list: First Author & Affiliation: Chen, Ping; Collaborative Innovation Center of Assessment toward Basic Education Quality, Beijing Normal University, Beijing, China. Release Date: 20170511. Correction Date: 20190211. Publication Type: Journal (0100), Peer Reviewed Journal (0110). Format Covered: Print. Document Type: Journal Article. Language: ChineseMajor Descriptor: Adaptive Testing; Item Response Theory; Maximum Likelihood; Multivariate Analysis; Computerized Assessment. Classification: Statistics & Mathematics (2240). Population: Human (10). Methodology: Scientific Simulation. References Available: Y. Page Count: 15. Issue Publication Date: Sep, 2016. 
AB  - With the development of computerized adaptive testing (CAT), many new issues and challenges have been raised. For example, as the test is continuously administered, some new items should be written, calibrated, and added to the item bank periodically to replace the flawed, obsolete, and overexposed items. The new items have to be precisely calibrated because the calibration precision will directly affect the accuracy of ability estimation. The technique of online calibration has been widely used to calibrate new items on-the-fly in CAT. since it offers several advantages over the traditional offline calibration approach. As the simplest and most straightforward online calibration method, Method A (Stocking, 1988) has an obvious theoretical limitation in that it treats the estimated abilities as true values and ignores the measurement errors in ability estimation. To overcome this weakness, we combined a full functional maximum likelihood estimator (FFMLE) and an estimator which made use of the consequences of sufficiency (ECSE) (Stefanski & Carroll, 1985) with Method A respectively to correct for the estimation error of ability, and the new methods are referred to as FFMLE-Method A and ECSE-Method A. A simulation study was conducted to compare the two new methods with three other methods: the original Method A [denoted as Method A (Original)], the original Method A which plugs in the true abilities of examinees [Method A (True)]. and the 'multiple EM cycles' method (MEM). These five methods were evaluated in terms of item-parameter recovery and calibration efficiency under three levels of sample sizes (1000, 2000 and 3000) and three levels of CAT test lengths (10. 20 and 30). assuming the new items are randomly assigned to examinees. Under the two-parameter logistic model, the true abilities for the three groups of examinees were randomly drawn from the standard normal distribution [A'^ (0,1)]. For all conditions. 1000 operational items were simulated to constitute the CAT item bank in which the item parameter vector were randomly generated from a multivariate normal distribution MVN (u. S) following the procedures of Chen and Xin (2014). Furthermore, the process of simulating and calibrating new items were replicated 100 times, and 20 new items were generated and the simulation method was the same as that of the operational items. Maximum Fisher Information method was employed to select the following items, and EAP method combined with MLE method was used to estimate the examinees' abilities. Fixed-length rule was utilized to stop the CAT test. The results showed that the two new approaches, FFMLE-Method A and ECSE-Method A, improved the calibration precision over the Method A (Original) in almost all conditions, and the magnitude of improvement reached maximum when the test length was small (e.g., 10). Furthermore, the performance of the two new methods was very close to that of the best-performing MEM for small and medium-sized test length (i.e.. 10 and 20), whereas ECSE-Method A had the best performance among all methods when the test length was relatively longer (i.e., 30). Also, larger sample size resulted in more precise item-parameter recovery for all online calibration methods. Though the simulation results are very promising, several future directions for research, such as variable-length CAT and more complex CAT conditions, merit investigation (e.g., including item exposure control, content balancing and allowing item review, etc.). (PsycINFO Database Record (c) 2019 APA, all rights reserved)
KW  - full functional maximum likelihood estimator
KW  - computerized adaptive testing
KW  - item response theory
KW  - online calibration
KW  - construction of item bank
KW  - Adaptive Testing
KW  - Item Response Theory
KW  - Maximum Likelihood
KW  - Multivariate Analysis
KW  - Computerized Assessment
DO  - 10.3724/SP.J.1041.2016.01184
UR  - https://search.ebscohost.com/login.aspx?direct=true&db=psyh&AN=2016-52745-012&lang=de&site=ehost-live
UR  - pchen@bnu.edu.cn
DP  - EBSCOhost
DB  - APA PsycInfo
ER  - 

TY  - THES
ID  - 2015-99131-045
AN  - 2015-99131-045
AU  - Hogan, Tiffany E.
T1  - Using a computer-adaptive test simulation to investigate test coordinators' perceptions of a high-stakes computer-based testing program
JF  - Dissertation Abstracts International Section A: Humanities and Social Sciences
JO  - Dissertation Abstracts International Section A: Humanities and Social Sciences
Y1  - 2015///
VL  - 76
IS  - 1-A(E)
PB  - ProQuest Information & Learning
SN  - 0419-4209
SN  - 978-1-321-18943-8
N1  - Accession Number: 2015-99131-045. Other Journal Title: Dissertation Abstracts International. Partial author list: First Author & Affiliation: Hogan, Tiffany E.; Georgia State U., US. Release Date: 20150706. Correction Date: 20221128. Publication Type: Dissertation Abstract (0400). Format Covered: Electronic. Document Type: Dissertation. Dissertation Number: AAI3583649. ISBN: 978-1-321-18943-8. Language: EnglishMajor Descriptor: Adaptive Testing; Computer Simulation; Educational Personnel; Computerized Assessment. Minor Descriptor: Elementary Schools; Perception. Classification: Educational & School Psychology (3500). Population: Human (10). Location: US. Age Group: Adulthood (18 yrs & older) (300). Methodology: Empirical Study; Interview; Nonclinical Case Study; Quantitative Study; Scientific Simulation. 
AB  - This case study examined the efficiency and precision of computer classification and adaptive testing to elicit responses from test coordinators on implementing high-stakes computer-based testing. Test coordinators from five elementary schools located in a Georgia school district participated in the study. The school district administered state-made, high-stakes tests using paper and pencil; locally-developed tests via the computer or paper and pencil. A post-hoc simulation program, Comprehensive Simulation of Computerized Adaptive Testing, used 586 student item responses to produce results with a variable termination point and a classification termination point. Results from the simulation were analyzed and used in the case study to elicit interview responses from test coordinators. The photographs of computer-labs and test schedule documents were collected and analyzed to validate school test coordinators’ responses. Test coordinators responded positively to the efficiency and precision of simulation results. Some test coordinators preferred the use of computer-adaptive tests for diagnostic purposes only. Test coordinators’ experiences focused on the security, the emotions, and the management of testing. The findings of this study will benefit those interested in implementing a high-stakes, computer-based testing program by recommending a simulation study be conducted and feedback be solicited from test coordinators prior to an operational test administration. (PsycInfo Database Record (c) 2022 APA, all rights reserved)
KW  - computer-adaptive tests
KW  - test coordinators
KW  - perceptions
KW  - computer-based testing efficiency
KW  - Adaptive Testing
KW  - Computer Simulation
KW  - Educational Personnel
KW  - Computerized Assessment
KW  - Elementary Schools
KW  - Perception
UR  - https://search.ebscohost.com/login.aspx?direct=true&db=psyh&AN=2015-99131-045&lang=de&site=ehost-live
DP  - EBSCOhost
DB  - APA PsycInfo
ER  - 

TY  - JOUR
ID  - 2008-17448-005
AN  - 2008-17448-005
AU  - Gibbons, Robert D.
AU  - Weiss, David J.
AU  - Kupfer, David J.
AU  - Frank, Ellen
AU  - Fagiolini, Andrea
AU  - Grochocinski, Victoria J.
AU  - Bhaumik, Dulal K.
AU  - Stover, Angela
AU  - Bock, R. Darrell
AU  - Immekus, Jason C.
T1  - Using computerized adaptive testing to reduce the burden of mental health assessment
JF  - Psychiatric Services
JO  - Psychiatric Services
JA  - Psychiatr Serv
Y1  - 2008/04//
VL  - 59
IS  - 4
SP  - 361
EP  - 368
PB  - American Psychiatric Assn
SN  - 1075-2730
SN  - 1557-9700
AD  - Gibbons, Robert D., Center for Health Statistics, University of Illinois at Chicago, Psychiatric Institute, 457, M/C 912, Chicago, IL, US, 60680-6998
N1  - Accession Number: 2008-17448-005. PMID: 18378832 Other Journal Title: Hospital & Community Psychiatry. Partial author list: First Author & Affiliation: Gibbons, Robert D.; Center for Health Statistics, University of Illinois at Chicago, Psychiatric Institute, Chicago, IL, US. Release Date: 20090216. Correction Date: 20160428. Publication Type: Journal (0100), Peer Reviewed Journal (0110). Format Covered: Electronic. Document Type: Journal Article. Language: EnglishMajor Descriptor: Adaptive Testing; Item Response Theory; Measurement; Mental Health; Outpatient Treatment. Minor Descriptor: Affective Disorders; Anxiety Disorders. Classification: Clinical Psychological Testing (2224); Outpatient Services (3371). Population: Human (10); Male (30); Female (40); Outpatient (60). Location: US. Age Group: Adulthood (18 yrs & older) (300); Young Adulthood (18-29 yrs) (320); Thirties (30-39 yrs) (340); Middle Age (40-64 yrs) (360); Aged (65 yrs & older) (380). Tests & Measures: Mood and Anxiety Spectrum Scales. Methodology: Empirical Study; Quantitative Study. References Available: Y. Page Count: 8. Issue Publication Date: Apr, 2008. 
AB  - Objective: This study investigated the combination of item response theory and computerized adaptive testing (CAT) for psychiatric measurement as a means of reducing the burden of research and clinical assessments. Methods: Data were from 800 participants in outpatient treatment for a mood or anxiety disorder; they completed 616 items of the 626-item Mood and Anxiety Spectrum Scales (MASS) at two times. The first administration was used to design and evaluate a CAT version of the MASS by using post hoc simulation. The second confirmed the functioning of CAT in live testing. Results: Tests of competing models based on item response theory supported the scale's bifactor structure, consisting of a primary dimension and four group factors (mood, panic-agoraphobia, obsessive-compulsive, and social phobia). Both simulated and live CAT showed a 95% average reduction (585 items) in items administered (24 and 30 items, respectively) compared with administration of the full MASS. The correlation between scores on the full MASS and the CAT version was .93. For the mood disorder subscale, differences in scores between two groups of depressed patients--one with bipolar disorder and one without--on the full scale and on the CAT showed effect sizes of .63 (p < .003) and 1.19 (p < .001) standard deviation units, respectively, indicating better discriminant validity for CAT. Conclusions: Instead of using small fixed-length tests, clinicians can create item banks with a large item pool, and a small set of the items most relevant for a given individual can be administered with no loss of information, yielding a dramatic reduction in administration time and patient and clinician burden. (PsycINFO Database Record (c) 2016 APA, all rights reserved)
KW  - item response theory
KW  - computerized adaptive testing
KW  - psychiatric measurement
KW  - mental health assessment
KW  - outpatient treatment
KW  - mood disorder
KW  - anxiety disorders
KW  - Adolescent
KW  - Adult
KW  - Aged
KW  - Agoraphobia
KW  - Anxiety Disorders
KW  - Bipolar Disorder
KW  - Diagnosis, Computer-Assisted
KW  - Female
KW  - Humans
KW  - Male
KW  - Mental Disorders
KW  - Middle Aged
KW  - Mood Disorders
KW  - Obsessive-Compulsive Disorder
KW  - Panic Disorder
KW  - Phobic Disorders
KW  - Reproducibility of Results
KW  - Surveys and Questionnaires
KW  - Time Factors
KW  - Adaptive Testing
KW  - Item Response Theory
KW  - Measurement
KW  - Mental Health
KW  - Outpatient Treatment
KW  - Affective Disorders
KW  - Anxiety Disorders
U1  - Sponsor: National Institute of Mental Health, US. Grant: R01-MH- 66302; R01-MH-30915. Recipients: No recipient indicated
DO  - 10.1176/appi.ps.59.4.361
UR  - https://search.ebscohost.com/login.aspx?direct=true&db=psyh&AN=2008-17448-005&lang=de&site=ehost-live
UR  - ORCID: 0000-0001-5827-0853
UR  - rdgib@uic.edu
DP  - EBSCOhost
DB  - APA PsycInfo
ER  - 

TY  - THES
ID  - 2016-37860-162
AN  - 2016-37860-162
AU  - Sommers, Lacey M.
T1  - Using item response theory to evaluate a scale of internalized distress for computer adaptive administration
JF  - Dissertation Abstracts International: Section B: The Sciences and Engineering
JO  - Dissertation Abstracts International: Section B: The Sciences and Engineering
Y1  - 2016///
VL  - 77
IS  - 4-B(E)
PB  - ProQuest Information & Learning
SN  - 0419-4217
SN  - 978-1339278872
N1  - Accession Number: 2016-37860-162. Other Journal Title: Dissertation Abstracts International. Partial author list: First Author & Affiliation: Sommers, Lacey M.; Palo Alto University, Psychology, US. Release Date: 20161128. Publication Type: Dissertation Abstract (0400). Format Covered: Electronic. Document Type: Dissertation. Dissertation Number: AAI3737795. ISBN: 978-1339278872. Language: EnglishMajor Descriptor: Distress; Evaluation; Item Response Theory; Psychological Assessment. Classification: General Psychology (2100). Population: Human (10); Male (30); Female (40). Age Group: Adulthood (18 yrs & older) (300); Middle Age (40-64 yrs) (360); Aged (65 yrs & older) (380). Methodology: Empirical Study; Quantitative Study. 
AB  - There has been a notable lack of change in psychological assessment practices over the last several decades despite vast improvements in science and technology. The use of item response theory (IRT) and computer adaptive testing (CAT) in other testing domains has yielded positive results, but these practices have not gained widespread use in psychological assessment. Research has shown that the item response theory methodology can yield important information about assessments not available to researchers through classical test theory, and computer adaptive testing can result in significant item reductions and shorter test administration times. Thus, the current study evaluated whether a single set of items that measure internalizing symptoms in a clinical sample with differential item functioning less than |+/-0.3| across the three demographic variables of gender, age, and education can be identified. Research has consistently shown that an internalized distress dimension is prevalent in the literature on self-rated mood, as well as in the MMPI-2 item pool. IRT analyses were used to investigate for age, gender, and education bias in a sample of 4,493 participants with 250 participants for most combinations of demographic variables (age [3], education [3], gender [2]). Age, education, and gender bias was most often seen in comparisons for participants in the 60 to 79 year old age group for all three scales. The Welsh Anxiety (A) scale showed the greatest number of items (19) with DIF greater than |+/-0.3|across the three demographic variables. Differential item functioning was most prevalent when there were significant disparities between the two groups being evaluated. Overall, results showed that there were 22 items that were relatively unbiased for each combination of demographic variables. Internalized Distress (ID) had the greatest number of unbiased items across the various combinations of demographic variables, with 19 items showing relatively no bias. These findings indicate that it is possible to develop a computer adaptive assessment to measure internalizing symptoms in a clinical sample with enough items that are free from bias for age, education, and gender. Further exploration of computer-adaptive administration of the MMPI-2 and other self-report inventories seems warranted because of its ability to provide comparable information frequently with considerable time saving for individuals. (PsycINFO Database Record (c) 2016 APA, all rights reserved)
KW  - item response theory
KW  - evaluation
KW  - distress
KW  - psychological assessment
KW  - Distress
KW  - Evaluation
KW  - Item Response Theory
KW  - Psychological Assessment
UR  - https://search.ebscohost.com/login.aspx?direct=true&db=psyh&AN=2016-37860-162&lang=de&site=ehost-live
DP  - EBSCOhost
DB  - APA PsycInfo
ER  - 

TY  - THES
ID  - 2010-99120-164
AN  - 2010-99120-164
AU  - Wang, Jia-Hwa
T1  - Using real-data simulations to compare computer adaptive testing and static short-form administrations of an upper extremity item bank
JF  - Dissertation Abstracts International: Section B: The Sciences and Engineering
JO  - Dissertation Abstracts International: Section B: The Sciences and Engineering
Y1  - 2010///
VL  - 70
IS  - 12-B
SP  - 7521
EP  - 7521
PB  - ProQuest Information & Learning
SN  - 0419-4217
SN  - 978-1-109-51372-1
N1  - Accession Number: 2010-99120-164. Other Journal Title: Dissertation Abstracts International. Partial author list: First Author & Affiliation: Wang, Jia-Hwa; U Florida, US. Release Date: 20100823. Correction Date: 20190211. Publication Type: Dissertation Abstract (0400). Format Covered: Electronic. Document Type: Dissertation. Dissertation Number: AAI3386014. ISBN: 978-1-109-51372-1. Language: EnglishMajor Descriptor: Adaptive Testing; Computers; Psychometrics; Simulation; Computerized Assessment. Classification: Health & Mental Health Treatment & Prevention (3300). Population: Human (10). Methodology: Empirical Study; Quantitative Study. Page Count: 1. 
AB  - Computerized adaptive testing (CAT), which administers only items relevant to respondents’ ability, has the advantage of measuring persons’ ability precisely with considerably fewer items than traditional tests. CAT has been proposed for use in healthcare to reduce the respondents’, administrators’ or researchers’ burden in clinics/clinical trials. However, there have been few studies in healthcare that have investigated the optimal characteristics of a CAT. The objective of my study was to investigate: (1) the psychometrics of an item bank measuring upper extremity (UE) disorders for CAT use, (2) how different testing procedures affect the ability estimates from CAT, and (3) whether CAT produce better ability estimates than a traditional static short assessment. The psychometrics of the item bank developed by combining items from the Disabilities of the Arm, Shoulder and Hand and the Upper Extremity Functional Index were examined by confirmatory factor analysis, item response theory analysis, and differential item functioning (DIF) analysis across body part impairments (neck, shoulder, elbow, and wrist/hand). Repeated-measures MANOVA was used to investigate the standard error (SE) and bias of ability estimates from CAT with different testing procedures. Structural equation modeling was implemented to examine the correlations between ability estimates from the full test and different CAT structures and the full test with a short form. Further, paired-sample t test was performed to investigate the SE and bias of ability estimates from CAT and a static short form. In general, the item bank was found essentially unidimensional, the generalized partial credit model fit to the data better than partial credit model and there was no significant DIF. The ability estimates from CAT with expected a posteriori ability estimation method was found to be more precise and more comparable to those from full test than ability estimates derived from the maximum likelihood estimation. Further, CAT had better precision, comparability to full test and sensitivity to detect change than a static short form. The findings from this study suggest that UE CAT differ across estimations methods and are significantly better than a short form version. (PsycINFO Database Record (c) 2019 APA, all rights reserved)
KW  - real data simulations
KW  - computer adaptive testing
KW  - static short-form administrations
KW  - upper extremity item bank
KW  - psychometrics
KW  - Adaptive Testing
KW  - Computers
KW  - Psychometrics
KW  - Simulation
KW  - Computerized Assessment
UR  - https://search.ebscohost.com/login.aspx?direct=true&db=psyh&AN=2010-99120-164&lang=de&site=ehost-live
DP  - EBSCOhost
DB  - APA PsycInfo
ER  - 

TY  - THES
ID  - 2021-92243-139
AN  - 2021-92243-139
AU  - Hobson, Ernest Guy
T1  - Using the Rasch model in a computer adaptive testing application to enhance the measurement quality of emotional intelligence
JF  - Dissertation Abstracts International: Section B: The Sciences and Engineering
JO  - Dissertation Abstracts International: Section B: The Sciences and Engineering
Y1  - 2022///
VL  - 83
IS  - 2-B
PB  - ProQuest Information & Learning
SN  - 0419-4217
SN  - 979-8744425616
N1  - Accession Number: 2021-92243-139. Other Journal Title: Dissertation Abstracts International. Partial author list: First Author & Affiliation: Hobson, Ernest Guy; University of Johannesburg (South Africa), South Africa. Release Date: 20211209. Publication Type: Dissertation Abstract (0400). Format Covered: Electronic. Document Type: Dissertation. Dissertation Number: AAI28332601. ISBN: 979-8744425616. Language: EnglishMajor Descriptor: Adaptive Testing; Emotional Intelligence; Item Response Theory. Minor Descriptor: Independence (Personality); Measurement; Self-Control. Classification: Cognitive Processes (2340); General Psychology (2100). Population: Human (10). Location: South Africa. Age Group: Adulthood (18 yrs & older) (300). Methodology: Empirical Study; Quantitative Study; Scientific Simulation. 
AB  - Background: The use of objective measurement via Rasch models in self-report scales has increased in the recent past, which in turn has enabled the development of computer adaptive tests (CATs). Despite significant advances in both objective measurement and computer adaptive applications few studies have ventured into the personality domain and the emergent field of emotional intelligence. The development of CATs of personality attributes holds advantages such as the reduction in items used for the assessment, reduction of respondent fatigue, and greater cooperation from respondents in the assessment. Research purpose: The aim of this study was to develop a computer adaptive test of the trait Self-control sub-scale of a trait-based emotional intelligence inventory (Trait Emotional Intelligence Questionnaire: TEIQue). Secondary objectives were to examine the functioning of the CAT by (a) comparing the CAT with a static version, and (b) to establish a practical approach to developing a computer adaptive solution to existing static fixed format self-report inventories. Research design: Participants were 681 working South African adults who participated in a local validation research project of the TEIQue. All respondents completed an informed consent form indicating willingness to participate in the research process. The self-control scale consists of 31 items. Each item employs a 7-point Likert-type response format. The data analysis entailed three steps. Step 1 focussed on establishing a benchmark based on the static measurement of trait based self-control. The analysis entailed calibrating and composing a core self-control item bank by means of a Rasch rating scale analysis. The Rasch analysis focussed on how well the observed data fitted the measurement model and identifying items that meet the criteria for inclusion into the item bank. The data were analysed for individual item fit, unidimensionality, local independence, and differential item functioning across gender. Items that did not meet the criteria for good fit were excluded from the item bank. The rating scale analysis yielded 16 items that met the requirements of the Rasch model. These items constituted the static item bank. Step 2 involved obtaining person measures and standard errors for the static item bank. Step 3 involved a post hoc CAT simulation of the responses of the 681 persons to the 16 items, where different item selection criteria and stopping rules were applied to obtain CAT based person measures and standard errors. All simulations were carried out with the Firestar software. The aim of this step was to evaluate the correspondence between static person measures (i.e. measures obtained with the full item bank) and adaptive person measures (i.e. measures obtained with the simulated CAT). Main findings: Overall, high correlations were found between person measures of the static inventory and the CAT version. With a 13 out of 16 item strategy a very high correlation (r = .97) was obtained. The 7 out of 16 item strategy yielded a satisfactory, but somewhat weaker result (r = .91). Results showed that on average about ten items were required to obtain a standard error of person measures < .40. Different initial item starting and subsequent item selection strategies yielded largely similar results with no one single strategy clearly appearing to be the best strategy. (PsycInfo Database Record (c) 2021 APA, all rights reserved)
KW  - computer adaptive testing
KW  - emotional intelligence
KW  - Rasch models
KW  - independence
KW  - measurement quality
KW  - Adaptive Testing
KW  - Emotional Intelligence
KW  - Item Response Theory
KW  - Independence (Personality)
KW  - Measurement
KW  - Self-Control
UR  - https://search.ebscohost.com/login.aspx?direct=true&db=psyh&AN=2021-92243-139&lang=de&site=ehost-live
DP  - EBSCOhost
DB  - APA PsycInfo
ER  - 

TY  - JOUR
ID  - 2016-08717-011
AN  - 2016-08717-011
AU  - Achtyes, Eric D.
AU  - Halstead, Scott
AU  - Smart, LeAnn
AU  - Moore, Tara
AU  - Frank, Ellen
AU  - Kupfer, David J.
AU  - Gibbons, Robert
T1  - Validation of computerized adaptive testing in an outpatient nonacademic setting: The VOCATIONS trial
JF  - Psychiatric Services
JO  - Psychiatric Services
JA  - Psychiatr Serv
Y1  - 2015/10/01/
VL  - 66
IS  - 10
SP  - 1091
EP  - 1096
PB  - American Psychiatric Assn
SN  - 1075-2730
SN  - 1557-9700
AD  - Achtyes, Eric D.
N1  - Accession Number: 2016-08717-011. PMID: 26030317 Other Journal Title: Hospital & Community Psychiatry. Partial author list: First Author & Affiliation: Achtyes, Eric D.; Pine Rest Christian Mental Health Services, Grand Rapids, MI, US. Release Date: 20160331. Correction Date: 20160428. Publication Type: Journal (0100), Peer Reviewed Journal (0110). Format Covered: Electronic. Document Type: Journal Article. Language: EnglishConference Information: Annual Meeting of the American Psychiatric Association, May, 2014, New York City, NY, US. Grant Information: Achtyes, Eric D. Conference Note: Some findings from this study were presented at the aforementioned conference at the annual meeting of the American Society of Clinical Psychopharmacology, Hollywood, Florida, June 16–18, 2014; and at the Institute on Psychiatric Services, San Francisco, October 30–November 2, 2014. Major Descriptor: Adaptive Testing; Outpatient Treatment; Psychometrics; Test Validity. Classification: Health Psychology Testing (2226); Outpatient Services (3371). Population: Human (10); Male (30); Female (40); Outpatient (60). Location: US. Age Group: Adulthood (18 yrs & older) (300); Young Adulthood (18-29 yrs) (320); Thirties (30-39 yrs) (340); Middle Age (40-64 yrs) (360); Aged (65 yrs & older) (380). Tests & Measures: Structured Clinical Interview for DSM-IV-TR; Hamilton Rating Scale for Depression-25; Center for Epidemiologic Studies Depression Scale; Global Assessment of Functioning Scale; Patient Health Questionnaire-9 DOI: 10.1037/t06165-000. Methodology: Empirical Study; Longitudinal Study; Prospective Study; Quantitative Study. Supplemental Data: Experimental Materials Internet. References Available: Y. Page Count: 6. Issue Publication Date: Oct 1, 2015. Publication History: First Posted Date: Jun 1, 2015; Accepted Date: Feb 4, 2015; Revised Date: Dec 20, 2014; First Submitted Date: Sep 4, 2014. 
AB  - Objective: Computerized adaptive testing (CAT) provides an alternative to fixed-length assessments. The study validated a suite of computerized adaptive tests for mental health (CAT-MH) in a community psychiatric sample. Methods: A total of 145 adults from a community outpatient clinic, including 19 with no history of a mental disorder (control group), were prospectively evaluated with CAT for depression (CAD-MDD and CAT-DI), mania (CAT-MANIA), and anxiety symptoms (CAT-ANX). Ratings were compared with gold-standard psychiatric assessments, including the Structured Clinical Interview for DSM-IV-TR (SCID), Hamilton Rating Scale for Depression (HAM-D-25), Patient Health Questionnaire (PHQ-9), Center for Epidemiologic Studies Depression Scale (CES-D), and Global Assessment of Functioning (GAF). Results: Sensitivity and specificity for CAD-MDD were .96 and .64, respectively (.96 and 1.00 for major depression versus the control group). CAT for depression severity (CATDI) correlated well with the HAM-D-25 (r = .79), PHQ-9 (r = .90), and CES-D (r = .90) and had an odds ratio (OR) of 27.88 across its range for current SCID major depressive disorder. CATANX correlated with the HAM-D-25 (r = .73), PHQ-9 (r = .78), and CES-D (r = .81) and had an OR of 11.52 across its range for current SCID generalized anxiety disorder. CAT-MANIA did not correlate well with the HAM-D-25 (r = .31), PHQ-9 (r = .37), and CES-D (r = .39), but it had an OR of 11.56 across its range for a current SCID bipolar diagnosis. Participants found the CAT-MH acceptable and easy to use, averaging 51.7 items and 9.4 minutes to complete the full battery. Conclusions: Compared with gold-standard diagnostic and assessment measures, CAT-MH provided an effective, rapidly administered assessment of psychiatric symptoms. (PsycINFO Database Record (c) 2016 APA, all rights reserved)
KW  - test validation
KW  - psychometrics
KW  - computerized adaptive testing
KW  - outpatient treatment
KW  - Adaptive Testing
KW  - Outpatient Treatment
KW  - Psychometrics
KW  - Test Validity
U1  - Sponsor: Pine Rest Foundation. Other Details: “CATDI/ SCID Assessment Tool”. Recipients: Achtyes, Eric D.; Halstead, Scott
U1  - Sponsor: National Institute of Mental Health, US. Grant: MH66302. Recipients: Gibbons, Robert
U1  - Sponsor: Michigan State University, US. Other Details: In-kind support (computer software). Recipients: Achtyes, Eric D.
DO  - 10.1176/appi.ps.201400390
UR  - https://search.ebscohost.com/login.aspx?direct=true&db=psyh&AN=2016-08717-011&lang=de&site=ehost-live
UR  - achtyes@msu.edu
DP  - EBSCOhost
DB  - APA PsycInfo
ER  - 

TY  - JOUR
ID  - 2016-10815-001
AN  - 2016-10815-001
AU  - Pilkonis, Paul A.
AU  - Yu, Lan
AU  - Dodds, Nathan E.
AU  - Johnston, Kelly L.
AU  - Lawrence, Suzanne M.
AU  - Daley, Dennis C.
T1  - Validation of the alcohol use item banks from the Patient-Reported Outcomes Measurement Information System (PROMIS®)
JF  - Drug and Alcohol Dependence
JO  - Drug and Alcohol Dependence
JA  - Drug Alcohol Depend
Y1  - 2016/04/01/
VL  - 161
SP  - 316
EP  - 322
PB  - Elsevier Science
SN  - 0376-8716
SN  - 1879-0046
AD  - Pilkonis, Paul A., Western Psychiatric Institute and Clinic, 3811 O’Hara Street, Pittsburgh, PA, US, 15213
N1  - Accession Number: 2016-10815-001. PMID: 26936412 Partial author list: First Author & Affiliation: Pilkonis, Paul A.; Department of Psychiatry, University of Pittsburgh School of Medicine, Pittsburgh, PA, US. Release Date: 20160303. Correction Date: 20230123. Publication Type: Journal (0100), Peer Reviewed Journal (0110). Format Covered: Electronic. Document Type: Journal Article. Language: EnglishGrant Information: Pilkonis, Paul A. Major Descriptor: Alcohol Use; Item Response Theory; Measurement; Test Validity. Minor Descriptor: Health Behavior. Classification: Clinical Psychological Testing (2224); Substance Abuse & Addiction (3233). Population: Human (10); Male (30); Female (40); Outpatient (60). Location: US. Age Group: Adulthood (18 yrs & older) (300). Tests & Measures: Comprehensive Alcohol Expectancies Questionnaire; Alcohol Use Disorders Identification Test DOI: 10.1037/t01528-000; CAGE Questionnaire DOI: 10.1037/t01522-000; Patient-Reported Outcomes Measurement Information System DOI: 10.1037/t06780-000. Methodology: Empirical Study; Followup Study; Longitudinal Study; Prospective Study; Interview; Quantitative Study. Supplemental Data: Experimental Materials Internet. References Available: Y. Page Count: 7. Issue Publication Date: Apr 1, 2016. Publication History: First Posted Date: Feb 17, 2016; Accepted Date: Feb 10, 2016; Revised Date: Feb 9, 2016; First Submitted Date: Oct 14, 2015. Copyright Statement: All rights reserved. Elsevier Ireland Ltd. 2016. 
AB  - Background: The Patient-Reported Outcomes Measurement Information System (PROMIS®) includes five item banks for alcohol use. There are limited data, however, regarding their validity (e.g., convergent validity, responsiveness to change). To provide such data, we conducted a prospective study with 225 outpatients being treated for substance abuse. Methods: Assessments were completed shortly after intake and at 1-month and 3-month follow-ups. The alcohol item banks were administered as computerized adaptive tests (CATs). Fourteen CATs and one six-item short form were also administered from eight other PROMIS domains to generate a comprehensive health status profile. After modeling treatment outcome for the sample as a whole, correlates of outcome from the PROMIS health status profile were examined. Results: For convergent validity, the largest correlation emerged between the PROMIS alcohol use score and the Alcohol Use Disorders Identification Test (r = .79 at intake). Regarding treatment outcome, there were modest changes across the target problem of alcohol use and other domains of the PROMIS health status profile. However, significant heterogeneity was found in initial severity of drinking and in rates of change for both abstinence and severity of drinking during follow-up. This heterogeneity was associated with demographic (e.g., gender) and health-profile (e.g., emotional support, social participation) variables. Conclusions: The results demonstrated the validity of PROMIS CATs, which require only 4–6 items in each domain. This efficiency makes it feasible to use a comprehensive health status profile within the substance use treatment setting, providing important prognostic information regarding abstinence and severity of drinking. (PsycInfo Database Record (c) 2023 APA, all rights reserved)
KW  - Alcohol use
KW  - Measurement
KW  - Item response theory
KW  - Patient-reported outcomes
KW  - PROMIS
KW  - Alcohol Drinking
KW  - Female
KW  - Health Status
KW  - Humans
KW  - Male
KW  - Outpatients
KW  - Patient Outcome Assessment
KW  - Prospective Studies
KW  - Substance-Related Disorders
KW  - Treatment Outcome
KW  - Alcohol Use
KW  - Item Response Theory
KW  - Measurement
KW  - Test Validity
KW  - Health Behavior
U1  - Sponsor: Patient-Centered Outcomes Research Institute. Grant: 1IP2PI000189. Recipients: Pilkonis, Paul A. (Prin Inv)
DO  - 10.1016/j.drugalcdep.2016.02.014
UR  - https://search.ebscohost.com/login.aspx?direct=true&db=psyh&AN=2016-10815-001&lang=de&site=ehost-live
UR  - ORCID: 0000-0001-5465-7081
UR  - pilkonispa@upmc.edu
DP  - EBSCOhost
DB  - APA PsycInfo
ER  - 

TY  - JOUR
ID  - 2014-24769-001
AN  - 2014-24769-001
AU  - Pilkonis, Paul A.
AU  - Yu, Lan
AU  - Dodds, Nathan E.
AU  - Johnston, Kelly L.
AU  - Maihoefer, Catherine C.
AU  - Lawrence, Suzanne M.
T1  - Validation of the depression item bank from the Patient-Reported Outcomes Measurement Information System (PROMIS®) in a three-month observational study
JF  - Journal of Psychiatric Research
JO  - Journal of Psychiatric Research
JA  - J Psychiatr Res
Y1  - 2014/09//
VL  - 56
SP  - 112
EP  - 119
PB  - Elsevier Science
SN  - 0022-3956
SN  - 1879-1379
AD  - Pilkonis, Paul A., Western Psychiatric Institute and Clinic, 3811 O'Hara Street, Pittsburgh, PA, US, 15213
N1  - Accession Number: 2014-24769-001. PMID: 24931848 Partial author list: First Author & Affiliation: Pilkonis, Paul A.; Department of Psychiatry, University of Pittsburgh School of Medicine, Pittsburgh, PA, US. Release Date: 20140616. Correction Date: 20140728. Publication Type: Journal (0100), Peer Reviewed Journal (0110). Format Covered: Electronic. Document Type: Journal Article. Language: EnglishMajor Descriptor: Information Systems; Major Depression; Self-Report; Test Validity; Treatment Outcomes. Minor Descriptor: Psychometrics. Classification: Clinical Psychological Testing (2224); Affective Disorders (3211). Population: Human (10); Male (30); Female (40); Outpatient (60). Location: US. Age Group: Adulthood (18 yrs & older) (300); Young Adulthood (18-29 yrs) (320); Thirties (30-39 yrs) (340); Middle Age (40-64 yrs) (360); Aged (65 yrs & older) (380). Tests & Measures: Hamilton Rating Scale for Depression DOI: 10.1037/t04100-000; Global Assessment of Functioning Scale; Center for Epidemiological Studies Depression Scale DOI: 10.1037/t02942-000; Patient Health Questionnaire-9 DOI: 10.1037/t06165-000; Patient-Reported Outcomes Measurement Information System DOI: 10.1037/t06780-000. Methodology: Empirical Study; Longitudinal Study; Prospective Study; Quantitative Study. References Available: Y. Page Count: 8. Issue Publication Date: Sep, 2014. Publication History: Accepted Date: May 13, 2014; Revised Date: May 13, 2014; First Submitted Date: Dec 23, 2013. Copyright Statement: All rights reserved. Elsevier Ltd. 2014. 
AB  - The Patient-Reported Outcomes Measurement Information System (PROMIS®) is an NIH Roadmap initiative devoted to developing better measurement tools for assessing constructs relevant to the clinical investigation and treatment of all diseases—constructs such as pain, fatigue, emotional distress, sleep, physical functioning, and social participation. Following creation of item banks for these constructs, our priority has been to validate them, most often in short-term observational studies. We report here on a three-month prospective observational study with depressed outpatients in the early stages of a new treatment episode (with assessments at intake, one-month follow-up, and three-month follow-up). The protocol was designed to compare the psychometric properties of the PROMIS depression item bank (administered as a computerized adaptive test, CAT) with two legacy self-report instruments: the Center for Epidemiological Studies Depression scale (CESD; Radloff, 1977) and the Patient Health Questionnaire (PHQ-9; Spitzer et al., 1999). PROMIS depression demonstrated strong convergent validity with the CESD and the PHQ-9 (with correlations in a range from .72 to .84 across all time points), as well as responsiveness to change when characterizing symptom severity in a clinical outpatient sample. Identification of patients as 'recovered' varied across the measures, with the PHQ-9 being the most conservative. The use of calibrations based on models from item response theory (IRT) provides advantages for PROMIS depression both psychometrically (creating the possibility of adaptive testing, providing a broader effective range of measurement, and generating greater precision) and practically (these psychometric advantages can be achieved with fewer items—a median of 4 items administered by CAT—resulting in less patient burden). (PsycINFO Database Record (c) 2016 APA, all rights reserved)
KW  - Patient Reported Outcomes Measurement Information System
KW  - depression item bank
KW  - validation
KW  - psychometric properties
KW  - Adolescent
KW  - Adult
KW  - Aged
KW  - Aged, 80 and over
KW  - Calibration
KW  - Computers
KW  - Depressive Disorder
KW  - Female
KW  - Follow-Up Studies
KW  - Humans
KW  - Male
KW  - Middle Aged
KW  - Outpatients
KW  - Patient Outcome Assessment
KW  - Prospective Studies
KW  - Psychometrics
KW  - Self Report
KW  - Young Adult
KW  - Information Systems
KW  - Major Depression
KW  - Self-Report
KW  - Test Validity
KW  - Treatment Outcomes
KW  - Psychometrics
DO  - 10.1016/j.jpsychires.2014.05.010
UR  - https://search.ebscohost.com/login.aspx?direct=true&db=psyh&AN=2014-24769-001&lang=de&site=ehost-live
UR  - pilkonispa@upmc.edu
DP  - EBSCOhost
DB  - APA PsycInfo
ER  - 

TY  - JOUR
ID  - 2020-49925-002
AN  - 2020-49925-002
AU  - Voltmer, Katharina
T1  - Validierung des Adaptiven Tests des Emotionswissens für drei- bis neunjährige Kinder (ATEM 3–9) = Validation of the Adaptive Test of Emotion Knowledge (ATEM) for 3–9-year-olds
JF  - Diagnostica
JO  - Diagnostica
Y1  - 2020/07//
VL  - 66
IS  - 3
SP  - 158
EP  - 171
PB  - Hogrefe Verlag GmbH & Co. KG
SN  - 0012-1924
SN  - 2190-622X
AD  - Voltmer, Katharina, Leuphana Universitat Luneburg, 21335, Luneburg, Germany
N1  - Accession Number: 2020-49925-002. Translated Title: Validation of the Adaptive Test of Emotion Knowledge (ATEM) for 3–9-year-olds. Partial author list: First Author & Affiliation: Voltmer, Katharina; Leuphana Universitat Luneburg, Luneburg, Germany. Release Date: 20200827. Publication Type: Journal (0100), Peer Reviewed Journal (0110). Format Covered: Electronic. Document Type: Journal Article. Language: GermanMajor Descriptor: Childhood Development; Emotions; Item Response Theory; Psychometrics; Test Validity. Classification: Developmental Scales & Schedules (2222); Psychosocial & Personality Development (2840). Population: Human (10). Age Group: Childhood (birth-12 yrs) (100); Preschool Age (2-5 yrs) (160); School Age (6-12 yrs) (180). Tests & Measures: Adaptive Test of Emotion Knowledge. Methodology: Empirical Study; Quantitative Study. Page Count: 14. Issue Publication Date: Jul, 2020. Publication History: First Posted Date: Dec 19, 2019. Copyright Statement: Hogrefe Verlag. 2020. 
AB  - Emotion knowledge in children includes emotion recognition in other peoples’ faces, as well as understanding the internal and external causes and consequences of emotions and strategies for regulating them. To date, there is no measure in the German-speaking countries that captures emotion knowledge in a differentiated but also reliable and valid way. The Adaptive Test of Emotion Knowledge (ATEM) aims to close this gap. The ATEM follows an adaptive design and six components of emotion knowledge. The 32 Rasch-scaled items are embedded in an attractive story and vary in their difficulty within and between the components. In a sample of 581 3–9-year-olds, the test showed a good fit to a six-dimensional model and very good psychometric properties. In a subsample of 254 3–5-year-olds, correlations between the ATEM and a measure of emotion knowledge and measures of expressive and receptive language abilities demonstrated good validity. The ATEM proved to be a good measure of emotion knowledge in children. (PsycInfo Database Record (c) 2020 APA, all rights reserved)
AB  - Das Emotionswissen von Kindern umfasst neben dem Ablesen von Emotionen aus Gesichtern anderer Menschen auch das Erkennen von externalen und internalen Ursachen von Emotionen sowie das Wissen über ihre Konsequenzen und Regulationsmöglichkeiten. Im deutschsprachigen Raum gibt es bisher kein Testverfahren, welches das Emotionswissen auf diese Weise differenziert und gleichzeitig reliabel und valide erfasst. Mit dem Adaptiven Test des Emotionswissens (ATEM) wird diese Lücke geschlossen. Der ATEM fragt in einem adaptiven Design 6 Teilaspekte des Emotionswissens ab, wobei die 32 Items in eine kinderfreundliche Geschichte eingebettet sind. Der nach der Item-Response-Theorie aufgebaute Test zeigt mit einer Gesamtstichprobe von N = 581 Kindern im Alter zwischen 3 und 9 Jahren gute psychometrische Eigenschaften. In einer Substichprobe von n = 254 Kindern im Alter zwischen 3 und 5 Jahren wurde der ATEM anhand von Korrelationen mit verschiedenen Variablen validiert. Insgesamt eignet sich der ATEM gut zur differenzierten Messung des Emotionswissens bei Kindern. (PsycInfo Database Record (c) 2020 APA, all rights reserved)
KW  - emotion knowledge
KW  - children
KW  - test
KW  - adaptive
KW  - item response theory
KW  - Childhood Development
KW  - Emotions
KW  - Item Response Theory
KW  - Psychometrics
KW  - Test Validity
DO  - 10.1026/0012-1924/a000244
UR  - https://search.ebscohost.com/login.aspx?direct=true&db=psyh&AN=2020-49925-002&lang=de&site=ehost-live
UR  - kvoltmer@leuphana.de
DP  - EBSCOhost
DB  - APA PsycInfo
ER  - 

TY  - JOUR
ID  - 2018-38721-006
AN  - 2018-38721-006
AU  - Sari, Halil Ibrahim
AU  - Raborn, Anthony
T1  - What information works best?: A comparison of routing methods
JF  - Applied Psychological Measurement
JO  - Applied Psychological Measurement
JA  - Appl Psychol Meas
Y1  - 2018/09//
VL  - 42
IS  - 6
SP  - 499
EP  - 515
PB  - Sage Publications
SN  - 0146-6216
SN  - 1552-3497
AD  - Sari, Halil Ibrahim, Measurement and Evaluation in Education Program, Muallim Rifat Education Faculty, Kilis 7 Aralik University, 79100, Kilis, Turkey
N1  - Accession Number: 2018-38721-006. Partial author list: First Author & Affiliation: Sari, Halil Ibrahim; Kilis 7 Aralik University, Kilis, Turkey. Release Date: 20180816. Publication Type: Journal (0100), Peer Reviewed Journal (0110). Format Covered: Electronic. Document Type: Journal Article. Language: EnglishMajor Descriptor: Adaptive Testing; Statistical Correlation; Statistical Estimation. Minor Descriptor: Information. Classification: Statistics & Mathematics (2240). Population: Human (10). Methodology: Empirical Study; Mathematical Model; Quantitative Study; Scientific Simulation. Supplemental Data: Other Internet. References Available: Y. Page Count: 17. Issue Publication Date: Sep, 2018. Copyright Statement: The Author(s). 2018. 
AB  - There are many item selection methods proposed for computerized adaptive testing (CAT) applications. However, not all of them have been used in computerized multistage testing (ca-MST). This study uses some item selection methods as a routing method in ca-MST framework. These are maximum Fisher information (MFI), maximum likelihood weighted information (MLWI), maximum posterior weighted information (MPWI), Kullback–Leibler (KL), and posterior Kullback–Leibler (KLP). The main purpose of this study is to examine the performance of these methods when they are used as a routing method in ca-MST applications. These five information methods under four ca-MST panel designs and two test lengths (30 items and 60 items) were tested using the parameters of a real item bank. Results were evaluated with overall findings (mean bias, root mean square error, correlation between true and estimated thetas, and module exposure rates) and conditional findings (conditional absolute bias, standard error of measurement, and root mean square error). It was found that test length affected the outcomes much more than other study conditions. Under 30-item conditions, 1-3 designs outperformed other panel designs. Under 60-item conditions, 1-3-3 designs were better than other panel designs. Each routing method performed well under particular conditions; there was no clear best method in the studied conditions. The recommendations for routing methods in any particular condition were provided for researchers and practitioners as well as the limitations of these results. (PsycINFO Database Record (c) 2018 APA, all rights reserved)
KW  - computerized adaptive testing
KW  - parameters
KW  - statistical correlation
KW  - information
KW  - Adaptive Testing
KW  - Statistical Correlation
KW  - Statistical Estimation
KW  - Information
DO  - 10.1177/0146621617752990
UR  - https://search.ebscohost.com/login.aspx?direct=true&db=psyh&AN=2018-38721-006&lang=de&site=ehost-live
UR  - ORCID: 0000-0002-8083-4739
UR  - hisari@kilis.edu.tr
DP  - EBSCOhost
DB  - APA PsycInfo
ER  - 

TY  - JOUR
ID  - 1999-01536-002
AN  - 1999-01536-002
AU  - Rammsayer, Thomas
T1  - Zum Zeitverhalten beim computer-gestützten adaptiven Testen: Antwortlatenzen bei richtigen und falschen Lösungen = Timing behavior in computerized adaptive testing: Response times as a function of correct and incorrect answers
JF  - Diagnostica
JO  - Diagnostica
Y1  - 1999///
VL  - 45
IS  - 4
SP  - 178
EP  - 183
PB  - Hogrefe Verlag GmbH & Co. KG
SN  - 0012-1924
SN  - 2190-622X
N1  - Accession Number: 1999-01536-002. Translated Title: Timing behavior in computerized adaptive testing: Response times as a function of correct and incorrect answers. Partial author list: First Author & Affiliation: Rammsayer, Thomas; Georg-August-U Göttingen, Georg-Elias-Müller-Inst für Psychologie, Goettingen, Germany. Release Date: 20000401. Correction Date: 20190211. Publication Type: Journal (0100), Peer Reviewed Journal (0110). Format Covered: Print. Document Type: Journal Article. Language: GermanMajor Descriptor: Reaction Time; Testing; Computerized Assessment. Classification: Tests & Testing (2220). Population: Human (10); Male (30). Location: Germany. Age Group: Adulthood (18 yrs & older) (300); Young Adulthood (18-29 yrs) (320); Thirties (30-39 yrs) (340). Tests & Measures: Eysenck Personality Inventory DOI: 10.1037/t02711-000. Methodology: Empirical Study. Page Count: 6. Issue Publication Date: 1999. 
AB  - Examined timing behavior in computerized adaptive testing. To examine whether correct answers require more processing time than incorrect answers, 120 male students (median age 25 yrs) living in Germany were confronted with perceptual and cognitive discrimination tasks. The results confirmed that response times for incorrect answers were significantly longer than those for correct answers. The findings support the assumption that timing behavior represents an independent personality trait. The tests used were the State-Trait-Anxiety Inventory and the Eysenck Personality Inventory Questionnaire. (PsycINFO Database Record (c) 2019 APA, all rights reserved)
KW  - response times for correct vs incorrect answers in computerized adaptive testing
KW  - male 20–35 yr olds
KW  - Reaction Time
KW  - Testing
KW  - Computerized Assessment
DO  - 10.1026//0012-1924.45.4.178
UR  - https://search.ebscohost.com/login.aspx?direct=true&db=psyh&AN=1999-01536-002&lang=de&site=ehost-live
DP  - EBSCOhost
DB  - APA PsycInfo
ER  - 
